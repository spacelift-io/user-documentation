{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Getting Started with Spacelift","text":""},{"location":"index.html#getting-started-with-spacelift","title":"Getting Started with Spacelift","text":"<p>Spacelift blends a regular CI's versatility with the methodological rigor of a specialized, security-conscious infrastructure tool.</p> <p>In this guide we will briefly introduce some key concepts that you need to know to work with Spacelift. These concepts will be followed by detailed instructions to help you create and configure your first run with Spacelift.</p>"},{"location":"index.html#main-concepts","title":"Main Concepts","text":"<ul> <li>Stacks: Spacelift's central entity that connects to your source control repository and manages the state of infrastructure. Stacks also facilitate integration with cloud providers (AWS, Azure, Google Cloud) and other important Spacelift components.</li> <li>State management: Infrastructure states can be managed by your backend, or (for OpenTofu/Terraform projects) imported into Spacelift. You are not required to manage your state with Spacelift.</li> <li>Worker pools: The underlying compute used by Spacelift is called a worker, managed in groups known as worker pools. You need to provision at least one worker pool to use Spacelift.</li> <li>Policies: Policies provide a way to express rules as code to manage your IaC environment, and help make common decisions for login, access, and execution. Policies are based on the Open Policy Agent project and can be defined using its rule language Rego.</li> <li>Cloud integration: Spacelift provides native integration with AWS. Integration with other cloud providers is also possible via OIDC Federation or programmatic connection with their identity services.</li> <li>VCS change workflow: Spacelift evaluates your version control system's (VCS's) pull requests (PRs) to provide a preview of changes being made to your infrastructure. These changes are deployed automatically when PRs are merged.</li> </ul>"},{"location":"index.html#step-1-install-spacelift","title":"Step 1: Install Spacelift","text":"<p>Follow the install guide to get Spacelift up and running.</p>"},{"location":"index.html#welcome-to-the-launchpad","title":"Welcome to the LaunchPad","text":"<p>Once you log in to your Spacelift account, you will be brought to the LaunchPad tab to configure your environment. The checklist has four steps to help you get started:</p> <ol> <li>Integrate source code</li> <li>Integrate your cloud account</li> <li>Create first stack</li> <li>Invite teammates</li> </ol> <p>Once your LaunchPad tasks are complete, you can start your first stack run.</p>"},{"location":"index.html#trigger-your-first-run","title":"Trigger your first run","text":"<p>Assuming your repository contains your infrastructure (or you're using our provided Terraform starter repository), you can start using Spacelift to start runs.</p> <p>Tip</p> <p>If you are using the Terraform starter repository, and you did not sign up for your Spacelift account with GitHub, you may need to add the environment variable <code>TF_VAR_github_app_namespace</code> with the value as your organization name or GitHub handle. You can do this under the <code>Environment</code> tab in the stack.</p> <p></p> <ol> <li>Navigate to the Stacks tab.</li> <li>Click the name of the stack you want to run.</li> <li>Click Trigger to start a Spacelift run. This will check the source code and run any commands on it, and you will be taken to the run view.</li> <li>Click Confirm to apply your changes.</li> <li>Wait for the run to finish.</li> </ol> <p></p> <p>Your output will look different based on your code repository and the resources it creates. If you used the starter repository, you should have a new stack in your account called <code>Managed stack</code> that can demonstrate the effectiveness of our plan policies. Play with it and see if you can fix the purposeful plan issue.</p>"},{"location":"concepts/spacectl.html","title":"spacectl, the Spacelift CLI","text":""},{"location":"concepts/spacectl.html#spacectl-the-spacelift-cli","title":"<code>spacectl</code>, the Spacelift CLI","text":"<p><code>spacectl</code> is a utility wrapping Spacelift's GraphQL API for easy programmatic access in command-line contexts - either in manual interactive mode (in your local shell), or in a predefined CI pipeline (GitHub actions, CircleCI, Jenkins etc).</p> <p>Its primary purpose is to help you explore and execute actions inside Spacelift. It provides limited functionality for creating or editing resources. To do that programmatically, you can use the Spacelift Terraform Provider.</p>"},{"location":"concepts/spacectl.html#installation","title":"Installation","text":""},{"location":"concepts/spacectl.html#officially-supported-packages","title":"Officially supported packages","text":"<p>Officially supported packages are maintained by Spacelift and are the preferred ways to install <code>spacectl</code></p>"},{"location":"concepts/spacectl.html#homebrew","title":"Homebrew","text":"<p>You can install <code>spacectl</code> using Homebrew on MacOS or Linux:</p> <pre><code>brew install spacelift-io/spacelift/spacectl\n</code></pre>"},{"location":"concepts/spacectl.html#windows","title":"Windows","text":"<p>You can install <code>spacectl</code> using winget:</p> <pre><code>winget install spacectl\n</code></pre> <p>or</p> <pre><code>winget install --id spacelift-io.spacectl\n</code></pre>"},{"location":"concepts/spacectl.html#docker-image","title":"Docker image","text":"<p><code>spacectl</code> is distributed as a Docker image, which can be used as follows:</p> <pre><code>docker run -it --rm ghcr.io/spacelift-io/spacectl stack deploy --id my-infra-stack\n</code></pre> <p>Don't forget to add the required environment variables in order to authenticate.</p>"},{"location":"concepts/spacectl.html#asdf","title":"asdf","text":"<pre><code>asdf plugin add spacectl\nasdf install spacectl latest\nasdf global spacectl latest\n</code></pre>"},{"location":"concepts/spacectl.html#github-release","title":"GitHub Release","text":"<p>Alternatively, <code>spacectl</code> is distributed through GitHub Releases as a zip file containing a self-contained statically linked executable built from the source in this repository. Binaries can be download directly from the Releases page.</p>"},{"location":"concepts/spacectl.html#usage-on-github-actions","title":"Usage on GitHub Actions","text":"<p>We have setup-spacectl GitHub Action that can be used to install <code>spacectl</code>:</p> <pre><code>steps:\n  - name: Install spacectl\n    uses: spacelift-io/setup-spacectl@main\n\n  - name: Deploy infrastructure\n    env:\n      SPACELIFT_API_KEY_ENDPOINT: https://mycorp.app.spacelift.io\n      SPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\n      SPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\n    run: spacectl stack deploy --id my-infra-stack\n</code></pre>"},{"location":"concepts/spacectl.html#community-supported-packages","title":"Community supported packages","text":"<p>Disclaimer: These packages are community-maintained, please verify the package integrity yourself before using them to install or update <code>spacectl</code>.</p>"},{"location":"concepts/spacectl.html#arch-linux","title":"Arch linux","text":"<p>Install <code>spacectl-bin</code>: from the Arch User Repository (AUR):</p> <pre><code>yay -S spacectl-bin\n</code></pre> <p>Please make sure to verify the <code>PKGBUILD</code> before installing/updating.</p>"},{"location":"concepts/spacectl.html#alpine-linux","title":"Alpine linux","text":"<p>Install <code>spacectl</code> from the Alpine Repository (alpine packages):</p> <pre><code>apk add spacectl --repository=https://dl-cdn.alpinelinux.org/alpine/edge/testing\n</code></pre> <p>Please make sure to verify the <code>APKBUILD</code> before installing/updating.</p>"},{"location":"concepts/spacectl.html#quick-start","title":"Quick Start","text":"<p>Authenticate using <code>spacectl profile login</code>:</p> <pre><code>&gt; spacectl profile login my-account\nEnter Spacelift endpoint (eg. https://unicorn.app.spacelift.io/): http://my-account.app.spacelift.tf\nSelect authentication flow:\n  1) for API key,\n  2) for GitHub access token,\n  3) for login with a web browser\nOption: 3\n</code></pre> <p>Use spacectl \ud83d\ude80:</p> <pre><code>&gt; spacectl stack list\nName                          | Commit   | Author        | State     | Worker Pool | Locked By\nstack-1                       | 1aa0ef62 | Adam Connelly | NONE      |             |\nstack-2                       | 1aa0ef62 | Adam Connelly | DISCARDED |             |\n</code></pre>"},{"location":"concepts/spacectl.html#getting-help","title":"Getting Help","text":"<p>To list all the commands available, use <code>spacectl help</code>:</p> <pre><code>&gt; spacectl help\nNAME:\n   spacectl - Programmatic access to Spacelift GraphQL API.\n\nUSAGE:\n   spacectl [global options] command [command options] [arguments...]\n\nVERSION:\n   0.26.0\n\nCOMMANDS:\n   module                   Manage a Spacelift module\n   profile                  Manage Spacelift profiles\n   provider                 Manage a Terraform provider\n   run-external-dependency  Manage Spacelift Run external dependencies\n   stack                    Manage a Spacelift stack\n   whoami                   Print out logged-in users information\n   version                  Print out CLI version\n   workerpool               Manages workerpools and their workers\n   help, h                  Shows a list of commands or help for one command\n\nGLOBAL OPTIONS:\n   --help, -h     show help\n   --version, -v  print the version\n</code></pre> <p>To get help about a particular command or subcommand, use the <code>-h</code> flag:</p> <pre><code>&gt; spacectl profile -h\nNAME:\n   spacectl profile - Manage Spacelift profiles\n\nUSAGE:\n   spacectl profile command [command options] [arguments...]\n\nCOMMANDS:\n     current       Outputs your currently selected profile\n     export-token  Prints the current token to stdout. In order not to leak, we suggest piping it to your OS pastebin\n     list          List all your Spacelift account profiles\n     login         Create a profile for a Spacelift account\n     logout        Remove Spacelift credentials for an existing profile\n     select        Select one of your Spacelift account profiles\n     help, h       Shows a list of commands or help for one command\n\nOPTIONS:\n   --help, -h  show help (default: false)\n</code></pre>"},{"location":"concepts/spacectl.html#example","title":"Example","text":"<p>The following screencast shows an example of using spacectl to run a one-off task in Spacelift:</p> <p></p>"},{"location":"concepts/spacectl.html#authentication","title":"Authentication","text":"<p><code>spacectl</code> is designed to work in two different contexts - a non-interactive scripting mode (eg. external CI/CD pipeline) and a local interactive mode, where you type commands into your shell. Because of this, it supports two types of credentials - environment variables and user profiles.</p> <p>We refer to each method of providing credentials as \"credential providers\" (like AWS), and details of each method are documented in the following sections.</p>"},{"location":"concepts/spacectl.html#authenticating-using-environment-variables","title":"Authenticating using environment variables","text":"<p>The CLI supports the following authentication methods via the environment:</p> <ul> <li>Spacelift API tokens.</li> <li>GitHub tokens.</li> <li>Spacelift API keys.</li> </ul> <p><code>spacectl</code> looks for authentication configurations in the order specified above, and will stop as soon as it finds a valid configuration. For example, if a Spacelift API token is specified, GitHub tokens and Spacelift API keys will be ignored, even if their environment variables are specified.</p>"},{"location":"concepts/spacectl.html#spacelift-api-tokens","title":"Spacelift API tokens","text":"<p>Spacelift API tokens can be specified using the <code>SPACELIFT_API_TOKEN</code> environment variable. When this variable is found, the CLI ignores all the other authentication environment variables because the token contains all the information needed to authenticate.</p> <p>NOTE: API tokens are generally short-lived and will need to be re-created often.</p>"},{"location":"concepts/spacectl.html#github-tokens","title":"GitHub tokens","text":"<p>GitHub tokens are only available to accounts that use GitHub as their identity provider, but are very convenient for use in GitHub actions. To use a GitHub token, set the following environment variables:</p> <ul> <li><code>SPACELIFT_API_KEY_ENDPOINT</code> - the URL to your Spacelift account, for example <code>https://mycorp.app.spacelift.io</code>.</li> <li><code>SPACELIFT_API_GITHUB_TOKEN</code> - a GitHub personal access token.</li> </ul>"},{"location":"concepts/spacectl.html#spacelift-api-keys","title":"Spacelift API keys","text":"<p>To use a Spacelift API key, set the following environment variables:</p> <ul> <li><code>SPACELIFT_API_KEY_ENDPOINT</code> - the URL to your Spacelift account, for example <code>https://mycorp.app.spacelift.io</code>.</li> <li><code>SPACELIFT_API_KEY_ID</code> - the ID of your Spacelift API key. Available via the Spacelift application.</li> <li><code>SPACELIFT_API_KEY_SECRET</code> - the secret for your API key. Only available when the secret is created.</li> </ul> <p>More information about API authentication can be found at our GraphQL API documentation.</p>"},{"location":"concepts/spacectl.html#authenticating-using-account-profiles","title":"Authenticating using account profiles","text":"<p>In order to make working with multiple Spacelift accounts easy in interactive scenarios, Spacelift supports account management through the <code>profile</code> family of commands:</p> <pre><code>\u276f spacectl profile\nNAME:\n   spacectl profile - Manage Spacelift profiles\n\nUSAGE:\n   spacectl profile command [command options] [arguments...]\n\nCOMMANDS:\n     current       Outputs your currently selected profile\n     export-token  Prints the current token to stdout. In order not to leak, we suggest piping it to your OS pastebin\n     list          List all your Spacelift account profiles\n     login         Create a profile for a Spacelift account\n     logout        Remove Spacelift credentials for an existing profile\n     select        Select one of your Spacelift account profiles\n     help, h       Shows a list of commands or help for one command\n\nOPTIONS:\n   --help, -h  show help (default: false)\n</code></pre> <p>Each of the subcommands requires an account alias, which is a short, user-friendly name for each set of credentials (account profiles). Profiles don't need to be unique - you can have multiple sets of credentials for a single account too.</p> <p>Account profiles support three authentication methods:</p> <ul> <li> <p>GitHub access tokens</p> </li> <li> <p>API keys</p> </li> <li> <p>Login with a browser (API token).</p> </li> </ul> <p>In order to authenticate to your first profile, type in the following (make sure to replace <code>${MY_ALIAS}</code> with the actual profile alias):</p> <pre><code>\u276f spacectl profile login ${MY_ALIAS}\nEnter Spacelift endpoint (eg. https://unicorn.app.spacelift.io/):\n</code></pre> <p>In the next step, you will be asked to choose which authentication method you are going to use. Note that if your account is using SAML-based SSO authentication, then API keys and login with a browser are your only options. After you're done entering credentials, the CLI will validate them against the server, and assuming that they're valid, will persist them in a credentials file in <code>.spacelift/${MY_ALIAS}</code>. It will also create a symlink in <code>${HOME}/.spacelift/current</code> pointing to the current profile.</p> <p>By default the login process is interactive, however, if that does not fit your workflow, the steps can be predefined using flags, for example:</p> <pre><code>\u276f spacectl profile login --method browser --endpoint https://unicorn.app.spacelift.io local-test\n</code></pre> <p>You can switch between account profiles by using <code>spacectl profile select ${MY_ALIAS}</code>. What this does behind the scenes is point <code>${HOME}/.spacelift/current</code> to the new location. You can also delete stored credentials for a given profile by using the <code>spacectl profile logout ${MY_ALIAS}</code> command.</p>"},{"location":"concepts/vcs-agent-pools.html","title":"VCS Agent Pools","text":""},{"location":"concepts/vcs-agent-pools.html#vcs-agent-pools","title":"VCS Agent Pools","text":"<p>By default, Spacelift communicates with your VCS provider directly. This is usually sufficient, but some users may have special requirements regarding infrastructure, security or compliance, and need to host their VCS system in a way that's only accessible internally, where Spacelift can't reach it. This is where VCS Agent Pools come into play.</p> <p>A single VCS Agent Pool is a way for Spacelift to communicate with a single VCS system on your side. You run VCS Agents inside of your infrastructure and configure them with your internal VCS system endpoint. They will then connect to a gateway on our backend, and we will be able to access your VCS system through them.</p> <p>On the Agent there are very conservative checks on what requests are let through and which ones are denied, with an explicit allowlist of paths that are necessary for Spacelift to work. All requests will be logged to standard output with a description about what they were used for.</p>"},{"location":"concepts/vcs-agent-pools.html#create-the-vcs-agent-pool","title":"Create the VCS Agent Pool","text":"<p>Navigate to the Integrations screen, then click View on the VCS Agent Pools card. Click Create VCS agent pool.</p> <p></p> <p>Give your VCS Agent Pool a name and description, and you're done! A configuration token will be downloaded.</p> <p></p>"},{"location":"concepts/vcs-agent-pools.html#running-the-vcs-agent","title":"Running the VCS Agent","text":""},{"location":"concepts/vcs-agent-pools.html#download-the-vcs-agent-binaries","title":"Download the VCS Agent binaries","text":"<p>The latest version of the VCS Agent binaries for Linux are available at Spacelift's CDN:</p> Binary name SHA256 checksum GPG signature spacelift-vcs-agent-x86_64 spacelift-vcs-agent-x86_64_SHA256SUMS spacelift-vcs-agent-x86_64_SHA256SUMS.sig spacelift-vcs-agent-aarch64 spacelift-vcs-agent-aarch64_SHA256SUMS spacelift-vcs-agent-aarch64_SHA256SUMS.sig <p>Binaries for other operating systems are available on the GitHub Releases page.</p>"},{"location":"concepts/vcs-agent-pools.html#checksum-verification","title":"Checksum verification","text":"<pre><code>curl https://keys.openpgp.org/vks/v1/by-fingerprint/175FD97AD2358EFE02832978E302FB5AA29D88F7 | gpg --import\n\ngpg --verify spacelift-vcs-agent-x86_64_SHA256SUMS.sig\nCHECKSUM=$(cut -f 1 -d ' ' spacelift-vcs-agent-x86_64_SHA256SUMS)\nACTUAL_CHECKSUM=$(sha256sum spacelift-vcs-agent-x86_64 | cut -f 1 -d ' ')\n\nif [ \"$CHECKSUM\" != \"$ACTUAL_CHECKSUM\" ]; then\n  echo \"Invalid checksum!\"\n  exit 1\nelse\n  echo \"Checksum verification succeeded.\"\nfi\n</code></pre>"},{"location":"concepts/vcs-agent-pools.html#run-via-docker","title":"Run via Docker","text":"<p>The VCS Agent is also available as a multi-arch (amd64 and arm64) Docker image:</p> <ul> <li><code>public.ecr.aws/spacelift/vcs-agent:latest</code></li> <li><code>public.ecr.aws/spacelift/vcs-agent:&lt;version&gt;</code></li> </ul> <p>The available versions are listed on the GitHub Releases page.</p> <pre><code>docker run -it --rm -e \"SPACELIFT_VCS_AGENT_POOL_TOKEN=&lt;VCS TOKEN&gt;\" \\\n  -e \"SPACELIFT_VCS_AGENT_TARGET_BASE_ENDPOINT=http://169.254.0.10:7990\" \\\n  -e \"SPACELIFT_VCS_AGENT_VENDOR=bitbucket_datacenter\" \\\n  public.ecr.aws/spacelift/vcs-agent\n</code></pre>"},{"location":"concepts/vcs-agent-pools.html#run-the-vcs-agent-inside-a-kubernetes-cluster","title":"Run the VCS Agent inside a Kubernetes Cluster","text":"<p>We have a VCS Agent Helm Chart that you can use to install the VCS Agent on top of your Kubernetes Cluster. After creating a VCS Agent Pool in Spacelift and generating a token, you can add our Helm chart repo and update your local cache using:</p> <pre><code>helm repo add spacelift https://downloads.spacelift.io/helm\nhelm repo update\n</code></pre> <p>Assuming your token, VCS endpoint and vendor are stored in the <code>SPACELIFT_VCS_AGENT_POOL_TOKEN</code>, <code>SPACELIFT_VCS_AGENT_TARGET_BASE_ENDPOINT</code> and <code>SPACELIFT_VCS_AGENT_VENDOR</code> environment variables, you can install the chart using the following command:</p> <pre><code>helm upgrade vcs-agent spacelift/vcs-agent --install --set \"credentials.token=$SPACELIFT_VCS_AGENT_POOL_TOKEN,credentials.endpoint=$SPACELIFT_VCS_AGENT_TARGET_BASE_ENDPOINT,credentials.vendor=$SPACELIFT_VCS_AGENT_VENDOR\"\n</code></pre>"},{"location":"concepts/vcs-agent-pools.html#configure-and-run-the-vcs-agent","title":"Configure and run the VCS Agent","text":"<p>A number of configuration variables is available to customize how your VCS Agent behaves. The ones marked as required are \u2026 well \u2026 required.</p> CLI Flag Environment Variable Status Default Value Description <code>--target-base-endpoint</code> <code>SPACELIFT_VCS_AGENT_TARGET_BASE_ENDPOINT</code> Required The internal endpoint of your VCS system, including the protocol, as well as port, if applicable. (e.g., <code>http://169.254.0.10:7990</code>) <code>--token</code> <code>SPACELIFT_VCS_AGENT_POOL_TOKEN</code> Required The token you\u2019ve received from Spacelift during VCS Agent Pool creation <code>--vendor</code> <code>SPACELIFT_VCS_AGENT_VENDOR</code> Required The vendor of your VCS system. Currently available options are <code>azure_devops</code>, <code>gitlab</code>, <code>bitbucket_datacenter</code> and <code>github_enterprise</code> <code>--allowed-projects</code> <code>SPACELIFT_VCS_AGENT_ALLOWED_PROJECTS</code> Optional <code>.*</code> Regexp matching allowed projects for API calls. Projects are in the form: 'group/repository'. <code>--bugsnag-api-key</code> <code>SPACELIFT_VCS_AGENT_BUGSNAG_API_KEY</code> Optional Override the Bugsnag API key used for error reporting. <code>--parallelism</code> <code>SPACELIFT_VCS_AGENT_PARALLELISM</code> Optional <code>4</code> Number of streams to create. Each stream can handle one request simultaneously. <code>--debug-print-all</code> <code>SPACELIFT_VCS_AGENT_DEBUG_PRINT_ALL</code> Optional <code>false</code> Makes vcs-agent print out all the requests and responses. <code>HTTPS_PROXY</code> Optional Hostname or IP address of the proxy server, including the protocol, as well as port, if applicable. (e.g., <code>http://10.10.1.1:8888</code>) <code>NO_PROXY</code> Optional Comma-separated list of host names that shouldn't go through any proxy is set in. <p>Congrats! Your VCS Agent should now connect to the Spacelift backend and start handling connections.</p> <p></p> <p>Within the VCS Agent Pools page, you will be able to see the number of active connections used by your pool.</p> <p></p> <p>Now whenever you need to specify an endpoint inside of Spacelift which should use your VCS Agent Pool, you should write it this way: <code>private://my-vcs-agent-pool-name/possible/path</code></p> <p></p> <p>When trying to use this integration, i.e. by opening the Stack creation form, you'll get a detailed log of the requests:</p> <p></p>"},{"location":"concepts/vcs-agent-pools.html#worker-pool-settings","title":"Worker pool settings","text":"<p>Since your source code is downloaded directly by Spacelift workers, you need to configure them to access your VCS instance. Instructions for doing this are available on the worker pools page.</p>"},{"location":"concepts/vcs-agent-pools.html#passing-metadata-tags","title":"Passing Metadata Tags","text":"<p>When the VCS Agent from a VCS Agent Pool is connecting to the gateway, you can send along some tags that will allow you to uniquely identify the process / machine for the purpose of debugging. Any environment variables using <code>SPACELIFT_METADATA_</code> prefix will be passed on. As an example, if you're running Spacelift VCS Agents in EC2, you can do the following just before you execute the VCS Agent binary:</p> <pre><code>export SPACELIFT_METADATA_instance_id=$(ec2-metadata --instance-id | cut -d ' ' -f2)\n</code></pre> <p>Doing so will set your EC2 instance ID as instance_id tag in your VCS Agent connections.</p>"},{"location":"concepts/vcs-agent-pools.html#debug-information","title":"Debug Information","text":"<p>Sometimes, it is helpful to display additional information to troubleshoot an issue. When that is needed, set the following environment variables:</p> <pre><code>export GODEBUG=http2debug=2\nexport GRPC_GO_LOG_SEVERITY_LEVEL=info\nexport GRPC_GO_LOG_VERBOSITY_LEVEL=99\n</code></pre> <p>You may want to tweak the values to increase or decrease verbosity.</p>"},{"location":"concepts/authorization.html","title":"Authorization &amp; RBAC","text":""},{"location":"concepts/authorization.html#authorization-rbac","title":"Authorization &amp; RBAC","text":"<p>Spacelift provides a comprehensive Role-Based Access Control (RBAC) system designed for enterprise infrastructure teams. RBAC enables fine-grained, customizable permissions giving you precise control over who can access what resources and perform which actions.</p>"},{"location":"concepts/authorization.html#evolution-from-legacy-system-roles","title":"Evolution from legacy system roles","text":"<p>Spacelift has evolved from simple legacy system roles (Read, Write, Admin) to a custom RBAC system that offers advanced access control:</p> <ul> <li>Custom roles: Create roles tailored to your organization's specific needs.</li> <li>Granular actions: Composable permissions like <code>run:trigger</code>, <code>stack:manage</code>.</li> <li>Flexible assignment: Assign roles to users, IdP groups, and API keys.</li> <li>Space-based control: All roles are bound to specific Spaces for organized access management.</li> </ul>"},{"location":"concepts/authorization.html#core-rbac-architecture","title":"Core RBAC architecture","text":"<p>RBAC operates on three fundamental concepts: actions, actors, and subjects.</p>"},{"location":"concepts/authorization.html#actions","title":"Actions","text":"<p>Actions are the smallest unit of permission granularity. They define specific operations that can be performed within Spacelift. Examples include:</p> <ul> <li><code>run:trigger</code>: Trigger stack runs.</li> <li><code>stack:manage</code>: Create and modify stacks.</li> </ul>"},{"location":"concepts/authorization.html#actors","title":"Actors","text":"<p>Actors are entities that perform actions in the system:</p> <ul> <li>Users: Individual team members authenticated through your identity provider.</li> <li>API Keys: Programmatic access tokens for automation.</li> <li>IdP Groups: Groups of users as defined by your identity provider.</li> </ul>"},{"location":"concepts/authorization.html#subjects","title":"Subjects","text":"<p>Subjects are the resources being acted upon. Examples include:</p> <ul> <li>Stacks: Infrastructure definitions and their runs.</li> <li>Contexts: Collections of environment variables and files.</li> <li>Policies: Rules that govern various Spacelift behaviors.</li> <li>Spaces: Organizational containers for resources.</li> </ul>"},{"location":"concepts/authorization.html#getting-started-with-rbac","title":"Getting started with RBAC","text":""},{"location":"concepts/authorization.html#for-new-spacelift-users","title":"For new Spacelift users","text":"<p>If you're new to Spacelift, you can start using RBAC right away. Follow these steps to set up your RBAC configuration:</p> <ol> <li>Navigate to Organization Settings \u2192 Access Control Center \u2192 Roles.</li> <li>Review the predefined roles (Space Admin, Space Writer, Space Reader). These are equivalent to legacy roles.</li> <li>(Optional) Create custom roles with specific actions for your use cases.</li> <li>Assign roles to users and spaces.</li> </ol>"},{"location":"concepts/authorization.html#existing-users-migration-from-legacy-roles","title":"Existing users: migration from legacy roles","text":"<p>If you're currently using legacy system roles (Read/Write/Admin), your existing configurations have been automatically migrated to equivalent RBAC roles:</p> <ul> <li>Reader \u2192 Space Reader</li> <li>Writer \u2192 Space Writer</li> <li>Admin \u2192 Space Admin</li> </ul>"},{"location":"concepts/authorization.html#authorization-strategies","title":"Authorization Strategies","text":"<p>Spacelift offers two primary approaches for managing user access:</p>"},{"location":"concepts/authorization.html#user-management-recommended-for-most-organizations","title":"User management (recommended for most organizations)","text":"<ul> <li>GUI or API based: Manage access using the Spacelift web interface or using the Terraform provider.</li> <li>User-friendly: Invite users and assign roles without writing policies.</li> <li>IdP Integration: Seamlessly integrate with your identity provider for user management.</li> </ul>"},{"location":"concepts/authorization.html#login-policies-advanced","title":"Login policies (advanced)","text":"<ul> <li>Policy-as-code: Define authorization rules using Open Policy Agent (OPA).</li> <li>Dynamic: Conditional role assignment based on user attributes.</li> <li>Flexible: Support for complex authorization logic.</li> </ul>"},{"location":"concepts/authorization.html#key-rbac-features","title":"Key RBAC features","text":""},{"location":"concepts/authorization.html#access-control-center","title":"Access control center","text":"<p>A dedicated section in Organization Settings for managing your RBAC configuration:</p> <ul> <li>Create and manage custom roles.</li> <li>Assign roles to users, groups, and API keys.</li> <li>Monitor role assignments across spaces.</li> </ul>"},{"location":"concepts/authorization.html#custom-roles","title":"Custom roles","text":"<p>Go beyond predefined roles by creating custom roles that match your organization's specific needs.</p>"},{"location":"concepts/authorization.html#space-bound-permissions","title":"Space-bound permissions","text":"<p>All roles are assigned to specific spaces, providing:</p> <ul> <li>Isolation: Permissions are contained within designated spaces.</li> <li>Inheritance: Leverage space hierarchies for permission flow.</li> <li>Scalability: Manage permissions at the appropriate organizational level.</li> </ul>"},{"location":"concepts/authorization.html#next-steps","title":"Next steps","text":"<p>Dive deeper into RBAC with these guides:</p> <ul> <li>RBAC System: Detailed explanation of Spacelift's RBAC implementation.</li> </ul>"},{"location":"concepts/authorization.html#related-topics","title":"Related Topics","text":"<ul> <li>User Management: Invite and manage team members.</li> <li>Spaces: Organize resources with spaces.</li> <li>Login Policies: Policy-based access control.</li> <li>Single Sign-On: Enterprise identity integration.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-api-keys.html","title":"API key role bindings","text":""},{"location":"concepts/authorization/assigning-roles-api-keys.html#api-key-role-bindings","title":"API key role bindings","text":"<p>API keys can receive roles through three methods:</p> <ul> <li>Direct assignment: Assign roles directly to the API key.</li> <li>IdP group assignment: Associate API keys with IdP groups to inherit group-based role assignment.</li> <li>Login policy assignment: Use OPA policies to assign roles based on API key attributes (this can include assignment based on IdP group membership).</li> </ul> <p>Immediate role changes</p> <p>Except for login policies, role assignments and changes to roles take effect immediately (they force re-authentication if needed).</p>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#assign-roles","title":"Assign roles","text":""},{"location":"concepts/authorization/assigning-roles-api-keys.html#assign-roles-to-api-keys-directly-using-the-web-ui","title":"Assign roles to API keys directly using the web UI","text":"<ol> <li>Verify you meet the prerequisites:<ol> <li>The selected management strategy for your organization must be User Management.</li> <li>The key must exist in your Spacelift organization.</li> <li>You must have appropriate permissions to manage API key roles.</li> <li>Spaces where you want to assign roles must exist.</li> </ol> </li> <li>Navigate to API Key Management:<ol> <li>Click your name in the bottom left corner of the Spacelift interface.</li> <li>Go to Organization Settings.</li> <li>Go to ** API Keys in the Identity Management section.</li> <li>Find the API key you want to assign roles to.</li> <li>Click on the API key row to open its details.</li> </ol> </li> <li>Access role management:<ol> <li>In the API key details page, click Manage Roles.</li> <li>This opens the role assignment interface for the API key.</li> </ol> </li> <li>Assign roles:<ol> <li>Select Role: Choose appropriate role for the automation.</li> <li>Select Space: Choose the space where the role applies.</li> <li>Save Assignment: Confirm the role assignment.</li> </ol> </li> </ol>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#assign-roles-to-api-keys-directly-using-the-terraform-provider","title":"Assign roles to API keys directly using the Terraform provider","text":"<p>Refer to Spacelift Terraform provider documentation for more details.</p>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#assign-roles-to-api-keys-directly-using-login-policies","title":"Assign roles to API keys directly using login policies","text":"<ol> <li>Verify you meet the prerequisites:<ol> <li>The selected management strategy for your organization must be Login Policies.</li> <li>You must have appropriate permissions to create or modify login policies.</li> <li>Understanding of OPA/Rego policy language.</li> </ol> </li> <li>Use the <code>roles</code> rule to assign roles to users:</li> </ol> <pre><code>package spacelift\n\n# Allow API key login\nallow {\n    input.session.login == \"api-key-name\"\n}\n\n# Assign role to API key\nroles[space][role_id] {\n    input.session.login == \"api-key-name\"\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#by-key-name","title":"By key name","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Assign role to specific API key\nroles[\"production\"][\"deployer-role-id\"] {\n    input.session.login == \"ci-cd-production\"\n}\n\nroles[\"infrastructure\"][\"provisioner-role-id\"] {\n    input.session.login == \"terraform-automation\"\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#by-key-pattern","title":"By key pattern","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Assign roles based on key naming patterns\nroles[\"production\"][\"deployer-role-id\"] {\n    startswith(input.session.login, \"ci-cd-\")\n}\n\nroles[\"monitoring\"][\"reader-role-id\"] {\n    endswith(input.session.login, \"-monitoring\")\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#separate-keys-per-environment","title":"Separate keys per environment","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Development environment keys\nroles[\"development\"][\"full-deployer-role-id\"] {\n    environment_keys := {\n        \"ci-cd-dev\",\n        \"terraform-dev\",\n        \"automation-dev\"\n    }\n    environment_keys[input.session.login]\n}\n\n# Production environment keys (more restrictive)\nroles[\"production\"][\"limited-deployer-role-id\"] {\n    production_keys := {\n        \"ci-cd-prod\",\n        \"terraform-prod\"\n    }\n    production_keys[input.session.login]\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#multi-environment-keys","title":"Multi-environment keys","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Keys that work across multiple environments\nroles[space][\"cross-env-role-id\"] {\n    cross_env_keys := {\"backup-service\", \"monitoring-agent\"}\n    cross_env_keys[input.session.login]\n\n    # Define allowed spaces\n    allowed_spaces := {\"development\", \"staging\", \"production\"}\n    allowed_spaces[space]\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#cicd-pipeline-keys","title":"CI/CD pipeline keys","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# GitHub Actions deployment key\nroles[\"applications\"][\"github-deployer-role-id\"] {\n    input.session.login == \"github-actions-deploy\"\n}\n\n# GitLab CI deployment key\nroles[\"applications\"][\"gitlab-deployer-role-id\"] {\n    input.session.login == \"gitlab-ci-deploy\"\n}\n\n# Jenkins deployment key\nroles[\"applications\"][\"jenkins-deployer-role-id\"] {\n    input.session.login == \"jenkins-deploy\"\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#infrastructure-tools","title":"Infrastructure tools","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Terraform Cloud integration\nroles[\"infrastructure\"][\"terraform-cloud-role-id\"] {\n    input.session.login == \"terraform-cloud-integration\"\n}\n\n# Ansible automation\nroles[\"configuration\"][\"ansible-role-id\"] {\n    input.session.login == \"ansible-automation\"\n}\n\n# Kubernetes operator\nroles[\"kubernetes\"][\"k8s-operator-role-id\"] {\n    input.session.login == \"k8s-spacelift-operator\"\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#conditional-api-key-access","title":"Conditional API key access","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Time-based API key restrictions\nroles[\"production\"][\"time-limited-role-id\"] {\n    input.session.login == \"scheduled-deployment\"\n    is_deployment_window\n}\n\n# Helper rule for deployment windows\nis_deployment_window {\n    now := input.request.timestamp_ns\n    clock := time.clock([now, \"UTC\"])\n\n    # Allow deployments only during maintenance window\n    # Tuesday and Thursday, 2-4 AM UTC\n    weekday := time.weekday(now)\n    maintenance_days := {\"Tuesday\", \"Thursday\"}\n    maintenance_days[weekday]\n\n    clock[0] &gt;= 2\n    clock[0] &lt;= 4\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#ip-restricted-api-keys","title":"IP-restricted API keys","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# API keys restricted to specific networks\nroles[\"production\"][\"secure-deployer-role-id\"] {\n    secure_keys := {\"production-deploy\", \"critical-automation\"}\n    secure_keys[input.session.login]\n\n    # Only allow from secure networks\n    is_secure_network\n}\n\nis_secure_network {\n    secure_cidrs := {\n        \"10.0.0.0/8\",        # Corporate network\n        \"192.168.100.0/24\"   # Secure CI/CD subnet\n    }\n    secure_cidrs[cidr]\n    net.cidr_contains(cidr, input.request.remote_ip)\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#assign-roles-to-api-keys-using-idp-groups","title":"Assign roles to API keys using IdP groups","text":"<p>See IdP group role bindings for details on how to assign roles to IdP groups. Once a role is assigned to an IdP group, all actors (api keys and users that your identity provider reports as being members of that group) will inherit the assigned roles.</p>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#remove-an-api-key-role-binding","title":"Remove an API key role binding","text":"<ol> <li>Navigate to API Key Management:<ol> <li>Click your name in the bottom left corner of the Spacelift interface.</li> <li>Go to Organization Settings.</li> <li>Go to API Keys in the Identity Management section.</li> <li>Find the API key you want to assign roles to.</li> <li>Click on the API key row to open its details.</li> </ol> </li> <li>Access role management:<ol> <li>In the API key details page, click Manage Roles.</li> <li>This opens the role assignment interface for the API key.</li> </ol> </li> <li>Remove role assignment:<ol> <li>Find the role assignment to remove.</li> <li>Click Unassign from the dropdown.</li> <li>Confirm the removal.</li> </ol> </li> </ol>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#multiple-roles","title":"Multiple roles","text":"<p>Actors can have multiple roles across different spaces:</p> <ul> <li>Different roles in different spaces for varied access levels.</li> <li>Multiple roles in the same space (permissions are additive).</li> <li>Roles inherited from group membership plus individual assignments.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#find-role-ids","title":"Find role IDs","text":"<p>To use custom roles in login policies, you need their role IDs:</p> <ol> <li>Navigate to Organization Settings \u2192 Access Control Center \u2192 Roles.</li> <li>Click on the custom role you want to use.</li> <li>Click Copy ID from the role detail page.</li> <li>Use this ID in your login policy.</li> </ol>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/authorization/assigning-roles-api-keys.html#api-key-authentication-failures","title":"API Key authentication failures","text":"<ul> <li>Verify key is active and not expired.</li> <li>Ensure key is being used with correct endpoints.</li> <li>Validate key format and encoding.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#permission-denied-errors","title":"Permission denied errors","text":"<ul> <li>Confirm API key has required role assignments.</li> <li>Verify role includes necessary actions for the operation.</li> <li>Check if operation is being performed in correct space.</li> <li>Ensure space exists and API key has access.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#inconsistent-behavior","title":"Inconsistent behavior","text":"<ul> <li>API key permissions don't require re-authentication.</li> <li>Changes to role assignments take effect immediately.</li> <li>Check for policy conflicts or syntax errors.</li> <li>Validate role IDs are correct in login policies.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#debugging","title":"Debugging","text":"<ol> <li>Test API key authentication: You can create an interactive session with an API key (as if it was a user) to test the key's permissions and actions. To do that, go to <code>&lt;your-spacelift-subdomain&gt;.spacelift.io/apikeytoken</code> and enter your API key.</li> <li>Check role assignments: Confirm key has correct roles in target spaces.</li> <li>Validate actions: Ensure assigned roles include required permissions.</li> <li>Test operations: Use API key to perform expected operations.</li> <li>Review audit Logs: Check for API key related errors or warnings.</li> <li>Policy validation: If using login policies, verify syntax and logic, use the sample and simulate feature.</li> </ol>"},{"location":"concepts/authorization/assigning-roles-api-keys.html#related-topics","title":"Related Topics","text":"<ul> <li>Assigning Roles to Users: Individual user role assignment</li> <li>Assigning Roles to IdP Groups: Group-based role assignment</li> <li>RBAC System: Understanding Spacelift's RBAC</li> <li>API Integration: Using Spacelift's GraphQL API</li> </ul>"},{"location":"concepts/authorization/assigning-roles-groups.html","title":"IdP group role bindings","text":""},{"location":"concepts/authorization/assigning-roles-groups.html#idp-group-role-bindings","title":"IdP group role bindings","text":"<p>IdP groups can receive roles through direct group assignment. Assign roles to the entire group, which automatically applies to all members of that group.</p>"},{"location":"concepts/authorization/assigning-roles-groups.html#assign-roles","title":"Assign roles","text":""},{"location":"concepts/authorization/assigning-roles-groups.html#assign-roles-to-idp-groups-using-the-web-ui","title":"Assign roles to IdP groups using the web UI","text":"<ol> <li>Verify you meet the prerequisites:<ol> <li>The selected management strategy for your organization must be User Management.</li> <li>Your identity provider must be connected to Spacelift.</li> <li>You must have appropriate permissions to manage user group roles.</li> <li>Target spaces must exist where you want to assign roles.</li> </ol> </li> <li>Navigate to IdP group mapping:<ol> <li>Click your name in the bottom left corner of the Spacelift interface.</li> <li>Go to Organization Settings \u2192 Identity Management -&gt; IdP group mapping.</li> </ol> </li> <li>Create IdP group mapping:<ol> <li>Click Map IdP group.</li> <li>Enter the id of the IdP group (this is the id of the group in your identity provider, e.g., GitHub team slug).</li> <li>Select the role you want to assign to the group.</li> <li>Select the space where the group should have this role.</li> <li>Click Add to add role assignment.</li> <li>Click Add to save the group mapping.</li> </ol> </li> <li>Access group role management:<ol> <li>Click on the group row in the group list.</li> <li>Click Manage Roles.</li> <li>This opens the group role assignment interface.</li> </ol> </li> </ol>"},{"location":"concepts/authorization/assigning-roles-groups.html#assign-roles-to-idp-groups-using-the-terraform-provider","title":"Assign roles to IdP groups using the Terraform provider","text":"<p>Refer to the Spacelift Terraform provider documentation for detailed instructions on creating IdP group mappings programmatically.</p>"},{"location":"concepts/authorization/assigning-roles-groups.html#assign-roles-to-idp-groups-using-the-login-policies","title":"Assign roles to IdP groups using the login policies","text":"<ol> <li>Verify you meet the prerequisites:<ol> <li>The selected management strategy for your organization must be Login Policies.</li> <li>You must have appropriate permissions to create or modify login policies.</li> <li>Understanding of OPA/Rego policy language.</li> </ol> </li> <li>Use the <code>roles</code> rule to assign roles to users:</li> </ol> <pre><code>package spacelift\n\nallow { input.session.member }\n\n# Assign role based on team membership\nroles[space][role_id] {\n    input.session.teams[_] == \"team-name\"\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-groups.html#individual-group-assignment","title":"Individual group assignment","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# DevOps team gets platform engineer role\nroles[\"infrastructure\"][\"platform-engineer-role-id\"] {\n    input.session.teams[_] == \"DevOps\"\n}\n\n# Frontend team gets developer role\nroles[\"frontend\"][\"developer-role-id\"] {\n    input.session.teams[_] == \"Frontend\"\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-groups.html#multiple-teams-same-role","title":"Multiple teams, same role","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Define team sets\ndeveloper_teams := {\"Frontend\", \"Backend\", \"Mobile\", \"QA\"}\nplatform_teams := {\"DevOps\", \"SRE\", \"Platform\"}\n\n# Assign developer access\nroles[\"applications\"][\"developer-role-id\"] {\n    developer_teams[input.session.teams[_]]\n}\n\n# Assign platform access\nroles[\"infrastructure\"][\"platform-role-id\"] {\n    platform_teams[input.session.teams[_]]\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-groups.html#hierarchical-team-access","title":"Hierarchical team access","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Junior developers: development only\nroles[\"development\"][\"junior-dev-role-id\"] {\n    input.session.teams[_] == \"Junior-Developers\"\n}\n\n# Senior developers: development + staging\nroles[space][\"senior-dev-role-id\"] {\n    input.session.teams[_] == \"Senior-Developers\"\n    senior_spaces := {\"development\", \"staging\"}\n    senior_spaces[space]\n}\n\n# Team leads: all environments\nroles[space][\"team-lead-role-id\"] {\n    input.session.teams[_] == \"Team-Leads\"\n    all_spaces := {\"development\", \"staging\", \"production\"}\n    all_spaces[space]\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-groups.html#department-based-access","title":"Department-based access","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Engineering department base access\nroles[\"development\"][\"engineer-role-id\"] {\n    input.session.teams[_] == \"Engineering\"\n}\n\n# Operations department infrastructure access\nroles[\"infrastructure\"][\"ops-role-id\"] {\n    input.session.teams[_] == \"Operations\"\n}\n\n# Security department audit access across all spaces\nroles[space][\"security-auditor-role-id\"] {\n    input.session.teams[_] == \"Security\"\n    # Apply to all spaces\n    space := input.spaces[_].id\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-groups.html#project-and-functional-groups","title":"Project and functional groups","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Project-based access\nroles[\"project-alpha\"][\"developer-role-id\"] {\n    input.session.teams[_] == \"Project-Alpha-Team\"\n}\n\nroles[\"project-beta\"][\"developer-role-id\"] {\n    input.session.teams[_] == \"Project-Beta-Team\"\n}\n\n# Functional role overlays\nroles[\"infrastructure\"][\"platform-role-id\"] {\n    input.session.teams[_] == \"Platform-Engineers\"\n}\n\nroles[space][\"security-role-id\"] {\n    input.session.teams[_] == \"Security-Champions\"\n    # Security champions get audit access everywhere\n    space := input.spaces[_].id\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-groups.html#multi-condition-team-assignment","title":"Multi-condition team assignment","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Production access requires both team membership and seniority\nroles[\"production\"][\"prod-deployer-role-id\"] {\n    deployment_teams := {\"DevOps\", \"SRE\", \"Platform\"}\n    deployment_teams[input.session.teams[_]]\n\n    # Additional condition: must also be in senior group\n    input.session.teams[_] == \"Senior-Engineers\"\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-groups.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/authorization/assigning-roles-groups.html#group-permissions-not-working","title":"Group permissions not working","text":"<ul> <li>Verify group-to-role assignments are correct.</li> <li>Check if user is actually a member of the group.</li> <li>Ensure user has re-authenticated since group assignment.</li> <li>Validate role includes required actions.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-groups.html#conflicting-group-permissions","title":"Conflicting group permissions","text":"<ul> <li>Multiple groups can provide different roles.</li> <li>Permissions are additive across group memberships.</li> <li>Regular audit of group role combinations.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-groups.html#debugging","title":"Debugging","text":"<ol> <li>Verify Group Membership: Check user is member of expected groups in IdP</li> <li>Validate Role Assignment: Confirm group has correct role assignments</li> <li>Review Audit Logs: Check for group-related permission errors</li> </ol>"},{"location":"concepts/authorization/assigning-roles-groups.html#related-topics","title":"Related Topics","text":"<ul> <li>Assigning Roles to Users: Individual user role assignment</li> <li>Assigning Roles to API Keys: Service account permissions</li> <li>Login Policies: Policy-based access control</li> <li>Single Sign-On: IdP integration setup</li> </ul>"},{"location":"concepts/authorization/assigning-roles-users.html","title":"User role bindings","text":""},{"location":"concepts/authorization/assigning-roles-users.html#user-role-bindings","title":"User role bindings","text":"<p>Users can get permissions from three sources:</p> <ul> <li>Direct Role Assignment: Assign roles directly to individual users.</li> <li>IdP Group Assignment: Assign roles based on group memberships defined in your identity provider.</li> <li>Login Policy: Use Open Policy Agent (OPA) policies to dynamically assign roles (this can include assignment based on IdP group membership).</li> </ul> <p>Immediate Role Changes</p> <p>Except for login policies, role assignments and changes to roles take effect immediately (they force re-authentication if needed).</p>"},{"location":"concepts/authorization/assigning-roles-users.html#assign-roles","title":"Assign roles","text":""},{"location":"concepts/authorization/assigning-roles-users.html#assign-roles-to-users-directly-using-the-web-ui","title":"Assign roles to users directly using the web UI","text":"<ol> <li>Verify you meet the prerequisites:<ul> <li>User must be invited to the Spacelift organization.</li> <li>You must have appropriate permissions to manage user roles.</li> <li>Target spaces must exist where you want to assign roles.</li> <li>The selected management strategy for your organization must be User Management.</li> </ul> </li> <li>Navigate to User Management:<ol> <li>Click your name in the bottom left corner of the Spacelift interface.</li> <li>Select Organization Settings.</li> <li>Navigate to Users in the Identity Management section.</li> <li>Find the user you want to modify.</li> </ol> </li> <li>Access role management:<ol> <li>Click on the user's row in the user list.</li> <li>Click the Manage Roles button.</li> <li>This opens the role assignment interface.</li> </ol> </li> <li>Assign roles:<ol> <li>Select Role: Choose from predefined roles or custom roles.</li> <li>Select Space: Choose the space where the role applies.</li> <li>Save Assignment: Click Add to confirm the assignment.</li> </ol> </li> </ol>"},{"location":"concepts/authorization/assigning-roles-users.html#assign-roles-to-users-directly-using-the-terraform-provider","title":"Assign roles to users directly using the Terraform provider","text":"<p>Refer to Spacelift Terraform provider documentation for more details.</p>"},{"location":"concepts/authorization/assigning-roles-users.html#assign-roles-to-users-directly-using-the-login-policies","title":"Assign roles to users directly using the login policies","text":"<ol> <li>Verify you meet the prerequisites:<ul> <li>The selected management strategy for your organization must be Login Policies.</li> <li>You have the appropriate permissions to create or modify login policies.</li> <li>You understand the OPA/Rego policy language.</li> </ul> </li> <li>Use the <code>roles</code> rule to assign roles to users:</li> </ol> <pre><code>package spacelift\n\n# Basic login permission\nallow { input.session.member }\n\n# Role assignment syntax\nroles[space_name][role_id] { condition }\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-users.html#individual-user-assignment","title":"Individual user assignment","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Assign platform engineer role to specific user\nroles[\"infrastructure\"][\"platform-engineer-role-id\"] {\n    input.session.login == \"alice@company.com\"\n}\n\n# Assign multiple roles to same user\nroles[\"development\"][\"developer-role-id\"] {\n    input.session.login == \"alice@company.com\"\n}\n\nroles[\"staging\"][\"developer-role-id\"] {\n    input.session.login == \"alice@company.com\"\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-users.html#multiple-users-with-same-role","title":"Multiple users with same role","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Define user set\nsenior_engineers := {\n    \"alice@company.com\",\n    \"bob@company.com\",\n    \"charlie@company.com\"\n}\n\n# Assign role to all users in set\nroles[\"production\"][\"senior-developer-role-id\"] {\n    senior_engineers[input.session.login]\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-users.html#environment-based-access","title":"Environment-based access","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Junior developers: development only\nroles[\"development\"][\"developer-role-id\"] {\n    input.session.teams[_] == \"Junior-Developers\"\n}\n\n# Senior developers: development + staging\nroles[space][\"developer-role-id\"] {\n    input.session.teams[_] == \"Senior-Developers\"\n    environment_spaces := {\"development\", \"staging\"}\n    environment_spaces[space]\n}\n\n# Lead developers: all environments\nroles[space][\"lead-developer-role-id\"] {\n    input.session.teams[_] == \"Lead-Developers\"\n    all_spaces := {\"development\", \"staging\", \"production\"}\n    all_spaces[space]\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-users.html#time-based-access","title":"Time-based access","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Production access only during business hours\nroles[\"production\"][\"prod-deployer-role-id\"] {\n    input.session.teams[_] == \"SRE\"\n    is_business_hours\n}\n\n# Helper rule for business hours\nis_business_hours {\n    now := input.request.timestamp_ns\n    clock := time.clock([now, \"America/Los_Angeles\"])\n    weekday := time.weekday(now)\n\n    # Monday through Friday, 9 AM to 5 PM\n    not weekend[weekday]\n    clock[0] &gt;= 9\n    clock[0] &lt;= 17\n}\n\nweekend := {\"Saturday\", \"Sunday\"}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-users.html#ip-based-access","title":"IP-based access","text":"<pre><code>package spacelift\n\nallow { input.session.member }\n\n# Sensitive operations only from office network\nroles[\"production\"][\"admin-role-id\"] {\n    input.session.teams[_] == \"DevOps\"\n    is_office_network\n}\n\n# Helper rule for office network\nis_office_network {\n    office_networks := {\n        \"192.168.1.0/24\",\n        \"10.0.0.0/8\"\n    }\n    office_networks[network]\n    net.cidr_contains(network, input.request.remote_ip)\n}\n</code></pre>"},{"location":"concepts/authorization/assigning-roles-users.html#assigning-roles-to-users-via-idp-groups","title":"Assigning roles to users via IdP groups","text":"<p>See IdP Group Role Bindings for details on how to assign roles to IdP groups. Once a role is assigned to an IdP group, all actors (API keys and users that your identity provider reports as being members of that group) will inherit the assigned roles.</p>"},{"location":"concepts/authorization/assigning-roles-users.html#removing-a-user-role-binding","title":"Removing a user role binding","text":"<ol> <li>Navigate to User Management:<ol> <li>Click your name in the bottom left corner of the Spacelift interface.</li> <li>Select Organization Settings.</li> <li>Navigate to Users in the Identity Management section.</li> <li>Find the user you want to modify.</li> </ol> </li> <li>Access role management:<ol> <li>Click on the user's row in the user list.</li> <li>Click the Manage Roles button.</li> <li>This opens the role assignment interface.</li> </ol> </li> <li>Remove role assignment:<ol> <li>Find the role assignment to remove.</li> <li>Click the Unassign button from the dropdown.</li> <li>Confirm the removal.</li> </ol> </li> </ol>"},{"location":"concepts/authorization/assigning-roles-users.html#multiple-roles","title":"Multiple roles","text":"<p>Actors can have multiple roles across different spaces:</p> <ul> <li>Different roles in different spaces for varied access levels.</li> <li>Multiple roles in the same space (permissions are additive).</li> <li>Roles inherited from group membership plus individual assignments.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-users.html#getting-role-ids","title":"Getting role IDs","text":"<p>To use custom roles in login policies, you need their role IDs:</p> <ol> <li>Navigate to Organization Settings \u2192 Access Control Center \u2192 Roles.</li> <li>Click on the custom role you want to use.</li> <li>Click Copy ID from the role detail page.</li> <li>Use this ID in your login policy.</li> </ol>"},{"location":"concepts/authorization/assigning-roles-users.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/authorization/assigning-roles-users.html#user-cannot-see-resources","title":"User cannot see resources","text":"<ul> <li>Verify user has <code>space:read</code> action.</li> <li>Check if user is assigned to correct space.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-users.html#role-assignment-not-taking-effect","title":"Role assignment not taking effect","text":"<ul> <li>Check login policy syntax for errors.</li> <li>Verify role ID is correct in login policies.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-users.html#permission-denied-errors","title":"Permission denied errors","text":"<ul> <li>Verify user has required actions for the operation.</li> <li>Check if operation is being performed in correct space.</li> <li>Confirm role includes necessary permissions.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-users.html#login-policy-not-working","title":"Login policy not working","text":"<ul> <li>Check policy syntax for errors.</li> <li>Verify input data structure matches policy conditions.</li> <li>Use policy sampling to debug input data.</li> <li>Check for typos in team names or user logins.</li> <li>Check for case sensitivity in team names and user logins.</li> </ul>"},{"location":"concepts/authorization/assigning-roles-users.html#related-topics","title":"Related Topics","text":"<ul> <li>Assigning Roles to IdP Groups: Group-based role assignment</li> <li>Assigning Roles to API Keys: Service account permissions</li> <li>RBAC System: Understanding Spacelift's RBAC</li> <li>User Management: User invitation and management</li> </ul>"},{"location":"concepts/authorization/rbac-system.html","title":"Role-Based Access Control (RBAC)","text":""},{"location":"concepts/authorization/rbac-system.html#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<p>Up until recently, Spacelift used legacy system roles with broad roles (Reader, Writer, Admin) to manage user permissions. This approach worked for many organizations but lacked the granularity and flexibility needed for modern infrastructure management. With the introduction of the Custom RBAC system, Spacelift has transformed how permissions are managed, enabling a more fine-grained, composable approach to advanced access control.</p>"},{"location":"concepts/authorization/rbac-system.html#custom-rbac-vs-legacy-system-roles","title":"Custom RBAC vs legacy system roles","text":""},{"location":"concepts/authorization/rbac-system.html#legacy-system-roles-previous-approach","title":"Legacy system roles (previous approach)","text":"<p>The legacy system used three broad roles:</p> <ul> <li>Reader: View-only access to resources.</li> <li>Writer: Reader permissions + ability to trigger runs and modify environment variables.</li> <li>Admin: Writer permissions + ability to create/modify stacks and attachable entities.</li> </ul> <p>Migration to custom RBAC</p> <p>Existing legacy system role assignments have been automatically migrated to equivalent custom RBAC roles:</p> Legacy Role RBAC Equivalent Reader Space Reader Writer Space Writer Admin Space Admin"},{"location":"concepts/authorization/rbac-system.html#custom-rbac-current-approach","title":"Custom RBAC (current approach)","text":"<p>Custom RBAC decomposes these broad roles into individual, composable actions.</p> <p>Custom RBAC Example</p> <p>Instead of giving an actor, like an API key, full Writer access (which includes many permissions that are not needed), you can create a custom \"Deployment Operator\" role with just these permissions:</p> <ul> <li><code>run:trigger</code>: Can trigger stack runs.</li> <li><code>run:read</code>: Can view run details.</li> </ul> <p>This approach provides exactly the access needed for deployment operations without extra permissions.</p>"},{"location":"concepts/authorization/rbac-system.html#core-architecture","title":"Core architecture","text":"<p>In Spacelift's RBAC system, actors are entities that perform actions on subjects within spaces. This architecture allows for precise control over who can do what, where, and how.</p>"},{"location":"concepts/authorization/rbac-system.html#actors-who-performs-actions","title":"Actors: who performs actions","text":"<p>Actors include users, API keys, and IdP groups.</p>"},{"location":"concepts/authorization/rbac-system.html#users","title":"Users","text":"<p>Individual team members who are authenticated through your identity provider (GitHub, GitLab, Microsoft, Google, or SAML/OIDC SSO).</p>"},{"location":"concepts/authorization/rbac-system.html#user-patterns","title":"User patterns","text":"<ul> <li>Use IdP groups for role assignment when possible.</li> <li>Limit individual user role assignments to exceptional cases.</li> <li>Regular access reviews and cleanup.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#api-keys","title":"API Keys","text":"<p>Programmatic access tokens for automation and CI/CD integration.</p>"},{"location":"concepts/authorization/rbac-system.html#api-key-patterns","title":"API key patterns","text":"<ul> <li>Create purpose-specific keys with minimal required permissions.</li> <li>Use specific custom roles rather than broad predefined roles.</li> <li>Use environment-specific keys rather than shared keys.</li> <li>Use descriptive names that indicate purpose (e.g., \"terraform-ci-prod\").</li> <li>Include environment or project context in the name.</li> <li>Implement key rotation policies.</li> <li>Document the purpose and owner of each API key.</li> <li>Monitor API key usage through audit trails.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#idp-groups","title":"IdP Groups","text":"<p>Groups of users as defined by your identity provider.</p>"},{"location":"concepts/authorization/rbac-system.html#examples-of-group-sources","title":"Examples of group sources","text":"<p>GitHub Teams:</p> <ul> <li>Passed in the users' token.</li> </ul> <p>SAML/OIDC Groups:</p> <ul> <li>Defined by your enterprise identity provider.</li> <li>Mapped through SAML assertions or OIDC claims.</li> <li>Group membership determined by your IdP's group policies.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#idp-group-patterns","title":"IdP group patterns","text":"<p>Functional Groups: Organize groups by job function across the organization.</p> <ul> <li><code>platform-engineers</code> \u2192 Full infrastructure management</li> <li><code>application-developers</code> \u2192 Deployment capabilities only</li> <li><code>security-auditors</code> \u2192 Read-only access across all spaces</li> </ul> <p>Project Groups: Organize groups by project or product.</p> <ul> <li><code>project-alpha-team</code> \u2192 Full access to \"Project Alpha\" space</li> <li><code>project-beta-team</code> \u2192 Full access to \"Project Beta\" space</li> </ul> <p>Hybrid Approach: Combine functional and project-based groups.</p> <ul> <li>Base permissions from functional groups.</li> <li>Additional project-specific permissions from project groups.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#actions-the-building-blocks-of-permissions","title":"Actions: the building blocks of permissions","text":"<p>Actions are the smallest unit of permission granularity in Spacelift's RBAC system. Each action defines a specific operation that can be performed:</p> Action Description Legacy Equivalent <code>run:trigger</code> Trigger stack runs Writer <code>stack:manage</code> Create and modify stacks Admin <code>stack:delete</code> Delete stacks Admin <code>context:read</code> View contexts Reader <code>context:manage</code> Create and modify contexts Admin <code>space:read</code> View space contents Reader <code>space:manage</code> Manage space settings Admin <p>Expanding action catalog</p> <p>The RBAC system supports a limited, but expanding set of actions. Spacelift continuously adds new actions based on user feedback and use cases.</p>"},{"location":"concepts/authorization/rbac-system.html#subjects-what-actions-are-performed-on","title":"Subjects: what actions are performed on","text":"<p>Subjects are the resources that actors interact with, for example:</p> <ul> <li>Stacks: Infrastructure definitions, runs, and associated metadata.</li> <li>Contexts: Environment variables, mounted files, and configuration collections.</li> <li>Policies: Rules governing Spacelift behavior (approval, notification, etc.).</li> </ul> <p>Space-level granularity</p> <p>Currently, RBAC operates at the space level. All roles are bound to specific spaces and apply equally to all subjects within that space. Entity-level granularity (e.g., permissions for individual stacks) is not yet supported.</p>"},{"location":"concepts/authorization/rbac-system.html#stack-access-patterns","title":"Stack access patterns","text":"<p>Development Stacks:</p> <ul> <li>Developers need full management capabilities.</li> <li>Frequent deployments and experimentation.</li> <li>Less restrictive approval requirements.</li> </ul> <p>Production Stacks:</p> <ul> <li>Limited management access to senior engineers.</li> <li>Strict approval workflows.</li> <li>Enhanced audit and monitoring.</li> </ul> <p>Shared Infrastructure Stacks:</p> <ul> <li>Platform team management.</li> <li>Application team read access.</li> <li>Cross-team coordination requirements.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#policy-access-patterns","title":"Policy access patterns","text":"<p>Centralized Governance:</p> <ul> <li>Security team manages all policies.</li> <li>Consistent rules across the organization.</li> <li>Limited policy creation permissions.</li> </ul> <p>Federated Governance:</p> <ul> <li>Teams manage policies for their own spaces.</li> <li>Organization-wide baseline policies.</li> <li>Team-specific additional policies.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#spaces-the-scope-of-permissions","title":"Spaces: the scope of permissions","text":"<p>All RBAC roles are space-bound, meaning:</p> <ul> <li>Roles are assigned to specific spaces.</li> <li>Permissions apply to all subjects within that space.</li> <li>Users need appropriate roles in each space they need to access.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#space-hierarchy","title":"Space hierarchy","text":"<p>Spaces can be organized hierarchically to reflect your organizational structure:</p> <pre><code>Root Space\n\u251c\u2500\u2500 Infrastructure (Platform team management)\n\u2502   \u251c\u2500\u2500 Networking\n\u2502   \u251c\u2500\u2500 Security\n\u2502   \u2514\u2500\u2500 Monitoring\n\u251c\u2500\u2500 Applications (Application teams)\n\u2502   \u251c\u2500\u2500 Frontend\n\u2502   \u251c\u2500\u2500 Backend\n\u2502   \u2514\u2500\u2500 Mobile\n\u2514\u2500\u2500 Sandbox (Development and testing)\n</code></pre>"},{"location":"concepts/authorization/rbac-system.html#space-design-patterns","title":"Space design patterns","text":"<p>Isolation requirements:</p> <ul> <li>Separate spaces for different environments (dev/staging/prod).</li> <li>Separate spaces for different teams or projects.</li> <li>Separate spaces for different compliance requirements.</li> </ul> <p>Permission boundaries:</p> <ul> <li>Align space boundaries with permission requirements.</li> <li>Consider who needs access to what resources.</li> <li>Plan for space hierarchy and inheritance patterns.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#roles","title":"Roles","text":""},{"location":"concepts/authorization/rbac-system.html#predefined-roles","title":"Predefined roles","text":"<p>Spacelift provides three predefined roles (corresponding to the legacy system roles):</p>"},{"location":"concepts/authorization/rbac-system.html#space-reader","title":"Space reader","text":"<p>Actions: Basic read permissions</p> <ul> <li>View stacks, contexts, policies, and runs.</li> <li>Add comments to runs for feedback.</li> <li>Cannot trigger actions or modify resources.</li> <li>Equivalent to legacy Reader role.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#space-writer","title":"Space writer","text":"<p>Actions: Space Reader + multiple execution permissions</p> <ul> <li>All Space Reader permissions.</li> <li>Trigger stack runs.</li> <li>Execute tasks.</li> <li>Modify stack environment variables.</li> <li>Equivalent to legacy Writer role.</li> </ul>"},{"location":"concepts/authorization/rbac-system.html#space-admin","title":"Space admin","text":"<p>Actions: Space Writer + management permissions</p> <ul> <li>All Space Writer permissions.</li> <li>Create and modify stacks.</li> <li>Create and modify contexts and policies.</li> <li>Manage space settings (when assigned to specific space).</li> <li>Equivalent to legacy Admin role.</li> </ul> <p>Root Space Admin</p> <p>Users with Space Admin role on the root space become Root Space Admins with account-wide privileges including SSO setup, VCS configuration, and audit trail management.</p>"},{"location":"concepts/authorization/rbac-system.html#custom-roles","title":"Custom roles","text":""},{"location":"concepts/authorization/rbac-system.html#create-custom-roles-using-the-web-ui","title":"Create custom roles using the web UI","text":"<ol> <li>Go to Organization Settings \u2192 Access Control Center \u2192 Roles.</li> <li>Click Create Role to start defining a new role.</li> <li>Define Role Properties:<ul> <li>Name: Descriptive role name (e.g., \"Infrastructure Developer\").</li> <li>Description: Clear explanation of the role's purpose.</li> <li>Actions: Select specific permissions needed.</li> </ul> </li> </ol> <p>Read access baseline</p> <p>The <code>space:read</code> action is required to view any subjects within a space. Without it, users cannot see other resources even if they have permissions for them.</p>"},{"location":"concepts/authorization/rbac-system.html#create-custom-roles-using-the-terraform-provider","title":"Create custom roles using the Terraform provider","text":"<p>Refer to the Spacelift Terraform provider documentation for detailed instructions on creating custom roles programmatically.</p>"},{"location":"concepts/authorization/rbac-system.html#role-bindings-assigning-roles-to-actors","title":"Role bindings (assigning roles to actors)","text":"<p>View more detailed instructions for assigning roles to:</p> <ul> <li>Individual users</li> <li>API keys</li> <li>IdP groups</li> </ul>"},{"location":"concepts/blueprint.html","title":"Blueprint","text":""},{"location":"concepts/blueprint.html#blueprint","title":"Blueprint","text":"<p>There are multiple ways to create stacks in Spacelift. We recommend using our Terraform provider to programmatically create stacks using an administrative stack.</p> <p>However, some users might not be comfortable using Terraform code to create stacks. This is where Blueprints come in handy.</p>"},{"location":"concepts/blueprint.html#what-is-a-blueprint","title":"What is a Blueprint?","text":"<p>A Blueprint is a template for a stack and its configuration. The template contains variables that can be filled in by providing inputs when creating a stack from the Blueprint. The template can also contain a list of other resources that will be created when the stack is created.</p> <p>You can configure the following resources in a Blueprint:</p> <ul> <li>All stack settings including:<ul> <li>Name, description, labels, space.</li> <li>Behavioral settings: administrative, auto-apply, auto-destroy, hooks, runner image, etc.</li> </ul> </li> <li>VCS configuration.<ul> <li>Both default and space-level VCS integrations. In GitHub, custom app installations can only be created by GitHub Enterprise accounts.</li> </ul> </li> <li>Vendor configuration for your IaC provider.</li> <li>Environment variables, both non-sensitive and sensitive.</li> <li>Mounted files.</li> <li>Attaching Contexts.</li> <li> <p>Attaching Policies.</p> </li> <li> <p>Attaching AWS integrations.</p> </li> <li> <p>Dependencies.</p> </li> <li>Schedules:<ul> <li>Drift detection.</li> <li>Task.</li> <li>Delete.</li> </ul> </li> </ul>"},{"location":"concepts/blueprint.html#blueprint-states","title":"Blueprint states","text":"<p>There are two states: draft and published.</p> <ul> <li>Draft: The default state, where blueprint \"development\" is in progress and it's not meant to be used. You cannot create a stack from a draft blueprint.</li> <li>Published: The blueprint is ready to be used once you click Publish in the UI.</li> </ul> <p>A published blueprint cannot be moved back to draft state. You need to clone the blueprint, edit it, and publish it.</p> <p>You can share published blueprints with other users in your organization. They can create stacks from the blueprint as long as they have the necessary permissions. To share a blueprint, click Share in the published blueprint view to generate a link. When users navigate to the link, they will be presented with the full-screen template form without the blueprint editor.</p> <p></p> <p>Shared blueprint</p>"},{"location":"concepts/blueprint.html#permissions","title":"Permissions","text":"<p>Blueprint permissions are managed by spaces. You can only create, update, and delete a blueprint in a Space where you have admin access, but it can be read by anyone with read access to the Space.</p> <p>Once the blueprint is published and you want to create a stack from it, read access will be enough as long as you have admin access to the Space where the stack will be created.</p>"},{"location":"concepts/blueprint.html#how-to-create-a-blueprint","title":"How to create a blueprint","text":"<p>Navigate to the Blueprints page and click Create blueprint. As of now, we only support YAML format. The template engine will be familiar to those who have used GitHub Actions before.</p> <p>The absolute minimum you'll need to provide is <code>name</code>, <code>space</code>, <code>vcs</code>, and <code>vendor</code>; all others are optional. Here's a small working example:</p> <pre><code>inputs:\n  - id: stack_name\n    name: Stack name\nstack:\n  name: ${{ inputs.stack_name }}\n  space: root\n  vcs:\n    branch: main\n    repository: my-repository\n    provider: GITHUB\n  vendor:\n    terraform:\n      manage_state: true\n      version: \"1.3.0\"\n</code></pre> <p></p> <p>Preview of a blueprint</p> <p>The Create stack button is inactive because the blueprint is in draft state. To create a stack from the blueprint, click Publish, then Create stack.</p> <p>Info</p> <p>Multiple stacks can be created using a single blueprint if the <code>stacks</code> array is used instead of the <code>stack</code> object. See the full schema below for more information.</p>"},{"location":"concepts/blueprint.html#comprehensive-blueprint-example","title":"Comprehensive blueprint example","text":"<p>This blueprint covers all available configuration options.</p> <pre><code>inputs:\n  - id: environment\n    name: Environment to deploy to\n    # type is not mandatory, defaults to short_text\n  - id: app\n    name: App name (used for naming convention)\n    type: short_text\n  - id: description\n    name: Description of the stack\n    type: long_text\n    # long_text means you'll have a bigger text area in the UI\n\n    ... # Expand the box below to view the entire blueprint\n</code></pre> Click to view the comprehensive blueprint example <pre><code>```yaml\ninputs:\n- id: environment\n    name: Environment to deploy to\n    # type is not mandatory, defaults to short_text\n- id: app\n    name: App name (used for naming convention)\n    type: short_text\n- id: description\n    name: Description of the stack\n    type: long_text\n    # long_text means you'll have a bigger text area in the UI\n\n    ... # Expand the box below to view the entire blueprint\n- id: connstring\n    name: Connection string to the database\n    type: secret\n    # secret means the input will be masked in the UI\n- id: tf_version\n    name: OpenTofu/Terraform version of the stack\n    type: select\n    options:\n    - \"1.3.0\"\n    - \"1.4.6\"\n    - \"1.5.0\"\n- id: manage_state\n    name: Should Spacelift manage the state of OpenTofu/Terraform\n    default: true\n    type: boolean\n- id: destroy_task_epoch\n    name: Epoch timestamp of when to destroy the resources\n    type: number\noptions:\n# If true, a tracked run will be triggered right after the stack is created\ntrigger_run: true\n# If true, the stack will not be created, useful when using inputs and multiple stacks in a single template.\ndo_not_create: false\nstack:\nname: ${{ inputs.app }}-{{ inputs.environment }}-stack\nspace: root\n# The single-quote is needed to avoid YAML parsing errors since the question mark\n# and the colon are reserved characters in YAML.\ndescription: '${{ inputs.environment == \"prod\" ? \"Production stack\" : \"Non-production stack\" }}. Stack created at ${{ string(context.time) }}.'\nis_disabled: ${{ inputs.environment != 'prod' }}\nlabels:\n    - Environment/${{ inputs.environment }}\n    - Vendor/Terraform\n    - Owner/${{ context.user.login }}\n    - Blueprint/${{ context.blueprint.name }}\n    - Space/${{ context.blueprint.space }}\nadministrative: false\nallow_promotion: false\nauto_deploy: false\nauto_retry: false\nlocal_preview_enabled: true\nsecret_masking_enabled: true\nprotect_from_deletion: false\nrunner_image: public.ecr.aws/mycorp/spacelift-runner:latest\nworker_pool: 01GQ29K8SYXKZVHPZ4HG00BK2E\nattachments:\n    contexts:\n    - id: my-first-context-vnfq2\n        priority: 1\n    clouds:\n    aws:\n        id: 01GQ29K8SYXKZVHPZ4HG00BK2E\n        read: true\n        write: true\n    azure:\n        id: 01GQ29K8SYXKZVHPZ4HG00BK2E\n        read: true\n        write: true\n        subscription_id: 12345678-1234-1234-1234-123456789012\n    policies:\n    - my-push-policy-1\n    - my-approval-policy-1\nenvironment:\n    variables:\n    - name: MY_ENV_VAR\n        value: my-env-var-value\n        description: This is my non-encrypted environment variable\n    - name: TF_VAR_CONNECTION_STRING\n        value: ${{ inputs.connstring }}\n        description: The connection string to the database\n        secret: true\n    mounted_files:\n    - path: a.json\n        content: |\n        {\n            \"a\": \"b\"\n        }\n        description: This is the configuration of x feature\n        secret: true\nhooks:\n    apply:\n    before: [\"sh\", \"-c\", \"echo 'before apply'\"]\n    after: [\"sh\", \"-c\", \"echo 'after apply'\"]\n    init:\n    before: [\"sh\", \"-c\", \"echo 'before init'\"]\n    after: [\"sh\", \"-c\", \"echo 'after init'\"]\n    plan:\n    before: [\"sh\", \"-c\", \"echo 'before plan'\"]\n    after: [\"sh\", \"-c\", \"echo 'after plan'\"]\n    perform:\n    before: [\"sh\", \"-c\", \"echo 'before perform'\"]\n    after: [\"sh\", \"-c\", \"echo 'after perform'\"]\n    destroy:\n    before: [\"sh\", \"-c\", \"echo 'before destroy'\"]\n    after: [\"sh\", \"-c\", \"echo 'after destroy'\"]\n    run:\n    # There is no before hook for run\n    after: [\"sh\", \"-c\", \"echo 'after run'\"]\nschedules:\n    drift:\n    cron:\n        - \"0 0 * * *\"\n        - \"5 5 * * 0\"\n    reconcile: true\n    ignore_state: true # If true, the schedule will run even if the stack is in a failed state\n    timezone: UTC\n    tasks:\n    # You need to provide either a cron or a timestamp_unix\n    - command: \"terraform apply -auto-approve\"\n        cron:\n        - \"0 0 * * *\"\n    - command: \"terraform apply -auto-approve\"\n        timestamp_unix: ${{ int(timestamp('2024-01-01T10:00:20.021-05:00')) }}\n    delete:\n    delete_resources: ${{ inputs.environment == 'prod' }}\n    timestamp_unix: ${{ inputs.destroy_task_epoch - 86400 }}\nvcs:\n    id: \"github-for-my-org\" # Optional, only needed if you want to use a Space-level VCS integration. Use the \"Copy ID\" button to get the ID.\n    branch: main\n    project_root: modules/apps/${{ inputs.app }}\n    project_globs:\n    - \"terraform/**\"\n    - \"k8s/**\"\n    namespace: \"my-namespace\" # The VCS organization name or project namespace\n    # Note that this is just the name of the repository, not the full URL\n    repository: my-repository\n    provider: GITHUB_ENTERPRISE # Possible values: GITHUB, GITLAB, BITBUCKET_DATACENTER, BITBUCKET_CLOUD, GITHUB_ENTERPRISE, AZURE_DEVOPS, RAW_GIT\n    repository_url: \"https://github.com/my-namespace/my-repository\" # This is only needed for RAW_GIT provider\nvendor:\n    terraform:\n    manage_state: ${{ inputs.manage_state }}\n    version: ${{ inputs.tf_version }}\n    workspace: workspace-${{ inputs.environment }}\n    use_smart_sanitization: ${{ inputs.environment != 'prod' }}\n    workflow_tool: OPEN_TOFU # Could be TERRAFORM_FOSS, OPEN_TOFU, or CUSTOM\n    ansible:\n    playbook: playbook.yml\n    cloudformation:\n    entry_template_file: cf/main.yml\n    template_bucket: template_bucket\n    stack_name: ${{ inputs.app }}-${{ inputs.environment }}\n    region: '${{ inputs.environment.contains(\"prod\") ? \"us-east-1\" : \"us-east-2\" }}'\n    kubernetes:\n    namespace: ${{ inputs.app }}\n    pulumi:\n    stack_name: ${{ inputs.app }}-${{ inputs.environment }}\n    login_url: https://app.pulumi.com\n    terragrunt:\n    use_smart_sanitization: true\n    terraform_version: \"1.5.7\"\n    terragrunt_version: \"0.55.0\"\n    use_run_all: true\n    terragrunt_tool: OPEN_TOFU # Could be OPEN_TOFU, TERRAFORM_FOSS, or MANUALLY_PROVISIONED\n```\n</code></pre> <p>If we attach an existing resource to the stack (such as Worker Pool, Cloud integration, Policy or Context) we use the unique identifier of the resource. Typically, there is a button for it in the UI, but you can also find it in the URL of the resource.</p> <p></p> <p>Example of resource IDs</p>"},{"location":"concepts/blueprint.html#attach-a-vcs","title":"Attach a VCS","text":"<p>We have the following VCS systems available:</p> <ul> <li><code>AZURE_DEVOPS</code></li> <li><code>BITBUCKET_CLOUD</code></li> <li><code>BITBUCKET_DATACENTER</code></li> <li><code>GITHUB</code>: This is the built-in GitHub integration that is used for SSO as well.</li> <li><code>GITHUB_ENTERPRISE</code>: Unlike the name suggests, it's not only for GitHub Enterprise, but for any additional GitHub installation.</li> <li><code>GITLAB</code></li> <li><code>RAW_GIT</code>: Enables you to use any public Git repository. When using this, you need to provide the full URL for the repository by setting the <code>repository_url</code> field.</li> </ul> <p>The <code>vcs</code> section is mandatory and you need to provide the <code>branch</code>, <code>repository</code> and <code>provider</code>. Additionally, if your VCS is anything other than <code>GITHUB</code> or <code>RAW_GIT</code>, you need to provide <code>namespace</code> as well. In GitHub, that's the organization name, in GitLab it's the group name, and in Bitbucket and Azure it's the project name.</p> <p>If the VCS is <code>RAW_GIT</code>, you need to provide the <code>repository_url</code> instead of the <code>namespace</code> and <code>repository</code>.</p> <p>The <code>id</code> is optional and only needed if you want to use a non-default integration. You can find the ID by clicking Copy ID in the VCS integration settings.</p> <pre><code>  vcs:\n    id: \"github-for-my-org\" # Optional, only needed if you want to use a non-default VCS integration. Use the \"Copy ID\" button to get the ID.\n    branch: main\n    project_root: modules/networking\n    project_globs: # Project globs do not mount the files or directories in your project root. They are used primarily for triggering your stack when for example there are changes to a module outside of the project root.\n      - \"terraform/**\"\n      - \"k8s/**\"\n    namespace: \"my-namespace\" # The VCS organization name or project namespace.\n    repository: my-repository # Name of the repository.\n    repository_url: \"https://www.github.com/my-namespace/my-repository\" # This is only needed for RAW_GIT\n    provider: GITHUB_ENTERPRISE # Possible values: AZURE_DEVOPS, BITBUCKET_CLOUD, BITBUCKET_DATACENTER, GITHUB, GITHUB_ENTERPRISE, GITLAB, RAW_GIT\n</code></pre>"},{"location":"concepts/blueprint.html#template-engine","title":"Template engine","text":"<p>We built our own variable substitution engine based on Google CEL. The library is available on GitHub.</p>"},{"location":"concepts/blueprint.html#functions-objects","title":"Functions, objects","text":"<p>In the giant example above, you might have noticed inline functions. CEL supports a couple of functions, such as: <code>contains</code>, <code>startsWith</code>, <code>endsWith</code>, <code>matches</code>, <code>size</code> and a bunch of others. You can find the full list in the language definition. It also supports some basic operators, such as: <code>*</code>, <code>/</code>, <code>-</code>, <code>+</code>, relations (<code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>), <code>&amp;&amp;</code>, <code>||</code>, <code>!</code>, <code>?:</code>, and <code>in</code>.</p> <p>Other than the built-in operators and functions, we also added the string extensions to the evaluator, which include <code>.replace()</code>, <code>.lowerAscii()</code>, <code>.split()</code> and other methods. Example:</p> <pre><code>stack:\n  name: ${{ inputs.app_name.replace(\" \", \"-\") }}\n</code></pre> <p>Hint</p> <p>Look into the unit tests of the library. Look for the invocations of <code>interpret</code> function.</p> <p>Remember to keep the YAML syntax valid.</p>"},{"location":"concepts/blueprint.html#yaml-syntax-validity","title":"YAML syntax validity","text":"<p>There are reserved characters in YAML, such as <code>&gt;</code> (multiline string) <code>|</code> (multiline string), <code>:</code> (key-value pair marker), <code>?</code> (mapping key), etc. If you use these characters as part of a CEL expression, you'll need to use quotes around the expression to escape it. For example:</p> Invalid templateValid template <pre><code>stack:\nname: ${{ 2 &gt; 1 ? \"yes\" : \"no\" }}-my-stack\n</code></pre> <p>See how the syntax highlighter is confused?</p> <pre><code>stack:\nname: '${{ 2 &gt; 1 ? \"yes\" : \"no\" }}-my-stack'\n</code></pre> <p>Results in:</p> <pre><code>stack:\nname: 'yes-my-stack'\n</code></pre>"},{"location":"concepts/blueprint.html#interaction-with-terraform-templatefile","title":"Interaction with Terraform <code>templatefile</code>","text":"<p>When using the Terraform <code>templatefile</code> function to generate a Blueprint template body, you can run into issues because the Blueprint template engine and <code>templatefile</code> both use <code>$</code> as template delimiters. This can result in error messages like:</p> <pre><code>\u2502 Error: Error in function call\n\u2502\n\u2502   on main.tf line 2, in output \"content\":\n\u2502    2:   value = templatefile(\"${path.module}/test.tftpl\", {\n\u2502    3:     SPACE = \"root\"\n\u2502    4:   })\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 path.module is \".\"\n\u2502\n\u2502 Call to function \"templatefile\" failed: ./test.tftpl:5,31-32: Missing key/value separator; Expected an equals\n\u2502 sign (\"=\") to mark the beginning of the attribute value.\n</code></pre> <p>To solve this you can use <code>$${}</code> to indicate that <code>templatefile</code> should not attempt to replace a certain piece of text.</p> <p>In this example, <code>$${{ inputs.stack_name }}</code> is escaped, whereas <code>${SPACE}</code> is not:</p> <pre><code>inputs:\n  - id: stack_name\n    name: Stack name\nstack:\n  name: $${{ inputs.stack_name }}\n  space: ${SPACE}\n  vcs:\n    branch: main\n    repository: my-repository\n    provider: GITHUB\n  vendor:\n    terraform:\n      manage_state: true\n      version: \"1.3.0\"\n</code></pre> <p>We can then use a call to <code>templatefile</code> to render this template:</p> <pre><code>templatefile(\"${path.module}/test.tftpl\", {\n  SPACE = \"root\"\n})\n</code></pre> <p>This results in the following output when the template is rendered:</p> <pre><code>inputs:\n  - id: stack_name\n    name: Stack name\nstack:\n  name: ${{ inputs.stack_name }}\n  space: root\n  vcs:\n    branch: main\n    repository: my-repository\n    provider: GITHUB\n  vendor:\n    terraform:\n      manage_state: true\n      version: \"1.3.0\"\n</code></pre>"},{"location":"concepts/blueprint.html#variables","title":"Variables","text":"<p>Since you probably don't want to create stacks with the exact same name and configuration, you'll use variables.</p>"},{"location":"concepts/blueprint.html#inputs","title":"Inputs","text":"<p>Inputs are defined in the <code>inputs</code> section of the template. You can use them in the template by prefixing them with <code>${{ inputs.</code> and suffixing them with <code>}}</code>. For example, <code>${{ inputs.environment }}</code> will be replaced with the value of the <code>environment</code> input. You can use these variables in CEL functions as well. For example, <code>trigger_run: ${{ inputs.environment == 'prod' }}</code> will be replaced with <code>trigger_run: true</code> or <code>trigger_run: false</code> depending on the value of the <code>environment</code> input. To ensure an input variable is always recognized as a string, you can enclose the value in quotes <code>\"${{ inputs.environment }}\"</code>.</p> <p>The input object has <code>id</code>, <code>name</code>, <code>description</code>, <code>type</code>, <code>default</code> and <code>options</code> fields. The mandatory fields are <code>id</code> and <code>name</code>.</p> <p>The <code>id</code> is used to refer to the input in the template. The <code>name</code> and the <code>description</code> are just helper fields for the user in the Stack creation tab. The <code>type</code> is the type of the input. The <code>default</code> is an optional default value of the input. The <code>options</code> is a list of options for the <code>select</code> input type.</p> <p>Example:</p> <pre><code>inputs:\n  - id: app_name\n    name: The name of the app\nstack:\n  name: ${{ inputs.app_name }}-my-stack\n</code></pre>"},{"location":"concepts/blueprint.html#input-types","title":"Input types","text":"<p>If the input <code>type</code> is not provided, it defaults to <code>short_text</code>. Other options are:</p> Type Description <code>short_text</code> A short text input. <code>long_text</code> A long text input. Typically used for multiline strings. <code>secret</code> A secret input. The value of the input will be masked in the UI. <code>number</code> An integer input. <code>boolean</code> A boolean input. <code>select</code> A multi option input. In case of <code>select</code>, it is mandatory to provide <code>options</code>. <code>float</code> A float input. <p>An example including all the types:</p> <pre><code>inputs:\n  - id: app_name\n    name: The name of the stack\n    # No type provided, defaults to short_text\n  - id: description\n    name: The description of the stack\n    type: long_text\n  - id: connstring\n    name: Connection string to the database\n    type: secret\n  - id: number_of_instances\n    name: The number of instances\n    type: number\n  - id: delete_protection\n    name: Is delete protection enabled?\n    type: boolean\n  - id: environment\n    name: The environment to deploy to\n    type: select\n    options:\n      - prod\n      - staging\n      - dev\n  - id: scale_factor\n    name: The scale factor of the app\n    type: float\n    # You can optionally provide a default value\n    default: 1.5\n</code></pre>"},{"location":"concepts/blueprint.html#maps","title":"Maps","text":"<p>Maps is an additional object which can be included in the template. The structure of maps is key-value pairs where the value is another map.</p> <p>Using maps you can preconfigure specific values in the template and allow users to set them in a determenistic way. Maps cannot be used in the <code>inputs</code> section, neither can <code>inputs</code> be used in the <code>maps</code> section.</p> <p>Example of using maps:</p> <pre><code>inputs:\n  - id: env\n    name: Env\n    # This type can also be a regular free text input (string)\n    type: select\n    options:\n      - prod\n      - dev\n\nmaps:\n  prod:\n   stack_name: prod-stack\n   descripiton: This is stack is in production\n   manage_state: true\n  dev:\n   stack_name: dev-stack\n   descripiton: This is a development stack\n   manage_state: false\n\nstacks:\n  - key: mystack\n    name: ${{ maps[inputs.env].stack_name }}\n    space: root\n    description: &gt;\n      ${{ maps[inputs.env].descripiton }}\n    vcs:\n      branch: master\n      repository: empty\n      provider: GITHUB\n    vendor:\n      terraform:\n        manage_state: ${{ maps[inputs.env].manage_state }}\n        version: \"1.3.0\"\n</code></pre>"},{"location":"concepts/blueprint.html#context","title":"Context","text":"<p>We also provide an input object called <code>context</code>. It contains the following properties:</p> Property Type Description <code>time</code> <code>google.protobuf.Timestamp</code> UTC time of the evaluation of the template. <code>random_string</code> <code>string</code> A random string of 6 characters (numbers and letters, no special characters). <code>random_number</code> <code>int</code> A random number between 0 and 1000000. <code>random_uuid</code> <code>string</code> A random UUID. <code>user.login</code> <code>string</code> The login of the person who triggered the blueprint creation; as provided by the SSO provider. <code>user.name</code> <code>string</code> The full name of the person who triggered the blueprint creation; as provided by the SSO provider. <code>user.account</code> <code>string</code> The account subdomain of the user who triggered the blueprint creation. <code>blueprint.name</code> <code>string</code> The name of the blueprint that was used to create the stack. <code>blueprint.space</code> <code>string</code> The space ID of the blueprint that was used to create the stack. <code>blueprint.created_at</code> <code>google.protobuf.Timestamp</code> The time when the blueprint was created. <code>blueprint.updated_at</code> <code>google.protobuf.Timestamp</code> The time when the blueprint was last updated. <code>blueprint.published_at</code> <code>google.protobuf.Timestamp</code> The time when the blueprint was published. <code>blueprint.labels</code> <code>list(string)</code> The labels of the blueprint. <p>Here is an example of using a few of them:</p> <pre><code>stack:\n  name: integration-tests-${{ inputs.app }}-${{ context.random_string }}\n  description: |\n    Temporary integration test stack for ${{ inputs.app }}. Deployed in ${{ context.time.getFullYear() }}.\n    The base blueprint was created at ${{ string(context.blueprint.created_at) }}.\n  labels:\n    - owner/${{ context.user.login }}\n    - blueprints/${{ context.blueprint.name }}\n  environment:\n    variables:\n      - name: DEPLOYMENT_ID\n        value: \"${{ context.random_uuid }}\"\n  schedules:\n    delete:\n      delete_resources: ${{ context.random_number % 2 == 0 }} # Russian roulette\n      timestamp_unix: ${{ int(context.time) + duration(\"30m\").getSeconds() }} # Delete the stack in 30 minutes\n</code></pre> <p>Results in:</p> <pre><code>stack:\n  name: integration-tests-my-app-vG3j3a\n  description: |\n    Temporary integration test stack for my-app. Deployed in 2023.\n    The base blueprint was created at 2020-01-01T10:00:20.021-05:00.\n  labels:\n    - owner/johndoe\n    - blueprints/my-blueprint\n  environment:\n    variables:\n      - name: DEPLOYMENT_ID\n        value: 6c9c4e3e-6b5d-4b3a-9c9c-4e3e6b5d4b3a\n  schedules:\n    delete:\n      delete_resources: true # Russian roulette\n      timestamp_unix: 1674139424 # Delete the stack in 30 minutes\n</code></pre> <p>Note that this is not a working example as it is missing a few things (<code>inputs</code> section, <code>vcs</code>, etc.), but it should give you an idea of what you can do.</p> <p>Tip</p> <p>What can you do with <code>google.protobuf.Timestamp</code> and <code>google.protobuf.Duration</code>? Check out the language definition, which contains all the methods and type conversions available.</p>"},{"location":"concepts/blueprint.html#stack-configuration","title":"Stack Configuration","text":"<p>Stacks can be configured in a similar fashion as if you were using the Terraform provider or the UI. Most of the options are straightforward; however, some require deeper knowledge and are covered in the sections below. For a full list of available options, please refer to the Schema section.</p>"},{"location":"concepts/blueprint.html#dependencies","title":"Dependencies","text":"<p>Dependencies follow the same rules and limitations as described in the Stack dependencies section. You cannot create dependency cycles, nor can you make a stack depend on itself.</p> <p>Dependencies can be configured using the:</p> <ul> <li><code>depends_on</code> field in the stack configuration.</li> <li><code>stack_dependency_references</code> field in the environment configuration.</li> </ul> <p>You can also mix the two approaches and define both fields in the same stack. If you define the same dependency in both <code>depends_on</code> and <code>stack_dependency_references</code>, the latter will take precedence.</p> <p>Also note that dependencies are defined on the stack <code>key</code> field and not the <code>name</code>. The value of this field is not unique and can be reused in multiple stacks; however, it is recommended to use unique keys for each stack when creating dependencies.</p>"},{"location":"concepts/blueprint.html#example-1","title":"Example 1","text":"<p>Here is an example of a blueprint with multiple stacks using the <code>stack_dependency_references</code> field:</p> <pre><code>inputs:\n  - id: stack_name\n    name: The name of the stack\n\nstacks:\n  - name: ${{ inputs.stack_name }}-1\n    key: stack1\n    space: root\n    environment:\n      stack_dependency_references:\n        - name: TF_VAR_stack3_connection_string\n          from_stack: stack3\n          output: connection_string\n        - name: TF_VAR_stack3_additional_setting\n          from_stack: stack3\n          output: additional_setting\n        - name: TF_VAR_stack2_connection_string\n          from_stack: stack2\n          output: connection_string\n    vcs:\n      branch: master\n      repository: empty\n      provider: GITHUB\n    vendor:\n      terraform:\n        manage_state: true\n        version: \"1.3.0\"\n\n  - name: ${{ inputs.stack_name }}-2\n    key: stack2\n    space: root\n    vcs:\n      branch: master\n      repository: empty\n      provider: GITHUB\n    vendor:\n      terraform:\n        manage_state: true\n        version: \"1.3.0\"\n\n  - name: ${{ inputs.stack_name }}-3\n    key: stack3\n    space: root\n    vcs:\n      branch: master\n      repository: empty\n      provider: GITHUB\n    vendor:\n      terraform:\n        manage_state: true\n        version: \"1.3.0\"\n</code></pre>"},{"location":"concepts/blueprint.html#example-2","title":"Example 2","text":"<p>Here is an example of a blueprint with multiple stacks using the <code>depends_on</code> field:</p> <pre><code>inputs:\n  - id: stack_name\n    name: The name of the stack\n\nstacks:\n  - name: ${{ inputs.stack_name }}-1\n    key: stack1\n    space: root\n    vcs:\n      branch: master\n      repository: empty\n      provider: GITHUB\n    vendor:\n      terraform:\n        manage_state: true\n        version: \"1.3.0\"\n    depends_on:\n      - stack2\n      - stack3\n\n  - name: ${{ inputs.stack_name }}-2\n    key: stack2\n    space: root\n    vcs:\n      branch: master\n      repository: empty\n      provider: GITHUB\n    vendor:\n      terraform:\n        manage_state: true\n        version: \"1.3.0\"\n    depends_on:\n      - stack3\n\n  - name: ${{ inputs.stack_name }}-3\n    key: stack3\n    space: root\n    vcs:\n      branch: master\n      repository: empty\n      provider: GITHUB\n    vendor:\n      terraform:\n        manage_state: true\n        version: \"1.3.0\"\n</code></pre>"},{"location":"concepts/blueprint.html#validation","title":"Validation","text":""},{"location":"concepts/blueprint.html#blueprint-validation","title":"Blueprint Validation","text":"<p>We do not validate drafted blueprints, so you can do whatever you want with them. However, if you publish your blueprint, we'll make sure it includes the required fields and you'll get an error if it doesn't.</p> <p>One caveat: we cannot validate fields that have variables because we don't know the value of the variable. On the other hand, if you try to create a stack from the blueprint and supply the inputs to the template, we'll be able to do the full validation. Let's say:</p> <pre><code>inputs:\n  - id: timestamp\n    name: Delete timestamp of the stack\n    type: number\nstack:\n  schedules:\n    delete:\n      timestamp_unix: ${{ inputs.timestamp }}\n</code></pre> <p>We cannot ensure that the input variable is indeed a proper 10-digit epoch timestamp; we will only find out once you supply the actual input.</p>"},{"location":"concepts/blueprint.html#input-validation","title":"Input Validation","text":"<p>You can add validations to your input fields to ensure users provide valid data. The available validations depend on the input type:</p>"},{"location":"concepts/blueprint.html#string-validation","title":"String Validation","text":"<pre><code>inputs:\n  - id: username\n    name: Username\n    type: short_text\n    validations:\n      required: true\n      min_length: 3\n      max_length: 20\n      pattern: \"^[a-zA-Z0-9_]+$\"\n</code></pre> <p>Available string validations:</p> <ul> <li><code>required</code>: Boolean indicating if the field must be filled.</li> <li><code>min_length</code>: Minimum number of characters.</li> <li><code>max_length</code>: Maximum number of characters.</li> <li><code>length_equal</code>: Exact number of characters required.</li> <li><code>pattern</code>: Regular expression pattern the input must match.</li> </ul>"},{"location":"concepts/blueprint.html#number-validation","title":"Number Validation","text":"<pre><code>inputs:\n  - id: age\n    name: Age\n    type: number\n    validations:\n      required: true\n      greater_than: 0\n      less_than_or_equal: 120\n</code></pre> <p>Available number validations:</p> <ul> <li><code>required</code>: Boolean indicating if the field must be filled.</li> <li><code>greater_than</code>: Value must be greater than this number.</li> <li><code>greater_than_or_equal</code>: Value must be greater than or equal to this number.</li> <li><code>less_than</code>: Value must be less than this number.</li> <li><code>less_than_or_equal</code>: Value must be less than or equal to this number.</li> <li><code>not_equal</code>: Value must not equal this number.</li> <li><code>step</code>: For integers, specifies the increment (e.g., <code>step: 2</code> allows only even numbers).</li> </ul>"},{"location":"concepts/blueprint.html#schema","title":"Schema","text":"<p>The up-to-date schema of a Blueprint is available through a GraphQL query for authenticated users:</p> <pre><code>{\n  blueprintSchema\n}\n</code></pre> <p>Tip</p> <p>Remember that there are multiple ways to interact with Spacelift. You can use the GraphQL API, the CLI, the Terraform Provider, or the web UI.</p> <p>For simplicity, here is the current schema, but it might change in the future:</p> Click to expand <pre><code>{\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"title\": \"Blueprint\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"inputs\": {\n            \"$ref\": \"#/definitions/inputs\"\n        },\n        \"options\": {\n            \"$ref\": \"#/definitions/options\"\n        },\n        \"maps\": {\n            \"$ref\": \"#/definitions/maps\"\n        },\n        \"stack\": {\n            \"$ref\": \"#/definitions/stack\"\n        },\n        \"stacks\": {\n            \"type\": \"array\",\n            \"maxItems\": 5,\n            \"items\": {\n                \"$ref\": \"#/definitions/stackWithKey\"\n            }\n        }\n    },\n    \"additionalProperties\": false,\n    \"allOf\": [\n        {\n            \"oneOf\": [\n                {\n                    \"required\": [\n                        \"stack\"\n                    ]\n                },\n                {\n                    \"required\": [\n                        \"stacks\"\n                    ]\n                }\n            ]\n        },\n        {\n            \"if\": {\n                \"required\": [\n                    \"stack\"\n                ]\n            },\n            \"then\": {\n                \"not\": {\n                    \"required\": [\n                        \"stacks\"\n                    ]\n                }\n            }\n        },\n        {\n            \"if\": {\n                \"required\": [\n                    \"stacks\"\n                ]\n            },\n            \"then\": {\n                \"not\": {\n                    \"required\": [\n                        \"stack\"\n                    ]\n                }\n            }\n        }\n    ],\n    \"definitions\": {\n        \"maps\": {\n            \"type\": \"object\",\n            \"additionalProperties\": {\n                \"type\": \"object\",\n                \"additionalProperties\": {\n                    \"oneOf\": [\n                        {\n                            \"type\": \"string\"\n                        },\n                        {\n                            \"type\": \"number\"\n                        },\n                        {\n                            \"type\": \"boolean\"\n                        }\n                    ]\n                }\n            }\n        },\n        \"inputs\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"$ref\": \"#/definitions/input\"\n            }\n        },\n        \"input\": {\n            \"type\": \"object\",\n            \"oneOf\": [\n                {\n                    \"additionalProperties\": false,\n                    \"required\": [\n                        \"id\",\n                        \"name\"\n                    ],\n                    \"properties\": {\n                        \"id\": {\n                            \"type\": \"string\"\n                        },\n                        \"name\": {\n                            \"type\": \"string\"\n                        },\n                        \"description\": {\n                            \"type\": \"string\"\n                        },\n                        \"default\": {\n                            \"oneOf\": [\n                                {\n                                    \"type\": \"string\"\n                                },\n                                {\n                                    \"type\": \"number\"\n                                },\n                                {\n                                    \"type\": \"boolean\"\n                                }\n                            ]\n                        },\n                        \"validations\": {\n                            \"$ref\": \"#/definitions/string_validations\"\n                        },\n                        \"type\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"short_text\",\n                                \"long_text\",\n                                \"secret\"\n                            ]\n                        }\n                    }\n                },\n                {\n                    \"additionalProperties\": false,\n                    \"required\": [\n                        \"id\",\n                        \"name\",\n                        \"type\"\n                    ],\n                    \"properties\": {\n                        \"id\": {\n                            \"type\": \"string\"\n                        },\n                        \"name\": {\n                            \"type\": \"string\"\n                        },\n                        \"description\": {\n                            \"type\": \"string\"\n                        },\n                        \"default\": {\n                            \"oneOf\": [\n                                {\n                                    \"type\": \"string\"\n                                },\n                                {\n                                    \"type\": \"number\"\n                                },\n                                {\n                                    \"type\": \"boolean\"\n                                }\n                            ]\n                        },\n                        \"validations\": {\n                            \"$ref\": \"#/definitions/number_validations\"\n                        },\n                        \"type\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"number\",\n                                \"float\"\n                            ]\n                        }\n                    }\n                },\n                {\n                    \"additionalProperties\": false,\n                    \"required\": [\n                        \"id\",\n                        \"name\",\n                        \"type\"\n                    ],\n                    \"properties\": {\n                        \"id\": {\n                            \"type\": \"string\"\n                        },\n                        \"name\": {\n                            \"type\": \"string\"\n                        },\n                        \"description\": {\n                            \"type\": \"string\"\n                        },\n                        \"default\": {\n                            \"oneOf\": [\n                                {\n                                    \"type\": \"string\"\n                                },\n                                {\n                                    \"type\": \"number\"\n                                },\n                                {\n                                    \"type\": \"boolean\"\n                                }\n                            ]\n                        },\n                        \"type\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"boolean\"\n                            ]\n                        }\n                    }\n                },\n                {\n                    \"additionalProperties\": false,\n                    \"required\": [\n                        \"id\",\n                        \"name\",\n                        \"type\",\n                        \"options\"\n                    ],\n                    \"properties\": {\n                        \"id\": {\n                            \"type\": \"string\"\n                        },\n                        \"name\": {\n                            \"type\": \"string\"\n                        },\n                        \"description\": {\n                            \"type\": \"string\"\n                        },\n                        \"default\": {\n                            \"oneOf\": [\n                                {\n                                    \"type\": \"string\"\n                                },\n                                {\n                                    \"type\": \"number\"\n                                },\n                                {\n                                    \"type\": \"boolean\"\n                                }\n                            ]\n                        },\n                        \"type\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"select\"\n                            ]\n                        },\n                        \"options\": {\n                            \"type\": \"array\",\n                            \"minItems\": 1,\n                            \"items\": {\n                                \"type\": \"string\"\n                            }\n                        }\n                    }\n                }\n            ]\n        },\n        \"stack\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"name\",\n                \"space\",\n                \"vcs\",\n                \"vendor\"\n            ],\n            \"properties\": {\n                \"name\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                },\n                \"description\": {\n                    \"type\": \"string\"\n                },\n                \"labels\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\"\n                    }\n                },\n                \"administrative\": {\n                    \"type\": \"boolean\"\n                },\n                \"allow_promotion\": {\n                    \"type\": \"boolean\"\n                },\n                \"auto_deploy\": {\n                    \"type\": \"boolean\"\n                },\n                \"auto_retry\": {\n                    \"type\": \"boolean\"\n                },\n                \"is_disabled\": {\n                    \"type\": \"boolean\"\n                },\n                \"local_preview_enabled\": {\n                    \"type\": \"boolean\"\n                },\n                \"protect_from_deletion\": {\n                    \"type\": \"boolean\"\n                },\n                \"runner_image\": {\n                    \"type\": \"string\"\n                },\n                \"secret_masking_enabled\": {\n                    \"type\": \"boolean\"\n                },\n                \"space\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                },\n                \"worker_pool\": {\n                    \"type\": \"string\"\n                },\n                \"attachments\": {\n                    \"$ref\": \"#/definitions/attachment\"\n                },\n                \"environment\": {\n                    \"type\": \"object\",\n                    \"additionalProperties\": false,\n                    \"properties\": {\n                        \"mounted_files\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                                \"$ref\": \"#/definitions/mounted_file\"\n                            }\n                        },\n                        \"variables\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                                \"$ref\": \"#/definitions/variable\"\n                            }\n                        },\n                        \"stack_dependency_references\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                                \"$ref\": \"#/definitions/dependency_reference\"\n                            }\n                        }\n                    }\n                },\n                \"hooks\": {\n                    \"type\": \"object\",\n                    \"additionalProperties\": false,\n                    \"properties\": {\n                        \"apply\": {\n                            \"$ref\": \"#/definitions/before_after_hook\"\n                        },\n                        \"init\": {\n                            \"$ref\": \"#/definitions/before_after_hook\"\n                        },\n                        \"plan\": {\n                            \"$ref\": \"#/definitions/before_after_hook\"\n                        },\n                        \"perform\": {\n                            \"$ref\": \"#/definitions/before_after_hook\"\n                        },\n                        \"destroy\": {\n                            \"$ref\": \"#/definitions/before_after_hook\"\n                        },\n                        \"run\": {\n                            \"$ref\": \"#/definitions/after_hook\"\n                        }\n                    }\n                },\n                \"schedules\": {\n                    \"type\": \"object\",\n                    \"additionalProperties\": false,\n                    \"properties\": {\n                        \"drift\": {\n                            \"$ref\": \"#/definitions/drift_detection_schedule\"\n                        },\n                        \"tasks\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                                \"$ref\": \"#/definitions/task_schedule\"\n                            }\n                        },\n                        \"delete\": {\n                            \"$ref\": \"#/definitions/delete_schedule\"\n                        }\n                    }\n                },\n                \"vcs\": {\n                    \"type\": \"object\",\n                    \"oneOf\": [\n                        {\n                            \"additionalProperties\": false,\n                            \"required\": [\n                                \"branch\",\n                                \"provider\",\n                                \"repository\"\n                            ],\n                            \"properties\": {\n                                \"branch\": {\n                                    \"type\": \"string\",\n                                    \"minLength\": 1\n                                },\n                                \"project_root\": {\n                                    \"type\": \"string\"\n                                },\n                                \"project_globs\": {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                        \"type\": \"string\"\n                                    }\n                                },\n                                \"provider\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                        \"GITHUB\",\n                                        \"GITLAB\",\n                                        \"BITBUCKET_DATACENTER\",\n                                        \"BITBUCKET_CLOUD\",\n                                        \"GITHUB_ENTERPRISE\",\n                                        \"SHOWCASE\",\n                                        \"AZURE_DEVOPS\"\n                                    ]\n                                },\n                                \"id\": {\n                                    \"type\": \"string\",\n                                    \"description\": \"The id of the VCS provider.\"\n                                },\n                                \"namespace\": {\n                                    \"type\": \"string\"\n                                },\n                                \"repository\": {\n                                    \"type\": \"string\",\n                                    \"minLength\": 1,\n                                    \"description\": \"The name of the repository.\"\n                                }\n                            }\n                        },\n                        {\n                            \"additionalProperties\": false,\n                            \"required\": [\n                                \"branch\",\n                                \"provider\",\n                                \"repository_url\"\n                            ],\n                            \"properties\": {\n                                \"branch\": {\n                                    \"type\": \"string\",\n                                    \"minLength\": 1\n                                },\n                                \"project_root\": {\n                                    \"type\": \"string\"\n                                },\n                                \"project_globs\": {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                        \"type\": \"string\"\n                                    }\n                                },\n                                \"provider\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                        \"RAW_GIT\"\n                                    ]\n                                },\n                                \"repository\": {\n                                    \"type\": \"string\",\n                                    \"description\": \"The name of the repository. If not provided, it'll be extracted from the repository_url.\"\n                                },\n                                \"namespace\": {\n                                    \"type\": \"string\",\n                                    \"description\": \"The namespace of the repository. If not provided, it'll be extracted from the repository_url.\"\n                                },\n                                \"repository_url\": {\n                                    \"type\": \"string\",\n                                    \"minLength\": 1,\n                                    \"description\": \"The URL of the repository. This is only used for the 'GIT' provider.\"\n                                }\n                            }\n                        }\n                    ]\n                },\n                \"vendor\": {\n                    \"type\": \"object\",\n                    \"additionalProperties\": false,\n                    \"properties\": {\n                        \"ansible\": {\n                            \"$ref\": \"#/definitions/ansible_vendor\"\n                        },\n                        \"cloudformation\": {\n                            \"$ref\": \"#/definitions/cloudformation_vendor\"\n                        },\n                        \"kubernetes\": {\n                            \"$ref\": \"#/definitions/kubernetes_vendor\"\n                        },\n                        \"pulumi\": {\n                            \"$ref\": \"#/definitions/pulumi_vendor\"\n                        },\n                        \"terraform\": {\n                            \"$ref\": \"#/definitions/terraform_vendor\"\n                        },\n                        \"terragrunt\": {\n                            \"$ref\": \"#/definitions/terragrunt_vendor\"\n                        }\n                    }\n                },\n                \"options\": {\n                    \"$ref\": \"#/definitions/options\"\n                },\n                \"depends_on\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\",\n                        \"minLength\": 1\n                    }\n                },\n                \"key\": {\n                    \"type\": \"string\"\n                }\n            }\n        },\n        \"stackWithKey\": {\n            \"allOf\": [\n                {\n                    \"$ref\": \"#/definitions/stack\"\n                },\n                {\n                    \"type\": \"object\",\n                    \"required\": [\n                        \"key\"\n                    ]\n                }\n            ]\n        },\n        \"attachment\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"properties\": {\n                \"contexts\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"$ref\": \"#/definitions/context\"\n                    }\n                },\n                \"clouds\": {\n                    \"type\": \"object\",\n                    \"additionalProperties\": false,\n                    \"properties\": {\n                        \"aws\": {\n                            \"$ref\": \"#/definitions/aws_attachment\"\n                        },\n                        \"azure\": {\n                            \"$ref\": \"#/definitions/azure_attachment\"\n                        }\n                    }\n                },\n                \"policies\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\"\n                    }\n                }\n            }\n        },\n        \"aws_attachment\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"id\",\n                \"read\",\n                \"write\"\n            ],\n            \"properties\": {\n                \"id\": {\n                    \"type\": \"string\"\n                },\n                \"read\": {\n                    \"type\": \"boolean\"\n                },\n                \"write\": {\n                    \"type\": \"boolean\"\n                }\n            }\n        },\n        \"azure_attachment\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"id\",\n                \"read\",\n                \"write\",\n                \"subscription_id\"\n            ],\n            \"properties\": {\n                \"id\": {\n                    \"type\": \"string\"\n                },\n                \"read\": {\n                    \"type\": \"boolean\"\n                },\n                \"write\": {\n                    \"type\": \"boolean\"\n                },\n                \"subscription_id\": {\n                    \"type\": \"string\"\n                }\n            }\n        },\n        \"context\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"id\"\n            ],\n            \"properties\": {\n                \"id\": {\n                    \"type\": \"string\"\n                },\n                \"priority\": {\n                    \"type\": \"integer\",\n                    \"minimum\": 0\n                }\n            }\n        },\n        \"mounted_file\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"path\",\n                \"content\"\n            ],\n            \"properties\": {\n                \"path\": {\n                    \"type\": \"string\"\n                },\n                \"content\": {\n                    \"type\": \"string\"\n                },\n                \"description\": {\n                    \"type\": \"string\"\n                },\n                \"secret\": {\n                    \"type\": \"boolean\"\n                }\n            }\n        },\n        \"dependency_reference\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"name\",\n                \"from_stack\",\n                \"output\"\n            ],\n            \"properties\": {\n                \"name\": {\n                    \"type\": \"string\"\n                },\n                \"from_stack\": {\n                    \"type\": \"string\"\n                },\n                \"output\": {\n                    \"type\": \"string\"\n                },\n                \"trigger_always\": {\n                    \"type\": \"boolean\"\n                }\n            }\n        },\n        \"variable\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"name\",\n                \"value\"\n            ],\n            \"properties\": {\n                \"name\": {\n                    \"type\": \"string\"\n                },\n                \"value\": {\n                    \"type\": \"string\"\n                },\n                \"description\": {\n                    \"type\": \"string\"\n                },\n                \"secret\": {\n                    \"type\": \"boolean\"\n                }\n            }\n        },\n        \"after_hook\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"properties\": {\n                \"after\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\"\n                    },\n                    \"minLength\": 1\n                }\n            }\n        },\n        \"before_after_hook\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"properties\": {\n                \"before\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\"\n                    },\n                    \"minLength\": 1\n                },\n                \"after\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\"\n                    },\n                    \"minLength\": 1\n                }\n            }\n        },\n        \"drift_detection_schedule\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"cron\",\n                \"reconcile\"\n            ],\n            \"properties\": {\n                \"cron\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"$ref\": \"#/definitions/cron_schedule\",\n                        \"maxLength\": 1\n                    }\n                },\n                \"reconcile\": {\n                    \"type\": \"boolean\"\n                },\n                \"ignore_state\": {\n                    \"type\": \"boolean\"\n                },\n                \"timezone\": {\n                    \"type\": \"string\"\n                }\n            }\n        },\n        \"task_schedule\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"command\"\n            ],\n            \"oneOf\": [\n                {\n                    \"required\": [\n                        \"command\",\n                        \"cron\"\n                    ]\n                },\n                {\n                    \"required\": [\n                        \"command\",\n                        \"timestamp_unix\"\n                    ]\n                }\n            ],\n            \"properties\": {\n                \"command\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                },\n                \"cron\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"$ref\": \"#/definitions/cron_schedule\",\n                        \"minLength\": 1\n                    }\n                },\n                \"timestamp_unix\": {\n                    \"type\": \"number\",\n                    \"minimum\": 1600000000\n                },\n                \"timezone\": {\n                    \"type\": \"string\"\n                }\n            }\n        },\n        \"cron_schedule\": {\n            \"type\": \"string\",\n            \"pattern\": \"^(\\\\*|\\\\d+|\\\\d+-\\\\d+|\\\\d+\\\\/\\\\d+)(\\\\s+(\\\\*|\\\\d+|\\\\d+-\\\\d+|\\\\d+\\\\/\\\\d+)){4}$\"\n        },\n        \"delete_schedule\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"timestamp_unix\"\n            ],\n            \"properties\": {\n                \"delete_resources\": {\n                    \"type\": \"boolean\"\n                },\n                \"timestamp_unix\": {\n                    \"type\": \"number\",\n                    \"minimum\": 1600000000\n                }\n            }\n        },\n        \"ansible_vendor\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"playbook\"\n            ],\n            \"properties\": {\n                \"playbook\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                }\n            }\n        },\n        \"cloudformation_vendor\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"entry_template_file\",\n                \"template_bucket\",\n                \"stack_name\",\n                \"region\"\n            ],\n            \"properties\": {\n                \"entry_template_file\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                },\n                \"template_bucket\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                },\n                \"stack_name\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                },\n                \"region\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                }\n            }\n        },\n        \"kubernetes_vendor\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"namespace\"\n            ],\n            \"properties\": {\n                \"namespace\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                }\n            }\n        },\n        \"pulumi_vendor\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"stack_name\",\n                \"login_url\"\n            ],\n            \"properties\": {\n                \"stack_name\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                },\n                \"login_url\": {\n                    \"type\": \"string\",\n                    \"minLength\": 1\n                }\n            }\n        },\n        \"terraform_vendor\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"required\": [\n                \"manage_state\"\n            ],\n            \"properties\": {\n                \"version\": {\n                    \"type\": \"string\"\n                },\n                \"workspace\": {\n                    \"type\": \"string\"\n                },\n                \"use_smart_sanitization\": {\n                    \"type\": \"boolean\"\n                },\n                \"manage_state\": {\n                    \"type\": \"boolean\"\n                },\n                \"workflow_tool\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                        \"TERRAFORM_FOSS\",\n                        \"CUSTOM\",\n                        \"OPEN_TOFU\"\n                    ]\n                }\n            }\n        },\n        \"terragrunt_vendor\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"properties\": {\n                \"terraform_version\": {\n                    \"type\": \"string\"\n                },\n                \"terragrunt_version\": {\n                    \"type\": \"string\"\n                },\n                \"use_run_all\": {\n                    \"type\": \"boolean\"\n                },\n                \"use_smart_sanitization\": {\n                    \"type\": \"boolean\"\n                },\n                \"terragrunt_tool\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                        \"TERRAFORM_FOSS\",\n                        \"OPEN_TOFU\",\n                        \"MANUALLY_PROVISIONED\"\n                    ]\n                }\n            }\n        },\n        \"string_validations\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"properties\": {\n                \"required\": {\n                    \"type\": \"boolean\"\n                },\n                \"min_length\": {\n                    \"type\": \"integer\",\n                    \"minimum\": 0\n                },\n                \"max_length\": {\n                    \"type\": \"integer\",\n                    \"minimum\": 0\n                },\n                \"length_equal\": {\n                    \"type\": \"integer\",\n                    \"minimum\": 0\n                },\n                \"pattern\": {\n                    \"type\": \"string\"\n                }\n            }\n        },\n        \"number_validations\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"properties\": {\n                \"required\": {\n                    \"type\": \"boolean\"\n                },\n                \"greater_than\": {\n                    \"type\": \"number\"\n                },\n                \"greater_than_or_equal\": {\n                    \"type\": \"number\"\n                },\n                \"less_than\": {\n                    \"type\": \"number\"\n                },\n                \"less_than_or_equal\": {\n                    \"type\": \"number\"\n                },\n                \"not_equal\": {\n                    \"type\": \"number\"\n                },\n                \"step\": {\n                    \"type\": \"integer\"\n                }\n            }\n        },\n        \"options\": {\n            \"type\": \"object\",\n            \"additionalProperties\": false,\n            \"properties\": {\n                \"trigger_run\": {\n                    \"type\": \"boolean\"\n                },\n                \"do_not_create\": {\n                    \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"concepts/configuration.html","title":"Configuration","text":""},{"location":"concepts/configuration.html#configuration","title":"Configuration","text":"<p>While Spacelift stacks typically link source code with infrastructure resources, it is often the broadly defined configuration that serves as the glue that's keeping everything together. These can be that access credentials, backend definitions or user-defined variables affecting the behavior of resource definitions found in the \"raw\" source code.</p> <p>This section focuses on three aspects of configuration, each of which warrants its own help article:</p> <ul> <li>Direct stack environment, that is environment variables and mounted files;</li> <li>Contexts, that is environments (often partially defined) shared between stacks and/or OpenTofu/Terraform modules;</li> <li>Runtime configuration as defined in the <code>.spacelift/config.yml</code> file;</li> </ul>"},{"location":"concepts/configuration.html#a-general-note-on-precedence","title":"A general note on precedence","text":"<p>Some configuration settings can be defined on multiple levels. If they're over-defined (the same setting is defined multiple times), the end result will depend on generic rules of precedence. These rules will be the same for all applicable settings:</p> <ul> <li>stack-specific runtime configuration will take the highest precedence;</li> <li>common runtime configuration will go second;</li> <li>configuration defined directly (either through the environment, or settings) on the stack will go next;</li> <li>anything defined at the context level will take the lowest precedence - furthermore, contexts can be attached with a priority level further defining the exact precedence;</li> </ul>"},{"location":"concepts/configuration/context.html","title":"Context","text":""},{"location":"concepts/configuration/context.html#context","title":"Context","text":"<p>Note</p> <p>For the sake of brevity we'll be using the term projects to refer to both stacks and modules.</p>"},{"location":"concepts/configuration/context.html#introduction","title":"Introduction","text":"<p>On a high level, context is a bundle of configuration elements (environment variables and mounted files) independent of any stack that can be managed separately and attached to as many or as few stacks as necessary. Contexts are only directly accessible to administrators from the account view:</p> <p></p> <p>The list of contexts merely shows the name and description of the context. Clicking on the name allows you to edit it.</p>"},{"location":"concepts/configuration/context.html#management","title":"Management","text":"<p>Managing a context is quite straightforward - you can create, edit, attach, detach and delete it. The below paragraphs will focus on doing that through the web GUI but doing it programmatically using our Terraform provider is an attractive alternative.</p>"},{"location":"concepts/configuration/context.html#creating","title":"Creating","text":"<p>As an account administrator you can create a new context from the Contexts screen as seen on the above screenshot by pressing the Add context button:</p> <p></p> <p>This takes you to a simple form where the only inputs are name and description:</p> <p></p> <p>The required name is what you'll see in the context list and in the dropdown when attaching the context. Make sure that it's informative enough to be able to immediately communicate the purpose of the context, but short enough so that it fits nicely in the dropdown, and no important information is cut off.</p> <p>The optional description is completely free-form and it supports Markdown. This is a good place perhaps for a thorough explanation of the purpose of the stack, perhaps a link or two, and/or a funny GIF. In the web GUI this description will only show on the Contexts screen so it's not a big deal anyway.</p> <p>Warning</p> <p>Based on the original name, Spacelift generates an immutable slug that serves as a unique identifier of this context. If the name and the slug diverge significantly, things may become confusing.</p> <p>So even though you can change the context name at any point, we strongly discourage all non-trivial changes.</p>"},{"location":"concepts/configuration/context.html#editing","title":"Editing","text":"<p>Editing the context is only a little more exciting. You can edit the context from its dedicated view by pressing the Edit button:</p> <p></p> <p>This switches the context into editing mode where you can change the name and description but also manage configuration elements the same way you'd do for the stack environment, only much simpler - without overrides and computed values:</p> <p></p>"},{"location":"concepts/configuration/context.html#attaching-and-detaching","title":"Attaching and detaching","text":"<p>Attaching and detaching contexts actually happens from the stack management view. To attach a context, select the Contexts tab. This should show you a dropdown with all the contexts available for attaching, and a slider to set the priority of the attachment:</p> <p></p> <p>Info</p> <p>A context can only be attached once to a given stack, so if it's already attached, it will not be visible in the dropdown menu.</p> <p>OK, let's attach the context with priority 0 and see what gives:</p> <p></p> <p>Now this attached context will also contribute to the stack environment...</p> <p></p> <p>...and be visible on the list of attached contexts:</p> <p></p> <p>In order to detach the context, you can just press the Detach button and the context will stop contributing to the stack's environment:</p> <p></p>"},{"location":"concepts/configuration/context.html#a-note-on-priority","title":"A note on priority","text":"<p>You may be wondering what the priority slider is for. A priority is a property of context-stack relationship - in fact, the only property. All the contexts attached to a stack are sorted by priority (lowest first), though values don't need to be unique. This ordering establishes precedence rules between contexts should there be a conflict and multiple contexts define the same value.</p> <p>You might notice that there is no priority picker for auto-attached contexts. The highest priority for all configuration elements (environment variables, mounted files and hooks) are at the stack level, then explicitly attached contexts (based on set priorities), then auto-attached alphabetically for environment variables, mounted files, and after phase hooks (before phase hooks are attached reverse alphabetically with more details about that here as well).</p>"},{"location":"concepts/configuration/context.html#deleting","title":"Deleting","text":"<p>Deleting a context is straightforward - by pressing the Delete button in the context view you can get rid of an unnecessary context:</p> <p></p> <p>Warning</p> <p>Deleting a context will also automatically detach it from all the projects it was attached to. Make sure you only delete contexts that are no longer useful. For security purposes we do not store historical stuff and actually remove the deleted data from all of our data storage systems.</p>"},{"location":"concepts/configuration/context.html#use-cases","title":"Use cases","text":"<p>We can see two main use cases for contexts, depending on whether the context data is supplied externally or produced by Spacelift.</p>"},{"location":"concepts/configuration/context.html#shared-setup","title":"Shared setup","text":"<p>If the data is external to Spacelift, it's likely that this is a form of shared setup - that is, configuration elements that are common to multiple stacks, and grouped as a context for convenience. One example of this use case is cloud provider configuration, either for OpenTofu/Terraform or Pulumi. Instead of attaching the same values - some of them probably secret and pretty sensitive - to individual stacks, contexts allow you to define those once and then have admins attach them to the stacks that need them.</p> <p>A variation of this use case is collections of OpenTofu/Terraform input variables that may be shared by multiple stacks - for example things relating to a particular system environment (staging, production etc). In this case the collection of variables can specify things like environment name, DNS domain name or a reference to it (eg. zone ID), tags, references to provider accounts and similar settings. Again, instead of setting these on individual stacks, an admin can group them into a context, and attach to the eligible stacks.</p>"},{"location":"concepts/configuration/context.html#remote-state-alternative-terraform-specific","title":"Remote state alternative (Terraform-specific)","text":"<p>If the data in the context is produced by one or more Spacelift stacks, contexts can be an attractive alternative to the Terraform remote state. In this use case, contexts can serve as outputs for stacks that can be consumed by (attached to) other stacks. So, instead of exposing the entire state, a stack can use Spacelift Terraform provider to define values on a context - either managed by the same stack , or managed externally. Managing a context externally can be particularly useful when multiple stacks contribute to a particular context.</p> <p>Info</p> <p>In order to use the Terraform provider to define contexts or its configuration elements the stack has to be marked as administrative.</p> <p>As an example of one such use case, let's imagine an organization where shared infrastructure (VPC, DNS, compute cluster etc.) is centrally managed by a DevOps team, which exposes it as a service to be used by individual product development teams. In order to be able to use the shared infrastructure, each team needs to address multiple entities that are generated by the central infra repo. In vanilla Terraform one would likely use remote state provider, but that might expose secrets and settings the DevOps team would rather keep it to themselves. Using a context on the other hand allows the team to decide (and hopefully document) what constitutes their \"external API\".</p> <p>The proposed setup for the above use case would involve two administrative stacks - one to manage all the stacks, and the other for the DevOps team. The management stack would programmatically define the DevOps one, and possibly also its context. The DevOps team would receive the context ID as an input variable, and use it to expose outputs as <code>spacelift_environment_variable</code> and/or <code>spacelift_mounted_file</code> resources. The management stack could then simply attach the context populated by the DevOps stack to other stacks it defines and manages.</p>"},{"location":"concepts/configuration/context.html#extending-terraform-cli-configuration-terraform-specific","title":"Extending Terraform CLI Configuration (Terraform-specific)","text":"<p>For some of our Terraform users, a convenient way to configure the Terraform CLI behavior is through customizing the <code>~/.terraformrc</code> file.</p> <p>Spacelift allows you to extend terraform CLI configuration through the use of mounted files.</p>"},{"location":"concepts/configuration/environment.html","title":"Environment","text":""},{"location":"concepts/configuration/environment.html#environment","title":"Environment","text":"<p>If you take a look at the Environment screen of a stack you will notice it's pretty busy - in fact it's the second busiest view in Spacelift (run being the undisputed winner). Ultimately though, all the records here are either environment variables or mounted files. The main part of the view represents the synthetic outcome determining what your run will \"see\" when executed. If this does not make sense yet, please hang on and read the remainder of this article.</p> <p></p>"},{"location":"concepts/configuration/environment.html#environment-variables","title":"Environment variables","text":"<p>The concept of environment variables is instinctively understood by all programmers. It's represented as a key-value mapping available to all processes running in a given environment. Both with Pulumi and OpenTofu/Terraform, environment variables are frequently used to configure providers. Additionally, when prefixed with <code>TF_VAR_</code> they are used in OpenTofu/Terraform to use environment variables as OpenTofu/Terraform input variables.</p> <p>Info</p> <p>Spacelift does not provide a dedicated mechanism of defining OpenTofu/Terraform input variables because the combination of <code>TF_VAR_</code> environment variables and mounted files should cover all use cases without the need to introduce an extra entity.</p> <p>Adding an environment variable is rather straightforward - don't worry yet about the visibility (difference between plain and secret variables). This is described in a separate section:</p> <p></p> <p>...and so is editing:</p> <p></p> <p></p>"},{"location":"concepts/configuration/environment.html#environment-variable-interpolation","title":"Environment variable interpolation","text":"<p>Note that environment variables can refer to other environment variables using simple interpolation. For example, if you have an environment variable <code>FOO</code> with a value of <code>bar</code> you can use it to define another environment variable <code>BAZ</code> as <code>${FOO}-baz</code> which will result in <code>bar-baz</code> being set as the value of <code>BAZ</code>. This interpolation is lazily and dynamically evaluated on the worker, and will work between environment variables defined in different ways, including contexts.</p>"},{"location":"concepts/configuration/environment.html#computed-values","title":"Computed values","text":"<p>You will possibly notice some environment variables being marked as <code>&lt;computed&gt;</code>, which means that their value is only computed at runtime. These are not directly set on the stack but come from various integrations - for example, AWS credentials (<code>AWS_ACCESS_KEY_ID</code> and friends) are set by the AWS integration and <code>SPACELIFT_API_TOKEN</code> is injected into each run to serve a number of purposes.</p> <p>You cannot set a computed value but you can override it - that is, explicitly set an environment variable on a stack that has the same name as the variable that comes from integration. This is due to precedence rules that warrant its own dedicated section.</p> <p>Overriding a computed value is almost like editing a regular stack variable, although worth noticing is Override replacing Edit and the lack of Delete action:</p> <p></p> <p>When you click Override, you can replace the value computed at runtime with a static one:</p> <p></p> <p>Note how it becomes a regular write-only variable upon saving:</p> <p></p> <p>If you delete this variable, it will again be replaced by the computed one. If you want to get rid of the computed variable entirely, you will need to disable the integration that originally led to its inclusion in this list.</p>"},{"location":"concepts/configuration/environment.html#spacelift-environment","title":"Spacelift environment","text":"<p>The Spacelift environment section lists a special subset of computed values that are injected into each run and that provide some Spacelift-specific metadata about the context of the job being executed. These are prefixed so that they can be used directly as input variables to OpenTofu/Terraform configuration, and their names always clearly suggest the content:</p> Environment Variable Type Description Possible Values / Format TF_VAR_spacelift_account_name string Spacelift account name. <code>&lt;ACCOUNT_NAME&gt;.app.spacelift.io</code> TF_VAR_spacelift_commit_branch string The commit branch in the Version Control System (VCS). e.g., <code>main</code>, <code>develop</code>, <code>feature/branch-name</code> TF_VAR_spacelift_commit_sha string The SHA of the commit in the Version Control System (VCS). Git commit SHA (40-character hash, e.g., <code>0123456789abcdef0123456789abcdef01234567</code>) TF_VAR_spacelift_local_preview boolean Indicates if the local-preview feature is enabled for the stack. <code>true</code> or <code>false</code> TF_VAR_spacelift_project_root string The project root defined in the stack for the Version Control System. Relative path (e.g., <code>src/app</code>) TF_VAR_spacelift_repository string The name of the repository in the Version Control System (VCS). Format: <code>owner/repo</code> TF_VAR_spacelift_run_id string The unique Run ID for the current run. UUID-like format (e.g., <code>01JKZAHVAQHFM8CC3EA5HWRMP1</code>) TF_VAR_spacelift_run_state enum The state of the current run. <code>QUEUED</code>, <code>CANCELED</code>, <code>INITIALIZING</code>, <code>PLANNING</code>, <code>FAILED</code>, <code>FINISHED</code>, <code>UNCONFIRMED</code>, <code>DISCARDED</code>, <code>CONFIRMED</code>, <code>APPLYING</code>, <code>PERFORMING</code>, <code>STOPPED</code>, <code>DESTROYING</code>, <code>PREPARING</code>, <code>PREPARING_APPLY</code>, <code>SKIPPED</code>, <code>REPLAN_REQUESTED</code>, <code>PENDING</code> (DEPRECATED), <code>READY</code>, <code>PREPARING_REPLAN</code>, <code>PENDING_REVIEW</code> TF_VAR_spacelift_run_trigger string The trigger information of the run. e.g., <code>Username</code>, <code>git commit</code>, <code>trigger policy</code>, <code>schedule identifier</code>, <code>API keys</code>, <code>reconciliation runs</code>, <code>modules</code> TF_VAR_spacelift_run_type enum The type of the current run. <code>PROPOSED</code>, <code>TRACKED</code>, <code>TASK</code>, <code>TESTING</code>, <code>DESTROY</code>, <code>PARSE</code> TF_VAR_spacelift_space_id string The ID of the Space that the stack belongs to. e.g., <code>root</code> (for the default space) TF_VAR_spacelift_stack_branch string The tracked branch for the stack. e.g., <code>main</code>, <code>production</code> TF_VAR_spacelift_stack_id string The ID of the stack. e.g., <code>myfirststack</code> TF_VAR_spacelift_stack_labels string The labels attached to the stack. Comma-separated list (e.g., <code>feature:add_plan_pr_comment,newlabel</code>) TF_VAR_spacelift_stack_labels_list string The labels attached to the stack expressed as list. HCL list (e.g., <code>[\"feature:add_plan_pr_comment\",\"newlabel\"]</code>) TF_VAR_spacelift_workspace_root string The workspace root information. Absolute path (e.g., <code>/mnt/workspace</code>) <p>Info</p> <p>Unless you know exactly what you're doing, we generally discourage overriding these dynamic variables, to avoid confusion.</p>"},{"location":"concepts/configuration/environment.html#per-stage-environment-variables","title":"Per-stage environment variables","text":"<p>The Spacelift flow can be broken down into a number of stages - most importantly:</p> <ul> <li>Initializing, where we prepare the workspace;</li> <li>Planning, which calculates the changes;</li> <li>Applying, which makes the actual changes;</li> </ul> <p>In this model, only the Applying phase makes any actual changes to your resources and your state and needs the credentials that support it. Yet frequently, the practice is to pass the same credentials to all stages. The reason for that is either the lack of awareness or - more often - the limitations in the tooling. Depending on your flow, this may be a potential security issue because even if you manually review every job before it reaches the Applying stage, the Planning phase can do a lot of damage.</p> <p>Spacelift supports a more security-conscious approach by allowing users to define variables that are passed to read (in practice, everything except for Applying) and write stages. By default, we pass an environment variable to all stages, but prefixes can be used to change the default behavior.</p> <p>An environment variable whose name starts with the <code>ro_</code> prefix is only passed to read stages but not to the write (Applying) stage. On the other hand, an environment variable whose name starts with the <code>wo_</code> prefix is only passed to the write (Applying) stage but not to the read ones.</p> <p>Combining the two prefixes makes it easy to create flows that limit the exposure of admin credentials to the code that has been thoroughly reviewed. The example below uses a <code>GITHUB_TOKEN</code> environment variable used by the GitHub Terraform provider variable split into two separate environment variables:</p> <p></p> <p>The first token will potentially be exposed to less-trusted code, so it makes sense to create it with read-only permissions. The second token on the other hand will only be exposed to the reviewed code and can be given write or admin permissions.</p> <p>A similar approach can be used for AWS, GCP, Azure, or any other cloud provider credentials.</p> <p>Info</p> <p>Newlines are not supported in environment variables. Alternatively mounted files can be used, base64 encoding / decoding, or removing the newlines where it would be possible to do so.</p>"},{"location":"concepts/configuration/environment.html#mounted-files","title":"Mounted files","text":"<p>Every now and then an environment variable is not what you need - you need a file instead. Terraform Kubernetes provider is a great example - one of the common ways of configuring it involves setting a <code>KUBECONFIG</code> variable pointing to the actual config file which needs to be present in your workspace as well.</p> <p>It's almost like creating an environment variable, though instead of typing (or pasting) the value you'll be uploading a file:</p> <p></p> <p></p> <p>Info</p> <p>Notice how you can give your file a name that's different to the name of the uploaded entity. In fact, you can use <code>/</code> characters in the file path to nest it deeper in directory tree - for example <code>a/b/c/d/e.json</code> is a perfectly valid file path.</p> <p>Similar to environment variables, mounted files can have different visibility settings - you can learn more about it here. One thing to note here is that plaintext files can be downloaded back straight from the UI or API while secret ones will only be visible to the run executed for the stack.</p> <p>Info</p> <p>Mounted files are limited to 2 MB in size. If you need to inject larger files into your workspace, we suggest that you make them part of the Docker runner image, or retrieve them dynamically using something like wget or curl.</p>"},{"location":"concepts/configuration/environment.html#project-structure","title":"Project structure","text":"<p>When discussing mounted files, it is important to understand the structure of the Spacelift workspace. Every Spacelift workload gets a dedicated directory <code>/mnt/workspace/</code>, which also serves as a root for all the mounted files.</p> <p>Your Git repository is cloned into <code>/mnt/workspace/source/</code>, which also serves as the working directory for your project, unless explicitly overridden by the project root configuration setting (either on the stack level or on in the runtime configuration).</p> <p>Warning</p> <p>Mounted files may be put into <code>/mnt/workspace/source/</code> as well and it's a legitimate use case, for example, to dynamically inject backend settings or even add extra infra definitions. Just beware of path clashes as mounted files will override your project source code in case of conflict. Sometimes this is what you want, sometimes not.</p>"},{"location":"concepts/configuration/environment.html#attached-contexts","title":"Attached contexts","text":"<p>While contexts are important enough to warrant their own dedicated article, it's also crucial to understand how they interact with environment variables and mounted files set directly on the stack, as well as with computed values. Perhaps you've noticed the blue labels on one of the earlier screenshots. If you haven't, here they are again, with a proper highlight:</p> <p></p> <p>The highlighted label is the name of the attached context that supplies those values. The sorted list of attached contexts is located below the calculated environment view, and each entry can be unfurled to see its exact content.</p> <p>Similar to computed values, those coming from contexts can also be overridden. Here's an example:</p> <p></p> <p></p> <p></p> <p>Note how we can now Delete the variable - this would revert it to the value defined by the context. Contexts can both provide environment variables as well as mounted files, and both can be overridden directly on the stack.</p> <p>Info</p> <p>If you want to get rid of the context-provided variable or file entirely, you will need to detach the context itself.</p>"},{"location":"concepts/configuration/environment.html#a-note-on-visibility","title":"A note on visibility","text":"<p>Perhaps you may have noticed how environment variables and mounted files come in two flavors - plain and secret. Here they are in the form for the new environment variable:</p> <p></p> <p>...and here they are in the form for the new mounted file:</p> <p></p> <p>Functionally, the difference between the two is pretty simple - plain values are accessible in the web GUI and through the API, and secret ones aren't - they're only made available to Runs and Tasks. Here's an example of two environment variables in the GUI - one plain, and one secret (also referred to as write-only):</p> <p></p> <p>Mounted files are similar - plain can be downloaded from the web GUI or through the API, and secret can't. Here's the difference in the GUI:</p> <p></p> <p>While the content of secret (write-only) environment variables and mounted files is not accessible through the GUI or API, the checksums are always available so if you have the value handy and just want to check if that's the same value as the one set in Spacelift, you can compare its checksum with the one reported by us - check out the most recent GraphQL API schema for more details.</p> <p>Info</p> <p>Though all of our data is encrypted both at rest and in transit, secret (write-only) values enjoy two extra layers of protection.</p>"},{"location":"concepts/configuration/runtime-configuration.html","title":"Runtime configuration","text":""},{"location":"concepts/configuration/runtime-configuration.html#runtime-configuration","title":"Runtime configuration","text":"<p>The runtime configuration is an optional setup applied to individual runs instead of being global to the stack. It's defined in <code>.spacelift/config.yml</code> YAML file at the root of your repository. A single file is used to define settings for all stacks associated with its host Git repository, so the file structure looks like this:</p> .spacelift/config.yml<pre><code>version: \"2\"\n\nstack_defaults:\n    runner_image: your/first:runner\n    # Note that tflint is not installed by\n    # default - this example assumes that your\n    # runner image has this available.\n    before_init:\n        - echo \"checking formatting\"\n        - terraform fmt -diff\n        - tflint\n\n# Note that every field in the configuration is\n# optional, and has a reasonable default. This file\n# allows you to override those defaults, and you can\n# merely override individual fields.\nstacks:\n    # The key of is the immutable slug of your stack\n    # which you will find in the URL.\n    babys-first-stack: &amp;shared\n        before_apply:\n            - hostname\n        project_root: infra\n        terraform_version: 0.12.4\n    babys-second-stack:\n        &lt;&lt;: *shared\n        terraform_version: 0.13.0\n        environment:\n            AWS_REGION: eu-west-1\n            TF_VAR_access_roles: '[productionsecurity, admin]'\n</code></pre> <p>The top level of the file contains three keys - <code>version</code> (defaults to <code>1</code> if not specified), <code>stacks</code> containing a mapping of immutable stack id to the stack configuration block and <code>stack_defaults</code>, containing the defaults common to all stacks using this source code repository. Note that corresponding stack-specific settings will override any stack defaults.</p> <p>Considering the precedence of settings, below is the order that will be followed, starting from the most important to the least important:</p> <ol> <li>The configuration for a specified stack defined in config.yml</li> <li>The stack defaults defined config.yml</li> <li>The stack configuration set in the Spacelift UI.</li> </ol> <p>Info</p> <p>Please note that the precedence of the runtime configuration file changed between versions 1 and 2. Previously the stack configuration set in the Spacelift UI had a higher precedence than the stack defaults defined in the config.yml file. Starting with version 2, the values in the config.yml file always take precedence over stack settings defined outside the config.yml file.</p> <p>In cases where there is no stack slug defined in the config, only the first two sources are considered:</p> <ol> <li>The stack configuration set in the Spacelift UI</li> <li>The stack defaults defined in config.yml</li> </ol> <p>Info</p> <p>Since we adopted everyone's favorite data serialization format, you can use all the YAML shenanigans you can think of - things like anchors and inline JSON can keep your config DRY and neat.</p>"},{"location":"concepts/configuration/runtime-configuration.html#purpose-of-runtime-configuration","title":"Purpose of runtime configuration","text":"<p>The whole concept of runtime configuration may initially sound unnecessary, but it ultimately allows flexibility that would otherwise be hard to achieve. In general, its purpose is to preview effects of changes not related to the source code (eg. OpenTofu/Terraform or Pulumi version upgrades, variable changes etc.), before they become an established part of your infra.</p> <p>While stack environment applies both to tracked and non-tracked branches, a runtime configuration change can be pushed to a feature branch, which triggers proposed runs allowing you to preview the changes before they have a chance to affect your state.</p> <p>Info</p> <p>If the runtime configuration file is not present or does not contain your stack, default values are used - refer to each setting for its respective default.</p>"},{"location":"concepts/configuration/runtime-configuration.html#stacks-configuration-block","title":"<code>Stacks</code> configuration block","text":""},{"location":"concepts/configuration/runtime-configuration.html#before_-and-after_-hooks","title":"<code>before_</code> and <code>after_</code> hooks","text":"<p>Info</p> <p>Each collection defaults to an empty array.</p> <p>These scripts allow customizing the Spacelift workflow - see the relevant documentation here. The following are available:</p> <ul> <li><code>before_init</code></li> <li><code>after_init</code></li> <li><code>before_plan</code></li> <li><code>after_plan</code></li> <li><code>before_apply</code></li> <li><code>after_apply</code></li> <li><code>before_perform</code></li> <li><code>after_perform</code></li> <li><code>before_destroy</code></li> <li><code>after_destroy</code></li> <li><code>after_run</code></li> </ul>"},{"location":"concepts/configuration/runtime-configuration.html#environment-map","title":"<code>environment</code> map","text":"<p>Info</p> <p>Defaults to an empty map.</p> <p>The environment allows you to declaratively pass some environment variables to the runtime configuration of the Stack. In case of a conflict, these variables will override both the ones passed via attached Contexts and those directly set in Stack's environment.</p>"},{"location":"concepts/configuration/runtime-configuration.html#project_root-setting","title":"<code>project_root</code> setting","text":"<p>Info</p> <p>Defaults to an empty string, pointing to the working directory for the run.</p> <p>Project root is the path of your project directory inside the Hub repository. You can use this setting to point Spacelift to the right place if the repo contains source code for multiple stacks in various folders or serves multiple purposes like those increasingly popular monorepos combining infrastructure definitions with source code, potentially even for multiple applications.</p>"},{"location":"concepts/configuration/runtime-configuration.html#runner_image-setting","title":"<code>runner_image</code> setting","text":"<p>Info</p> <p>Defaults to <code>public.ecr.aws/spacelift/runner-terraform:latest</code>. See this section for more details.</p> <p>The runner image is the Docker image used to run your workloads. By making it a runtime setting, Spacelift allows testing the image before it modifies your infrastructure.</p>"},{"location":"concepts/configuration/runtime-configuration.html#terraform_version-setting","title":"<code>terraform_version</code> setting","text":"<p>Info</p> <p>Defaults to the latest known supported Terraform version.</p> <p>This setting is only valid on Terraform stacks and specifies the Terraform version that the run will use. The main use case is testing a newer version of Terraform before you use it to change the state since the way back is very hard. This version can only be equal to or higher than the one already used to apply state changes. For more details on Terraform version management, please refer to its dedicated help section.</p>"},{"location":"concepts/configuration/runtime-configuration.html#opentofu_version-setting","title":"<code>opentofu_version</code> setting","text":"<p>Info</p> <p>Defaults to the latest known supported OpenTofu version.</p> <p>This setting specifies the OpenTofu version to be used during the run and is only applicable to OpenTofu/Terraform stacks. It is considered when <code>terraform_workflow_tool</code> is set to <code>OPEN_TOFU</code>. To specify a version, ensure it aligns with those officially supported and tested by Spacelift.</p> <p>Example: <code>opentofu_version: \"1.6.2\"</code></p>"},{"location":"concepts/configuration/runtime-configuration.html#terraform_workflow_tool-setting","title":"<code>terraform_workflow_tool</code> setting","text":"<p>This setting determines the Terraform implementation used in the workflow. Choose based on your project needs and the specific features or support each option offers:</p> <ul> <li><code>TERRAFORM_FOSS</code> (default) - Utilizes the official Terraform by HashiCorp. Best for standard Terraform operations and official provider support.</li> <li><code>OPEN_TOFU</code> - An open-source Terraform fork. Choose this for enhanced features or modifications not available in the official version.</li> <li><code>CUSTOM</code> - Enables the use of Custom Workflows for highly customized or unique deployment processes.</li> </ul> <p>Pulumi version management is based on Docker images.</p>"},{"location":"concepts/configuration/runtime-configuration.html#git_sparse_checkout_paths-setting","title":"<code>git_sparse_checkout_paths</code> setting","text":"<p>Info</p> <p>Defaults to an empty list. If empty, the value from the stack option will be applied.</p> <p>Git sparse checkout paths allow you to specify a list of directories and files that will be used in sparse checkout, meaning that only the specified paths from the list will be cloned. This can help reduce the size of the workspace by only downloading the parts of the repository that are needed for the stack.</p> <p>Only path values are allowed; glob patterns are not supported.</p> <p>Example valid paths:</p> <ul> <li><code>./infrastructure/</code></li> <li><code>infrastructure/</code></li> <li><code>infrastructure</code></li> <li><code>./infrastructure/main.tf</code></li> </ul> <p>Example invalid path (glob pattern):</p> <ul> <li><code>./infrastructure/*</code></li> </ul>"},{"location":"concepts/configuration/runtime-configuration.html#terragrunt-setting","title":"<code>terragrunt</code> setting","text":"<p>This setting determines the Terragrunt parametrization.</p> <ul> <li><code>terragrunt_tool</code> - to choose the tool that should be used by Terragrunt: <code>TERRAFORM_FOSS</code> (default), <code>OPEN_TOFU</code> and <code>MANUALLY_PROVISIONED</code></li> <li><code>terragrunt_version</code> - Terragrunt version. Defaults to the latest version.</li> <li><code>terraform_version</code> - the Terraform version. Must not be provided when tool is set to <code>MANUALLY_PROVISIONED</code>. Defaults to the latest available OpenTofu/Terraform version.</li> <li><code>use_run_all</code> - whether to use <code>terragrunt run-all</code> instead of <code>terragrunt</code>. Defaults to <code>false</code>.</li> <li><code>use_smart_sanitization</code> - indicates whether runs on this will use Terraform's sensitive value system to sanitize the outputs of Terraform state and plans in spacelift instead of sanitizing all fields. Defaults to <code>false</code>.</li> </ul> <p>Warning</p> <p>You must set all necessary fields because if the <code>terragrunt</code> section is present in the config, it replaces all Terragrunt values rather than merging them. For example, if you set <code>terraform_version</code> only, all other fields will be reset to their defaults.</p> <p>Info</p> <p>Terragrunt specific parameters only work when the stack is a Terragrunt stack. Adding this does not override the stack type. For example, Ansible stacks don't become Terragrunt stacks.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html","title":"YAML reference","text":""},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#yaml-reference","title":"YAML reference","text":"<p>This document is a reference for the Spacelift configuration keys that are used in the <code>.spacelift/config.yml</code> file to configure one or more Stacks.</p> <p>Warning</p> <p>The <code>.spacelift/config.yml</code> file must be located at the root of your repository, not at the project root.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#stack-settings","title":"Stack settings","text":"<ul> <li><code>version</code></li> <li><code>stack_defaults</code></li> <li><code>stacks</code></li> </ul>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#module-settings","title":"Module settings","text":"<ul> <li><code>version</code></li> <li><code>module_version</code></li> <li><code>test_defaults</code></li> <li><code>tests</code></li> </ul>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#version","title":"<code>version</code>","text":"<p>The version property is optional and currently ignored but for the sake of completeness you may want to set the value to \"1\".</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#stack_defaults","title":"<code>stack_defaults</code>","text":"<p><code>stack_defaults</code> represent default settings that will apply to every stack defined in the id-settings map. Any default setting is overridden by an explicitly set stack-specific value.</p> Key Required Type Description <code>after_apply</code> N list&lt;string&gt; List of commands executed after applying changes. <code>after_destroy</code> N list&lt;string&gt; List of commands executed after destroying managed resources. <code>after_init</code> N list&lt;string&gt; List of commands executed after first interacting with the backend (eg. terraform init). <code>after_perform</code> N list&lt;string&gt; List of commands executed after performing a custom task <code>after_plan</code> N list&lt;string&gt; List of commands executed after planning changes <code>after_run</code> N list&lt;string&gt; List of commands executed after every run, regardless of its outcome <code>before_apply</code> N list&lt;string&gt; List of commands executed before applying changes. <code>before_destroy</code> N list&lt;string&gt; List of commands executed before destroying managed resources. <code>before_init</code> N list&lt;string&gt; List of commands executed before first interacting with the backend (eg. <code>terraform init</code>). <code>before_perform</code> N list&lt;string&gt; List of commands executed before performing a custom task <code>before_plan</code> N list&lt;string&gt; List of commands executed before planning changes <code>environment</code> N map&lt;string, string&gt; Map of extra environment variables and their values passed to the job <code>project_root</code> N string Optional folder inside the repository serving as the root of your stack <code>runner_image</code> N string Name of the custom runner image, if any <code>terraform_version</code> N string For Terraform stacks, Terraform version number to be used"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#stacks","title":"<code>stacks</code>","text":"<p>The stacks section is a map using stack public ID (slug) as keys and stack settings - described in this section - as values. If you're using this mapping together with stack defaults, note that any default setting is overridden by an explicitly set stack-specific value. This is particularly important for list and map fields where one may assume that these are merged. In practice, they're not merged - they're replaced. If you want merging semantics, YAML provides native methods to merge arrays and maps.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#module_version","title":"<code>module_version</code>","text":"<p>Module version is a required string value that must conform to the semantic versioning scheme. Note that pre-releases and builds/nightlies are not supported - only the standard <code>$major.$minor.$patch</code> format will work.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#test_defaults","title":"<code>test_defaults</code>","text":"<p>Test defaults are runtime settings following this scheme that will apply to all test cases for a module.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#tests","title":"<code>tests</code>","text":"<p>The <code>tests</code> section represents a list of test cases for a module, each containing the standard runtime settings in addition to the test-specific settings:</p> Key Required Type Description name Y string Unique name of the test case negative N bool Indicates whether the test is negative (expected to fail) id N string Unique identifier of the test case which can be used to refer to the test case depends_on N list&lt;string&gt; List of test case <code>id</code>s this test depends on"},{"location":"concepts/policy.html","title":"Policy","text":""},{"location":"concepts/policy.html#policy","title":"Policy","text":"<p>Policy-as-code is the idea of expressing rules using a high-level programming language and treating them as you normally treat code, which includes version control as well as continuous integration and deployment. This approach extends the infrastructure-as-code approach to also cover the rules governing this infrastructure, and the platform that manages it.</p> <p>Spacelift as a development platform is built around this concept and allows defining policies that involve various decision points in the application. User-defined policies can decide:</p> <ul> <li>Login: who gets to log in to your Spacelift account and with what level of access;</li> <li>Access: who gets to access individual Stacks and with what level of access;</li> <li>Approval: who can approve or reject a run and how a run can be approved;</li> <li>Initialization: which Runs and Tasks can be started;</li> <li>Notification: routing and filtering notifications;</li> <li>Plan: which changes can be applied;</li> <li>Push: how Git push events are interpreted;</li> <li>Task: which one-off commands can be executed;</li> <li>Trigger: what happens when blocking runs terminate;</li> </ul> <p>Please refer to the following table for information on what each policy types returns, and the rules available within each policy.</p> Type Purpose Types Returns Rules Login Allow or deny login, grant admin access Positive and negative <code>boolean</code> <code>allow</code>, <code>admin</code>, <code>deny</code>, <code>deny_admin</code> Access Grant or deny appropriate level of stack access Positive and negative <code>boolean</code> <code>read</code>, <code>write</code>, <code>deny</code>, <code>deny_write</code> Approval Who can approve or reject a run and how a run can be approved Positive and negative <code>boolean</code> <code>approve, reject</code> Initialization Blocks suspicious runs before they start Negative <code>set&lt;string&gt;</code> <code>deny</code> Notification Routes and filters notifications Positive <code>map&lt;string, any&gt;</code> <code>inbox</code>, <code>slack</code>, <code>webhook</code> Plan Gives feedback on runs after planning phase Negative <code>set&lt;string&gt;</code> <code>deny</code>, <code>warn</code> Push Determines how a Git push event is interpreted Positive and negative <code>boolean</code> <code>track</code>, <code>propose</code>, <code>ignore</code>, <code>ignore_track</code>, <code>notrigger</code>, <code>notify</code> Task Blocks suspicious tasks from running Negative <code>set&lt;string&gt;</code> <code>deny</code> Trigger Selects stacks for which to trigger a tracked run Positive <code>set&lt;string&gt;</code> <code>trigger</code> <p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. For up to date policy input information you can also refer to official Spacelift policy contract schema.</p> <p>If you cannot find what you are looking for, please reach out to our support and we will craft a policy to do exactly what you need.</p>"},{"location":"concepts/policy.html#how-it-works","title":"How it works","text":"<p>Spacelift uses an open-source project called Open Policy Agent and its rule language, Rego, to execute policies at various decision points.</p> <p>You can think of policies as snippets of code that receive some JSON-formatted input (the data needed to make a decision) and are allowed to produce an output in a predefined form.  Each policy type exposes slightly different data, so please refer to their respective schemas for more information.</p> <p>Login policies which are global. All other policy types operate on the stack level and can be attached to multiple stacks, like contexts, which facilitates code reuse and allows flexibility. Policies only affect stacks they're attached to.</p> <p>Multiple policies of the same type can be attached to a single stack, in which case they are evaluated separately to avoid having their code (like local variables and helper rules) affect one another. Once these policies are evaluated against the same input, their results are combined. So if you allow user login from one policy but deny it from another, the result will still be a denial.</p>"},{"location":"concepts/policy.html#opa-version","title":"OPA version","text":"<p>We update the version of OPA that we are using regularly, to find out the version we are currently running, you can use the following query:</p> <pre><code>query getOPAVersion{\n    policyRuntime {\n        openPolicyAgentVersion\n    }\n}\n</code></pre> <p>For more detailed information about the GraphQL API and its integration, please refer to the API documentation.</p>"},{"location":"concepts/policy.html#policy-language","title":"Policy language","text":"<p>Rego, the language we're using to execute policies, is a very elegant, Turing incomplete data query language. If you know SQL and <code>jq</code>, you should find Rego familiar and only need a few hours to understand its quirks. For each policy, we also provide examples you can tweak to achieve your goals, and many of those examples comes with a link allowing you to execute it in the Rego playground.</p>"},{"location":"concepts/policy.html#rego-constraints","title":"Rego constraints","text":"<p>To keep policies functionally pure and relatively snappy, we disabled some Rego built-ins that can query external or runtime data. These are:</p> <ul> <li><code>http.send</code></li> <li><code>opa.runtime</code></li> <li><code>rego.parse_module</code></li> <li><code>time.now_ns</code></li> <li><code>trace</code></li> </ul> <p>Policies must be self-contained and cannot refer to external resources (e.g. files in a VCS repository).</p> <p>Info</p> <p>Disabling <code>time.now_ns</code> may seem surprising, but depending on the current timestamp it can make your policies impure and thus tricky to test. We encourage you to test your policies thoroughly!</p> <p>The current timestamp in Rego-compatible form (Unix nanoseconds) is available as <code>spacelift.request.timestamp_ns</code> in plan policy payloads, so please use it instead.</p>"},{"location":"concepts/policy.html#policy-returns-and-rules","title":"Policy returns and rules","text":"<p>Please refer to the following table for information on what each policy types returns, and the rules available within each policy.</p> Type Purpose Types Returns Rules Login Allow or deny login, grant admin access Positive and negative <code>boolean</code> <code>allow</code>, <code>admin</code>, <code>deny</code>, <code>deny_admin</code> Access Grant or deny appropriate level of stack access Positive and negative <code>boolean</code> <code>read</code>, <code>write</code>, <code>deny</code>, <code>deny_write</code> Approval Who can approve or reject a run and how a run can be approved Positive and negative <code>boolean</code> <code>approve</code>, <code>reject</code> Initialization Blocks suspicious runs before they start Negative <code>set&lt;string&gt;</code> <code>deny</code> Notification Routes and filters notifications Positive <code>map&lt;string, any&gt;</code> <code>inbox</code>, <code>slack</code>, <code>webhook</code> Plan Gives feedback on runs after planning phase Negative <code>set&lt;string&gt;</code> <code>deny</code>, <code>warn</code> Push Determines how a Git push event is interpreted Positive and negative <code>boolean</code> <code>track</code>, <code>propose</code>, <code>ignore</code>, <code>ignore_track</code>, <code>notrigger</code>, <code>notify</code> Task Blocks suspicious tasks from running Negative <code>set&lt;string&gt;</code> <code>deny</code> Trigger Selects stacks for which to trigger a tracked run Positive <code>set&lt;string&gt;</code> <code>trigger</code> <p>Tip</p> <p>We maintain a library of example policies that you can tweak to meet your specific needs or use as-is.</p> <p>If you cannot find what you are looking for, please reach out to our support and we will craft a policy to do exactly what you need.</p>"},{"location":"concepts/policy.html#boolean","title":"Boolean","text":"<p>Login and access policies expect rules to return a boolean value (true or false). Each type of policy defines its own set of rules corresponding to different access levels. In these cases, various types of rules can be positive or negative (that is, they can explicitly allow or deny access).</p>"},{"location":"concepts/policy.html#set-of-strings","title":"Set of strings","text":"<p>The second group of policies (initialization, plan, and task) is expected to generate a set of strings that serve as direct feedback to the user. Those rules are generally negative in that they can only block certain actions. Only their lack counts as an implicit success.</p> <p>Here's a practical difference between the two types:</p> Boolean returnsString returns boolean.rego<pre><code>package spacelift\n\n# This is a simple deny rule.\n# When it matches, no feedback is provided.\ndeny {\n  true\n}\n</code></pre> string.rego<pre><code>package spacelift\n\n# This is a deny rule with string value.\n# When it matches, that value is reported to the user.\ndeny[\"the user will see this\"] {\n  true\n}\n</code></pre> <p>For the policies that generate a set of strings, you want these strings to be both informative and relevant, so you'll see this pattern a lot in the examples:</p> <pre><code>package spacelift\n\nwe_dont_create := { \"scary\", \"resource\", \"types\" }\n\n# This is an example of a plan policy.\ndeny[sprintf(\"some rule violated (%s)\", [resource.address])] {\n  some resource\n  created_resources[resource]\n\n  we_dont_create[resource.type]\n}\n</code></pre>"},{"location":"concepts/policy.html#complex-objects","title":"Complex objects","text":"<p>Notification policies will generate and return more complex objects, typically JSON objects. In terms of syntax, the returned values are still very similar to policies that return sets of strings, but they provide additional information inside the returned decision.</p> <p>For example, this rule which will return a JSON object to be used when creating a custom notification:</p> <pre><code>package spacelift\n\ninbox[{\n  \"title\": \"Tracked run finished!\",\n  \"body\": sprintf(\"Run ID: %s\", [run.id]),\n  \"severity\": \"INFO\",\n}] {\n  run := input.run_updated.run\n  run.type == \"TRACKED\"\n  run.state == \"FINISHED\"\n}\n</code></pre>"},{"location":"concepts/policy.html#helper-functions","title":"Helper functions","text":"<p>The following helper functions can be used in Spacelift policies:</p> Name Description <code>output := sanitized(x)</code> <code>output</code> is the string <code>x</code> sanitized using the same algorithm we use to sanitize secrets. <code>result := exec(x)</code> Executes the command <code>x</code>. <code>result</code> is an object containing <code>status</code>, <code>stdout</code> and <code>stderr</code>. Only applicable for run initialization policies for private workers."},{"location":"concepts/policy.html#creating-policies","title":"Creating policies","text":"<p>You can create policies through the web UI and the Terraform provider. We generally suggest the latter, as it's much easier to manage down the line and allows proper unit testing.</p>"},{"location":"concepts/policy.html#with-the-terraform-provider","title":"With the Terraform provider","text":"<p>Here's how you would define a plan policy in Terraform and attach it to a stack (also created here with minimal configuration):</p> <pre><code>resource \"spacelift_stack\" \"example-stack\" {\n  name       = \"Example stack\"\n  repository = \"example-stack\"\n  branch     = \"master\"\n}\n\n# This example assumes that you have Rego policies in a separate\n# folder called \"policies\".\nresource \"spacelift_policy\" \"example-policy\" {\n  name = \"Example policy\"\n  body = file(\"${path.module}/policies/example-policy.rego\")\n  type = \"TERRAFORM_PLAN\"\n}\n\nresource \"spacelift_policy_attachment\" \"example-attachment\" {\n  stack_id  = spacelift_stack.example-stack.id\n  policy_id = spacelift_policy.example-policy.id\n}\n</code></pre>"},{"location":"concepts/policy.html#with-the-web-ui","title":"With the web UI","text":"<p>You must be a Spacelift admin to manage policies.</p> <p>You can create approval, push, plan, trigger, and notification policies in the web UI. Login policies are created in a different section of the UI.</p> <p></p> <ol> <li>Navigate to the Policies tab in Spacelift.</li> <li>Click Create policy.</li> <li>Fill in the required policy details:<ol> <li>Name: Enter a unique, descriptive name for the policy.</li> <li>Type: Select the type of policy from the dropdown.</li> <li>Space: Select the space to create the policy in.</li> <li>Description (optional): Enter a (markdown-supported) description for the policy.</li> <li>Labels (optional): Add labels to help sort and filter your policies. You can use <code>autoattach:label</code> to attach the policy to stacks or modules with the chosen <code>label</code> automatically.</li> </ol> </li> <li>Click Continue to edit the policy.<ul> <li>You'll see an example policy body based on the type you chose. Remove the comments from any rules you'd like to apply and add or change rules as needed.</li> </ul> </li> <li>Click Create policy. You can always edit policies as needed.</li> </ol>"},{"location":"concepts/policy.html#policy-structure","title":"Policy structure","text":"<p>We prepend variable definitions to each policy. These variables can be different for each type, but the prepended code is very similar. Here's an example for the Approval policy:</p> <pre><code>package spacelift\n\n# This is what Spacelift will query for when evaluating policies.\nresult = {\n  \"approve\": approve,\n  \"reject\": reject,\n  \"flag\": flag,\n  \"sample\": sample,\n}\n\n# Default to ensure that \"approve\" is defined.\ndefault approve = false\n\n# Default to ensure that \"reject\" is defined.\ndefault reject = false\n\n# Default to ensure that \"sample\" is defined.\ndefault sample = false\n\n# Placeholder to ensure that \"flag\" will be a set.\nflag[\"never\"] {\n  false\n}\n</code></pre> <p>Warning</p> <p>You can't change predefined variable types. Doing so will result in a policy validation error and the policy won't be saved.</p>"},{"location":"concepts/policy.html#attaching-policies","title":"Attaching policies","text":""},{"location":"concepts/policy.html#automatically-with-labels","title":"Automatically (with labels)","text":"<p>With the exception of login policies, policies can be automatically attached to stacks using the <code>autoattach:label</code> special label. Replace <code>label</code> with the name of a label attached to stacks and/or modules in your Spacelift account you wish the policy to be attached to.</p>"},{"location":"concepts/policy.html#policy-attachment-example","title":"Policy attachment example","text":"<p>Adding <code>autoattach:needs_approval</code> label to your policy will automatically attach the policy to all stacks/modules with the label <code>needs_approval</code>. This can be done with any label you're using on your stacks and modules.</p> <p></p>"},{"location":"concepts/policy.html#wildcard-policy-attachments","title":"Wildcard policy attachments","text":"<p>You can also attach a policy to stacks/modules using a wildcard. For example, using <code>autoattach:*</code> as a label will attach the policy to all stacks/modules.</p>"},{"location":"concepts/policy.html#manually","title":"Manually","text":"<p>In the web UI, attaching policies is done in the stack management view:</p> <p></p> <ol> <li>Navigate to the Stacks tab.</li> <li>Click the name of the stack to attach a policy to.</li> <li>Click the Policies tab, then click Attach policy.</li> <li>Select policy details:     <ul> <li>Policy type: Select the type of policy from the dropdown list.</li> <li>Select policy: Choose the specific policy to add from the dropdown list.</li> </ul> </li> <li>Click Attach.</li> </ol>"},{"location":"concepts/policy.html#policy-workbench","title":"Policy workbench","text":"<p>Sometimes, it takes trial and error to get policies working as intended. This is due to several factors: an unfamiliarity with the concept, a learning curve with the policy language, and/or the slow feedback cycle. Generally, feedback can easily take hours as you iterate through writing a plan policy, making a code change, triggering a run, verifying policy behavior, and rinsing and repeating.</p> <p>Enter policy workbench. Policy workbench captures policy evaluation events so that you can adjust the policy independently, shortening the entire cycle. In order to use the workbench, you will first need to sample policy inputs.</p>"},{"location":"concepts/policy.html#sample-policy-inputs","title":"Sample policy inputs","text":"<p>Each of Spacelift's policies supports an additional boolean rule called <code>sample</code>. Returning <code>true</code> from this rule means that the input to the policy evaluation is captured, along with the policy body at the time and the exact result of the policy evaluation. You can, for example, capture every evaluation with a simple:</p> <pre><code>sample { true }\n</code></pre> <p>If that feels a bit simplistic, you can adjust this rule to capture only certain types of inputs. For example, in this case we only want to capture evaluations that returned in an empty list for <code>deny</code> reasons (e.g. with a plan or task policy):</p> <pre><code>sample { count(deny) == 0 }\n</code></pre> <p>You can also sample a certain percentage of policy evaluations. Given that we don't generally allow nondeterministic evaluations, you'd need to depend on a source of randomness internal to the input. In this example, we will use the timestamp turned into milliseconds from nanoseconds to get a better spread. We'll also sample every 10th evaluation:</p> <pre><code>sample {\n  millis := round(input.spacelift.request.timestamp_ns / 1e6)\n  millis % 100 &lt;= 10\n}\n</code></pre>"},{"location":"concepts/policy.html#why-sample","title":"Why sample?","text":"<p>Capturing all evaluations sounds tempting, but it will also be extremely messy. Spacelift only shows the 100 most recent evaluations from the past 7 days. If you capture everything, the most valuable samples can be drowned by irrelevant or uninteresting ones. Also, sampling adds a smaller performance penalty to your operations.</p>"},{"location":"concepts/policy.html#policy-workbench-in-practice","title":"Policy workbench in practice","text":"<p>To show you how to work with the policy workbench, we are going to use a task policy that allowlists just two tasks: an innocent <code>ls</code>, and tainting a particular resource. It also only samples successful evaluations, where the list of <code>deny</code> reasons is empty.</p> <p>Info</p> <p>This example comes from our test Terraform repo, which gives you hands-on experience with most Spacelift functionalities within 10-15 minutes.</p> <ol> <li>On the Policies tab, click the name of the policy to edit in the workbench.</li> <li>Click Show simulation panel on the right-hand side of the screen.     </li> <li>If your policy has been evaluated and sampled, you will see the policy body on the left-hand side of the screen and a dropdown with timestamped evaluations (inputs) on the right-hand side. The evaluations are color-coded based on their outcomes.     </li> <li>Select one of the inputs in the dropdown, then click Simulate.     </li> <li>While running simulations, you can modify both the input and the policy body. If you change the policy body, or select an input that was evaluated using a different policy version, a warning will appear:     </li> <li>Click Show changes in the warning to view the exact differences between the policy body in the editor and the one used for the selected input.     </li> <li>When you are satisfied with your updated policy, click Save changes so future evaluations use the new policy body.</li> </ol>"},{"location":"concepts/policy.html#filtering-samples","title":"Filtering samples","text":"<p>The samples view offers powerful filtering options to help you quickly locate relevant samples. You can filter samples by:</p> <ul> <li>Stack name</li> <li>Outcome</li> <li>Pull request ID</li> <li>Push branch</li> <li>Stack repository</li> </ul> <p></p> <p>Once you have identified the sample you want, click the three dots beside its outcome and select Use to simulate to run a simulation with that sample.</p>"},{"location":"concepts/policy.html#is-policy-sampling-safe","title":"Is policy sampling safe?","text":"<p>Policy sampling is perfectly safe. Session data may contain some personal information like username, name, and IP, but that data is only persisted for 7 days. Most importantly, in plan policies, the inputs hash all the string attributes of resources, ensuring that no sensitive data leaks through this means.</p> <p>Last but not least, the policy workbench (including access to previous inputs) is only available to Spacelift account administrators.</p>"},{"location":"concepts/policy.html#policy-library","title":"Policy library","text":"<p>OPA can be difficult to learn, especially if you are just starting out with it. The policy workbench is great for helping you get policies right, but with the policy library, we take it up a notch.</p> <p>The policy library gives you the ability to import templates as regular policies, directly inside your Spacelift account, that can be easily modified to meet your needs.</p> <p>On the Policies tab in Spacelift, you will see a new section: Templates.</p> <p></p> <p>You can filter the policies based on the policy type or labels. There are examples available for all supported policy types.</p>"},{"location":"concepts/policy.html#import-policy-templates","title":"Import policy templates","text":"<ol> <li>When you find a policy template you would like to add to your account, click Import.</li> <li>Edit policy details (optional):     <ol> <li>Name: Enter a descriptive name for the policy.</li> <li>Type: Ensure the policy type is correct.</li> <li>Space: Select the space to create the policy in.</li> <li>Description (optional): Enter a (markdown-supported) description for the policy.</li> <li>Labels (optional): Add labels to help sort and filter your policies. You can use <code>autoattach:label</code> to attach the policy to stacks or modules with the chosen <code>label</code> automatically.</li> </ol> </li> <li>Click Continue to edit the policy.     <ul> <li>You'll see an example policy body based on the type you chose. Remove the comments from any rules you'd like to apply and add or change rules as needed.</li> </ul> </li> <li>Click Create policy. You can always edit policies as needed.</li> </ol>"},{"location":"concepts/policy.html#testing-policies","title":"Testing policies","text":"<p>Info</p> <p>We invite you to play around with policy examples and inputs in the Rego playground. However, this is not a replacement for proper unit testing.</p> <p>The whole point of policy-as-code is being able to handle it as code, which involves testing. Testing policies is crucial to make sure they aren't allowing unauthorized actions or too restrictive.</p> <p>Spacelift uses a well-documented and well-supported open-source language, Rego, which has built-in support for testing. Testing Rego is extensively covered in their documentation so in this section we'll only look at things specific to Spacelift.</p> <p>Let's define a simple login policy that denies access to non-members, and write a test for it:</p> deny-non-members.rego<pre><code>package spacelift\n\ndeny { not input.session.member }\n</code></pre> <p>You'll see that we simply mock out the <code>input</code> received by the policy:</p> deny-non-members_test.rego<pre><code>package spacelift\n\ntest_non_member {\n    deny with input as { \"session\": { \"member\": false } }\n}\n\ntest_member_not_denied {\n    not deny with input as { \"session\": { \"member\": true } }\n}\n</code></pre> <p>We can then test it in the console using <code>opa test</code> command (note the glob, which captures both the source and its associated test):</p> <pre><code>\u276f opa test deny-non-members*\nPASS: 2/2\n</code></pre> <p>Testing policies that provide feedback to the users is only slightly more complex. Instead of checking for boolean values, you'll be testing for set equality. Let's define a simple run initialization policy that denies commits to a particular branch:</p> deny-sandbox.rego<pre><code>package spacelift\n\ndeny[sprintf(\"don't push to %s\", [branch])] {\n  branch := input.commit.branch\n  branch == \"sandbox\"\n}\n</code></pre> <p>In the test, we will check that the set return by the deny rule either has the expected element for the matching input, or is empty for non-matching one:</p> deny-sandbox_test.rego<pre><code>package spacelift\n\ntest_sandbox_denied {\n  expected := { \"don't push to sandbox\" }\n\n  deny == expected with input as { \"commit\": { \"branch\": \"sandbox\" } }\n}\n\ntest_master_not_denied {\n  expected := set()\n\n  deny == expected with input as { \"commit\": { \"branch\": \"master\" } }\n}\n</code></pre> <p>Again, we can test the policy in the console using <code>opa test</code> (note the glob, which captures both the source and its associated test):</p> <pre><code>\u276f opa test deny-sandbox*\nPASS: 2/2\n</code></pre> <p>Tip</p> <p>We suggest you always unit test your policies and apply the same continuous integration principles as with your application code. You can set up a CI project using your vendor of choice for the same repository that's linked to the Spacelift project that's defining those policies, to get an external validation.</p>"},{"location":"concepts/policy.html#policy-flags","title":"Policy flags","text":"<p>By default, each policy is completely self-contained and does not depend on the result of previous policies. However, there are some situations where you want to introduce a chain of policies passing data to one another.</p> <p>Different types of policies have access to different types of data required to make a decision, and you can use policy flags to pass that data between them.</p> <p>Say you have a push policy with access to the list of files affected by a push or a PR event. You want to introduce a form of ownership control, where changes to different files need approval from different users. For example, a change in the <code>network</code> directory may require approval from the network team, while a change in the <code>database</code> directory needs an approval from the DBAs.</p> <p>Approvals are handled by an approval policy but it doesn't retain access to the list of affected files you need. This is where policy flags come in: set arbitrary review flags on the run in the push policy. This can be a separate push policy as in this example, or part of one of your pre-existing push policies. For simplicity, our example will only focus on <code>network</code>.</p> flag_for_review.rego<pre><code>package spacelift\n\nnetwork_review_flag = \"review:network\"\n\nflag[network_review_flag] {\n  startswith(input.push.affected_files[_], \"network/\")\n}\n\nflag[network_review_flag] {\n  startswith(input.pull_request.diff[_], \"network/*\")\n}\n</code></pre> <p>Now, we can introduce a network approval policy using this flag.</p> network-review.rego<pre><code>package spacelift\n\nnetwork_review_required {\n  input.run.flags[_] == \"review:network\"\n}\n\napprove { not network_review_required }\napprove {\n  input.reviews.current.approvals[_].session.teams[_] == \"DBA\"\n}\n</code></pre> <p>There are a few things worth knowing about flags:</p> <ul> <li>They are arbitrary strings and Spacelift makes no assumptions about their format or content.</li> <li>They can be reset by policies that set them (see policy flag reset for details).</li> <li>They are passed between policy types. If you have multiple policies of the same type, they will not be able to see each other's flags.</li> <li>They can be set by any policies that explicitly touch a run: push, approval, plan and trigger.</li> <li>They are always accessible through <code>run</code>'s <code>flags</code> property whenever the <code>run</code> resource is present in the input document.</li> </ul> <p>Flags are shown in the Spacelift GUI, so even if you're not using them to explicitly pass the data between different types of policies, they can still be useful for debugging purposes. Here's an example of an approval policy exposing decision-making details:</p> <p></p>"},{"location":"concepts/policy.html#backwards-compatibility","title":"Backwards-compatibility","text":"<p>Policies, like the rest of Spacelift functionality, are generally kept fully backwards-compatible. Input fields of policies aren't removed and existing policy \"invocation sites\" are kept in place.</p> <p>Occasionally policies might be deprecated, and once unused, disabled, but this is a process in which we work very closely with any affected users to make sure they have ample time to migrate and aren't negatively affected.</p> <p>However, we do reserve the right to add new fields to policy inputs and introduce additional invocation sites. For example, we could introduce a new input event type to the push policy, and existing push policies will start getting those events. Thus, users are expected to write their policies in a way that new input types are handled gracefully, by checking for the event type in their rules.</p> <p>For example, in a push policy, you might write a rule as follows:</p> backwards-compatibility.rego<pre><code>track {\n  not is_null(input.pull_request)\n  input.pull_request.labels[_] == \"deploy\"\n}\n</code></pre> <p>As you can see, the first line in the <code>track</code> rule makes sure that we only respond to events that contain the <code>pull_request</code>j field.</p>"},{"location":"concepts/policy/approval-policy.html","title":"Approval policy","text":""},{"location":"concepts/policy/approval-policy.html#approval-policy","title":"Approval policy","text":"<p>Approval policies allow organizations to create sophisticated run review and approval flows that reflect their preferred workflow, security goals, and business objectives. Without an explicit approval policy, anyone with write access to a stack can create a run or task. An approval policy can make this process more granular and contextual.</p> <p>Runs can be reviewed by approval policies when they enter one of three states: queued, unconfirmed, or pending review. If a stack has autodeploy enabled, then the approval policy will not be evaluated here, and you should use a plan policy to warn, which will force the stack into an unconfirmed state, at which point the approval policy will get evaluated as a safeguard.</p> <p>When a queued run needs approval, it will not be scheduled until that approval is received. If the run is of a blocking type, it will block newer runs from scheduling too. A queued run that's pending approval can be canceled at any point.</p>"},{"location":"concepts/policy/approval-policy.html#review-a-run","title":"Review a run","text":"<p>In this example, a queued run is waiting for a human review, and the last approval policy evaluation returned an Undecided decision.</p> <ol> <li>Navigate to the Runs tab and select the run that needs review.<ul> <li>You can also access runs for a specific stack by navigating to the Stacks tab, clicking the name of the stack, and clicking the run to review in the Tracked Run section.</li> </ul> </li> <li>Click Review in the top right.     </li> <li>Review the changes, then click Approve if the run can continue, or Reject if the run should be canceled.<ul> <li>If desired, you can leave a comment from your review in the text box. </li> </ul> </li> <li>Click Submit Review.</li> <li>If you approved the run, the approval policy will evaluate to Approved, thus unblocking the run.     </li> </ol> <p>When an unconfirmed run needs approval, you will not be able to confirm it until that approval is received. The run can, however, be discarded at any point:</p> <p></p> <p>The run review and approval process is very similar to GitHub's Pull Request review. The only exception is that it's the Rego policy (rather than a set of checkboxes and dropdowns) that defines the exact conditions to approve the run.</p> <p>Tip</p> <p>If separate run approval and confirmation steps sound confusing, think about how GitHub's Pull Requests work. You can approve a PR before merging it in a separate step. A PR approval means \"I'm ok with this being merged\", while a run approval means \"I'm OK with that action being executed\".</p>"},{"location":"concepts/policy/approval-policy.html#rules","title":"Rules","text":"<p>Your approval policy can define the following boolean rules:</p> <ul> <li>approve: The run is approved and no longer requires (or allows) review.</li> <li>reject: The run fails immediately.</li> </ul> <p>While the <code>approve</code> rule must be defined in order for the run to be able to progress, you don't need to define the <code>reject</code> rule. If <code>reject</code> rules are undefined, runs that look invalid can be canceled or discarded manually.</p> <p>Any given policy evaluation can also return <code>false</code> on both <code>approve</code> and <code>reject</code> rules. This only means that the result is yet <code>undecided</code>, and more reviews will be necessary. A perfect example would be a policy that requires 2 approvals for a given job; the first review is not supposed to set the <code>approve</code> value to <code>true</code>.</p> <p>Info</p> <p>Users must have <code>write</code> or <code>admin</code> access to the stack to be able to approve changes.</p>"},{"location":"concepts/policy/approval-policy.html#how-it-works","title":"How it works","text":"<p>When a user reviews the run, Spacelift persists the review and passes it to the approval policy (along with other reviews), plus some information about the run and its stack.</p> <p>The same user can review the same run as many times as they want, but only their newest review will be presented to the approval policy. This mechanism allows you to change your mind, very similar to Pull Request reviews in GitHub.</p>"},{"location":"concepts/policy/approval-policy.html#data-input-schema","title":"Data input schema","text":"<p>This schema is an informational example, as JSON doesn't support comments by design. You can play with this sample (and others) using the policy workbench or policy templates.</p> <p>Each approval policy request will receive this data input schema:</p> <p>Official Schema Reference</p> <p>For the most up-to-date and complete schema definition, please refer to the official Spacelift policy contract schema under the <code>APPROVAL</code> policy type.</p> <pre><code>{\n  \"reviews\": { // run reviews\n    \"current\": { // reviews for the current state\n      \"approvals\": [{ // positive reviews\n        \"author\": \"string - reviewer username\",\n        \"request\": { // request data of the review\n          \"remote_ip\": \"string - user IP\",\n          \"timestamp_ns\": \"number - review creation Unix timestamp in nanoseconds\"\n        },\n        \"session\": { // session data of the review\n          \"login\": \"string - username of the reviewer\",\n          \"name\": \"string - full name of the reviewer\",\n          \"teams\": [\"string - names of teams the reviewer was a member of\"]\n        },\n        \"state\": \"string - the state of the run at the time of the approval\"\n      }],\n      \"rejections\": [/* negative reviews, see \"approvals\" for schema */]\n    },\n    \"older\": [/* reviews for previous state(s), see \"current\" for schema */]\n  },\n  \"run\": { // the run metadata\n    \"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n    \"branch\": \"string - the branch the run was triggered from\",\n    \"changes\": [\n      {\n        \"action\": \"string enum - added | changed | deleted\",\n        \"entity\": {\n          \"address\": \"string - full address of the entity\",\n          \"name\": \"string - name of the entity\",\n          \"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n          \"entity_vendor\": \"string - the name of the vendor\",\n          \"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n          \"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n        },\n        \"phase\": \"string enum - plan | apply\"\n      }\n    ],\n    \"command\": \"string or null, set when the run type is TASK\",\n    \"commit\": {\n      \"author\": \"string - GitHub login if available, name otherwise\",\n      \"branch\": \"string - branch to which the commit was pushed\",\n      \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n      \"hash\": \"string - the commit hash\",\n      \"message\": \"string - commit message\",\n      \"exist_on_tracked_branch\": \"boolean - true if commit with this hash exist on tracked branch\"\n    },\n    \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n    \"creator_session\": {\n      \"admin\": \"boolean - is the current user a Spacelift admin\",\n      \"creator_ip\": \"string - IP address of the user who created the session\",\n      \"login\": \"string - username of the creator\",\n      \"name\": \"string - full name of the creator\",\n      \"teams\": [\"string - names of teams the creator was a member of\"],\n      \"machine\": \"boolean - whether the run was initiated by a human or a machine\"\n    },\n    \"drift_detection\": \"boolean - is this a drift detection run\",\n    \"flags\": [\"string - list of flags set on the run by other policies\"],\n    \"id\": \"string - the run ID\",\n    \"runtime_config\": {\n      \"before_init\": [\"string - command to run before run initialization\"],\n      \"project_root\": \"string - root of the Terraform project\",\n      \"runner_image\": \"string - Docker image used to execute the run\",\n      \"terraform_version\": \"string - Terraform version used for the run\"\n    },\n    \"state\": \"string - the current run state\",\n    \"triggered_by\": \"string or null - user, trigger policy, or dependent stack that triggered the run. For a dependent stack, this is set to the stack ID that triggered it.\",\n    \"type\": \"string - type of the run\",\n    \"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n    \"user_provided_metadata\": [\n      \"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n    ]\n  },\n  \"stack\": { // the stack metadata\n    \"administrative\": \"boolean - is the stack administrative\",\n    \"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n    \"branch\": \"string - tracked branch of the stack\",\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n    \"name\": \"string - name of the stack\",\n    \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n    \"project_root\": \"optional string - project root as set on the Stack, if any\",\n    \"repository\": \"string - name of the source GitHub repository\",\n    \"state\": \"string - current state of the stack\",\n    \"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n    \"worker_pool\": {\n      \"id\": \"string - the worker pool ID, if it is private\",\n      \"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n      \"name\": \"string - name of the worker pool, if it is private\",\n      \"public\": \"boolean - is the worker pool public\"\n    }\n  }\n}\n</code></pre>"},{"location":"concepts/policy/approval-policy.html#approval-policy-examples","title":"Approval policy examples","text":"<p>These policy examples can be imported directly into your Spacelift.</p> <p>Tip</p> <p>We maintain a library of example policies ready to use or alter to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p>"},{"location":"concepts/policy/approval-policy.html#two-approvals-no-rejections","title":"Two approvals, no rejections","text":"<p>In this example, each Unconfirmed run (including proposed runs triggered by Git events) will require two approvals. Additionally, the run should have no rejections. Anyone who rejects the run will need to go back and approve it in order for the run to go through.</p> <p>Info</p> <p>We suggest requiring more than one review because one approval should come from the run/commit author to indicate that they're aware of what they're doing, especially if their VCS handle is different than their IdP handle. This is something we practice internally at Spacelift.</p> <pre><code>package spacelift\n\napprove { input.run.state != \"UNCONFIRMED\" }\n\napprove {\n  count(input.reviews.current.approvals) &gt; 1\n  count(input.reviews.current.rejections) == 0\n}\n</code></pre>"},{"location":"concepts/policy/approval-policy.html#two-to-approve-two-to-reject","title":"Two to approve, two to reject","text":"<p>This is a variation of the above policy that will automatically fail any run that receives more than one rejection.</p> <pre><code>package spacelift\n\napprove { input.run.state != \"UNCONFIRMED\" }\napprove { count(input.reviews.current.approvals) &gt; 1 }\nreject  { count(input.reviews.current.rejections) &gt; 1 }\n</code></pre>"},{"location":"concepts/policy/approval-policy.html#require-approval-for-a-task-command-not-on-the-allowlist","title":"Require approval for a task command not on the allowlist","text":"<pre><code>package spacelift\n\nallowlist := [\"ps\", \"ls\", \"rm -rf /\"]\n\n# Approve when not a task.\napprove { input.run.type != \"TASK\" }\n\n# Approve when allowlisted.\napprove { input.run.command == allowlist[_] }\n\n# Approve with two or more approvals.\napprove { count(input.reviews.current.approvals) &gt; 1 }\n</code></pre> <p>Options for input.run.type include <code>PROPOSED</code>, <code>TRACKED</code>, <code>TASK</code>, <code>TESTING</code>, <code>DESTROY</code>.</p>"},{"location":"concepts/policy/approval-policy.html#combining-multiple-rules","title":"Combining multiple rules","text":"<p>Usually, you will apply different rules to different types of jobs. Since approval policies are attached to stacks, you'll want to be smart about how you combine different rules. Here's how you can combine two of the above approval flows as an example:</p> <pre><code>package spacelift\n\n# First, define all conditions that require explicit\n# user approval.\nrequires_approval { input.run.state == \"UNCONFIRMED\" }\nrequires_approval { input.run.type == \"TASK\" }\n\n# Then, automatically approve all other jobs.\napprove { not requires_approval }\n\n# Autoapprove some task commands. We don't check for run type\n# because only tasks will the have \"command\" field set.\ntask_allowlist := [\"ps\", \"ls\", \"rm -rf /\"]\napprove { input.run.command == task_allowlist[_] }\n\n# Two approvals and no rejections to approve.\napprove {\n  count(input.reviews.current.approvals) &gt; 1\n  count(input.reviews.current.rejections) == 0\n}\n</code></pre>"},{"location":"concepts/policy/approval-policy.html#role-based-approval","title":"Role-based approval","text":"<p>Sometimes you want to give specific roles (but not others) the power to approve certain workloads. The policy below approves an unconfirmed run or a task when either a Director approves it, or both DevOps and Security roles approve it:</p> <pre><code>package spacelift\n\n# First, define all conditions that require explicit\n# user approval.\nrequires_approval { input.run.state == \"UNCONFIRMED\" }\nrequires_approval { input.run.type == \"TASK\" }\napprove           { not requires_approval }\n\napprovals := input.reviews.current.approvals\n\n# Define what it means to be approved by a Director, DevOps and Security.\ndirector_approval { approvals[_].session.teams[_] == \"Director\" }\ndevops_approval   { approvals[_].session.teams[_] == \"DevOps\" }\nsecurity_approval { approvals[_].session.teams[_] == \"Security\" }\n\n# Approve when a single Director approves:\napprove { director_approval }\n\n# Approve when both DevOps and Security approve:\napprove { devops_approval; security_approval }\n</code></pre>"},{"location":"concepts/policy/approval-policy.html#require-private-worker-pool","title":"Require private worker pool","text":"<p>You might want to ensure that your runs are always scheduled on a private worker pool. You could use an approval policy similar to this ones:</p> <pre><code>package spacelift\n\n# Approve any runs on private workers\napprove { not input.stack.worker_pool.public }\n\n# Reject any runs on public workers\nreject { input.stack.worker_pool.public }\n</code></pre> <p>You may want to auto-attach this policy to some, if not all, of your stacks.</p>"},{"location":"concepts/policy/approval-policy.html#use-more-descriptive-approvals","title":"Use more descriptive approvals","text":"<p>Sometimes it is worth adding notes about approval/rejection to see why without rego code analysis.</p> <pre><code>package spacelift\n\nallowlist := [\"ps\", \"ls\"]\ndenylist := [\"rm -rf /\"]\n\napprove_with_note[note] {\n  input.run.type == \"TASK\"\n  input.run.command == allowlist[_]\n  note := sprintf(\"always approve tasks with command %s\", [input.run.command])\n}\n\nreject_with_note[note] {\n  input.run.type == \"TASK\"\n  input.run.command == denylist[_]\n  note := sprintf(\"always reject tasks with command %s\", [input.run.command])\n}\n</code></pre>"},{"location":"concepts/policy/login-policy.html","title":"Login policy","text":""},{"location":"concepts/policy/login-policy.html#login-policy","title":"Login policy","text":""},{"location":"concepts/policy/login-policy.html#purpose","title":"Purpose","text":"<p>Login policies provide policy-as-code authorization for Spacelift, allowing users to log in and assign RBAC roles programmatically. Unlike all other policy types, login policies are global and can't be attached to individual stacks. They take effect immediately once they're created and affect all future login attempts.</p> <p>Login policies are one of two authorization strategies available in Spacelift. The other is User Management for GUI-based permission management.</p> <p>API Keys are treated as virtual users and evaluated with login policy unless they are in the \"root\" space set with an admin key.</p> <p>Tip</p> <p>GitHub or SSO admins and private account owners always have admin access to their respective Spacelift accounts, regardless of login policies, so login policy errors can't lock everyone out of the account.</p> <p>Warning</p> <p>Any changes (create, update, or delete) made to a login policy will invalidate all active sessions except the session making the change.</p>"},{"location":"concepts/policy/login-policy.html#authentication-rules","title":"Authentication rules","text":"<p>A login policy can define the following types of boolean rules:</p> <ul> <li>allow: Allows the user to log in as a non-admin</li> <li>admin: Allows the user to log in as an account-wide admin. You don't need to explicitly allow admin users</li> <li>deny: Denies the login attempt, regardless of other (allow and admin) rules</li> <li>deny_admin: Denies the current user admin access to the stack, regardless of other rules</li> </ul>"},{"location":"concepts/policy/login-policy.html#rbac-role-assignment","title":"RBAC Role Assignment","text":"<ul> <li>roles: Assigns RBAC roles to users for specific spaces. This is the modern   approach for fine-grained permissions.</li> </ul>"},{"location":"concepts/policy/login-policy.html#legacy-space-rules-deprecated","title":"Legacy space rules (deprecated)","text":"<p>space_admin/space_write/space_read:</p> <p>Legacy Space Rules</p> <p>The <code>space_admin</code>, <code>space_write</code>, and <code>space_read</code> rules are deprecated. Use the <code>roles</code> rule to assign RBAC roles for better granularity and security.</p> <p>If no rules match, the default action will be to deny a login attempt.</p> <p>Define Restrictions</p> <p>Any time you define an allow or admin rule, consider restricting access using a deny rule, too. See an example below.</p> <p>Admins have significant privileges and can create and delete stacks; trigger runs or tasks; create, delete, and attach contexts and policies; etc. Unless full admin access is necessary, grant users limited admin access using the space_admin rule.</p>"},{"location":"concepts/policy/login-policy.html#data-input","title":"Data input","text":"<p>Each policy request will receive this data input:</p> <p>Official Schema Reference</p> <p>For the most up-to-date and complete schema definition, please refer to the official Spacelift policy contract schema under the <code>LOGIN</code> policy type.</p> data_input_schema.json<pre><code>{\n  \"request\": {\n    \"remote_ip\": \"string - IP of the user trying to log in\",\n    \"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n  },\n  \"session\": {\n    \"creator_ip\": \"string - IP address of the user who created the session\",\n    \"login\": \"string - username of the user trying to log in\",\n    \"member\": \"boolean - is the user a member of the account\",\n    \"name\": \"string - full name of the user trying to log in - may be empty\",\n    \"teams\": [\n      \"string - names of teams the user is a member of\"\n    ]\n  },\n  \"spaces\": [\n    {\n      \"id\": \"string - ID of the space\",\n      \"name\": \"string - name of the space\",\n      \"labels\": [\n        \"string - label of the space\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>Tip</p> <p>OPA string comparisons are case-sensitive. Use the proper case as defined in your Identity Provider when comparing values.</p> <p>We recommend enabling sampling on the policy to see the exact values passed by the Identity Provider.</p> <p>Two fields in the <code>session</code> object require further explanation: member and teams.</p>"},{"location":"concepts/policy/login-policy.html#member","title":"member","text":"Using GitHub (Default Identity Provider)Using Single Sign-On <p>When you first log in to Spacelift, we retrieve some details (such as username) from GitHub, our default identity provider. Each Spacelift account is linked to only one GitHub account. Thus, when you log in to a Spacelift account, we're checking if you're a member of the associated GitHub account.</p> <p>When the GitHub account is an organization, we can explicitly query for your organization membership. If you're a member, the <code>member</code> field is set to true. If you're not, it's false. Private accounts only have one member, so the check is even simpler: if your login is the same as the name of the linked GitHub account, the member field is set to true. If not, it's false.</p> <p>When using Single Sign-On with SAML, every successful login attempt will require that the <code>member</code> field is set to true. If the linked IdP could verify you, you must be a member.</p> <p>Tip</p> <p>The <code>member</code> field is useful for your deny rules.</p>"},{"location":"concepts/policy/login-policy.html#teams","title":"teams","text":"Using GitHub (Default Identity Provider)Using Single Sign-On <p>When using the default identity provider (GitHub), teams are only queried for organization accounts. If you're a member of the GitHub organization linked to a Spacelift account, Spacelift will query the GitHub API for the full list of teams you're a member of. This list will be available in the <code>session.teams</code> field. For private accounts and non-members, the field will be empty.</p> <p>Spacelift treats GitHub team membership as transitive. For example, let's assume Charlie is a member of the Badass team, which is a child of team Awesome. Charlie's list of teams includes both Awesome and Badass, even though he's not a direct member of the team Awesome.</p> <p>For Single Sign-On, the list of teams depends on how the SAML assertion attribute is mapped to your user record on the IdP end. Please see Single Sign-On for more details.</p> <p>Tip</p> <p>The <code>teams</code> field is useful for your allow and admin rules.</p>"},{"location":"concepts/policy/login-policy.html#login-policy-examples","title":"Login policy examples","text":"<p>Example Policies Library</p> <p>We maintain a library of example policies that are ready to use as-is or tweak to meet your specific needs.</p> <p>If you can't find what you're looking for below or in the library, please reach out to Spacelift support and we will craft a policy to meet your needs.</p> <p>There are three main use cases for login policies: managing access within an organization, managing access for external contributors, or restricting access in specific circumstances.</p> <p>We recommend having only one login policy to avoid unexpected results when multiple policies are merged.</p>"},{"location":"concepts/policy/login-policy.html#managing-access-levels-within-an-organization","title":"Managing access levels within an organization","text":"<p>In high-security environments where the principle of least privilege is applied, it's possible nobody on the infrastructure team is given admin access to GitHub. Still, it would be useful for the infrastructure team to be in charge of your Spacelift account.</p> <p>Let's create a login policy that will give every member of the DevOps team admin access and everyone in Engineering regular access. We'll give them more granular access to individual stacks later using stack access policies. We'll also explicitly deny access to all non-members, just to be on the safe side.</p> <pre><code>package spacelift\n\nteams := input.session.teams\n\n# Make sure to use the GitHub team names, not IDs (e.g., \"Example Team\" not \"example-team\")\n# and omit the GitHub organization name\nadmin { teams[_] == \"DevOps\" }\nallow { teams[_] == \"Engineering\" }\ndeny  { not input.session.member }\n</code></pre> <p>Here's an example to play with.</p> <p>This is also important for Single Sign-On integrations: only the integration creator gets administrative permissions by default, so all other administrators must be granted their access using a login policy.</p>"},{"location":"concepts/policy/login-policy.html#granting-access-to-external-contributors","title":"Granting access to external contributors","text":"<p>Warning</p> <p>This feature is not available when using Single Sign-On because your identity provider must be able to successfully validate each user trying to log in to Spacelift.</p> <p>Sometimes people who are not members of your organization, such as consultants, need access to your Spacelift account. Other times, a group of friends working on a project in a personal GitHub account could use access to Spacelift. Here are examples of a policy that grants several allowlisted people regular access, and one person admin privileges:</p> GitHubGoogle <p>This example uses GitHub usernames to grant access to Spacelift.</p> <pre><code>package spacelift\n\nadmins  := { \"alice\" }\nallowed := { \"bob\", \"charlie\", \"danny\" }\nlogin   := input.session.login\n\nadmin { admins[login] }\nallow { allowed[login] }\ndeny  { not admins[login]; not allowed[login] }\n</code></pre> <p>Here's an example to play with.</p> <p>This example uses email addresses managed by Google to grant access to Spacelift.</p> <pre><code>package spacelift\n\nadmins  := { \"alice@example.com\" }\nlogin   := input.session.login\n\nadmin { admins[login] }\nallow { endswith(input.session.login, \"@example.com\") }\ndeny  { not admins[login]; not allow }\n</code></pre> <p>Warning</p> <p>Granting access to individuals is more risky than granting access to only teams and account members. In the latter case, when an account member loses access to your GitHub organization, they automatically lose access to Spacelift. But when allowlisting individuals and not restricting access to members only, you'll need to explicitly remove the individuals from your Spacelift login policy.</p>"},{"location":"concepts/policy/login-policy.html#restricting-access-in-specific-circumstances","title":"Restricting access in specific circumstances","text":"<p>Stable and secure infrastructure is crucial to business continuity. All changes to your infrastructure carry some risk, so you may want to restrict access to it. The example below shows a comprehensive policy that restricts Spacelift access to users logging in from the office IP during business hours. You may want to use elements of this policy to create your own (less draconian) version, or keep it this way to support everyone's work-life balance.</p> <p>This example only defines deny rules, so you'll likely want to add some allow and admin rules, either in this policy or in a separate one.</p> <pre><code>package spacelift\n\nnow     := input.request.timestamp_ns\nclock   := time.clock([now, \"America/Los_Angeles\"])\nweekend := { \"Saturday\", \"Sunday\" }\nweekday := time.weekday(now)\nip      := input.request.remote_ip\n\ndeny { weekend[weekday] }\ndeny { clock[0] &lt; 9 }\ndeny { clock[0] &gt; 17 }\ndeny { not net.cidr_contains(\"12.34.56.0/24\", ip) }\n</code></pre> <p>Here's an example to play with.</p>"},{"location":"concepts/policy/login-policy.html#granting-limited-admin-access","title":"Granting limited admin access","text":"<p>Sometimes, you want to give a user admin access limited to a certain set of resources, so that they can manage them without having access to all other resources in that account. You can find more on this use case in Spaces.</p>"},{"location":"concepts/policy/login-policy.html#rewriting-teams","title":"Rewriting teams","text":"<p>In addition to boolean rules regulating access to your Spacelift account, the login policy exposes the team rule, which allows you to dynamically rewrite the list of teams and define Spacelift roles independent of the identity provider. To illustrate this use case, imagine you want to define a <code>Superwriter</code> role for someone who's:</p> <ul> <li>logging in from an office VPN</li> <li>a member of the DevOps team, as defined by your IdP</li> <li>not a member of the Contractors team, as defined by your IdP</li> </ul> Defining Superwriter<pre><code>package spacelift\n\nteam[\"Superwriter\"] {\n  office_vpn\n  devops\n  not contractor\n}\n\ncontractor { input.session.teams[_] == \"Contractors\" }\ndevops     { input.session.teams[_] == \"DevOps\" }\noffice_vpn { net.cidr_contains(\"12.34.56.0/24\", input.request.remote_ip)  }\n</code></pre> <p>Here, the team rule overwrites the original list of teams, so if it evaluates to a non-empty collection, it will replace the original list of teams in the session. In the above example, the <code>Superwriter</code> role will become the only team for the evaluated user session.</p> <p>If you want to retain the original list of teams, you can modify the above example:</p> Defining Superwriter While Retaining Teams List<pre><code>package spacelift\n\n# This rule will copy each of the existing teams to the\n# new modified list.\nteam[name] { name := input.session.teams[_] }\n\nteam[\"Superwriter\"] {\n  office_vpn\n  devops\n  not contractor\n}\n\ncontractor { input.session.teams[_] == \"Contractors\" }\ndevops     { input.session.teams[_] == \"DevOps\" }\noffice_vpn { net.cidr_contains(\"12.34.56.0/24\", input.request.remote_ip)  }\n</code></pre> <p>Here's an example to play with.</p> <p>Tip</p> <p>Because the user session is updated, the rewritten teams are available in the data input provided to the policy types that receive user information. For example, the rewritten teams can be used in Access policies.</p>"},{"location":"concepts/policy/login-policy.html#default-login-policy","title":"Default login policy","text":"<p>If no login policies are defined on the account, Spacelift behaves as if it had this policy and allows all users:</p> <pre><code>package spacelift\n\nallow { input.session.member }\n</code></pre>"},{"location":"concepts/policy/notification-policy.html","title":"Notification policy","text":""},{"location":"concepts/policy/notification-policy.html#notification-policy","title":"Notification policy","text":""},{"location":"concepts/policy/notification-policy.html#purpose","title":"Purpose","text":"<p>Info</p> <p>Please note, we currently don't support importing rego.v1. For more details, refer to the policy introduction.</p> <p>Notification policies can filter, route, and adjust the body of notification messages sent by Spacelift. The policy works at the space level, meaning that it does not need to be attached to a specific stack. Notification policices always verify the space they're in can be accessed by whatever action is being evaluated. If you are new to spaces, consider exploring our spaces documentation.</p> <p>All notifications go through the policy evaluation, so any of them can be redirected to the routes defined in the policy.</p> <p>A notification policy can define the following rules:</p> <ul> <li>inbox: Allows messages to be routed to the Spacelift notification inbox.</li> <li>slack: Allows messages to be routed to a given Slack channel.</li> <li>webhook: Allows messages to be routed to a given webhook.</li> <li>pull_request: Allows messages to be routed to one or more pull requests.</li> </ul> <p>If no rules match, no action is taken.</p>"},{"location":"concepts/policy/notification-policy.html#data-input-schema","title":"Data input schema","text":"<p>This is the schema of the data input that each policy request can receive:</p> <p>Official Schema Reference</p> <p>For the most up-to-date and complete schema definition, please refer to the official Spacelift policy contract schema under the <code>NOTIFICATION</code> policy type.</p> <pre><code>{\n  \"account\": {\n    \"name\": \"string\"\n  },\n  \"module_version\": {\n    \"module\": {\n      \"id\": \"string - unique ID of the module\",\n      \"administrative\": \"boolean - is the module administrative\",\n      \"branch\": \"string - tracked branch of the module\",\n      \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n      \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n      \"name\": \"string - name of the module\",\n      \"project_root\": \"optional string - project root as set on the Module, if any\",\n      \"repository\": \"string - name of the source repository\",\n      \"terraform_provider\": \"string - name of the main Terraform provider used by the module\",\n      \"space\": {\n        \"id\": \"string - id of a space\",\n        \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n        \"name\": \"string - name of a the space\"\n      },\n      \"worker_pool\": {\n        \"public\": \"boolean - worker pool information\",\n        \"id\": \"string - unique ID of the worker pool\",\n        \"name\": \"string - name of the worker pool\",\n        \"labels\": [\"string - list of arbitrary, user-defined selectors\"]\n      }\n    },\n    \"version\": {\n      \"commit\": {\n        \"author\": \"string\",\n        \"branch\": \"string\",\n        \"created_at\": \"number (timestamp in nanoseconds)\",\n        \"hash\": \"string\",\n        \"message\": \"string\",\n        \"url\": \"string\"\n      },\n      \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n      \"id\": \"string - id of the version being created\",\n      \"latest\": \"boolean - is the module version latest\",\n      \"number\": \"string - semver version number\",\n      \"state\": \"string - current module state: ACTIVE, FAILED\",\n      \"test_runs\": [{\n        \"created_at\": \"number (timestamp in nanoseconds)\",\n        \"id\": \"string - id of the test\",\n        \"state\": \"string - state of the test\",\n        \"title\": \"string - title of the test\",\n        \"updated_at\": \"number (timestamp in nanoseconds)\",\n      }]\n    }\n  },\n  \"run_updated\": {\n    \"state\": \"string\",\n    \"username\": \"string\",\n    \"note\": \"string\",\n    \"run\":{\n      \"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n      \"branch\": \"string - the branch the run was triggered from\",\n      \"changes\": [\n        {\n          \"action\": \"string enum - added | changed | deleted\",\n          \"entity\": {\n            \"address\": \"string - full address of the entity\",\n            \"data\": \"object - detailed information about the entity, shape depends on the vendor and type\",\n            \"name\": \"string - name of the entity\",\n            \"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n            \"entity_vendor\": \"string - the name of the vendor\",\n            \"entity_type\": \"string - the type of entity, possible values depend on the vendor\"\n          },\n          \"phase\": \"string enum - plan | apply\"\n        }\n      ],\n      \"command\": \"string\",\n      \"commit\": {\n        \"author\": \"string\",\n        \"branch\": \"string\",\n        \"created_at\": \"number (timestamp in nanoseconds)\",\n        \"hash\": \"string\",\n        \"message\": \"string\",\n        \"url\": \"string\"\n      },\n      \"created_at\": \"number (timestamp in nanoseconds)\",\n      \"creator_session\": {\n        \"admin\": \"boolean\",\n        \"creator_ip\": \"string\",\n        \"login\": \"string\",\n        \"name\": \"string\",\n        \"teams\": [\"string\"],\n        \"machine\": \"boolean - whether the run was kicked off by a human or a machine\"\n      },\n      \"drift_detection\": \"boolean\",\n      \"flags\": [\"string\"],\n      \"id\": \"string\",\n      \"runtime_config\": {\n        \"after_apply\": [\"string\"],\n        \"after_destroy\": [\"string\"],\n        \"after_init\": [\"string\"],\n        \"after_perform\": [\"string\"],\n        \"after_plan\": [\"string\"],\n        \"after_run\": [\"string\"],\n        \"before_apply\": [\"string\"],\n        \"before_destroy\": [\"string\"],\n        \"before_init\": [\"string\"],\n        \"before_perform\": [\"string\"],\n        \"before_plan\": [\"string\"],\n        \"environment\": \"map[string]string\",\n        \"project_root\": \"string\",\n        \"runner_image\": \"string\",\n        \"terraform_version\": \"string\"\n      },\n      \"policy_receipts\": [{\n         \"flags\": [\"string - flag assigned to the policy\"],\n         \"name\": \"string - name of the policy\",\n         \"outcome\": \"string - outcome of the policy\",\n         \"type\": \"string - type of the policy\"\n       }],\n      \"state\": \"string\",\n      \"triggered_by\": \"string or null\",\n      \"type\": \"string - PROPOSED or TRACKED\",\n      \"updated_at\": \"number (timestamp in nanoseconds)\",\n      \"user_provided_metadata\": [\"string\"]\n    },\n    \"stack\": {\n      \"administrative\": \"boolean\",\n      \"autodeploy\": \"boolean\",\n      \"autoretry\": \"boolean\",\n      \"branch\": \"string\",\n      \"id\": \"string\",\n      \"labels\": [\"string\"],\n      \"locked_by\": \"string or null\",\n      \"name\": \"string\",\n      \"namespace\": \"string or null\",\n      \"project_root\": \"string or null\",\n      \"repository\": \"string\",\n      \"space\": {\n        \"id\": \"string\",\n        \"labels\": [\"string\"],\n        \"name\": \"string\"\n      },\n      \"state\": \"string\",\n      \"terraform_version\": \"string or null\",\n      \"tracked_commit\": {\n        \"author\": \"string\",\n        \"branch\": \"string\",\n        \"created_at\": \"number (timestamp in nanoseconds)\",\n        \"hash\": \"string\",\n        \"message\": \"string\",\n        \"url\": \"string\"\n      }\n    },\n    \"timing\": [\n      {\n        \"duration\": \"number (in nanoseconds)\",\n        \"state\": \"string\"\n      }\n    ],\n    \"urls\": {\n      \"run\": \"string - URL to the run\"\n    }\n  },\n  \"webhook_endpoints\": [\n    {\n      \"id\": \"custom-hook2\",\n      \"labels\": [\n        \"example-label1\",\n        \"example-label2\"\n      ]\n    }\n  ],\n  \"internal_error\": {\n    \"error\": \"string\",\n    \"message\": \"string\",\n    \"severity\": \"string - INFO, WARNING, ERROR\"\n  }\n}\n</code></pre> <p>The final JSON object received as input will depend on the type of notification being sent. Event-dependent objects will only be present when those events happen.</p> <p>The best way to see what input your Notification policy received is to enable sampling and check the policy workbench. You can also use the table in the next section as a reference.</p>"},{"location":"concepts/policy/notification-policy.html#ansible-vendor-changes-structure","title":"Ansible vendor changes structure","text":"<p>For Ansible runs, the <code>changes</code> array contains objects with different <code>action</code> values and <code>entity</code> objects that contain Ansible-specific fields:</p> <pre><code>\"changes\": [\n  {\n    \"action\": \"string enum - changed | ok | skipped | rescued | ignored | unreachable | failed\",\n    \"entity\": {\n      \"host_name\": \"string\",\n      \"playbook_name\": \"string\",\n      \"role_name\": \"string\",\n      \"task_name\": \"string\",\n      \"task_action\": \"string\"\n    },\n    \"phase\": \"string enum - plan | apply\"\n  }\n]\n</code></pre> Object Received Event <code>account</code> Any event <code>webhook_endpoints</code> Any event <code>run_updated</code> Run Updated <code>internal_error</code> Internal error occurred <code>module_version</code> Module version updated"},{"location":"concepts/policy/notification-policy.html#notification-policy-in-practice","title":"Notification policy in practice","text":"<p>Using the notification policy, you can completely re-write notifications or control where and when they are sent. Let's look into how the policy works for each of the defined routes.</p>"},{"location":"concepts/policy/notification-policy.html#choose-a-space-for-your-policy","title":"Choose a space for your policy","text":"<p>When creating notification policies, take into account the space in which you're creating them. Generally the policy follows the same conventions as any other Spacelift component, with a few small caveats.</p> Run update notificationsInternal errors <p>Run update messages will rely on the space that the run is happening in. It will check any policies in that space, including policies inherited from other spaces.</p> <p>Most internal errors will check for notification policies inside of the root space. However if the policy is reporting about a component that belongs to a certain space (and can determine which space it is), then it will check for policies in that or any inherited spaces.</p> <p>Here is a list of components it will check in order:</p> <ul> <li>Stack</li> <li>Worker pool</li> <li>AWS integration</li> <li>Policy</li> </ul>"},{"location":"concepts/policy/notification-policy.html#inbox-notifications","title":"Inbox notifications","text":"<p>Inbox notifications are what you receive in your Spacelift notification inbox. By default, these are errors that happened during action execution inside Spacelift and are always sent even if you do not have a policy created.</p> <p>However, with notification policices you can alter the body of those errors to add additional context or create your own unique notifications.</p> <p>The inbox rule accepts multiple configurable parameters, all optional:</p> <ul> <li><code>title</code>: A custom title for the message.</li> <li><code>body</code>: A custom message body.</li> <li><code>severity</code>: The severity level for the message.</li> </ul>"},{"location":"concepts/policy/notification-policy.html#create-new-inbox-notifications","title":"Create new inbox notifications","text":"<p>This inbox rule will send <code>INFO</code> level notification messages to your inbox when a tracked run has finished:</p> <pre><code>package spacelift\n\n inbox[{\n  \"title\": \"Tracked run finished!\",\n  \"body\": sprintf(\"http://example.app.spacelift.io/stack/%s/run/%s has finished\", [stack.id, run.id]),\n  \"severity\": \"INFO\",\n }] {\n   stack := input.run_updated.stack\n   run := input.run_updated.run\n   run.type == \"TRACKED\"\n   run.state == \"FINISHED\"\n }\n</code></pre>"},{"location":"concepts/policy/notification-policy.html#slack-messages","title":"Slack messages","text":"<p>Slack messages can also be controlled using the notification policy. Before creating any policies that interact with Slack, you will need to add the slack integration to your Spacelift account.</p> <p>Info</p> <p>The documentation about Slack contains more information about available actions, Slack access policies, and more.</p> <p>The rules for Slack require a <code>channel_id</code> to be defined, which can be found at the bottom of a channel's About section in Slack:</p> <p></p> <p>Once the integration is configured in your account, you should be ready to define rules for routing Slack messages. Slack rules allow you to make the same filtering decisions as any other rule in the policy. They also allow you to edit the message bodies themselves to create custom messages.</p> <p>The Slack rules accept multiple configurable parameters:</p> <ul> <li><code>channel_id</code>: The Slack channel to which the message will be delivered.</li> <li><code>message</code> (optional): A custom message to be sent.</li> <li><code>mention_users</code> (optional): An array of users to mention in the default message.</li> <li><code>mention_groups</code> (optional): An array of groups to mention in the default message.</li> </ul>"},{"location":"concepts/policy/notification-policy.html#filtering-and-routing-messages","title":"Filtering and routing messages","text":"<p>To receive notifications for finished runs only on a specific Slack channel, define a rule like this:</p> <pre><code>package spacelift\n\nslack[{\"channel_id\": \"C0000000000\"}] {\n  input.run_updated != null\n\n  run := input.run_updated.run\n  run.state == \"FINISHED\"\n}\n</code></pre>"},{"location":"concepts/policy/notification-policy.html#changing-the-message-body","title":"Changing the message body","text":"<p>To send a custom message when a run which tries to attach a policy requires confirmation, define a rule like this:</p> <pre><code>package spacelift\n\nslack[{\n  \"channel_id\": \"C0000000000\",\n  \"message\": sprintf(\"http://example.app.spacelift.io/stack/%s/run/%s is trying to attach a policy!\", [stack.id, run.id]),\n}] {\n  stack := input.run_updated.stack\n  run := input.run_updated.run\n  run.type == \"TRACKED\"\n  run.state == \"UNCONFIRMED\"\n  change := run.changes[_]\n  change.phase == \"plan\"\n  change.entity.type == \"spacelift_policy_attachment\"\n}\n</code></pre>"},{"location":"concepts/policy/notification-policy.html#mentioning-users-and-groups","title":"Mentioning users and groups","text":"<p>You can provide an array of user IDs and group IDs to mention in the default message. To mention a user and a group (conditionally) in a Slack message, define a rule like this:</p> <pre><code>package spacelift\n\nusers := {\n  \"bob\": \"U08MA4D50RY\"\n}\ngroups := {\n  \"devs\": \"S08MA51EW4S\"\n}\n\nmention_groups(run) = [groups.devs] {\n  run.state == \"UNCONFIRMED\"\n} else = []\n\nslack[{\n  \"channel_id\": \"C08MA83LAD9\",\n  \"mention_users\": [users.bob],\n  \"mention_groups\": mention_groups(run)\n}] {\n  run := input.run_updated.run\n  run.type == \"TRACKED\"\n}\n</code></pre>"},{"location":"concepts/policy/notification-policy.html#organizing-messages-in-threads","title":"Organizing messages in threads","text":"<p>Use the <code>thread_key</code> parameter to group related messages in a single Slack thread. When the first message with a specific <code>thread_key</code> is sent, it creates a new message in the channel. Subsequent messages with the same <code>thread_key</code> will be sent as replies in that thread, instead of creating new messages.</p> <p>Here's how to use <code>thread_key</code> to group all updates for a single run in one thread:</p> <pre><code>package spacelift\n\nslack[{\"channel_id\": \"C08TULY2SBS\", \"thread_key\": run.id}] {\n run := input.run_updated.run\n run.type == \"TRACKED\"\n}\n</code></pre> <p>This will send a message every time the run is updated, with each update appearing as a reply in the same thread.</p> <p>Note</p> <p>Using <code>thread_key</code> changes the default behavior of run update notifications. Without <code>thread_key</code>, only one message is sent per run and that message is updated every time the run is updated. With <code>thread_key</code>, every state change will be sent as a separate message in the thread.</p>"},{"location":"concepts/policy/notification-policy.html#webhook-requests","title":"Webhook requests","text":"<p>Info</p> <p>You must configure at least one named webhook before using webhook notifications.</p> <p>With webhook notifications, you can receive webhooks on specific events that happen in Spacelift and craft unique requests to be consumed by some third-party.</p> <p>The notification policy relies on named webhooks, which can be created and managed in the webhooks section of Spacelift. Any policy evaluation will always receive a list of possible webhooks together with their labels as input. The data received in the policy input should be used to determine which webhook will be used when sending the request.</p> <p>The webhook policy accepts multiple configurable parameters:</p> <ul> <li><code>endpoint_id</code>: Endpoint id (slug) to which the webhook will be delivered.</li> <li><code>headers</code> (optional): A key value map to append to request headers.</li> <li><code>payload</code> (optional): A custom valid JSON object to be sent as request body.</li> <li><code>method</code> (optional): The HTTP method to use when sending the request.</li> </ul>"},{"location":"concepts/policy/notification-policy.html#filtering-webhook-requests","title":"Filtering webhook requests","text":"<p>Filter and select webhooks using the received input data. You can create rules where only specific actions trigger a webhook being sent. For example, we could define a rule to allow a webhook to be sent about any drift detection run:</p> <pre><code>package spacelift\n\nwebhook[{\"endpoint_id\": endpoint.id}] {\n  endpoint := input.webhook_endpoints[_]\n  endpoint.id == \"drift-hook\"\n  input.run_updated.run.drift_detection == true\n  input.run_updated.run.type == \"PROPOSED\"\n}\n</code></pre>"},{"location":"concepts/policy/notification-policy.html#creating-a-custom-webhook-request","title":"Creating a custom webhook request","text":"<p>All requests sent will include the default headers for verification, a payload appropriate for the message type, and the <code>method</code> set as <code>POST</code>. However, by using the webhook rule we can modify the body of the request, change the method, or add additional headers.</p> <p>To set up a completely custom request for a tracked run, define a rule like this:</p> <pre><code>package spacelift\n\nwebhook[wbdata] {\n  endpoint := input.webhook_endpoints[_]\n  endpoint.id == \"testing-notifications\"\n  wbdata := {\n    \"endpoint_id\": endpoint.id,\n    \"payload\": {\n      \"custom_field\": \"This is a custom message\",\n      \"run_type\": input.run_updated.run.type,\n      \"run_state\": input.run_updated.run.state,\n      \"updated_at\": input.run_updated.run.updated_at,\n    },\n    \"method\": \"PUT\",\n    \"headers\": {\n      \"custom-header\": \"custom\",\n    },\n  }\n\n  input.run_updated.run.type == \"TRACKED\"\n}\n</code></pre> <p>Using custom webhook requests also makes it easy to integrate Spacelift with any third-party webhook consumer.</p>"},{"location":"concepts/policy/notification-policy.html#including-run-logs-in-webhook-requests","title":"Including run logs in webhook requests","text":"<p>You can include logs from various run phases in your webhook requests by using placeholders in any value field of the payload. Placeholders in JSON keys will be ignored:</p> <ul> <li><code>spacelift::logs::initializing</code> placeholder will be replaced with logs from the initializing phase.</li> <li><code>spacelift::logs::preparing</code> placeholder will be replaced with logs from the preparing phase.</li> <li><code>spacelift::logs::planning</code> placeholder will be replaced with logs from the planning phase.</li> <li><code>spacelift::logs::applying</code> placeholder will be replaced with logs from the applying phase.</li> </ul> <p>Here's an example that includes logs from different phases in the webhook payload:</p> <pre><code>package spacelift\n\nwebhook[wbdata] {\n  endpoint := input.webhook_endpoints[_]\n  endpoint.id == \"test\"\n  wbdata := {\n    \"endpoint_id\": endpoint.id,\n    \"payload\": {\n      \"initializing\": \"spacelift::logs::initializing\",\n      \"preparing\": \"You can embed the placeholder within text like this: spacelift::logs::preparing\",\n      \"planning_and_applying\": [\"The placeholders also work in lists:\", \"spacelift::logs::planning\", \"spacelift::logs::applying\"],\n    },\n    \"method\": \"PUT\",\n    \"headers\": {\n      \"custom-header\": \"custom\",\n    },\n  }\n\n  input.run_updated.run.type == \"TRACKED\"\n  input.run_updated.run.state == \"FINISHED\"\n}\n</code></pre>"},{"location":"concepts/policy/notification-policy.html#discord-integration-using-custom-webhook-requests","title":"Discord integration using custom webhook requests","text":"<p>Discord can be integrated to receive updates about Spacelift by creating a new webhook endpoint in your Discord server's integrations section and providing that as the endpoint when creating a new named webhook in Spacelift.</p> <p>Info</p> <p> For more information about making Discord webhooks, follow their official webhook guide.</p> <p>After creating the webhook on both Discord and Spacelift, you will need to define a new webhook rule like this:</p> <pre><code># Send updates about tracked runs to discord.\nwebhook[wbdata] {\n  endpoint := input.webhook_endpoints[_]\n  endpoint.id == \"YOUR_WEBHOOK_ID_HERE\"\n  stack := input.run_updated.stack\n  run := input.run_updated.run\n  wbdata := {\n    \"endpoint_id\": endpoint.id,\n    \"payload\": {\n      \"embeds\": [{\n        \"title\": \"Tracked run triggered!\",\n        \"description\": sprintf(\"Stack: [%s](http://example.app.spacelift.io/stack/%s)\\nRun ID: [%s](http://example.app.spacelift.io/stack/%s/run/%s)\\nRun state: %s\", [stack.name,stack.id,run.id,stack.id, run.id,run.state]),\n        }]\n     }\n  }\n  input.run_updated.run.type == \"TRACKED\"\n}\n</code></pre> <p>Once the rule is created, you should receive updates about tracked runs on your Discord server:</p> <p></p>"},{"location":"concepts/policy/notification-policy.html#pull-request-notifications","title":"Pull request notifications","text":"<p>With pull request notifications, you can target a single pull request as well as pull requests targeting a specific branch or commit.</p> <p>The pull request rule accepts multiple configurable parameters, all optional:</p> <ul> <li><code>id</code>: A pull request ID.</li> <li><code>commit</code>: The target commit SHA.</li> <li><code>branch</code>: The target branch.</li> <li><code>body</code>: A custom comment body.</li> <li><code>deduplication_key</code>: A deduplication key for updating PR comments.</li> </ul>"},{"location":"concepts/policy/notification-policy.html#creating-a-pr-comment","title":"Creating a PR comment","text":"<p>Here's a rule which will add a comment (containing a default body) to the pull request that triggered the run:</p> <pre><code>package spacelift\n\nimport future.keywords.contains\nimport future.keywords.if\n\npull_request contains {\"commit\": run.commit.hash} if {\n run := input.run_updated.run\n run.state == \"FINISHED\"\n}\n</code></pre> <p>Hint</p> <p>Pull request notifications work best in combination with a push policy to create proposed runs on pull requests.</p>"},{"location":"concepts/policy/notification-policy.html#updating-an-existing-pr-comment","title":"Updating an existing PR comment","text":"<p>Here's a rule to add a comment to the pull request that triggered the run and update that comment for every run state change, instead of creating new comments.</p> <pre><code>package spacelift\n\nimport future.keywords\n\npull_request contains {\n \"commit\": run.commit.hash,\n \"body\": sprintf(\"Run %s is %s\", [run.id, run.state]),\n \"deduplication_key\": deduplication_key,\n} if {\n run := input.run_updated.run\n deduplication_key := input.run_updated.stack.id\n}\n</code></pre> <p>The essential aspect for updating existing comments is the <code>deduplication_key</code>, which must have a constant value throughout the PR's lifetime to update the same comment. If the <code>deduplication_key</code> changes but a comment was created with the old deduplication_key, a new comment will be created and updated.</p> <p>The <code>deduplication_key</code> is associated with the PR using it, so using the same <code>deduplication_key</code> on different PRs is safe and will not cause collisions.</p> <p>To use this with existing policies, simply add <code>deduplication_key := input.run_updated.stack.id</code> (or another constant value) to the condition of the <code>pull_request</code> rule.</p>"},{"location":"concepts/policy/notification-policy.html#adding-a-comment-to-prs-targeting-a-specific-commit","title":"Adding a comment to PRs targeting a specific commit","text":"<p>Specify a target commit SHA using the <code>commit</code> parameter:</p> <pre><code>package spacelift\n\nimport future.keywords.contains\nimport future.keywords.if\n\npull_request contains {\n \"commit\": run.commit.hash,\n \"body\": sprintf(\"https://%s.app.spacelift.io/stack/%s/run/%s has finished\", [input.account.name, stack.id, run.id]),\n} if {\n stack := input.run_updated.stack\n run := input.run_updated.run\n run.state == \"FINISHED\"\n}\n</code></pre>"},{"location":"concepts/policy/notification-policy.html#adding-a-comment-to-prs-targeting-a-specific-branch","title":"Adding a comment to PRs targeting a specific branch","text":"<p>Provide the branch name using the <code>branch</code> parameter:</p> <pre><code>package spacelift\n\nimport future.keywords.contains\nimport future.keywords.if\n\npull_request contains {\n \"branch\": \"main\",\n \"body\": sprintf(\"https://%s.app.spacelift.io/stack/%s/run/%s has finished\", [input.account.name, stack.id, run.id]),\n} if {\n stack := input.run_updated.stack\n run := input.run_updated.run\n run.state == \"FINISHED\"\n}\n</code></pre> <p>Hint</p> <p><code>branch</code> is the base branch of the pull request. For example, if it's <code>\"branch\": input.run_updated.stack.branch</code>, the policy would comment into every pull request that targets the tracked branch of the stack.</p>"},{"location":"concepts/policy/notification-policy.html#changing-the-comment-body","title":"Changing the comment body","text":"<p>You can customize the comment body and even include logs from various run phases by including a corresponding placeholder:</p> <ul> <li><code>spacelift::logs::initializing</code> placeholder will be replaces with logs from the initializing phase.</li> <li><code>spacelift::logs::preparing</code> placeholder will be replaces with logs from the preparing phase.</li> <li><code>spacelift::logs::planning</code> placeholder will be replaced with logs from the planning phase.</li> <li><code>spacelift::logs::applying</code> placeholder will be replaced with logs from the applying phase.</li> </ul> <pre><code>package spacelift\n\nimport future.keywords.contains\nimport future.keywords.if\n\npull_request contains {\n \"commit\": run.commit.hash,\n \"body\": body,\n} if {\n stack := input.run_updated.stack\n run := input.run_updated.run\n run.state == \"FINISHED\"\n\n body := sprintf(\n  `https://%s.app.spacelift.io/stack/%s/run/%s has finished\n\n## Planning logs\n\n%s\nspacelift::logs::planning\n%s\n`,\n  [input.account.name, stack.id, run.id, \"```\", \"```\"],\n )\n}\n</code></pre>"},{"location":"concepts/policy/notification-policy.html#adding-a-comment-to-a-pr-about-changed-resources","title":"Adding a comment to a PR about changed resources","text":"<p>This more complex example will add a comment to a pull request where it will list all the resources that were added, changed, deleted, imported, moved, or forgotten.</p> <p>Note</p> <p>This is an example policy you can customize completely. If you would like to do so, add <code>sample := true</code> to the the policy and use the policy workbench to see what data is available. This example is also specifically for GitHub.</p> <pre><code>package spacelift\n\nimport future.keywords  # Import future syntax features.\n\n# Define a variable `header` to store a formatted string using the `sprintf` function.\n# This string will include dynamic information such as the URL for resource changes and\n# badges for different actions (add, change, destroy, import, move, forget) based on their counts.\nheader := sprintf(\"### Resource changes ([link](%s))\\n\\n![add](https://img.shields.io/badge/add-%d-brightgreen) ![change](https://img.shields.io/badge/change-%d-yellow) ![destroy](https://img.shields.io/badge/destroy-%d-red) ![import](https://img.shields.io/badge/import-%d-blue) ![move](https://img.shields.io/badge/move-%d-purple) ![forget](https://img.shields.io/badge/forget-%d-b07878) \\n\\n| Action | Resource | Changes |\\n| --- | --- | --- |\", [input.run_updated.urls.run, count(added), count(changed), count(deleted), count(imported), count(moved), count(forgotten)])\n\n# Create strings of resources by joining all resource items in respective categories with newline characters.\naddedresources := concat(\"\\n\", added)        # Join all 'added' resources with newline characters.\nchangedresources := concat(\"\\n\", changed)    # Join all 'changed' resources with newline characters.\ndeletedresources := concat(\"\\n\", deleted)    # Join all 'deleted' resources with newline characters.\nimportedresources := concat(\"\\n\", imported)  # Join all 'imported' resources with newline characters.\nmovedresources := concat(\"\\n\", moved)        # Join all 'moved' resources with newline characters.\nforgottenresources := concat(\"\\n\", forgotten) # Join all 'forgotten' resources with newline characters.\n\n# Define rules to populate the `added` collection with formatted rows when specific conditions are met.\nadded contains row if {\n    some x in input.run_updated.run.changes  # Iterate over each change in the run's updates.\n    row := sprintf(\"| Added | `%s` | &lt;details&gt;&lt;summary&gt;Value&lt;/summary&gt;`%s`&lt;/details&gt; |\", [x.entity.address, x.entity.data.values])  # Format a row for an 'added' resource.\n    x.action == \"added\"                     # Check if the action for the change is 'added'.\n    x.entity.entity_type == \"resource\"      # Ensure the entity type is a 'resource'.\n    not x.moved                             # Ensure the resource was not moved.\n}\n\n# Additional conditions to consider other types of \"added\" actions based on replacements.\nadded contains row if {\n    some x in input.run_updated.run.changes\n    row := sprintf(\"| Added | `%s` | &lt;details&gt;&lt;summary&gt;Value&lt;/summary&gt;`%s`&lt;/details&gt; |\", [x.entity.address, x.entity.data.values])\n    x.action == \"destroy-Before-create-replaced\"  # Check if the action is 'destroy-Before-create-replaced'.\n    x.entity.entity_type == \"resource\"\n    not x.moved\n}\n\nadded contains row if {\n    some x in input.run_updated.run.changes\n    row := sprintf(\"| Added | `%s` | &lt;details&gt;&lt;summary&gt;Value&lt;/summary&gt;`%s`&lt;/details&gt; |\", [x.entity.address, x.entity.data.values])\n    x.action == \"create-Before-destroy-replaced\"  # Check if the action is 'create-Before-destroy-replaced'.\n    x.entity.entity_type == \"resource\"\n    not x.moved\n}\n\n# Define a rule to populate the `changed` collection with formatted rows when specific conditions are met.\nchanged contains row if {\n    some x in input.run_updated.run.changes\n    row := sprintf(\"| Changed | `%s` | &lt;details&gt;&lt;summary&gt;New value&lt;/summary&gt;`%s`&lt;/details&gt; |\", [x.entity.address, x.entity.data.values])  # Format a row for a 'changed' resource.\n    x.entity.entity_type == \"resource\"      # Ensure the entity type is a 'resource'.\n    x.action == \"changed\"                   # Check if the action for the change is 'changed'.\n    not x.moved                             # Ensure the resource was not moved.\n}\n\n# Define rules to populate the `deleted` collection with formatted rows when specific conditions are met.\ndeleted contains row if {\n    some x in input.run_updated.run.changes\n    row := sprintf(\"| Deleted | `%s` | :x: |\", [x.entity.address])  # Format a row for a 'deleted' resource.\n    x.action == \"deleted\"                   # Check if the action for the change is 'deleted'.\n    x.entity.entity_type == \"resource\"      # Ensure the entity type is a 'resource'.\n    not x.moved                             # Ensure the resource was not moved.\n}\n\n# Additional conditions to consider other types of \"deleted\" actions based on replacements.\ndeleted contains row if {\n    some x in input.run_updated.run.changes\n    row := sprintf(\"| Deleted | `%s` | :x: |\", [x.entity.address])\n    x.action == \"destroy-Before-create-replaced\"  # Check if the action is 'destroy-Before-create-replaced'.\n    x.entity.entity_type == \"resource\"\n    not x.moved\n}\n\ndeleted contains row if {\n    some x in input.run_updated.run.changes\n    row := sprintf(\"| Deleted | `%s` | :x: |\", [x.entity.address])\n    x.action == \"create-Before-destroy-replaced\"  # Check if the action is 'create-Before-destroy-replaced'.\n    x.entity.entity_type == \"resource\"\n    not x.moved\n}\n\n# Define a rule to populate the `imported` collection with formatted rows when specific conditions are met.\nimported contains row if {\n    some x in input.run_updated.run.changes\n    row := sprintf(\"| Imported | `%s` | &lt;details&gt;&lt;summary&gt;New value&lt;/summary&gt;`%s`&lt;/details&gt; |\", [x.entity.address, x.entity.data.values])  # Format a row for an 'imported' resource.\n    x.action == \"import\"                    # Check if the action for the change is 'import'.\n    x.entity.entity_type == \"resource\"      # Ensure the entity type is a 'resource'.\n    not x.moved                             # Ensure the resource was not moved.\n}\n\n# Define a rule to populate the `moved` collection with formatted rows when specific conditions are met.\nmoved contains row if {\n    some x in input.run_updated.run.changes\n    row := sprintf(\"| Moved | `%s` | &lt;details&gt;&lt;summary&gt;New value&lt;/summary&gt;`%s`&lt;/details&gt; |\", [x.entity.address, x.entity.data.values])  # Format a row for a 'moved' resource.\n    x.entity.entity_type == \"resource\"      # Ensure the entity type is a 'resource'.\n    x.moved                                 # Check if the resource was moved.\n}\n\n# Define a rule to populate the `forgotten` collection with formatted rows when specific conditions are met.\nforgotten contains row if {\n    some x in input.run_updated.run.changes\n    row := sprintf(\"| Forgotten | `%s` | :x: |\", [x.entity.address])  # Format a row for a 'forgotten' resource.\n    x.action == \"forget\"                    # Check if the action for the change is 'forget'.\n    x.entity.entity_type == \"resource\"      # Ensure the entity type is a 'resource'.\n    not x.moved                             # Ensure the resource was not moved.\n}\n\n# Define a rule to create a pull request object if certain conditions are met.\npull_request contains {\n \"commit\": input.run_updated.run.commit.hash,\n \"body\": replace(replace(concat(\"\\n\", [header, addedresources, changedresources, deletedresources, importedresources, movedresources, forgottenresources]), \"\\n\\n\\n\", \"\\n\"), \"\\n\\n\", \"\\n\"),\n } if {\n    input.run_updated.run.state == \"FINISHED\"  # Ensure the run state is 'FINISHED'.\n    input.run_updated.run.type == \"PROPOSED\"   # Ensure the run type is 'PROPOSED'.\n}\n</code></pre> <p>The output will look like this:</p> <p></p>"},{"location":"concepts/policy/policy-flag-reset.html","title":"Policy Flag Reset","text":""},{"location":"concepts/policy/policy-flag-reset.html#policy-flag-reset","title":"Policy Flag Reset","text":"<p>Policies can now reset flags they have previously set using the <code>reset_flag</code> rule.</p>"},{"location":"concepts/policy/policy-flag-reset.html#basic-usage","title":"Basic usage","text":"<pre><code>package spacelift\n\n# Set a flag when creating S3 buckets\nflag[\"s3-review\"] {\n    input.terraform.resource_changes[_].type == \"aws_s3_bucket\"\n}\n\n# Reset the flag when no S3 buckets are being created\nreset_flag[\"s3-review\"] {\n    not input.terraform.resource_changes[_].type == \"aws_s3_bucket\"\n}\n</code></pre>"},{"location":"concepts/policy/policy-flag-reset.html#multi-owner-security","title":"Multi-owner security","text":"<ul> <li>Multiple policies can own the same flag when they all set it</li> <li>ALL owners must agree to reset a flag for it to be removed</li> <li>Prevents malicious policies from hijacking flags set by other policies</li> </ul>"},{"location":"concepts/policy/policy-flag-reset.html#processing-order","title":"Processing order","text":"<p>Within each policy evaluation:</p> <ol> <li>Reset flags are processed first</li> <li>Add flags are processed second</li> </ol>"},{"location":"concepts/policy/policy-flag-reset.html#legacy-policies","title":"Legacy policies","text":"<p>Policies without PolicyULID can set flags but cannot reset any flags.</p>"},{"location":"concepts/policy/policy-flag-reset.html#use-with-targeted-replans","title":"Use with targeted replans","text":"<p>Flag reset is particularly useful during targeted replans, where plan and approval policies can be evaluated multiple times within the same run as the plan changes. Policies can dynamically set and reset flags based on the current plan content, allowing for adaptive approval workflows that respond to the actual changes being deployed.</p>"},{"location":"concepts/policy/policy-flag-reset.html#supported-policy-types","title":"Supported policy types","text":"<p>Available in push, approval, plan, and trigger policies.</p>"},{"location":"concepts/policy/run-initialization-policy.html","title":"Initialization policy","text":""},{"location":"concepts/policy/run-initialization-policy.html#initialization-policy","title":"Initialization policy","text":"<p>Warning</p> <p>This feature is deprecated. New users should not use this feature and existing users are encouraged to migrate to the approval policy, which offers a much more flexible and powerful way to control which runs are allowed to proceed. A migration guide is available here.</p>"},{"location":"concepts/policy/run-initialization-policy.html#purpose","title":"Purpose","text":"<p>Initialization policy can prevent a Run or a Task from being initialized, thus blocking any custom code or commands from being executed. It superficially looks like a plan policy in that it affects an existing Run and prints feedback to logs, but it does not get access to the plan. Instead, it can be used to protect your stack from unwanted changes or enforce organizational rules concerning how and when runs are supposed to be triggered.</p> <p>Warning</p> <p>Server-side initialization policies are being deprecated. We will be replacing them with worker-side policies that can be set by using the launcher run initialization policy flag (<code>SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY</code>).</p> <p>For a limited time period we will be running both types of initialization policy checks but ultimately we're planning to move the pre-flight checks to the worker node, thus allowing customers to block suspicious looking jobs on their end.</p> <p>Let's create a simple initialization policy, attach it to the stack, and see what gives:</p> <pre><code>package spacelift\n\ndeny[\"you shall not pass\"] {\n  true\n}\n</code></pre> <p>...and boom:</p> <p></p>"},{"location":"concepts/policy/run-initialization-policy.html#rules","title":"Rules","text":"<p>Initialization policies are simple in that they only use a single rule - deny - with a string message. A single result for that rule will fail the run before it has a chance to start - as we've just witnessed above.</p>"},{"location":"concepts/policy/run-initialization-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request will receive:</p> <pre><code>{\n  \"commit\": {\n    \"author\": \"string - GitHub login if available, name otherwise\",\n    \"branch\": \"string - branch to which the commit was pushed\",\n    \"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n    \"message\": \"string - commit message\"\n  },\n  \"request\": {\n    \"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n  },\n  \"run\": {\n    \"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n    \"changes\": [\n      {\n        \"action\": \"string enum - added | changed | deleted\",\n        \"entity\": {\n          \"address\": \"string - full address of the entity\",\n          \"name\": \"string - name of the entity\",\n          \"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n          \"entity_vendor\": \"string - the name of the vendor\",\n          \"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n          \"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n        },\n        \"phase\": \"string enum - plan | apply\"\n      }\n    ],\n    \"commit\": {\n      \"author\": \"string - GitHub login if available, name otherwise\",\n      \"branch\": \"string - branch to which the commit was pushed\",\n      \"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n      \"hash\": \"string - the commit hash\",\n      \"message\": \"string - commit message\"\n    },\n    \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n    \"flags\" : [\"string - list of flags set on the run by other policies\" ],\n    \"id\": \"string - the run ID\",\n    \"runtime_config\": {\n      \"before_init\": [\"string - command to run before run initialization\"],\n      \"project_root\": \"string - root of the Terraform project\",\n      \"runner_image\": \"string - Docker image used to execute the run\",\n      \"terraform_version\": \"string - Terraform version used to for the run\"\n    },\n    \"state\": \"string - the current run state\",\n    \"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n    \"type\": \"string - PROPOSED or TRACKED\",\n    \"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n    \"user_provided_metadata\": [\n      \"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n    ]\n  },\n  \"stack\": {\n    \"administrative\": \"boolean - is the stack administrative\",\n    \"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n    \"branch\": \"string - tracked branch of the stack\",\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n    \"name\": \"string - name of the stack\",\n    \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n    \"project_root\": \"optional string - project root as set on the Stack, if any\",\n    \"repository\": \"string - name of the source GitHub repository\",\n    \"state\": \"string - current state of the stack\",\n    \"terraform_version\": \"string or null - last Terraform version used to apply changes\"\n  }\n}\n</code></pre>"},{"location":"concepts/policy/run-initialization-policy.html#aliases","title":"Aliases","text":"<p>In addition to our helper functions, we provide aliases for commonly used parts of the input data:</p> Alias Source <code>commit</code> <code>input.commit</code> <code>run</code> <code>input.run</code> <code>runtime_config</code> <code>input.run.runtime_config</code> <code>stack</code> <code>input.stack</code>"},{"location":"concepts/policy/run-initialization-policy.html#use-cases","title":"Use cases","text":"<p>There are two main use cases for run initialization policies - protecting your stack from unwanted changes and enforcing organizational rules. Let's look at these one by one.</p>"},{"location":"concepts/policy/run-initialization-policy.html#protect-your-stack-from-unwanted-changes","title":"Protect your stack from unwanted changes","text":"<p>While specialized, Spacelift is still a CI/CD platform and thus allows running custom code before Terraform initialization phase using <code>before_init</code>scripts. This is a very powerful feature, but as always, with great power comes great responsibility. Since those scripts get full access to your Terraform environment, how hard is it to create a commit on a feature branch that would run <code>terraform destroy -auto-approve</code>? Sure, all Spacelift runs are tracked and this prank will sooner or later be tracked down to the individual who ran it, but at that point do you still have a business?</p> <p>That's where initialization policies can help. Let's explicitly blacklist all Terraform commands if they're running as <code>before_init</code> scripts. OK, let's maybe add a single exception for a formatting check.</p> <pre><code>package spacelift\n\ndeny[sprintf(\"don't use Terraform please (%s)\", [command])] {\n  command := input.run.runtime_config.before_init[_]\n\n  contains(command, \"terraform\")\n  command != \"terraform fmt -check\"\n}\n</code></pre> <p>Feel free to play with this example in the Rego playground.</p> <p>OK, but what if someone gets clever and creates a Docker image that symlinks something very innocent-looking to <code>terraform</code>? Well, you have two choices - you could replace a blacklist with a whitelist, but a clever attacker can be really clever. So the other choice is to make sure that a known good Docker is used to execute the run. Here's an example:</p> <pre><code>package spacelift\n\ndeny[sprintf(\"unexpected runner image (%s)\", [image])] {\n  image := input.run.runtime_config.runner_image\n\n  image != \"spacelift/runner:latest\"\n}\n</code></pre> <p>Here's the above example in the Rego playground.</p> <p>Danger</p> <p>Obviously, if you're using an image other than what we control, you still have to ensure that the attacker can't push bad code to your Docker repo. Alas, this is beyond our control.</p>"},{"location":"concepts/policy/run-initialization-policy.html#enforce-organizational-rules","title":"Enforce organizational rules","text":"<p>While the previous section was all about making sure that bad stuff does not get executed, this use case presents run initialization policies as a way to ensure best practices - ensuring that the right things get executed the right way and at the right time.</p> <p>One of the above examples explicitly whitelisted OpenTofu/Terraform formatting check. Keeping your code formatted in a standard way is generally a good idea, so let's make sure that this command always gets executed first. Note that as per Anna Karenina principle this check is most elegantly defined as a negation of another rule matching the required state of affairs:</p> <pre><code>package spacelift\n\ndeny[\"please always run formatting check first\"] {\n  not formatting_first\n}\n\nformatting_first {\n  input.run.runtime_config.before_init[i] == \"terraform fmt -check\"\n  i == 0\n}\n</code></pre> <p>Here's this example in the Rego playground.</p> <p>This time we'll skip the mandatory \"don't deploy on weekends\" check because while it could also be implemented here, there are probably better places to do it. Instead, let's enforce a feature branch naming convention. We'll keep this example simple, requiring that feature branches start with either <code>feature/</code> or <code>fix/</code>, but you can go fancy and require references to Jira tickets or even look at commit messages:</p> <pre><code>package spacelift\n\ndeny[sprintf(\"invalid feature branch name (%s)\", [branch])] {\n  branch := input.commit.branch\n\n  input.run.type == \"PROPOSED\"\n  not re_match(\"^(fix|feature)\\/.*\", branch)\n}\n</code></pre> <p>Here's this example in the Rego playground.</p>"},{"location":"concepts/policy/run-initialization-policy.html#migration-guide","title":"Migration guide","text":"<p>A run initialization policy can be expressed as an approval policy if it defines a single <code>reject</code> rule, and an <code>approve</code> rule that is its negation. Below you will find equivalents of the examples above expressed as approval policies.</p>"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-enforcing-opentofuterraform-check","title":"Migration example: enforcing OpenTofu/Terraform check","text":"<pre><code>package spacelift\n\nreject { not formatting_first}\n\napprove { not reject }\n\nformatting_first {\n  input.run.runtime_config.before_init[i] == \"terraform fmt -check\"\n  i == 0\n}\n</code></pre>"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-disallowing-before-init-opentofuterraform-commands-other-than-formatting","title":"Migration example: disallowing before-init OpenTofu/Terraform commands other than formatting","text":"<pre><code>package spacelift\n\nreject {\n  command := input.run.runtime_config.before_init[_]\n  contains(command, \"terraform\"); command != \"terraform fmt -check\"\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-enforcing-runner-image","title":"Migration example: enforcing runner image","text":"<pre><code>package spacelift\n\nreject {\n  input.run.runtime_config.runner_image != \"spacelift/runner:latest\"\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-enforcing-feature-branch-naming-convention","title":"Migration example: enforcing feature branch naming convention","text":"<pre><code>package spacelift\n\nreject {\n  branch := input.run.commit.branch\n  input.run.type == \"PROPOSED\"\n  not re_match(\"^(fix|feature)\\/.*\", branch)\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/stack-access-policy.html","title":"Access policy","text":""},{"location":"concepts/policy/stack-access-policy.html#access-policy","title":"Access policy","text":"<p>Danger</p> <p>Access policies are deprecated in favour of Space access rules in the login policy. See Spaces for more details.</p>"},{"location":"concepts/policy/stack-access-policy.html#purpose","title":"Purpose","text":"<p>By default, non-admin users have no access to any Stacks or Modules and must be granted that explicitly. There are two levels of non-admin access - reader and writer, and the exact meaning of these roles is covered in a separate section. For now all we need to care about is that access policies are what we use to give appropriate level of access to individual stacks to non-admin users in your account.</p> <p>This type of access control is typically done either by building a separate user management system on your end or piggy-backing on one created by your identity provider. Both solutions have their limitations - a separate user management system makes it more difficult for organizations to onboard and offboard users, and the last thing we want is for a guy that was just fired to log in to Spacelift and have their revenge. User management systems are also pretty difficult to get right, too, especially if granular and sophisticated access controls are required.</p> <p>Piggy-backing on the identity provider is probably a safer bet and is used by many DevTools vendors. With this approach, having some access level to a GitHub repo would give you the same access level to all Spacelift stacks and/or modules associated with it. That's somewhat reasonable, but not as flexible as having your own fancy user management system. Imagine having two stacks linked to one repo, representing two environments - staging and production. It's quite possible that you'd appreciate separate access controls for these two.</p> <p>Access policies offer the best of both worlds - they give you a tool to build your own access management system using data obtained either from our identity provider (GitHub), or from your identity provider if using Single Sign-On integration. In subsequent sections we'll dive deeper into what data is exposed to your policies, how you can define access policies with different levels of access, and what those levels actually mean.</p>"},{"location":"concepts/policy/stack-access-policy.html#rules","title":"Rules","text":"<p>Your access policy can define the following boolean rules:</p> <ul> <li>write: gives the current user write access to the stack or module;</li> <li>read: gives the current user read access to the stack or module;</li> <li>deny: denies the current user all access to the stack or module, no matter the outcome of other rules;</li> <li>deny_write:  denies the current user write access to the stack or module, no matter the outcome of other rules;</li> </ul> <p>Note that write access automatically assumes read permissions, too, so there's no need to define separate read policies for writers.</p> <p>Another thing to keep in mind when defining access policies is that they are executed quickly. Internally, we expect that running all access policies on all the stacks in one request (<code>stacks</code> in the GraphQL API) will take less than 500 milliseconds - otherwise the request fails. That's actually plenty for modern computers, but think twice before creating fancy regex rules in your access policies.</p>"},{"location":"concepts/policy/stack-access-policy.html#readers-and-writers","title":"Readers and writers","text":"<p>There are two levels of non-admin access to a Spacelift stack or module - reader and writer. These are pretty intuitive for most developers, but this section will cover them in more detail to avoid any possible confusion. But first, let's try to understand the use case for different levels of access.</p> <p>In every non-trivial organization there will be different roles - folks who build and manage shared infrastructure, folks who build and manage their team or project-level infrastructure, and folks who use this infrastructure to build great things. The first group is probably the people who manage your Spacelift accounts - the admins. They need to be able to set up everything - create stacks, contexts and policies, and attach them accordingly. You'd normally use login policies to manage their access.</p> <p>The second group - folks who manage their team or project-level infrastructure - should have a reasonable level of access to their project. They should be able to define the environment, set up various integrations, trigger and confirm runs, execute tasks. This level of access is granted by the writer permission. However, writers should still operate within the boundaries defined by admins, who do that mainly by attaching contexts and policies to the stacks.</p> <p>Last but not least the third group - folks who build things on top of existing infra - don't necessarily need to define the infra, but they need to understand what's available and when things are changing. You'll probably want to allow them to contribute to infra definitions, too, and allow them to see feedback from proposed runs. They can't do anything, but they can see everything. These are the readers. Most modern organizations tend to provide this level of access to as many stakeholders as possible to maintain transparency and facilitate collaboration.</p>"},{"location":"concepts/policy/stack-access-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request will receive:</p> <p>Official Schema Reference</p> <p>For the most up-to-date and complete schema definition, please refer to the official Spacelift policy contract schema under the <code>ACCESS</code> policy type.</p> <pre><code>{\n  \"request\": {\n    \"remote_ip\": \"string - IP of the user making a request\",\n    \"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n  },\n  \"session\": {\n    \"admin\": \"boolean - is the current user a Spacelift admin\",\n    \"creator_ip\": \"string - IP address of the user who created the session\",\n    \"login\": \"string - GitHub username of the logged in user\",\n    \"name\":  \"string - full name of the logged in GitHub user - may be empty\",\n    \"teams\": [\"string - names of org teams the user is a member of\"],\n    \"machine\": \"boolean - whether the creator is a machine or a user\"\n  },\n  \"stack\": { // when access to a stack is being evaluated\n    \"id\": \"string - unique ID of the stack\",\n    \"administrative\": \"boolean - is the stack administrative\",\n    \"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n    \"branch\": \"string - tracked branch of the stack\",\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n    \"name\": \"string - name of the stack\",\n    \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n    \"project_root\": \"optional string - project root as set on the Stack, if any\",\n    \"repository\": \"string - name of the source repository\",\n    \"state\": \"string - current state of the stack\",\n    \"terraform_version\": \"string or null - last Terraform version used to apply changes\"\n  },\n  \"module\": { // when access to a module is being evaluated\n    \"id\": \"string - unique ID of the module\",\n    \"administrative\": \"boolean - is the stack administrative\",\n    \"branch\": \"string - tracked branch of the module\",\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n    \"repository\": \"string - name of the source repository\",\n    \"terraform_provider\": \"string - name of the main Terraform provider used by the module\"\n  }\n}\n</code></pre>"},{"location":"concepts/policy/stack-access-policy.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p> <p>With all the above theory in mind, let's jump straight to the code and define some access policies. This section will cover some common examples that can be copied more or less directly, and some contrived ones to serve as an inspiration.</p> <p>Info</p> <p>Remember that access policies must be attached to a stack or a module to take effect.</p>"},{"location":"concepts/policy/stack-access-policy.html#read-access-to-everyone-in-engineering","title":"Read access to everyone (in Engineering)","text":"<p>I get read access, you get read access, everyone gets read access. As long as they're members of the Engineering team:</p> <pre><code>package spacelift\n\nread { input.session.teams[_] == \"Engineering\" }\n</code></pre> <p>OK, that was simple. But let's also see it in the Rego playground.</p>"},{"location":"concepts/policy/stack-access-policy.html#in-case-things-go-wrong-we-want-you-to-be-there","title":"In case things go wrong, we want you to be there","text":"<p>You know when things go wrong it's usually because someone did something. Like an infra deployment. Let's try to make sure they're in the office when doing so and restrict write access to business hours and office IP range. This policy is best combined with one that gives read access.</p> <pre><code>package spacelift\n\nnow     := input.request.timestamp_ns\nclock   := time.clock([now, \"America/Los_Angeles\"])\nweekend := { \"Saturday\", \"Sunday\" }\nweekday := time.weekday(now)\nip      := input.request.remote_ip\n\nwrite      { input.session.teams[_] == \"Product team\" }\ndeny_write { weekend[weekday] }\ndeny_write { clock[0] &lt; 9 }\ndeny_write { clock[0] &gt; 17 }\ndeny_write { not net.cidr_contains(\"12.34.56.0/24\", ip) }\n</code></pre> <p>Here is this example in Rego playground.</p>"},{"location":"concepts/policy/stack-access-policy.html#protect-administrative-stacks","title":"Protect administrative stacks","text":"<p>Administrative stacks are powerful - getting write access to one is almost as good as being an admin - you can define and attach contexts and policies. So let's deny write access to them entirely. This works since access policies are not evaluated for admin users.</p> <pre><code>package spacelift\n\ndeny_write { input.stack.administrative }\n</code></pre> <p>And here's the necessary Rego playground example.</p>"},{"location":"concepts/policy/task-run-policy.html","title":"Task policy","text":""},{"location":"concepts/policy/task-run-policy.html#task-policy","title":"Task policy","text":"<p>Warning</p> <p>This feature is deprecated. New users should not use this feature and existing users are encouraged to migrate to the approval policy, which offers a much more flexible and powerful way to control which tasks are allowed to proceed. A migration guide is available here.</p>"},{"location":"concepts/policy/task-run-policy.html#purpose","title":"Purpose","text":"<p>Spacelift tasks are a handy feature that allows an arbitrary command to be executed within the context of your fully initialized stack. This feature is designed to make running one-off administrative tasks (eg. resource tainting) safer and more convenient. It can also be an attack vector allowing evil people to do bad things, or simply a footgun allowing well-meaning people to err in a spectacular way.</p> <p>Enter task policies. The sole purpose of task policies is to prevent certain commands from being executed, to prevent certain groups or individuals from executing any commands, or to prevent certain commands from being executed by certain groups or individuals.</p> <p>Info</p> <p>Preventing admins from running tasks using policies can only play an advisory role and should not be considered a safety measure. A bad actor with admin privileges can detach a policy from the stack and run whatever they want. Choose your admins wisely.</p> <p>Task policies are simple in that they only use a single rule - deny - with a string message. A single match for that rule will prevent a run from being created, with an appropriate API error. Let's see how that works in practice by defining a simple rule and attaching it to a stack:</p> <pre><code>package spacelift\n\ndeny[\"not in my town, you don't\"] { true }\n</code></pre> <p>And here's the outcome when trying to run a task:</p> <p></p>"},{"location":"concepts/policy/task-run-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request will receive:</p> <p>Official Schema Reference</p> <p>For the most up-to-date and complete schema definition, please refer to the official Spacelift policy contract schema under the <code>TASK</code> policy type.</p> <pre><code>{\n  \"request\": {\n    \"command\": \"string - command that the user is trying to execute as task\",\n    \"remote_ip\": \"string - IP of the user trying to log in\",\n    \"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n  },\n  \"session\": {\n    \"admin\": \"boolean - is the current user a Spacelift admin\",\n    \"creator_ip\": \"string - IP address of the user who created the session\",\n    \"login\": \"string - GitHub username of the current user\",\n    \"name\": \"string - full name of the current user\",\n    \"teams\": [\"string - names of org teams the current user is a member of\"],\n    \"machine\": \"boolean - whether the creator is a machine or a user\"\n  },\n  \"stack\": {\n    \"administrative\": \"boolean - is the stack administrative\",\n    \"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n    \"branch\": \"string - tracked branch of the stack\",\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n    \"name\": \"string - name of the stack\",\n    \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n    \"project_root\": \"optional string - project root as set on the Stack, if any\",\n    \"repository\": \"string - name of the source GitHub repository\",\n    \"state\": \"string - current state of the stack\",\n    \"terraform_version\": \"string or null - last Terraform version used to apply changes\"\n  }\n}\n</code></pre>"},{"location":"concepts/policy/task-run-policy.html#aliases","title":"Aliases","text":"<p>In addition to our helper functions, we provide aliases for commonly used parts of the input data:</p> Alias Source <code>request</code> <code>input.request</code> <code>session</code> <code>input.session</code> <code>stack</code> <code>input.stack</code>"},{"location":"concepts/policy/task-run-policy.html#examples","title":"Examples","text":"<p>Let's have a look at a few examples to see what you can accomplish with task policies. You've seen one example already - disabling tasks entirely. That's perhaps both heavy-handed and naive given that admins can detach the policy if needed. So let's only block non-admins from running tasks:</p> <pre><code>package spacelift\n\ndeny[\"only admins can run tasks\"] { not input.session.admin }\n</code></pre> <p>Let's look at an example of this simple policy in the Rego playground.</p> <p>That's still pretty harsh. We could possibly allow writers to run some commands we consider safe - like resource tainting and untainting. Let's try then, and please excuse the regex:</p> <pre><code>package spacelift\n\ndeny[sprintf(\"command not allowed (%s)\", [command])] {\n  command := input.request.command\n\n  not input.session.admin\n  not regex.match(\"^terraform\\\\s(un)?taint\\\\s[\\\\w\\\\-\\\\.]*$\", command)\n}\n</code></pre> <p>Feel free to play with the above example in the Rego playground.</p> <p>If you want to keep whitelisting different commands, it may be more elegant to flip the rule logic, create a series of allowed rules, and define one deny rule as <code>not allowed</code>. Let's have a look at this approach, and while we're at it let's remind everyone not to run anything during the weekend:</p> <pre><code>package spacelift\n\ncommand := input.request.command\n\ndeny[sprintf(\"command not allowed (%s)\", [command])] { not allowed }\n\ndeny[\"no tasks on weekends\"] {\n  today   := time.weekday(input.request.timestamp_ns)\n  weekend := { \"Saturday\", \"Sunday\" }\n\n  weekend[today]\n}\n\nallowed { input.session.admin }\nallowed { regex.match(\"^terraform\\\\s(un)?taint\\\\s[\\\\w\\\\-\\\\.]*$\", command) }\nallowed { regex.match(\"^terraform\\\\simport\\\\s[\\\\w\\\\-\\\\.]*$\", command) }\n</code></pre> <p>As usual, this example is available to play around with.</p>"},{"location":"concepts/policy/task-run-policy.html#migration-guide","title":"Migration guide","text":"<p>A task policy can be expressed as an approval policy if it defines a single <code>reject</code> rule, and an <code>approve</code> rule that is its negation. Below you will find equivalents of the examples above expressed as approval policies.</p>"},{"location":"concepts/policy/task-run-policy.html#migration-example-only-allow-opentofuterraform-taint-and-untaint","title":"Migration example: only allow OpenTofu/Terraform taint and untaint","text":"<pre><code>package spacelift\n\nreject {\n  command := input.run.command\n  not regex.match(\"^terraform\\\\s(un)?taint\\\\s[\\\\w\\\\-\\\\.]*$\", command)\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/task-run-policy.html#migration-example-no-tasks-on-weekends","title":"Migration example: no tasks on weekends","text":"<pre><code>package spacelift\n\nreject {\n  today   := time.weekday(input.run.created_at)\n  weekend := { \"Saturday\", \"Sunday\" }\n\n  weekend[today]\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html","title":"Plan policy","text":""},{"location":"concepts/policy/terraform-plan-policy.html#plan-policy","title":"Plan policy","text":""},{"location":"concepts/policy/terraform-plan-policy.html#purpose","title":"Purpose","text":"<p>Plan policies are evaluated during a planning phase after a vendor-specific change preview command (e.g. <code>terraform plan</code>) executes successfully. The body of the change is exported to JSON and parts of it are combined with Spacelift metadata to form the data input to the policy.</p> <p>Plan policies are the only ones with access to the actual changes to the managed resources, making them the best place to enforce organizational rules and best practices as well as do automated code review.</p> <p>There are two types of rules here that Spacelift will care about: deny and warn. Each of them must come with an appropriate message that will be shown in the logs.</p> <ul> <li>Deny rules: Print in red. Automatically fail the run.</li> <li>Warn rules: Print in yellow. At most, mark the run for human review if the change affects the tracked branch and the stack is set to autodeploy.</li> </ul> <p>This simple policy will show both types of rules in action:</p> <pre><code>package spacelift\n\ndeny[\"you shall not pass\"] {\n  true # true means \"match everything\"\n}\n\nwarn[\"hey, you look suspicious\"] {\n  true\n}\n</code></pre> <p>If you create this policy, attach it to a stack, and trigger a run, you will see something like this:</p> <p></p> <p>It works, but it's not terribly useful unless you want to block all changes to your stack in a really clumsy way.</p> <p>Let's dig deeper into the data input document that each plan policy receives, two possible use cases (rule enforcement and automated code review) and some examples.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#data-input","title":"Data input","text":"<p>This is the data input schema each policy request will receive. If the policy is executed for the first time, the <code>previous_run</code> field will be missing.</p> <p>Official Schema Reference</p> <p>For the most up-to-date and complete schema definition, please refer to the official Spacelift policy contract schema under the <code>PLAN</code> policy type.</p> <pre><code>{\n  \"spacelift\": {\n    \"commit\": {\n      \"author\": \"string - GitHub login if available, name otherwise\",\n      \"branch\": \"string - branch to which the commit was pushed\",\n      \"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n      \"hash\": \"string - the commit hash\",\n      \"message\": \"string - commit message\"\n    },\n    \"request\": {\n      \"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n    },\n    \"previous_run\": {\n      \"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n      \"branch\": \"string - the branch the run was triggered from\",\n      \"changes\": [\n        {\n          \"action\": \"string enum - added | changed | deleted\",\n          \"entity\": {\n            \"address\": \"string - full address of the entity\",\n            \"name\": \"string - name of the entity\",\n            \"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n            \"entity_vendor\": \"string - the name of the vendor\",\n            \"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n            \"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n          },\n          \"phase\": \"string enum - plan | apply\"\n        }\n      ],\n      \"commit\": {\n        \"author\": \"string - GitHub login if available, name otherwise\",\n        \"branch\": \"string - branch to which the commit was pushed\",\n        \"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n        \"hash\": \"string - the commit hash\",\n        \"message\": \"string - commit message\"\n      },\n      \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n      \"drift_detection\": \"boolean - is this a drift detection run\",\n      \"flags\" : [\"string - list of flags set on the run by other policies\" ],\n      \"id\": \"string - the run ID\",\n      \"runtime_config\": {\n        \"before_init\": [\"string - command to run before run initialization\"],\n        \"project_root\": \"string - root of the Terraform project\",\n        \"runner_image\": \"string - Docker image used to execute the run\",\n        \"terraform_version\": \"string - Terraform version used to for the run\"\n      },\n      \"state\": \"string - the current run state\",\n      \"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n      \"type\": \"string - type of the run\",\n      \"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n      \"user_provided_metadata\": [\n        \"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n      ]\n    },\n    \"run\": {\n      \"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n      \"branch\": \"string - the branch the run was triggered from\",\n      \"changes\": [\n        {\n          \"action\": \"string enum - added | changed | deleted\",\n          \"entity\": {\n            \"address\": \"string - full address of the entity\",\n            \"name\": \"string - name of the entity\",\n            \"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n            \"entity_vendor\": \"string - the name of the vendor\",\n            \"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n            \"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n          },\n          \"phase\": \"string enum - plan | apply\"\n        }\n      ],\n      \"commit\": {\n        \"author\": \"string - GitHub login if available, name otherwise\",\n        \"branch\": \"string - branch to which the commit was pushed\",\n        \"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n        \"hash\": \"string - the commit hash\",\n        \"message\": \"string - commit message\"\n      },\n      \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n      \"drift_detection\": \"boolean - is this a drift detection run\",\n      \"flags\" : [\"string - list of flags set on the run by other policies\" ],\n      \"id\": \"string - the run ID\",\n      \"runtime_config\": {\n        \"before_init\": [\"string - command to run before run initialization\"],\n        \"project_root\": \"string - root of the Terraform project\",\n        \"runner_image\": \"string - Docker image used to execute the run\",\n        \"terraform_version\": \"string - Terraform version used to for the run\"\n      },\n      \"state\": \"string - the current run state\",\n      \"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n      \"type\": \"string - type of the run\",\n      \"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n      \"user_provided_metadata\": [\n        \"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n      ]\n    },\n    \"stack\": {\n      \"administrative\": \"boolean - is the stack administrative\",\n      \"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n      \"branch\": \"string - tracked branch of the stack\",\n      \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n      \"name\": \"string - name of the stack\",\n      \"repository\": \"string - name of the source GitHub repository\",\n      \"state\": \"string - current state of the stack\",\n      \"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n      \"tracked_commit\": {\n        \"author\": \"string - GitHub login if available, name otherwise\",\n        \"branch\": \"string - branch to which the commit was pushed\",\n        \"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n        \"hash\": \"string - the commit hash\",\n        \"message\": \"string - commit message\"\n      },\n      \"worker_pool\": {\n        \"id\": \"string - the worker pool ID, if it is private\",\n        \"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n        \"name\": \"string - name of the worker pool, if it is private\",\n        \"public\": \"boolean - is the worker pool public\"\n      }\n    }\n  },\n  \"terraform\": {\n    \"resource_changes\": [\n      {\n        \"address\": \"string - full address of the resource, including modules\",\n        \"type\": \"string - type of the resource, eg. aws_iam_user\",\n        \"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n        \"name\": \"string - name of the resource, without type\",\n        \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n        \"project_root\": \"optional string - project root as set on the Stack, if any\",\n        \"provider_name\": \"string - provider managing the resource, eg. aws\",\n        \"change\": {\n          \"actions\": [\"string - create, update, delete or no-op\"],\n          \"before\": \"optional object - content of the resource\",\n          \"after\": \"optional object - content of the resource\"\n        }\n      }\n    ],\n    \"terraform_version\": \"string\"\n  }\n}\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html#aliases","title":"Aliases","text":"<p>In addition to our helper functions, we provide aliases for commonly used parts of the input data:</p> Alias Description <code>affected_resources</code> List of the resources that will be created, deleted, and updated by Terraform <code>created_resources</code> List of the resources that will be created by Terraform <code>deleted_resources</code> List of the resources that will be deleted by Terraform <code>recreated_resources</code> List of the resources that will be deleted and then created by Terraform <code>updated_resources</code> List of the resources that will be updated by Terraform"},{"location":"concepts/policy/terraform-plan-policy.html#string-sanitization","title":"String sanitization","text":"<p>Sensitive properties in <code>\"before\"</code> and <code>\"after\"</code> objects will be sanitized to protect secret values. Sanitization hashes the value with the sha256 algorithm and takes the last 8 bytes of the hash.</p> <p>If you need to compare a string property to a constant, use the <code>sanitized(string)</code> helper function.</p> <pre><code>package spacelift\n\ndeny[\"must not target the forbidden endpoint: forbidden.endpoint/webhook\"] {\n  resource := input.terraform.resource_changes[_]\n\n  actions := {\"create\", \"delete\", \"update\"}\n  actions[resource.change.actions[_]]\n\n  resource.change.after.endpoint == sanitized(\"forbidden.endpoint/webhook\")\n}\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html#custom-inputs","title":"Custom inputs","text":"<p>Sometimes you need to pass some additional data to your policy input. For example, you may want to pass the result of a third-party API to tool call to the <code>configuration</code> data from the Terraform plan. To use custom inputs:</p> <ol> <li>Generate a JSON file with the data you need at the root of your project.</li> <li>The file name must follow the pattern <code>$key.custom.spacelift.json</code> and represent a valid JSON object.<ul> <li>The file name is case-sensitive.</li> </ul> </li> <li>The object will be merged with the rest of the input data, as <code>input.third_party_metadata.custom.$key</code>.</li> </ol> <p>Below are two examples, one exposing Terraform configuration and the other exposing the result of a third-party security tool.</p> <p>Tip</p> <p>To learn more about integrating security tools with Spacelift using custom inputs, refer to our blog post.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#exposing-terraform-configuration-to-the-plan-policy","title":"Exposing Terraform configuration to the plan policy","text":"<p>To expose the Terraform configuration to the plan policy to ensure that only the \"blessed\" modules are used to provision resources, add this command to the list of <code>after_plan</code> hooks:</p> <pre><code>terraform show -json spacelift.plan | jq -c '.configuration' &gt; configuration.custom.spacelift.json\n</code></pre> <p>The data will be available in the policy input as <code>input.third_party_metadata.custom.configuration</code>. This depends on the <code>jq</code> tool being available in the runner image. It is installed by default on our standard image.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#passing-custom-tool-output-to-the-plan-policy","title":"Passing custom tool output to the plan policy","text":"<p>For this example, we will generate warnings (from the open-source Terraform security scanner tfsec) as JSON and have them reported and processed using the plan policy.</p> <p>Run <code>tfsec</code> as a <code>before_init</code> hook and save the output to a file:</p> <pre><code>tfsec -s --format=json . &gt; tfsec.custom.spacelift.json\n</code></pre> <p>The data will be available in the policy input as <code>input.third_party_metadata.custom.tfsec</code>. This depends on the <code>tfsec</code> tool being available in the runner image, which you will need to install either directly on the image or as part of your <code>before_init</code> hook.</p> <p>Some vulnerability scanning tools, like <code>tfsec</code>, will return a non-zero exit code when they encounter vulnerabilities, which will result in a stack failure. The majority of these tools provide a soft scanning option that will show all the vulnerabilities without considering the command as failed, which we will use instead.</p> <p>If your tool doesn't offer soft scanning, append <code>|| true</code> at the end of the command, which always returns a zero exit code.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#use-cases","title":"Use cases","text":"<p>Since plan policies have access to the infrastructure changes that are about to be introduced, you can run all sorts of checks against those changes. There are two main use cases for those checks:</p> <ul> <li>Organizational rule enforcement to prevent rules that go against organizational policies.</li> <li>Automated code review to augment human decision-making.</li> </ul>"},{"location":"concepts/policy/terraform-plan-policy.html#organizational-rule-enforcement","title":"Organizational rule enforcement","text":"<p>In every organization, there are some things you do not touch, such as:</p> <ul> <li>A particular line of code surrounded by comments warning, if you change it, the site will go down and the on-call personnel will be after you.</li> <li>Potential security vulnerabilities that can expose all your infrastructure to the wrong crowd.</li> </ul> <p>Spacelift can turn these organizational hard rules into policies that can't be broken. You will most likely want to exclusively use deny rules.</p> <p>In this example, we introduce a simple rule: never create static AWS credentials.</p> <pre><code>package spacelift\n\n# The message here is dynamic and captures resource address to provide\n# appropriate context to anyone affected by this policy. For the sake of your\n# sanity and that of your colleagues, always add a message when denying a change.\ndeny[sprintf(message, [resource.address])] {\n  message := \"static AWS credentials are evil (%s)\"\n\n  resource := input.terraform.resource_changes[_]\n  resource.change.actions[_] == \"create\"\n\n  # This is what decides whether the rule captures a resource.\n  # There may be an arbitrary number of conditions, and they all must\n  # succeed for the rule to take effect.\n  resource.type == \"aws_iam_access_key\"\n}\n</code></pre> <p>This slightly more sophisticated policy states that when some resources are recreated, they should be created before they're destroyed or an outage will follow. We found this to be an issue with <code>aws_batch_compute_environment</code>, among other resources.</p> <pre><code>package spacelift\n\n# This is what Rego calls a set. You can add further elements to it as necessary.\nalways_create_first := { \"aws_batch_compute_environment\" }\n\ndeny[sprintf(message, [resource.address])] {\n  message  := \"always create before deleting (%s)\"\n  resource := input.terraform.resource_changes[_]\n\n  # Make sure the type is on the list.\n  always_create_first[resource.type]\n\n  some i_create, i_delete\n  resource.change.actions[i_create] == \"create\"\n  resource.change.actions[i_delete] == \"delete\"\n\n\n  i_delete &lt; i_create\n}\n</code></pre> <p>While in most cases you'll want your rules to only look at resources affected by the change, you're not limited to doing so. You can look at all resources and force teams to remove certain resources. Here's an example where until AWS resources are all removed in one go, no further changes can take place:</p> <pre><code>package spacelift\n\ndeny[sprintf(message, [resource.address])] {\n  message  := \"we've moved to GCP, find an equivalent there (%s)\"\n  resource := input.terraform.resource_changes[_]\n\n  resource.provider_name == \"aws\"\n\n  # If you're just deleting, all good.\n  resource.change.actions != [\"delete\"]\n}\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html#automated-code-review","title":"Automated code review","text":"<p>In addition to enforcing hard rules, plan policy rules can help humans understand changes better and make informed decisions on what looks good and what does not.</p> <p>Warn rules look like this:</p> <p></p> <p>The warn rule won't fail your plan and can provide great help to a human reviewer, especially when multiple changes are introduced. Also, if a stack is set to autodeploy, the presence of a single warning is enough to flag the run for a human review.</p> <p>The best way to use warn and deny rules together depends on your preferred Git workflow. We've found short-lived feature branches with Pull Requests to the tracked branch to work relatively well. In this scenario, the <code>type</code> of the run is important: PROPOSED for commits to feature branches, and TRACKED on commits to the tracked branch. Your rules should use this mechanism to balance comprehensive feedback on Pull Requests and the flexibility of being able to deploy things that humans deem appropriate.</p> <p>As a general rule when using plan policies for code review, deny when run type is PROPOSED and warn when it is TRACKED. Denying tracked runs unconditionally may be a good idea for most egregious violations, but when this approach is taken to an extreme it can make your life difficult.</p> <p>We suggest that you at most deny when the run is PROPOSED, which will send a failure status to the GitHub commit, then give the reviewer a chance to approve the change anyways. If you want a human to take another look before those changes go live, either set stack autodeploy to false or explicitly warn about potential violations. Here's an example of how to reuse the same rule to deny or warn depending on the run type:</p> <pre><code>package spacelift\n\nproposed := input.spacelift.run.type == \"PROPOSED\"\n\ndeny[reason] { proposed; reason := iam_user_created[_] }\nwarn[reason] { not proposed; reason := iam_user_created[_] }\n\niam_user_created[sprintf(\"do not create IAM users: (%s)\", [resource.address])] {\n  resource := input.terraform.resource_changes[_]\n  resource.change.actions[_] == \"create\"\n\n  resource.type == \"aws_iam_user\"\n}\n</code></pre> <p>Predictably, this fails when committed to a non-tracked (feature) branch:</p> <p></p> <p>...but as a GitHub repo admin you can still merge it if you've set your branch protection rules accordingly:</p> <p></p> <p>If we squash and merge:</p> <p></p> <p>The run stopped to await a human decision. At this point, we still have a choice to either confirm or discard the run. In the latter case, you will likely want to revert the commit that caused the problem, otherwise all subsequent runs will be affected.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies that are ready to use or alter to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#require-human-review-when-resources-are-changed","title":"Require human review when resources are changed","text":"<p>Adding resources may cost a lot of money, but it's usually safe from an operational perspective. Let's use a <code>warn</code> rule to allow changes with only added resources to get automatically applied, and require all others to get a human review:</p> <pre><code>package spacelift\n\nwarn[sprintf(message, [action, resource.address])] {\n  message := \"action '%s' requires human review (%s)\"\n  review  := {\"update\", \"delete\"}\n\n  resource := input.terraform.resource_changes[_]\n  action   := resource.change.actions[_]\n\n  review[action]\n}\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html#automatically-deploy-changes-from-selected-individuals","title":"Automatically deploy changes from selected individuals","text":"<p>Sometimes changes introduced by trusted individuals can be deployed automatically, especially if they already went through code review. This example allows commits from allowlisted individuals to be deployed automatically (and assumes the stack is set to autodeploy):</p> <pre><code>package spacelift\n\nwarn[sprintf(message, [author])] {\n  message     := \"%s is not on the allowlist - human review required\"\n  author      := input.spacelift.commit.author\n  allowlisted := { \"alice\", \"bob\", \"charlie\" }\n\n  not allowlisted[author]\n}\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html#require-commits-to-be-reasonably-sized","title":"Require commits to be reasonably sized","text":"<p>Massive changes make reviewers miserable. In this example, we automatically fail all changes that affect more than 50 resources but allow them to be deployed with mandatory human review:</p> <pre><code>package spacelift\n\nproposed := input.spacelift.run.type == \"PROPOSED\"\n\ndeny[msg] { proposed; msg := too_many_changes[_] }\nwarn[msg] { not proposed; msg := too_many_changes[_] }\n\ntoo_many_changes[msg] {\n  threshold := 50\n\n  res := input.terraform.resource_changes\n  ret := count([r | r := res[_]; r.change.actions != [\"no-op\"]])\n  msg := sprintf(\"more than %d changes (%d)\", [threshold, ret])\n\n  ret &gt; threshold\n}\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html#back-of-the-envelope-blast-radius","title":"Back-of-the-envelope blast radius","text":"<p>This is a fancy contrived example building on top of the previous one. However, rather than looking at the total number of affected resources, it attempts to create a metric called a \"blast radius\": how much the change will affect the whole stack.</p> <p>It assigns special multipliers to some types of resources changed and treats different types of changes differently: deletes and updates are more \"expensive\" because they affect live resources, while new resources are generally safer and thus \"cheaper\". Per our automated code review pattern, we will fail Pull Requests with changes violating this policy, but require human action through warnings when these changes hit the tracked branch.</p> <pre><code>package spacelift\n\nproposed := input.spacelift.run.type == \"PROPOSED\"\n\ndeny[msg] { proposed; msg := blast_radius_too_high[_] }\nwarn[msg] { not proposed; msg := blast_radius_too_high[_] }\n\nblast_radius_too_high[sprintf(\"change blast radius too high (%d/100)\", [blast_radius])] {\n  blast_radius := sum([blast |\n                        resource := input.terraform.resource_changes[_];\n                        blast := blast_radius_for_resource(resource)])\n\n  blast_radius &gt; 100\n}\n\nblast_radius_for_resource(resource) = ret {\n  blasts_radii_by_action := { \"delete\": 10, \"update\": 5, \"create\": 1, \"no-op\": 0 }\n\n    ret := sum([value | action := resource.change.actions[_]\n                    action_impact := blasts_radii_by_action[action]\n                    type_impact := blast_radius_for_type(resource.type)\n                    value := action_impact * type_impact])\n}\n\n# Let's give some types of resources special blast multipliers.\nblasts_radii_by_type := { \"aws_ecs_cluster\": 20, \"aws_ecs_user\": 10, \"aws_ecs_role\": 5 }\n\n# By default, blast radius has a value of 1.\nblast_radius_for_type(type) = 1 {\n    not blasts_radii_by_type[type]\n}\n\nblast_radius_for_type(type) = ret {\n    blasts_radii_by_type[type] = ret\n}\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html#cost-management","title":"Cost management","text":"<p>Thanks to our Infracost integration, you can take cost information into account when deciding whether to ask for human approval or to block changes entirely.</p>"},{"location":"concepts/policy/trigger-policy.html","title":"Trigger policy","text":""},{"location":"concepts/policy/trigger-policy.html#trigger-policy","title":"Trigger policy","text":"<p>Tip</p> <p>We now have the stack dependencies feature available which should mostly cover the use cases described below. It's a simpler and more intuitive way to define dependencies between stacks.</p>"},{"location":"concepts/policy/trigger-policy.html#purpose","title":"Purpose","text":"<p>Frequently, your infrastructure consists of a number of projects (stacks in Spacelift parlance) that are connected in some way - either depend logically on one another, or must be deployed in a particular order for some other reason - for example, a rolling deploy in multiple regions.</p> <p>Enter trigger policies. Trigger policies are evaluated at the end of each stack-blocking run (which includes tracked runs and tasks) as well as on module version releases and allow you to decide if some tracked Runs should be triggered. This is a very powerful feature, effectively turning Spacelift into a Turing machine.</p> <p>Warning</p> <p>Note that in order to support various use cases this policy type is currently evaluated every time a blocking Run reaches a terminal state, which includes states like Canceled, Discarded, Stopped or Failed in addition to the more obvious Finished. This allows for very interesting and complex workflows (eg. automated retry logic) but please be aware of that when writing your own policies.</p> <p>All runs triggered - directly or indirectly - by trigger policies as a result of the same initial run are grouped into a so-called workflow. In the trigger policy you can access all other runs in the same workflow as the currently finished run, regardless of their Stack. This lets you coordinate executions of multiple Stacks and build workflows which require multiple runs to finish in order to commence to the next stage (and trigger another Stack).</p>"},{"location":"concepts/policy/trigger-policy.html#data-input","title":"Data input","text":"<p>When triggered by a run, this is the schema of the data input that each policy request will receive:</p> <p>Official Schema Reference</p> <p>For the most up-to-date and complete schema definition, please refer to the official Spacelift policy contract schema under the <code>TRIGGER</code> policy type.</p> <pre><code>{\n  \"run\": {\n    \"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n    \"branch\": \"string - the branch the run was triggered from\",\n    \"changes\": [\n      {\n        \"action\": \"string enum - added | changed | deleted\",\n        \"entity\": {\n          \"address\": \"string - full address of the entity\",\n          \"name\": \"string - name of the entity\",\n          \"type\": \"string - full resource type or 'output' for outputs\",\n          \"entity_vendor\": \"string - the name of the vendor\",\n          \"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n          \"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n        },\n        \"phase\": \"string enum - plan | apply\"\n      }\n    ],\n    \"commit\": {\n      \"author\": \"string - GitHub login if available, name otherwise\",\n      \"branch\": \"string - branch to which the commit was pushed\",\n      \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n      \"hash\": \"string - the commit hash\",\n      \"message\": \"string - commit message\"\n    },\n    \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n    \"creator_session\": {\n      \"admin\": \"boolean - is the current user a Spacelift admin\",\n      \"creator_ip\": \"string - IP address of the user who created the session\",\n      \"login\": \"string - username of the creator\",\n      \"name\": \"string - full name of the creator\",\n      \"teams\": [\"string - names of teams the creator was a member of\"],\n      \"machine\": \"boolean - whether the run was initiated by a human or a machine\"\n    },\n    \"drift_detection\": \"boolean - is this a drift detection run\",\n    \"flags\": [\"string - list of flags set on the run by other policies\"],\n    \"id\": \"string - the run ID\",\n    \"runtime_config\": {\n      \"before_init\": [\"string - command to run before run initialization\"],\n      \"project_root\": \"string - root of the Terraform project\",\n      \"runner_image\": \"string - Docker image used to execute the run\",\n      \"terraform_version\": \"string - Terraform version used for the run\"\n    },\n    \"state\": \"string - the current run state\",\n    \"states_history\": [\n      {\n        \"new_state\": \"string - state transition that occurred\"\n      }\n    ],\n    \"triggered_by\": \"string - user or trigger policy who triggered the run, if applicable\",\n    \"type\": \"string - type of the run\",\n    \"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n    \"user_provided_metadata\": [\n      \"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n    ]\n  },\n  \"stack\": {\n    \"administrative\": \"boolean - is the stack administrative\",\n    \"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n    \"branch\": \"string - tracked branch of the stack\",\n    \"id\": \"string - unique stack identifier\",\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n    \"name\": \"string - name of the stack\",\n    \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n    \"project_root\": \"optional string - project root as set on the Stack, if any\",\n    \"repository\": \"string - name of the source GitHub repository\",\n    \"state\": \"string - current state of the stack\",\n    \"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n    \"tracked_commit\": {\n      \"author\": \"string - GitHub login if available, name otherwise\",\n      \"branch\": \"string - branch to which the commit was pushed\",\n      \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n      \"hash\": \"string - the commit hash\",\n      \"message\": \"string - commit message\"\n    },\n    \"worker_pool\": {\n      \"id\": \"string - the worker pool ID, if it is private\",\n      \"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n      \"name\": \"string - name of the worker pool, if it is private\",\n      \"public\": \"boolean - is the worker pool public\"\n    }\n  },\n  \"stacks\": [\n    {\n      \"administrative\": \"boolean - is the stack administrative\",\n      \"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n      \"branch\": \"string - tracked branch of the stack\",\n      \"id\": \"string - unique stack identifier\",\n      \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n      \"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n      \"name\": \"string - name of the stack\",\n      \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n      \"project_root\": \"optional string - project root as set on the Stack, if any\",\n      \"repository\": \"string - name of the source GitHub repository\",\n      \"state\": \"string - current state of the stack\",\n      \"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n      \"tracked_commit\": {\n        \"author\": \"string - GitHub login if available, name otherwise\",\n        \"branch\": \"string - branch to which the commit was pushed\",\n        \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n        \"hash\": \"string - the commit hash\",\n        \"message\": \"string - commit message\"\n      },\n      \"worker_pool\": {\n        \"id\": \"string - the worker pool ID, if it is private\",\n        \"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n        \"name\": \"string - name of the worker pool, if it is private\",\n        \"public\": \"boolean - is the worker pool public\"\n      }\n    }\n  ],\n  \"workflow\": [\n    {\n      \"id\": \"string - Unique ID of the Run\",\n      \"stack_id\": \"string - unique stack identifier\",\n      \"state\": \"state - one of the states of the Run\",\n      \"type\": \"string - TRACKED or TASK\"\n    }\n  ]\n}\n</code></pre> <p>Info</p> <p>Note the presence of two similar keys: <code>stack</code> and <code>stacks</code>. The former is the Stack that the newly finished Run belongs to. The other is a list of all Stacks in the account. The schema for both is the same.</p> <p>When triggered by a new module version, this is the schema of the data input that each policy request will receive:</p> <pre><code>{\n  \"module\": { // Module for which the new version was released\n    \"administrative\": \"boolean - is the stack administrative\",\n    \"branch\": \"string - tracked branch of the module\",\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"current_version\": \"Newly released module version\",\n    \"id\": \"string - unique ID of the module\",\n    \"name\": \"string - name of the stack\",\n    \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n    \"project_root\": \"optional string - project root as set on the Module, if any\",\n    \"repository\": \"string - name of the source GitHub repository\",\n    \"space\": {\n        \"id\": \"string\",\n        \"labels\": [\"string\"],\n        \"name\": \"string\"\n      },\n    \"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n    \"worker_pool\": {\n      \"id\": \"string - the worker pool ID, if it is private\",\n      \"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n      \"name\": \"string - name of the worker pool, if it is private\",\n      \"public\": \"boolean - is the worker pool public\"\n    }\n  }\n\n  \"stacks\": [ // List of consumers of the newest available module version\n    {\n      \"administrative\": \"boolean - is the stack administrative\",\n      \"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n      \"branch\": \"string - tracked branch of the stack\",\n      \"id\": \"string - unique stack identifier\",\n      \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n      \"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n      \"name\": \"string - name of the stack\",\n      \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n      \"project_root\": \"optional string - project root as set on the Stack, if any\",\n      \"repository\": \"string - name of the source GitHub repository\",\n      \"state\": \"string - current state of the stack\",\n      \"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n      \"tracked_commit\": {\n        \"author\": \"string - GitHub login if available, name otherwise\",\n        \"branch\": \"string - branch to which the commit was pushed\",\n        \"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n        \"hash\": \"string - the commit hash\",\n        \"message\": \"string - commit message\"\n      },\n      \"worker_pool\": {\n        \"id\": \"string - the worker pool ID, if it is private\",\n        \"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n        \"name\": \"string - name of the worker pool, if it is private\",\n        \"public\": \"boolean - is the worker pool public\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"concepts/policy/trigger-policy.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p> <p>Since trigger policies turn Spacelift into a Turing machine, you could probably use them to implement Conway's Game of Life, but there are a few more obvious use cases. Let's have a look at two of them - interdependent Stacks and automated retries.</p>"},{"location":"concepts/policy/trigger-policy.html#interdependent-stacks","title":"Interdependent stacks","text":"<p>The purpose here is to create a complex workflow that spans multiple Stacks. We will want to trigger a predefined list of Stacks when a Run finishes successfully. Here's our first take:</p> <pre><code>package spacelift\n\ntrigger[\"stack-one\"]   { finished }\ntrigger[\"stack-two\"]   { finished }\ntrigger[\"stack-three\"] { finished }\n\nfinished {\n  input.run.state == \"FINISHED\"\n  input.run.type == \"TRACKED\"\n}\n</code></pre> <p>Here's a minimal example of this rule in the Rego playground. But it's far from ideal. We can't be guaranteed that stacks with these IDs still exist in this account. Spacelift will handle that just fine, but you'll likely find if confusing. Also, for any new Stack that appears you will need to explicitly add it to the list. That's annoying.</p> <p>We can do better, and to do that, we'll use Stack labels. Labels are completely arbitrary strings that you can attach to individual Stacks, and we can use them to do something magical - have \"client\" Stacks \"subscribe\" to \"parent\" ones.</p> <p>So how's that:</p> <pre><code>package spacelift\n\ntrigger[stack.id] {\n  stack := input.stacks[_]\n  input.run.state == \"FINISHED\"\n  input.run.type == \"TRACKED\"\n  stack.labels[_] == concat(\"\", [\"depends-on:\", input.stack.id])\n}\n</code></pre> <p>Here's a minimal example of this rule in the Rego playground. The benefit of this policy is that you can attach it to all your stacks, and it will just work for your entire organization.</p> <p>Can we do better? Sure, we can even have stacks use labels to decide which types of runs or state changes they care about. Here's a mind-bending example:</p> <pre><code>package spacelift\n\ntrigger[stack.id] {\n  stack := input.stacks[_]\n  input.run.type == \"TRACKED\"\n  stack.labels[_] == concat(\"\", [\n    \"depends-on:\", input.stack.id,\n    \"|state:\", input.run.state],\n  )\n}\n</code></pre> <p>Another Rego example to play with. Now, how cool is that?</p>"},{"location":"concepts/policy/trigger-policy.html#automated-retries","title":"Automated retries","text":"<p>Here's another use case - sometimes OpenTofu/Terraform or Pulumi deployments fail for a reason that has nothing to do with the code - think eventual consistency between various cloud subsystems, transient API errors etc. It would be great if you could restart the failed run. Oh, and let's make sure new runs are not created in a crazy loop - since policy-triggered runs trigger another policy evaluation:</p> <pre><code>package spacelift\n\ntrigger[stack.id] {\n  stack := input.stack\n  input.run.state == \"FAILED\"\n  input.run.type == \"TRACKED\"\n  is_null(input.run.triggered_by)\n}\n</code></pre> <p>Info</p> <p>Note that this will also prevent user-triggered runs from being retried. Which is usually what you want in the first place, because a triggering human is probably already babysitting the Stack anyway.</p>"},{"location":"concepts/policy/trigger-policy.html#diamond-problem","title":"Diamond Problem","text":"<p>The diamond problem happens when your stacks and their dependencies form a shape like in the following diagram:</p> <pre><code>graph LR\n  1  --&gt; 2a;\n  1  --&gt; 2b;\n  2a --&gt; 3;\n  2b --&gt; 3;</code></pre> <p>Which means that Stack 1 triggers both Stack 2a and 2b, and we only want to trigger Stack 3 when both predecessors finish. This can be elegantly solved using workflows.</p> <p>First we'll have to create a trigger policy for Stack 1:</p> <pre><code>package spacelift\n\ntrigger[\"stack-2a\"] {\n  tracked_and_finished\n}\n\ntrigger[\"stack-2b\"] {\n  tracked_and_finished\n}\n\ntracked_and_finished {\n  input.run.state == \"FINISHED\"\n  input.run.type == \"TRACKED\"\n}\n</code></pre> <p>This will trigger both Stack 2a and 2b whenever a run finishes on Stack 1.</p> <p>Now onto a trigger policy for Stack 2a and 2b:</p> <pre><code>package spacelift\n\ntrigger[\"stack-3\"] {\n  run_a := input.workflow[_]\n  run_b := input.workflow[_]\n  run_a.stack_id == \"stack-2a\"\n  run_b.stack_id == \"stack-2b\"\n  run_a.state == \"FINISHED\"\n  run_b.state == \"FINISHED\"\n}\n</code></pre> <p>Here we trigger Stack 3, whenever the runs in Stack 2a and 2b are both finished.</p> <p>You can also easily extend this to work with a label-based approach, so that you could define Stack 3's dependencies by attaching a <code>depends-on:stack-2a,stack-2b</code>label to it:</p> <pre><code>package spacelift\n\n# Helper with stack_id's of workflow runs which have already finished.\nalready_finished[run.stack_id] {\n  run := input.workflow[_]\n  run.state == \"FINISHED\"\n}\n\ntrigger[stack.id] {\n  input.run.state == \"FINISHED\"\n  input.run.type == \"TRACKED\"\n\n  # For each Stack which has a depends-on label,\n  # get a list of its dependencies.\n  stack := input.stacks[_]\n  label := stack.labels[_]\n  startswith(label, \"depends-on:\")\n  dependencies := split(trim_prefix(label, \"depends-on:\"), \",\")\n\n  # The current Stack is one of the dependencies.\n  input.stack.id == dependencies[_]\n\n  finished_dependencies := [dependency |\n                                       dependency := dependencies[_]\n                                       already_finished[dependency]]\n\n  # Check if all dependencies have finished.\n  count(finished_dependencies) == count(dependencies)\n}\n</code></pre>"},{"location":"concepts/policy/trigger-policy.html#module-updates","title":"Module updates","text":"<p>Trigger policies can be attached to modules as well. Modules track the consumers of each of their versions. When a new module version is released, the consumers of the previously newest version are assumed to be potential consumers of the newly released one. Hence, the trigger policy for a module can be used to trigger a run on all of these stacks. The module version will be updated as long as the version constraints allow the newest version to be used.</p> <p>Here is a simple trigger policy that will trigger a run on all stacks that use the latest version of the module when a new version is released:</p> <pre><code>package spacelift\n\ntrigger[stack.id] { stack := input.stacks[_] }\n</code></pre> <p>Note that the stack would need to be have been triggered once successfully (initialized) prior to module start tracking it. It's also possible to combine this with a tag driven module version release push policy and here is a link to an example of that.</p>"},{"location":"concepts/policy/push-policy.html","title":"Push policy","text":""},{"location":"concepts/policy/push-policy.html#push-policy","title":"Push policy","text":"<p>Git push policies are triggered on a per-stack basis to determine the action that should be taken for each individual stack or module in response to a Git push or Pull Request notification. There are three possible outcomes:</p> <ul> <li>track: Set the new head commit on the stack or module and create a tracked run (one that can be applied).</li> <li>propose: Create a proposed run against a proposed version of infrastructure.</li> <li>ignore: Do not schedule a new run.</li> </ul> <p>You can create sophisticated, custom-made setups using push policies. We can think of two main (not mutually exclusive) use cases:</p> <ul> <li>Ignore changes to certain paths. This is something you'd find useful both with classic monorepos and repositories containing multiple Terraform projects under different paths.</li> <li>Apply only a subset of changes, such as only commits tagged in a certain way.</li> </ul>"},{"location":"concepts/policy/push-policy.html#git-push-policy-and-tracked-branch","title":"Git push policy and tracked branch","text":"<p>Each stack and module points at a particular Git branch called a tracked branch. By default, any push to the tracked branch that changes a file in the project root triggers a tracked run that can be applied. This logic can be changed entirely by a Git push policy, but the tracked branch is always reported as part of the stack input to the policy evaluator and can be used as a point of reference.</p> <p></p> <p>When a push policy does not track a new push, the head commit of the stack/module will not be set to the tracked branch head commit. Sync the tracked branch head commit with the head commit of the stack/module by navigating to that stack and pressing the sync button.</p>"},{"location":"concepts/policy/push-policy.html#push-and-pull-request-events","title":"Push and Pull Request events","text":"<p>Spacelift can currently react to two types of events: push and pull request (also called merge request by GitLab). Push events are the default; even if you don't have a push policy set up, we will respond to those events. Pull request events are supported for some VCS providers and are generally received when you open, synchronize (push a new commit), label, or merge the pull request.</p> <p>There are some valid reasons to use pull request events in addition or instead of push ones. For example, when making decisions based on the paths of affected files, push events are often confusing:</p> <ul> <li>They contain affected files for all commits in a push, not just the head commit.</li> <li>They are not context-aware, making it hard to work with pull requests; if a given push is ignored on an otherwise relevant PR, then the Spacelift status check is not provided.</li> </ul> <p>Here are a few samples of PR-driven policies from real-life use cases, each reflecting a slightly different way of structuring your workflow.</p> <p>First, let's only trigger proposed runs if a PR exists, and allow any push to the tracked branch to trigger a tracked run:</p> <pre><code>package spacelift\n\ntrack   { input.push.branch == input.stack.branch }\npropose { not is_null(input.pull_request) }\nignore  { not track; not propose }\n</code></pre> <p>If you want to enforce that tracked runs are always created from PR merges (and not from direct pushes to the tracked branch), you can tweak the above policy accordingly to ignore all non-PR events:</p> <pre><code>package spacelift\n\ntrack   { is_pr; input.push.branch == input.stack.branch }\npropose { is_pr }\nignore  { not is_pr }\nis_pr   { not is_null(input.pull_request) }\n</code></pre> <p>Here's another example where you respond to a particular PR label (\"deploy\") to automatically deploy changes:</p> <pre><code>package spacelift\n\ntrack   { is_pr; labeled }\npropose { true }\nis_pr   { not is_null(input.pull_request) }\nlabeled { input.pull_request.labels[_] == \"deploy\" }\n</code></pre> <p>Info</p> <p>When a run is triggered from a GitHub Pull Request and the Pull Request is mergeable (meaning there are no merge conflicts), we check out the code for something called the \"potential merge commit\". This virtual commit represents the potential result of merging the Pull Request into its base branch and should provide higher quality, less confusing feedback.</p>"},{"location":"concepts/policy/push-policy.html#deduplicating-events","title":"Deduplicating events","text":"<p>If you're using pull requests in your flow, Spacelift might receive duplicate events. For example, if you push to a feature branch and then open a pull request, Spacelift first receives a push event, then a separate pull request (opened) event. When you push another commit to that feature branch, we again receive two events: push and pull request (synchronized). When you merge the pull request, we get two more: push and pull request (closed).</p> <p>Push policies could resolve to the same actionable (not ignore) outcome (e.g. track or propose). In those cases, instead of creating two separate runs, we debounce the events by deduplicating runs created by them on a per-stack basis.</p> <p>The deduplication key consists of the commit SHA and run type. If your policy returns two different actionable outcomes for two different events associated with a given SHA, both runs will be created. In practice, this would be an unusual corner case and a reason to revisit your workflow.</p> <p>When events are deduplicated and you're sampling policy evaluations, you may notice that there are two samples for the same SHA, each with different input. You can generally assume that the first one creates a run.</p>"},{"location":"concepts/policy/push-policy.html#canceling-in-progress-runs","title":"Canceling in-progress runs","text":"<p>You can use push policies to pre-empt any in-progress runs with the new run. The input document includes the <code>in_progress</code> key, which contains an array of runs that are currently either still queued, ready, or awaiting human confirmation. You can use it in conjunction with the cancel rule like this:</p> <pre><code>cancel[run.id] { run := input.in_progress[_] }\n</code></pre> <p>Of course, you can use a more sophisticated approach and only choose to cancel a certain type of run, or runs in a particular state. For example, this rule will only cancel proposed runs that are currently queued (waiting for the worker):</p> <pre><code>cancel[run.id] {\n  run := input.in_progress[_]\n  run.type == \"PROPOSED\"\n  run.state == \"QUEUED\"\n}\n</code></pre> <p>You can also compare branches and cancel proposed runs in queued state pointing to a specific branch:</p> <pre><code>cancel[run.id] {\n  run := input.in_progress[_]\n  run.type == \"PROPOSED\"\n  run.state == \"QUEUED\"\n  run.branch == input.pull_request.head.branch\n}\n</code></pre>"},{"location":"concepts/policy/push-policy.html#cancelation-restrictions","title":"Cancelation restrictions","text":"<p>There are some restrictions on cancelation:</p> <ul> <li>Module test runs cannot be canceled. Only proposed and tracked stack runs can be canceled.</li> <li>Cancelation works based on a new run pre-empting existing runs. What this means is that if your push policy does not result in any runs being triggered, the cancelation will have no effect.</li> <li>A run can only cancel other runs of the same type. For example a proposed run can only cancel other proposed runs, not tracked runs.</li> <li>Cancelation is best effort and not guaranteed. If the run is picked up by the worker or approved by a human in the meantime, the cancelation itself is canceled.</li> </ul>"},{"location":"concepts/policy/push-policy.html#configuring-ignore-event-behavior","title":"Configuring Ignore event behavior","text":""},{"location":"concepts/policy/push-policy.html#customize-vcs-check-messages-for-ignored-run-events","title":"Customize VCS check messages for Ignored Run events","text":"<p>To customize the messages sent back to your VCS when Spacelift runs are ignored, use the <code>message</code> function within your Push policy. See the example policy for reference.</p>"},{"location":"concepts/policy/push-policy.html#customize-check-status-for-ignored-run-events","title":"Customize check status for Ignored Run events","text":"<p>By default, ignored runs on a stack will return a <code>skipped</code> status check event, rather than a fail event. If you want ignored run events to have a <code>failed</code> status check on your VCS, set the <code>fail</code> function value to <code>true</code> in your Push policy.</p>"},{"location":"concepts/policy/push-policy.html#example-policy","title":"Example policy","text":"<p>The following push policy does not trigger any run within Spacelift. Using this policy, we can ensure that the status check within our VCS (in this case, GitHub) fails and returns the message \"I love bacon.\"</p> <pre><code>fail { true }\nmessage[\"I love bacon\"] { true }\n</code></pre> <p>With this policy, users would see this behavior within their GitHub status check:</p> <p></p> <p>Info</p> <p>This behavior (customization of the message and failing of the check within the VCS), is only applicable when runs do not take place within Spacelift.</p>"},{"location":"concepts/policy/push-policy.html#tag-driven-terraform-module-release-flow","title":"Tag-driven Terraform module release flow","text":"<p>You can use a simple push policy to manage your Terraform module versions using git tags and push your module to the Spacelift module registry using git tag events. Use the <code>module_version</code> block within a push policy attached your module, and then set the version using the tag information from the git push event.</p> <p>For example, this push policy will trigger a tracked run when a tag event is detected, then parses the tag event data and uses that value for the module version. We remove a git tag prefixed with <code>v</code> as the Terraform module registry only supports versions in a numeric <code>X.X.X</code> format.</p> <p>For this policy, you will need to provide a mock, non-existent version for proposed runs. This precaution has been taken to ensure that pull requests do not encounter check failures due to the existence of versions that are already in use.</p> <pre><code>package spacelift\n\nmodule_version := version {\n    version := trim_prefix(input.push.tag, \"v\")\n    not propose\n}\n\nmodule_version := \"&lt;X.X.X&gt;\" {\n    propose\n}\n\npropose {\n  not is_null(input.pull_request)\n  }\n</code></pre> <p>To add a track rule to your push policy, this will start a tracked run when the module version is not empty and the push branch is the same as the one the module branch is tracking:</p> <pre><code>track {\n  module_version != \"\"\n  input.push.branch == input.module.branch\n }\n</code></pre>"},{"location":"concepts/policy/push-policy.html#allow-forks","title":"Allow forks","text":"<p>By default, Spacelift doesn't trigger runs when a forked repository opens a pull request against your repository. This is to prevent a security incident. For example, if your infrastructure were open source, someone could fork it, implement unwanted code, then open a pull request for the original repository that would automatically run.</p> <p>Info</p> <p>The cause is very similar to GitHub Actions, where they don't expose repository secrets when forked repositories open pull requests.</p> <p>If you want to allow forks to trigger runs, you can explicitly do it with <code>allow_fork</code> rule. For example, if you trust certain people or organizations, this rule allows a forked repository to run only if the owner of the forked repo is <code>johnwayne</code> or <code>microsoft</code>:</p> <pre><code>propose { true }\nallow_fork {\n  validOwners := {\"johnwayne\", \"microsoft\"}\n  validOwners[input.pull_request.head_owner]\n}\n</code></pre> <p>The <code>head_owner</code> field means different things in different VCS providers:</p> VCS provider Meaning of <code>head_owner</code> field Where to find it GitHub/GitHub Enterprise The organization or person who owns the forked repo. In the URL: <code>https://github.com/&lt;head_owner&gt;/&lt;forked_repository&gt;</code>. GitLab The group of the repository. In the URL: <code>https://gitlab.com/&lt;head_owner&gt;/&lt;forked_repository&gt;</code>. Azure DevOps The ID of the forked repo's project (a UUID). Open <code>https://dev.azure.com/&lt;organization&gt;/_apis/projects</code> in your browser to see all projects and their unique IDs. Bitbucket Cloud Workspace. In the URL: <code>https://www.bitbucket.org/&lt;workspace&gt;/&lt;forked_repository&gt;</code> Bitbucket Datacenter/Server The project key of the repo. The display name of the project and its abbreviation (all caps)."},{"location":"concepts/policy/push-policy.html#approval-and-mergeability","title":"Approval and mergeability","text":"<p>The <code>pull_request</code> property on the input to a push policy contains the following fields:</p> <ul> <li><code>approved</code>: Indicates whether the PR has been approved.</li> <li><code>mergeable</code>: Indicates whether the PR can be merged.</li> <li><code>undiverged</code>: Indicates that the PR branch is not behind the target branch.</li> </ul> <p>This push policy will automatically deploy a PR's changes once it has been approved, any required checks have completed, and the PR has a <code>deploy</code> label added to it:</p> <pre><code>package spacelift\n\n# Trigger a tracked run if a change is pushed to the stack branch\ntrack {\n  affected\n  input.push.branch == input.stack.branch\n}\n\n# Trigger a tracked run if a PR is approved, mergeable, undiverged and has a deploy label\ntrack {\n  is_pr\n  is_clean\n  is_approved\n  is_marked_for_deploy\n}\n\n# Trigger a proposed run if a PR is opened\npropose {\n  is_pr\n}\n\nis_pr {\n  not is_null(input.pull_request)\n}\n\nis_clean {\n  input.pull_request.mergeable\n  input.pull_request.undiverged\n}\n\nis_approved {\n  input.pull_request.approved\n}\n\nis_marked_for_deploy {\n  input.pull_request.labels[_] == \"deploy\"\n}\n</code></pre> <p>Each source control provider has slightly different features, and because of this, the exact definition of <code>approved</code> and <code>mergeable</code> varies slightly.</p>"},{"location":"concepts/policy/push-policy.html#github-github-enterprise","title":"GitHub / GitHub Enterprise","text":"<ul> <li><code>approved</code>: The PR has at least one approval, and also meets any minimum approval requirements for the repo.</li> <li><code>mergeable</code>: The PR branch has no conflicts with the target branch, and any branch protection rules have been met.</li> </ul>"},{"location":"concepts/policy/push-policy.html#gitlab","title":"GitLab","text":"<ul> <li><code>approved</code>: The PR has at least one approval. If approvals are required, it is only <code>true</code> when all required approvals have been made.</li> <li><code>mergeable</code>: The PR branch has no conflicts with the target branch, any blocking discussions have been resolved, and any required approvals have been made.</li> </ul>"},{"location":"concepts/policy/push-policy.html#azure-devops","title":"Azure DevOps","text":"<ul> <li><code>approved</code>: The PR has at least one approving review (including approved with suggestions).</li> <li><code>mergeable</code>: The PR branch has no conflicts with the target branch, and any blocking policies are approved.</li> </ul> <p>Info</p> <p>We are unable to calculate divergence across forks in Azure DevOps, so the <code>undiverged</code> property will always be <code>false</code> for PRs created from forks.</p>"},{"location":"concepts/policy/push-policy.html#bitbucket-cloud","title":"Bitbucket Cloud","text":"<ul> <li><code>approved</code>: The PR has at least one approving review from someone other than the PR author.</li> <li><code>mergeable</code>: The PR branch has no conflicts with the target branch.</li> </ul>"},{"location":"concepts/policy/push-policy.html#bitbucket-datacenterserver","title":"Bitbucket Datacenter/Server","text":"<ul> <li><code>approved</code>: The PR has at least one approving review from someone other than the PR author.</li> <li><code>mergeable</code>: The PR branch has no conflicts with the target branch.</li> </ul>"},{"location":"concepts/policy/push-policy.html#data-input-schema","title":"Data input schema","text":"<p>As input, Git push policy receives the following:</p> <p>Official Schema Reference</p> <p>For the most up-to-date and complete schema definition, please refer to the official Spacelift policy contract schema under the <code>GIT_PUSH</code> policy type.</p> <pre><code>{\n  \"in_progress\": [{\n    \"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n    \"branch\": \"string - the branch this run is based on\",\n    \"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n    \"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n    \"type\": \"string - run type: proposed, tracked, task, etc.\",\n    \"state\": \"string - run state: queued, unconfirmed, etc.\",\n    \"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n    \"user_provided_metadata\": [\"string - blobs of metadata provided using spacectl or the API when interacting with this run\"]\n  }],\n  \"pull_request\": {\n    \"action\": \"string - opened, reopened, closed, merged, edited, labeled, synchronize, unlabeled\",\n    \"action_initiator\": \"string\",\n    \"approved\": \"boolean - indicates whether the PR has been approved\",\n    \"author\": \"string\",\n    \"base\": {\n      \"affected_files\": [\"string\"],\n      \"author\": \"string\",\n      \"branch\": \"string\",\n      \"created_at\": \"number (timestamp in nanoseconds)\",\n      \"message\": \"string\",\n      \"tag\": \"string\"\n    },\n    \"closed\": \"boolean\",\n    \"diff\": [\"string - list of files changed between base and head commit\"],\n    \"draft\": \"boolean - indicates whether the PR is marked as draft\",\n    \"head\": {\n      \"affected_files\": [\"string\"],\n      \"author\": \"string\",\n      \"branch\": \"string\",\n      \"created_at\": \"number (timestamp in nanoseconds)\",\n      \"message\": \"string\",\n      \"tag\": \"string\"\n    },\n    \"head_owner\": \"string\",\n    \"id\": \"number\",\n    \"labels\": [\"string\"],\n    \"mergeable\": \"boolean - indicates whether the PR can be merged\",\n    \"title\": \"string\",\n    \"undiverged\": \"boolean - indicates whether the PR is up to date with the target branch\"\n  },\n  \"push\": {\n    // For Git push events, this contains the pushed commit.\n    // For Pull Request events,\n    // this contains the head commit or merge commit if available (merge event).\n    \"affected_files\": [\"string\"],\n    \"author\": \"string\",\n    \"branch\": \"string\",\n    \"created_at\": \"number (timestamp in nanoseconds)\",\n    \"message\": \"string\",\n    \"tag\": \"string\"\n  },\n  \"stack\": {\n    \"additional_project_globs\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"administrative\": \"boolean\",\n    \"autodeploy\": \"boolean\",\n    \"branch\": \"string\",\n    \"id\": \"string\",\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n    \"name\": \"string\",\n    \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n    \"project_root\": \"optional string - project root as set on the Stack, if any\",\n    \"repository\": \"string\",\n    \"state\": \"string\",\n    \"terraform_version\": \"string or null\",\n    \"tracked_commit\": {\n      \"author\": \"string\",\n      \"branch\": \"string\",\n      \"created_at\": \"number (timestamp in nanoseconds)\",\n      \"hash\": \"string\",\n      \"message\": \"string\"\n    },\n    \"worker_pool\": {\n      \"public\": \"boolean - indicates whether the worker pool is public or not\"\n    }\n  },\n  \"vcs_integration\": {\n    \"id\": \"string - ID of the VCS integration\",\n    \"name\": \"string - name of the VCS integration\",\n    \"provider\": \"string - possible values are AZURE_DEVOPS, BITBUCKET_CLOUD, BITBUCKET_DATACENTER, GIT, GITHUB, GITHUB_ENTERPRISE, GITLAB\",\n    \"description\": \"string - description of the VCS integration\",\n    \"is_default\": \"boolean - indicates whether the VCS integration is the default one or Space-level\",\n    \"space\": {\n      \"id\": \"string\",\n      \"labels\": [\"string\"],\n      \"name\": \"string\"\n    },\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"updated_at\": \"number (timestamp in nanoseconds)\",\n    \"created_at\": \"number (timestamp in nanoseconds)\"\n  }\n}\n</code></pre>"},{"location":"concepts/policy/push-policy.html#new-module-version-schema","title":"New module version schema","text":"<p>When triggered by a new module version, this is the schema of the data input that each policy request will receive:</p> <pre><code>{\n  \"module\": { // Module for which the new version was released\n    \"administrative\": \"boolean - is the stack administrative\",\n    \"branch\": \"string - tracked branch of the module\",\n    \"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n    \"current_version\": \"Newly released module version\",\n    \"id\": \"string - unique ID of the module\",\n    \"name\": \"string - name of the stack\",\n    \"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n    \"project_root\": \"optional string - project root as set on the Module, if any\",\n    \"repository\": \"string - name of the source GitHub repository\",\n    \"space\": {\n        \"id\": \"string\",\n        \"labels\": [\"string\"],\n        \"name\": \"string\"\n      },\n    \"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n    \"worker_pool\": {\n      \"id\": \"string - the worker pool ID, if it is private\",\n      \"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n      \"name\": \"string - name of the worker pool, if it is private\",\n      \"public\": \"boolean - is the worker pool public\"\n    }\n  },\n  \"pull_request\": {\n    \"action\": \"string - opened, reopened, closed, merged, edited, labeled, synchronize, unlabeled\",\n    \"action_initiator\": \"string\",\n    \"approved\": \"boolean - indicates whether the PR has been approved\",\n    \"author\": \"string\",\n    \"base\": {\n      \"affected_files\": [\"string\"],\n      \"author\": \"string\",\n      \"branch\": \"string\",\n      \"created_at\": \"number (timestamp in nanoseconds)\",\n      \"message\": \"string\",\n      \"tag\": \"string\"\n    }\n  },\n  \"vcs_integration\": {\n    \"id\": \"bitbucket-for-payments-team\",\n    \"name\": \"Bitbucket for Payments Team\",\n    \"provider\": \"BITBUCKET_CLOUD\",\n    \"description\": \"### Payments Team BB integration\\n\\nThis integration should be **only** used by the Payments Integrations team. If you need access, drop [Joe](https://mycorp.slack.com/users/432JOE435) a message on Slack.\",\n    \"is_default\": false,\n    \"labels\": [\"bitbucketcloud\", \"paymentsorg\"],\n    \"space\": {\n      \"id\": \"paymentsteamspace-01HN0BF3GMYZQ4NYVNQ1RKQ9M7\",\n      \"labels\": [],\n      \"name\": \"PaymentsTeamSpace\"\n    },\n    \"created_at\": 1706187931079960000,\n    \"updated_at\": 1706274820310231000\n  }\n}\n</code></pre> <p>Based on this input, the policy may define boolean <code>track</code>, <code>propose</code> and <code>ignore</code> rules.</p> <ul> <li>The positive outcome of at least one <code>ignore</code> rule causes the push to be ignored, no matter the outcome of other rules.</li> <li>The positive outcome of at least one <code>track</code> rule triggers a tracked run.</li> <li>The positive outcome of at least one <code>propose</code> rule triggers a proposed run.</li> </ul> <p>If no rules are matched, the default is to ignore the push. Always supply an exhaustive set of policies, making sure that they define what to track and what to propose in addition to defining what they ignore.</p> <p>You can also define an auxiliary rule called <code>ignore_track</code>, which overrides a positive outcome of the <code>track</code> rule but does not affect other rules, most notably <code>propose</code>. This can be used to turn some of the pushes that would otherwise be applied into test runs.</p>"},{"location":"concepts/policy/push-policy.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies ready to use or alter to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p>"},{"location":"concepts/policy/push-policy.html#ignoring-certain-paths","title":"Ignoring certain paths","text":"<p>Ignoring changes to certain paths is something you'd find useful both with classic monorepos and repositories containing multiple OpenTofu/Terraform projects under different paths. When evaluating a push, we determine the list of affected files by looking at all the files touched by any of the commits in a given push.</p> <p>Info</p> <p>This list may include false positives, for example in a situation where you delete a given file in one commit, bring it back in another commit, then push multiple commits at once. This is a safer default than trying to figure out the exact scope of each push.</p> <p>Imagine a situation where you only want to look at changes to Terraform definitions (in HCL or JSON) inside one the <code>production/</code> or <code>modules/</code> directory, and have <code>track</code> and <code>propose</code> use their default settings:</p> <pre><code>package spacelift\n\ntrack   { input.push.branch == input.stack.branch }\npropose { input.push.branch != \"\" }\nignore  { not affected }\n\naffected {\n  some i, j, k\n\n  tracked_directories := {\"modules/\", \"production/\"}\n  tracked_extensions := {\".tf\", \".tf.json\"}\n\n  path := input.push.affected_files[i]\n\n  startswith(path, tracked_directories[j])\n  endswith(path, tracked_extensions[k])\n}\n</code></pre> <p>To keep the example readable we had to define <code>ignore</code> in a negative way, per the Anna Karenina principle.</p>"},{"location":"concepts/policy/push-policy.html#status-checks-and-ignored-pushes","title":"Status checks and ignored pushes","text":"<p>By default when the push policy instructs Spacelift to ignore a certain change, no commit status check is sent back to the VCS. This behavior is explicitly designed to prevent noise in monorepo scenarios where a large number of stacks are linked to the same Git repo.</p> <p>However, you may still be interested in learning that the push was ignored, or getting a commit status check for a given stack when it's set as required by GitHub's branch protection rules or your internal organization rules.</p> <p>In that case, you can use the <code>notify</code> rule to override default notification settings. So if you want to notify your VCS vendor even when a commit is ignored, you can define it like this:</p> <pre><code>package spacelift\n\n# other rules (including ignore), see above\n\nnotify { ignore }\n</code></pre> <p>Info</p> <p>The <code>notify</code> rule (false by default) only applies to ignored pushes, so you can't set it to <code>false</code> to silence commit status checks for proposed runs.</p>"},{"location":"concepts/policy/push-policy.html#applying-from-a-tag","title":"Applying from a tag","text":"<p>Another use case for a Git push policy would be to apply from a newly created tag rather than from a branch. This can be useful in multiple scenarios. For example, a staging/QA environment could be deployed every time a certain tag type is applied to a tested branch, thereby providing inline feedback on a GitHub Pull Request from the actual deployment rather than a plan/test. You could also constrain production to only apply from tags unless a run is explicitly triggered by the user.</p> <p>Here's an example:</p> <pre><code>package spacelift\n\ntrack   { re_match(`^\\d+\\.\\d+\\.\\d+$`, input.push.tag) }\npropose { input.push.branch != input.stack.branch }\n</code></pre>"},{"location":"concepts/policy/push-policy.html#set-head-commit-without-triggering-a-run","title":"Set head commit without triggering a run","text":"<p>The <code>track</code> decision sets the new head commit on the affected stack or module. This head commit is used when a tracked run is manually triggered or a task is started on the stack. In this case, you normally want to have a new tracked run, so that's what we do by default.</p> <p>However, sometimes you want to trigger tracked runs in a specific order or under specific circumstances either manually or using a trigger policy. So what you want is an option to set the head commit without triggering a run. The boolean <code>notrigger</code> rule will work in conjunction with the <code>track</code> decision and prevent the tracked run from being created.</p> <p><code>notrigger</code> does not depend in any way on the <code>track</code> rule; they're entirely independent. Spacelift will only look at <code>notrigger</code> if <code>track</code> evaluates to true when interpreting the result of the policy.</p> <p>Here's an example of using the two rules together to always set the new commit on the stack, but not trigger a run. You would use this when the run is always triggered manually, through the API, or using a trigger policy:</p> <pre><code>track     { input.push.branch == input.stack.branch }\npropose   { not track }\nnotrigger { true }\n</code></pre>"},{"location":"concepts/policy/push-policy.html#default-git-push-policy","title":"Default Git push policy","text":"<p>If no Git push policies are attached to a stack or a module, the default behavior is equivalent to this policy:</p> <pre><code>package spacelift\n\ntrack {\n  affected\n  input.push.branch == input.stack.branch\n}\n\npropose { affected }\npropose { affected_pr }\n\nignore  {\n    not affected\n    not affected_pr\n}\nignore  { input.push.tag != \"\" }\n\naffected {\n    filepath := input.push.affected_files[_]\n    startswith(normalize_path(filepath), normalize_path(input.stack.project_root))\n}\n\naffected {\n    filepath := input.push.affected_files[_]\n    glob_pattern := input.stack.additional_project_globs[_]\n    glob.match(glob_pattern, [\"/\"], normalize_path(filepath))\n}\n\naffected_pr {\n    filepath := input.pull_request.diff[_]\n    startswith(normalize_path(filepath), normalize_path(input.stack.project_root))\n}\n\naffected_pr {\n    filepath := input.pull_request.diff[_]\n    glob_pattern := input.stack.additional_project_globs[_]\n    glob.match(glob_pattern, [\"/\"], normalize_path(filepath))\n}\n\n# Helper function to normalize paths by removing leading slashes\nnormalize_path(path) = trim(path, \"/\")\n</code></pre>"},{"location":"concepts/policy/push-policy.html#waiting-for-cicd-artifacts","title":"Waiting for CI/CD artifacts","text":"<p>There are cases where you want pushes to your repo to trigger a run in Spacelift, but only after a CI/CD pipeline (or a part of it) has completed. An example would be when you want to trigger an infra deploy after some Docker image has been built and pushed to a registry.</p> <p>You can use push policies' external dependencies feature to achieve this.</p>"},{"location":"concepts/policy/push-policy.html#prioritization","title":"Prioritization","text":"<p>Although we generally recommend using our default scheduling order (tracked runs and tasks, then proposed runs, then drift detection runs), you can use push policies to prioritize certain runs over others. For example, you may want to prioritize runs triggered by a certain user or a certain branch.</p> <p>Use the boolean <code>prioritize</code> rule to mark a run as prioritized:</p> <pre><code>package spacelift\n\n# other rules (including ignore), see above\n\nprioritize { input.stack.labels[_] == \"prioritize\" }\n</code></pre> <p>This example will prioritize runs on any stack that has the <code>prioritize</code> label set. Run prioritization only works for private worker pools. An attempt to prioritize a run on a public worker pool using this policy will not work.</p>"},{"location":"concepts/policy/push-policy.html#stack-locking","title":"Stack locking","text":"<p>Stack locking can be particularly useful in workflows heavily reliant on pull requests. The push policy enables you to lock and unlock a stack based on specific criteria using the <code>lock</code> and <code>unlock</code> rules.</p> <p><code>lock</code> rule behavior when a non-empty string is returned:</p> <ul> <li>Lock the stack if it's currently unlocked.</li> <li>No change if the stack is already locked by the same owner.</li> <li>Reject any runs if the stack is locked by a different owner and you attempt to lock it.</li> </ul> <p><code>unlock</code> rule behavior when a non-empty string is returned:</p> <ul> <li>No change if the stack is currently unlocked.</li> <li>Unlock the stack if it's locked by the same owner.</li> <li>No change if the stack is locked by a different owner.</li> </ul> <p>Info</p> <p>Runs are only rejected if the push policy rules result in an attempt to acquire a lock on an already locked stack with a different lock key. If the <code>lock</code> rule is undefined or results in an empty string, runs will not be rejected.</p> <p>This example policy snippet locks a stack when a pull request is opened or synchronized, and unlocks it when the pull request is closed or merged. Add <code>import future.keywords</code> to your policy to use this exact snippet.</p> <pre><code>lock_id := sprintf(\"PR_ID_%d\", [input.pull_request.id])\n\nlock := lock_id {\n    input.pull_request.action in [\"opened\", \"synchronize\"]\n}\n\nunlock := lock_id {\n    input.pull_request.action in [\"closed\", \"merged\"]\n}\n</code></pre> <p>You can customize selectively locking and unlocking the stacks whose project root or project globs are set to track the files in the pull request:</p> <pre><code>lock_id := sprintf(\"PR_ID_%d\", [input.pull_request.id])\n\nlock := lock_id if {\n    input.pull_request.action in [\"opened\", \"synchronize\"]\n    affected_pr\n}\n\nunlock := lock_id if {\n    input.pull_request.action in [\"closed\", \"merged\"]\n    affected_pr\n}\n\naffected_pr if {\n    some filepath in input.pull_request.diff\n    startswith(filepath, input.stack.project_root)\n}\n\naffected_pr if {\n    some filepath in input.pull_request.diff\n    some glob_pattern in input.stack.additional_project_globs\n    glob.match(glob_pattern, [\"/\"], filepath)\n}\n</code></pre> <p>You can also lock and unlock through comments:</p> <pre><code>unlock := lock_id {\n    input.pull_request.action == \"commented\"\n    input.pull_request.comment == concat(\" \", [\"/spacelift\", \"unlock\", input.stack.id])\n}\n</code></pre> <p>You can then unlock your stack by commenting something such as:</p> <pre><code>/spacelift unlock my-stack-id\n</code></pre>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html","title":"External dependencies","text":""},{"location":"concepts/policy/push-policy/run-external-dependencies.html#external-dependencies","title":"External dependencies","text":"<p>External dependencies in push policies allow a user to define a set of dependencies that, while being external to Spacelift, must be completed before a Spacelift run can start.</p> <p>A common use case of this feature is making Spacelift wait for a CI/CD pipeline to complete before executing a run.</p>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#how-it-works","title":"How it works","text":"<p>To use this feature, you must:</p> <ul> <li>Define dependencies in push policies.</li> <li>Mark dependencies as finished or failed using spacectl.</li> </ul>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#define-dependencies","title":"Define dependencies","text":"<p>To define dependencies, you need to add a <code>external_dependency</code> rule to your push policy definition. This way, any run that gets created via this policy also has the dependency defined.</p> <p>The following rule adds a dependency to all runs created by a policy.</p> <pre><code>external_dependency[sprintf(\"%s-binary-build\", [input.push.hash])] { true }\n</code></pre> <p>You can have more complex rules that decide on the set of external dependencies based on data such as the current stack's labels.</p> <p>Warning</p> <p>Include unique strings (such as commit hashes) in the dependencies names, as this is the only way to ensure that the dependency is unique for each source control event.</p>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#mark-dependencies-as-finished-or-failed","title":"Mark dependencies as finished or failed","text":"<p>To mark a dependency as <code>finished</code>, <code>failed</code>, or <code>skipped</code>, use the spacectl command line tool with following commands:</p> <pre><code>spacectl run-external-dependency mark-completed --id \"&lt;commit-sha&gt;-binary-build\" --status finished\n\nspacectl run-external-dependency mark-completed --id \"&lt;commit-sha&gt;-binary-build\" --status failed\n\nspacectl run-external-dependency mark-completed --id \"&lt;commit-sha&gt;-binary-build\" --status skipped \n</code></pre> <p>The run will be eligible for execution only after all of its dependencies are marked as finished or skipped. If any of the dependencies has failed, the run will be marked as failed as well.</p> <p>Warning</p> <p>To mark a run dependency as finished or failed, spacectl needs to be authenticated and have write access to all the spaces that have runs with the given dependency defined.</p>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#example-with-github-actions","title":"Example with GitHub Actions","text":"<p>The following example shows how to use external dependencies with GitHub Actions. First, define a push policy with dependencies. This example defines two dependencies: one for a binary build and one for a Docker image build.</p> <pre><code>package spacelift\n\ntrack {\n    input.push != null\n    input.push.branch == input.stack.branch\n}\n\nexternal_dependency[sprintf(\"%s-binary-build\", [input.push.hash])] { true }\nexternal_dependency[sprintf(\"%s-docker-image-build\", [input.push.hash])] { true }\n</code></pre> <p>Next, create a GitHub Action pipeline that will mark the dependencies as <code>finished</code> or <code>failed</code>. This pipeline will define two jobs, one for each dependency. We will use <code>sleep</code> to mock the build process.</p> <pre><code>name: Build\n\non:\n  push:\n\njobs:\n  build-binaries:\n    name: Build binaries\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Install spacectl\n        uses: spacelift-io/setup-spacectl@main\n\n      - name: Check out repository code\n        uses: actions/checkout@v5\n\n      - name: Build binaries\n        run: |\n          sleep 15\n          echo \"building binaries done\"\n\n      - name: Notify Spacelift of build completion (success)\n        if: success()\n        env:\n          SPACELIFT_API_KEY_ENDPOINT: https://&lt;youraccount&gt;.app.spacelift.io\n          SPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\n          SPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\n        run: spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-binary-build\" --status finished\n\n      - name: Notify Spacelift of build completion (failed)\n        if: failure()\n        env:\n          SPACELIFT_API_KEY_ENDPOINT: https://&lt;youraccount&gt;.app.spacelift.io\n          SPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\n          SPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\n        run: spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-binary-build\" --status failed\n\n  build-docker-images:\n    name: Build docker images\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Install spacectl\n        uses: spacelift-io/setup-spacectl@main\n\n      - name: Check out repository code\n        uses: actions/checkout@v5\n\n      - name: Build docker images\n        run: |\n          sleep 30\n          echo \"building images done\"\n\n      - name: Notify Spacelift of build completion (success)\n        if: success()\n        env:\n          SPACELIFT_API_KEY_ENDPOINT: https://&lt;youraccount&gt;.app.spacelift.io\n          SPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\n          SPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\n        run: spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-docker-image-build\" --status finished\n\n      - name: Notify Spacelift of build completion (failed)\n        if: failure()\n        env:\n          SPACELIFT_API_KEY_ENDPOINT: https://&lt;youraccount&gt;.app.spacelift.io\n          SPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\n          SPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\n        run: spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-docker-image-build\" --status failed\n</code></pre> <p>Warning</p> <p>Replace <code>&lt;youraccount&gt;</code> with your Spacelift account name and fill in necessary secrets if you decide to use this example.</p>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#testing-example","title":"Testing example","text":"<p>With the policy and the pipeline defined, we can test it. Creating a new commit in the repository will trigger the pipeline.</p> <ol> <li>A run was created in Spacelift, but it's in queued state. The run will not start until all the dependencies are marked as finished or skipped.     </li> <li>After the <code>docker-image-build</code> dependency has been marked as skipped the run is still queued, as the <code>binary-build</code> dependency is still not resolved.     </li> <li>The run starts only after all the dependencies reach a terminal state.     </li> </ol> <p>We can also test what happens if a step in the pipeline fails by changing the <code>build-binaries</code> job in the pipeline from:</p> <pre><code>      - name: Build binaries\n        run: |\n          sleep 15\n          echo \"building binaries done\"\n</code></pre> <p>to:</p> <pre><code>      - name: Build binaries\n        run: |\n          sleep 15\n          echo \"building binaries failed\"\n          exit 1\n</code></pre> <p>Now, when we push a commit to the repo, the new run will be marked as failed with a note explaining that one of the dependencies is marked as failed.</p> <p></p>"},{"location":"concepts/resources.html","title":"Resources","text":""},{"location":"concepts/resources.html#resources","title":"Resources","text":"<p>One major benefit of specialized tools like Spacelift - as opposed to general-purpose CI/CD platforms - is that they intimately understand the material they're working with. With regards to infra-as-code, the most important part of this story is understanding your managed resources in-depth. Both from the current perspective, but also being able to put each resource in its historical context.</p> <p>The Resources view is the result of multiple months of meticulous work understanding and documenting the lifecycle of each resource managed by Spacelift, regardless of the technology used - OpenTofu/Terraform, Terragrunt, Pulumi or AWS CloudFormation.</p>"},{"location":"concepts/resources.html#stack-level-resources","title":"Stack-level resources","text":"<p>This screen shows you the stack-level resources view. By default, resources are grouped to help you understand the structure of each of your infrastructure projects.</p> <p></p> <p>Resources can be grouped by provider and type. Let's group by provider:</p> <p></p> <p>We can see lots of AWS resources, <code>random</code>, <code>null_resource</code>, and a one from TLS. Let's now filter just the TLS one.</p> <p></p> <p>Let's now take a look at this one:</p> <p></p> <p>The panel that is now showing on the right hand side of the Resources view shows the details of a single resource, which is now highlighted using a blue background color. On this panel, we see two noteworthy things.</p> <p>Starting with the lower right hand corner, we have the vendor-specific representation of the resource. Note how for security purposes all string values are sanitized. In fact, we never get to see them directly - we only see first 7 characters of their checksum. If you know the possible value, you can easily do the comparison. If you don't, then the secret is safe.</p> <p>More importantly, though, you can drill down to see the runs that either created, or last updated each of the managed resources. Let's now go back to our <code>tls-key</code>, and click on the ID of the run shown in the Updated by section. This will take you to the run in question:</p> <p></p> <p>One extra click on the commit SHA will take you to the GitHub commit. Depending on your Git flow, the commit may be linked to a Pull Request, giving you the ultimate visibility into the infrastructure change management process:</p> <p></p>"},{"location":"concepts/resources.html#account-level-resources","title":"Account-level resources","text":"<p>A view similar to stack-level resources is available for the entire account, too.</p> <p></p> <p>We can unfold stack resources by clicking on the arrow:</p> <p></p> <p>By default we group by stack, giving you the same view as for stack-level resources. But you will notice that there are more rows in the table representing different stacks.</p> <p>In this view you can also filter and group by different properties.</p> <p></p>"},{"location":"concepts/resources.html#shared-via-a-link","title":"Shared via a link","text":"<p>You can share a resource via a link by clicking the icon in the top-right corner.</p> <p></p> <p>The other way is to click on three dots on the table row and choose the Copy link option.</p> <p></p> <p>It's possible from both views - stack-level and account-level.</p> <p></p>"},{"location":"concepts/resources.html#add-to-filters-cell-option","title":"Add to filters - cell option","text":"<p>On the left side, you can see the filter menu. You can also use filters by clicking on the three dots for a column.</p> <p> </p>"},{"location":"concepts/resources/configuration-management.html","title":"Configuration Management","text":""},{"location":"concepts/resources/configuration-management.html#configuration-management","title":"Configuration Management","text":"<p>Configuration Management View, is a view designed to enhance visibility, control, and monitoring of Ansible tasks across your stacks and runs.</p> <p>This feature is available only for Ansible stacks, offering a focused way to monitor the last status of each item in your Ansible inventory. Below is an overview of the updates and functionality:</p>"},{"location":"concepts/resources/configuration-management.html#where-youll-find-it","title":"Where You\u2019ll Find It","text":""},{"location":"concepts/resources/configuration-management.html#stack-view","title":"Stack View","text":"<p>The Configuration Management View is available in the Stack View, replacing the Resources View for Ansible stacks.</p> <p></p> <p>Info</p> <p>Since the new changes replace Resources View you will not be able to see Resources View in Ansible stacks by default. If you'd like to see it (e.g. because you have some historical resources you want to investigate there), you can toggle between the Configuration Management View and the previous Resources View using the \u201cEnable configuration management view\u201d toggle.</p>"},{"location":"concepts/resources/configuration-management.html#resources-view","title":"Resources View","text":"<p>The Configuration Management View is also added as a separate tab in the Resources View.</p> <p></p>"},{"location":"concepts/resources/configuration-management.html#run-view","title":"Run View","text":"<p>The Tasks Tab in the Run View provides detailed visibility into Ansible task execution during a run. </p>"},{"location":"concepts/resources/configuration-management.html#key-features","title":"Key Features","text":""},{"location":"concepts/resources/configuration-management.html#task-monitoring","title":"Task Monitoring","text":"<ul> <li>View the last status of every item in your Ansible inventory, showing the outcome of the most recent run.</li> <li>Navigate seamlessly through tasks to analyze their status, logs, and execution details.</li> </ul>"},{"location":"concepts/resources/configuration-management.html#enhanced-run-list-view","title":"Enhanced Run List View","text":"<ul> <li>The updated run list now includes Ansible statuses, providing immediate insights without diving into individual runs.</li> </ul>"},{"location":"concepts/resources/configuration-management.html#detailed-logs-in-task-details","title":"Detailed Logs in Task Details","text":"<p>Access detailed logs for task execution in the task details tab to diagnose and debug issues efficiently.</p>"},{"location":"concepts/resources/configuration-management.html#why-use-the-configuration-management-view","title":"Why Use the Configuration Management View?","text":"<ul> <li>Increase Visibility: Unified workflows for all tools, including Terraform, OpenTofu, and Ansible.</li> <li>Encourage Automation: Seamlessly integrate infrastructure control and configuration management with stack dependencies.</li> <li>Improve Audit Capabilities: Collect, analyze, and filter data from execution logs.</li> <li>Understand Tasks Intuitively: Visualize and filter tasks with ease.</li> </ul>"},{"location":"concepts/run.html","title":"Run","text":""},{"location":"concepts/run.html#run","title":"Run","text":"<p>Every job that can touch your Spacelift-managed infrastructure is called a Run. There are four main types of runs, and each of them warrants a separate section.</p> <p>Three of them are children of Stacks:</p> <ul> <li>task, which is a freeform command you can execute on your infrastructure;</li> <li>proposed run, which serves as a preview of introduced changes;</li> <li>tracked run, which is a form of deployment;</li> </ul> <p>There's also a fourth type of run - module test case. Very similar to a tracked run, it's executed on a OpenTofu/Terraform module.</p>"},{"location":"concepts/run.html#execution-model","title":"Execution model","text":"<p>In Spacelift, each run is executed on a worker node, inside a Docker container. We maintain a number of these worker nodes (collectively known as the public worker pool) that are available to all customers, but also allow individual customers to run our agent on their end, for their exclusive use. You can read more about worker pools here.</p> <p>Regardless of whether you end up using a private or a public worker pool, each Spacelift run involves a handover between spacelift.io (which we like to call the mothership) and the worker node. After the handover, the worker node is fully responsible for running the job and communicating the results of the job back to the mothership.</p> <p>Info</p> <p>It's important to know that it's always the worker node executing the run and accessing your infrastructure, never the mothership.</p>"},{"location":"concepts/run.html#common-run-states","title":"Common run states","text":"<p>Regardless of the type of the job performed, some phases and terminal states are common. We discuss them here, so that we can refer to them when describing various types of runs in more detail.</p>"},{"location":"concepts/run.html#queued","title":"Queued","text":"<p>Queued means that the run is not Ready for processing, as it's either blocked, waiting for dependencies to finish, or requires additional action from a user.</p> <p>Spacelift serializes all state-changing operations to the Stack. Both tracked runs and tasks have the capacity to change the state, they're never allowed to run in parallel. Instead, each of them gets an exclusive lock on the stack, blocking others from starting.</p> <p>If your run or task is currently blocked by something else holding the lock on the stack, you'll see the link to the blocker in run state list:</p> <p></p> <p>There can also be other reasons why the run is in this state and is not being promoted to state Ready:</p> <ul> <li>It needs to be approved</li> <li>It's waiting for dependant stacks to finish</li> <li>It's waiting for external dependencies to finish</li> </ul> <p>Queued is a passive state meaning no operations are performed while a run is in this state. The user can also discard the run while it's still queued, transitioning it to the terminal Discarded state.</p>"},{"location":"concepts/run.html#ready","title":"Ready","text":"<p>Ready state means that the run is eligible for processing and is waiting for a worker to become available. A run will stay in this state until it's picked up by a worker.</p> <p>When using the public worker pool, you will have to wait until a worker becomes available. For private workers, please refer to the worker pools documentation for troubleshooting advice.</p> <p>Ready is a passive state meaning no operations are performed while a run is in this state. When a worker is available, the state will automatically transition to Preparing. The user is also able to discard the run even if it's ready for processing, transitioning it to the terminal Discarded state.</p>"},{"location":"concepts/run.html#discarded","title":"Discarded","text":"<p>Discarded state means that the user has manually stopped a Queued run or task even before it had the chance to be picked up by the worker.</p> <p>Discarded is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can follow it.</p>"},{"location":"concepts/run.html#preparing","title":"Preparing","text":"<p>The preparing state is the first one where real work is done. At this point both the run is eligible for processing and there's a worker node ready to process it. The preparing state is all about the handover and dialog between spacelift.io and the worker.</p> <p>Here's an example of one such handover:</p> <p></p> <p>Note that Ground Control refers to the bit directly controlled by us, in a nod to late David Bowie. The main purpose of this phase is for Ground Control to make sure that the worker node gets everything that's required to perform the job, and that it can take over the execution.</p> <p>Once the worker is able to pull the Docker image and use it to start the container, this phase is over and the initialization phase begins. If the process fails for whatever reason, the run is marked as failed.</p>"},{"location":"concepts/run.html#initializing","title":"Initializing","text":"<p>The last phase where actual work is done and which is common to all types of run is the initialization. This phase is handled exclusively by the worker and involves running pre-initialization hooks and vendor-specific initialization process. For Terraform stacks it would mean running <code>terraform init</code>, in the right directory and with the right parameters.</p> <p>Important thing to note with regards to pre-initialization hooks and the rest of the initialization process is that all these run in the same shell session, so environment variables exported by pre-initialization hooks are accessible to the vendor-specific initialization process. This is often the desired outcome when working with external secret managers like HashiCorp Vault.</p> <p>If this phase fails for whatever reason, the run is marked as failed. Otherwise, the next step is determined by the type of the run being executed.</p>"},{"location":"concepts/run.html#failed","title":"Failed","text":"<p>If a run transitions into the failed state means that something, at some point went wrong and this state can follow any state. In most cases this will be something related to your project like:</p> <ul> <li>errors in the source code;</li> <li>pre-initialization checks failing;</li> <li>plan policies rejecting your change;</li> <li>deployment-time errors;</li> </ul> <p>In rare cases errors in Spacelift application code or third party dependencies can also make the job fail. These cases are clearly marked by a notice corresponding to the failure mode, reported through our exception tracker and immediately investigated.</p> <p>Failed is a passive state meaning no operations are performed while the run is in this state. It's also a terminal state meaning that no further state can supersede it.</p>"},{"location":"concepts/run.html#finished","title":"Finished","text":"<p>Finished state means that the run was successful, though the success criteria will depend on the type of run. Please read the documentation for the relevant run type for more details.</p> <p>Finished is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it.</p>"},{"location":"concepts/run.html#full-list-of-potential-run-states","title":"Full List of Potential Run States","text":"<p>NONE - The stack is created but no initial runs have been triggered. This is not offically a run state but is listed in the UI for informational purposes.</p> <p>QUEUED - The run is queued and is either waiting for an available worker or for the stack lock to be released by another run.</p> <p>CANCELED - The run has been canceled by the user.</p> <p>INITIALIZING - The run's workspace is currently initializing on the worker.</p> <p>PLANNING - The worker is planning the run.</p> <p>FAILED - The run has failed.</p> <p>FINISHED - The run or task has finished successfully.</p> <p>UNCONFIRMED - The run has planned successfully, it reports changes and is waiting for the user to review those.</p> <p>DISCARDED - The run's plan has been rejected by the user.</p> <p>CONFIRMED - The run's plan has been confirmed by the user. The run is now waiting for the worker to pick it up and start processing.</p> <p>APPLYING - A worker is currently applying the run's changes.</p> <p>PERFORMING - The worker is currently performing the task requested by the user.</p> <p>STOPPED - The run has been stopped by the user.</p> <p>DESTROYING - The worker is currently performing a destroy operation on the managed resources.</p> <p>PREPARING - The workspace is currently being prepared for the run.</p> <p>PREPARING_APPLY - The workspace is currently being prepared for the change deployment.</p> <p>SKIPPED - The run was skipped.</p> <p>REPLAN_REQUESTED - The run is pending a replan and should get picked up by the scheduler.</p> <p>PENDING - Deprecated state.</p> <p>PREPARING_REPLAN - The run is being prepared to be replanned.</p> <p>READY - The run is ready to be started.</p> <p>PENDING_REVIEW - The proposed run is waiting for post-planning approval policy sign-off.</p>"},{"location":"concepts/run.html#stopping-runs","title":"Stopping runs","text":"<p>Some types of runs in some phases may safely be interrupted. We allow sending a stop signal from the GUI and API to the run, which is then passed to the worker handling the job. It's then up to the worker to handle or ignore that signal.</p> <p>Stopped state indicates that a run has been stopped while Initializing or Planning, either manually by the user or - for proposed changes - also by Spacelift. Proposed changes will automatically be stopped when a newer version of the code is pushed to their branch. This is mainly designed to limit the number of unnecessary API calls to your resource providers, though it saves us a few bucks on EC2, too.</p> <p>Here's an example of a run manually stopped while Initializing:</p> <p></p> <p>Stopped is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it.</p>"},{"location":"concepts/run.html#logs-retention","title":"Logs Retention","text":"<p>Run logs are kept for a configurable retention period. By default, run logs have a retention period of 60 days. If there's a need to customize the retention period, you can do so by configuring a custom S3 bucket configuration for the run logs.</p>"},{"location":"concepts/run.html#run-prioritization","title":"Run prioritization","text":"<p>Runs have different priorities based on their type.</p> <p>The highest priority is assigned to blocking runs (tracked runs, destroy runs, and tasks), followed by proposed runs, and then drift detection runs. Runs of the same type are processed in the order they were created, oldest first.</p> <p>You can manually change the priority of a run while it's in a non-terminal state if a private worker pool is processing the run. We will process prioritized runs before runs from other stacks. However, a stack's standard order of run types (tasks, tracked, and proposed runs) execution will still be respected.</p> <p>The priority of a run can be changed from the run's view and in the worker pool queue view.</p>"},{"location":"concepts/run.html#run-view","title":"Run view","text":""},{"location":"concepts/run.html#worker-pool-queue-view","title":"Worker pool queue view","text":"<p>Runs can be prioritized manually or automatically, using a push policy. Note that we only recommend automatic prioritization in special circumstances. For most users, the default prioritization (tracked runs, then proposed runs, then drift detection runs) provides an optimal user experience.</p>"},{"location":"concepts/run.html#limit-runs","title":"Limit Runs","text":"<p>You can also limit the number of runs being created for a VCS event by navigating to Organization settings and clicking on Limits.</p> <p></p> <p>You can set a maximum number of 500 runs to be triggered by a VCS event for your account.</p> <p>If the number of runs that are going to be triggered exceeds the limit you have set in Spacelift, you will be able to see this as the reason for failure in your VCS provider.</p>"},{"location":"concepts/run.html#zero-trust-model","title":"Zero Trust Model","text":"<p>For your most sensitive Stacks you can use additional verification of Runs based on arbitrary metadata you can provide to Runs when creating or confirming them.</p> <p>This metadata can be passed through the API or by using the spacectl CLI to create or confirm runs. Every time such an interaction happens, you can add a new piece of metadata, which will form a list of metadata blobs inside of the Run. This will then be available in policies, including the private-worker side initialization policy.</p> <p>This way you can i.e. sign the runs when you confirm them and later verify this signature inside of the private worker, through the initialization policy. There you can use the exec function, which lets you run an arbitrary binary inside of the docker image. This binary would verify that the content of the run and signature match and that the signature is a proper signature of somebody from your company.</p> <p>This works for any kind of Run, including tasks.</p>"},{"location":"concepts/run/ai.html","title":"AI","text":""},{"location":"concepts/run/ai.html#ai","title":"AI","text":"<p>Spacelift now provides a way to harvest the power of AI to summarize failed runs. By clicking the <code>Explain</code> button in the runs history page, Saturnhead will use an advanced LLM to digest the logs of your failed runs and provide useful insights into what went wrong.</p>"},{"location":"concepts/run/ai.html#enabling-saturnhead-features","title":"Enabling Saturnhead features","text":"<p>Saturnhead features have to be enabled by an admin user. Admins can locate the settings screen in <code>Organization settings &gt; Artificial Intelligence</code>. When enabling the Saturnhead Assist features for the first time, you'll be asked to accept the Terms and Conditions. Once accepted, all the users with read access to runs will be able to ask for assistance on failed runs.</p> <p></p> <p>Furthermore, admins can select the LLM model that will be used to provide assistance. Play around with different models to see which one works best for your organization.</p>"},{"location":"concepts/run/ai.html#ai-empowered-run-summaries","title":"AI empowered run summaries","text":"<p>After enabling the feature, the users with read access to the runs will be able to summarize failed executions.</p> <p>Info</p> <p>This feature is only supported on stacks using OpenTofu or Terraform.</p> <p>The results of a summary change on each case due to the LLM being non-deterministic, but they generally provide the following information:</p> <ul> <li>Human readable summary of the logs.</li> <li>Detailed information on what went wrong</li> <li>Code snippet suggestions to help solve the issue.</li> </ul> <p>There are 2 types of summary: Summarize and Explain.</p> <ul> <li>Summarize uses the output logs of the current phase to generate a human-readable summary. They're faster to execute and provide guidance on what went wrong in a specific step.</li> </ul> <p></p> <ul> <li>By clicking Explain, Saturnhead will use the logs of the \"Initialize\", \"Plan\" and \"Apply\" phases combined to generate a global summary. This is ideal for catching issues that span across multiple phases. This summary is slower to run, but it is very helpful when dealing with more complex issues.</li> </ul> <p></p> <p>Once Saturnhead has analyzed the results of the run, it will display a message with the cause of the issue and potential fix solutions for the problem.</p> <p></p>"},{"location":"concepts/run/ignored-triggers.html","title":"Ignored run warnings","text":""},{"location":"concepts/run/ignored-triggers.html#ignored-run-warnings","title":"Ignored run warnings","text":"<p>Ignored run warnings will help you identify why a run has not been triggered when a commit is pushed to your repository.</p> <p>A run can be ignored because of the following reasons:</p> <ul> <li>A change has been made outside of the project root.</li> <li>A tag is pushed.</li> <li>The default push policy is disabled, and no custom push policies are attached.</li> <li>One or many custom push policies were applied and dismissed the commit.</li> </ul>"},{"location":"concepts/run/ignored-triggers.html#limitations","title":"Limitations","text":"<ul> <li>We are currently only showing ignored runs for commits pushed to stack tracked branches.</li> </ul>"},{"location":"concepts/run/ignored-triggers.html#tracked-runs-tab","title":"Tracked runs tab","text":"<p>If you go to the main stack view where you can see the list of runs, a warning box will be shown in case your stack is not in sync. This warning will explain why the commit has been ignored.</p> <p></p> <p>The warning will automatically disappear if you push another commit that triggers a run. You can also dismiss this warning box, and click on the checkbox to not show ignored run warnings again.</p> <p>In case you want to re-enable it, you can find a toggle in your personal settings.</p> <p></p>"},{"location":"concepts/run/ignored-triggers.html#ignored-runs-tab","title":"Ignored runs tab","text":"<p>If you want to go back in time and try to understand why a run has not been triggered, you can open the Ignored runs tab.</p> <p>Info</p> <p>The retention period for those ignored runs is 7 days.</p> <p></p>"},{"location":"concepts/run/proposed.html","title":"Proposed run (preview)","text":""},{"location":"concepts/run/proposed.html#proposed-run-preview","title":"Proposed run (preview)","text":"<p>Proposed runs are previews of changes that would be applied to your infrastructure if the new code was to somehow become canonical, for example by pushing it to the tracked branch.</p> <p>Proposed runs are generally triggered by Git push events. By default, whenever a push occurs to any branch other than the tracked branch, a proposed run is started - one for each of the affected stacks. This default behavior can be extensively customized using our push policies.</p> <p>The purpose of proposed runs is not to make changes to your infrastructure but to merely preview and report them during the planning phase.</p>"},{"location":"concepts/run/proposed.html#planning","title":"Planning","text":"<p>Once the workspace is prepared by the Initializing phase, planning runs a vendor-specific preview command and interprets the results. For OpenTofu that command is <code>tofu plan</code>, for Terraform - <code>terraform plan</code>, for Pulumi - <code>pulumi preview</code>. The result of the planning phase is the collection of currently managed resources and outputs as well as planned changes. This is used as an input to plan policies (optional) and to calculate the delta - always.</p> <p>Note that the Planning phase can be safely stopped by the user.</p> <p>On Ansible stacks, this phase can be skipped without execution by setting the <code>SPACELIFT_SKIP_PLANNING</code> environment variable to true in the stack's environment variables.</p>"},{"location":"concepts/run/proposed.html#plan-policies","title":"Plan policies","text":"<p>If any plan policies are attached to the current stack, each of these policies is evaluated to automatically determine whether the change is acceptable according to the rules adopted by your organization. Here is an example of an otherwise successful planning phase that still fails due to policy violations:</p> <p></p> <p>You can read more about plan policies here.</p>"},{"location":"concepts/run/proposed.html#pending-review","title":"Pending Review","text":"<p>If any plan policy results in a warning and there are approval policies attached, the run will enter a pending review state after planning, in which approval policies will be evaluated. The run will only finish once all approval policies approve.</p> <p></p> <p>This is useful to e.g. block pull requests related to a proposed run when the changes made by the run should be reviewed by another team, like security.</p>"},{"location":"concepts/run/proposed.html#delta","title":"Delta","text":"<p>If the planning phase is successful (which includes policy evaluation), Spacelift analyses the diff and counts the resources and outputs that would be added, changed and deleted if the changes were to be applied. Here's one example of one such delta being reported:</p> <p></p>"},{"location":"concepts/run/proposed.html#success-criteria","title":"Success criteria","text":"<p>The planning phase will fail if:</p> <ul> <li>infrastructure definitions are incorrect - eg. malformed, invalid etc.;</li> <li>external APIs (eg. AWS, GCP, Azure etc.) fail when previewing changes;</li> <li>plan policies return one or more deny reasons;</li> <li>a worker node crashes - eg. you kill a private worker node while it's executing the job;</li> </ul> <p>If that happens, the run will transition to the failed state. Otherwise, the proposed run terminates in the finished state.</p>"},{"location":"concepts/run/proposed.html#reporting","title":"Reporting","text":"<p>The results of proposed runs are reported in multiple ways:</p> <ul> <li>always - in VCS, as commit statuses and pull request comments - please refer to GitHub and GitLab documentation for the exact details;</li> <li>through Slack notifications - if set up;</li> <li>through webhooks - if set up;</li> </ul>"},{"location":"concepts/run/pull-request-comments.html","title":"Pull Request Comments","text":""},{"location":"concepts/run/pull-request-comments.html#pull-request-comments","title":"Pull Request Comments","text":""},{"location":"concepts/run/pull-request-comments.html#pull-request-plan-commenting","title":"Pull Request Plan Commenting","text":""},{"location":"concepts/run/pull-request-comments.html#via-notification-policy","title":"Via Notification policy","text":"<p>We have a nice example in our Notification policy documentation that shows how to add a comment to a pull request about changed resources. It is fully customizable, so you can change the message to your liking.</p>"},{"location":"concepts/run/pull-request-comments.html#via-stack-label-legacy","title":"Via Stack label (legacy)","text":"<p>To enable this feature, simply add the label <code>feature:add_plan_pr_comment</code> to the stacks you wish to have plan commenting enabled for on pull requests.</p> <p></p> <p>Once enabled, on any future pull request activity, the result of the plan will be commented onto the pull request.</p> <p></p>"},{"location":"concepts/run/pull-request-comments.html#pull-request-comment-driven-actions","title":"Pull Request Comment-Driven Actions","text":"<p>To enable support for pull request comment events in Spacelift, you will need to ensure the following permissions are enabled within your VCS app integration. Note that if your VCS integration was created using the Spacelift VCS setup wizard, then these permissions have already been set up automatically, and no action is needed.</p> <ul> <li>Read access to <code>issues</code> repository permissions</li> <li>Subscribe to <code>issues:comments</code> event</li> </ul> <p>Assuming the above permissions are configured on your VCS application, you can then access pull request comment event data from within your Push policy, and build customizable workflows using this data.</p> <p>Warning</p> <p>Please note that Spacelift will only evaluate comments that begin with<code>/spacelift</code> to prevent users from unintended actions against their resources managed by Spacelift. Furthermore, Spacelift only processes event data for new comments, and will not receive event data for edited or deleted comments.</p> <p>Example Push policy to trigger a tracked run from a pull request comment event:</p> <pre><code>package spacelift\n\ntrack {\n    input.pull_request.action == \"commented\"\n    input.pull_request.comment == concat(\" \", [\"/spacelift\", \"deploy\", input.stack.id])\n}\n</code></pre> <p>Using a Push policy such as the example above, a user could trigger a tracked run on their Spacelift stack by commenting something such as:</p> <pre><code>/spacelift deploy my-stack-id\n</code></pre> <p>Events triggered by comments are subject to the same deduplication logic as other VCS events. This means that if the commit data remains unchanged, a new run will not be created. However there is an exception for pull request comment events: if push policy results in a proposed decision and the comment starts with the <code>/spacelift</code> command, deduplication rules do not apply and the run will be created regardless. This allows you to trigger an unlimited amount of proposed runs from a single commit, example:</p> <pre><code>package spacelift\n\npropose {\n    input.pull_request.action == \"commented\"\n    input.pull_request.comment == concat(\" \", [\"/spacelift\", \"propose\", input.stack.id])\n}\n</code></pre>"},{"location":"concepts/run/run-promotion.html","title":"Run Promotion","text":""},{"location":"concepts/run/run-promotion.html#run-promotion","title":"Run Promotion","text":""},{"location":"concepts/run/run-promotion.html#what-is-run-promotion","title":"What is Run Promotion?","text":"<p>As a quick summary of the differences between the two types of runs: proposed runs only display changes to be made, while tracked runs apply (deploy) the proposed changes.</p> <p>Promoting a proposed run is triggering a tracked run for the same Git commit.</p>"},{"location":"concepts/run/run-promotion.html#using-run-promotion","title":"Using Run Promotion","text":""},{"location":"concepts/run/run-promotion.html#pre-requisites","title":"Pre-Requisites","text":"<ol> <li> <p>For a run to be promote-able, the proposed run must point to a commit that is newer than the stack's current commit.</p> </li> <li> <p>To promote a run, you first need to ensure that you have <code>Allow run promotion</code> enabled in the stack settings of your stack(s) in which you'd like to promote runs.</p> </li> <li> <p>You should have a write permission to promote a run.</p> </li> </ol> <p>Stack Settings &gt; Behavior &gt; Allow Run Promotion</p> <p></p>"},{"location":"concepts/run/run-promotion.html#promote-from-proposed-run-view","title":"Promote from Proposed Run View","text":"<p>Assuming you've enabled Run Promotion within the stack settings, and the commit to be promoted is newer than the stack's current commit. On a given proposed run, you will then see the \"Promote\" button as seen in the screenshot below. You simply need to click this button to promote the proposed run into a tracked run.</p> <p></p>"},{"location":"concepts/run/run-promotion.html#promote-from-a-pull-request","title":"Promote from a Pull Request","text":"<p>For Spacelift users utilizing GitHub, a similar feature is available directly from the GitHub Pull Request. Assuming the same criteria is met as mentioned previously: 1) The commit to be promoted is newer than the stack's current commit 2) Run Promotion is enabled on the stack - Then, you will see a <code>Deploy</code> button available within the Checks tab of the pull request. This button will promote your proposed run into a tracked run.</p> <p></p>"},{"location":"concepts/run/task.html","title":"Task","text":""},{"location":"concepts/run/task.html#task","title":"Task","text":"<p>While tasks enjoy the privilege of having their own GUI screen, they're just another type of run. The core difference is that after the common initialization phase, a task will run your custom command instead of a string of preordained vendor-specific commands.</p>"},{"location":"concepts/run/task.html#the-purpose-of-tasks","title":"The purpose of tasks","text":"<p>The main purpose of task is to perform arbitrary changes to your infrastructure in a coordinated, safe and audited way. Tasks allow ultimate flexibility and can be used to check the environment (see the humble <code>ls -la</code> on the above screenshot), perform benign read-only operations like showing parts of the Terraform state, or even make changes to the state itself, like tainting a resource.</p> <p>Given that thanks to the Docker integration you have full control over the execution environment of your workloads, there's hardly a limit to what you can do.</p> <p>Danger</p> <p>Obvious abuse of shared workers will get you kicked out of the platform. But you can abuse private workers all you like.</p> <p>With the above caveat, let's go through the main benefits of using Spacelift tasks.</p>"},{"location":"concepts/run/task.html#coordinated","title":"Coordinated","text":"<p>Tasks are always treated as operations that may change the underlying state, and are thus serialized. No two tasks will ever run simultaneously, nor will a task execute while a tracked run is in progress. This prevents possible concurrent updates to the state that would be possible without a centrally managed mutex.</p> <p>What's more, some tasks will be more sensitive than others. While a simple <code>ls</code> is probably nothing to be afraid of, the two-way state migration described above could have gone wrong in great many different ways. The stack locking mechanism thus allows taking exclusive control over one or more stacks by a single individual, taking the possibility of coordination to a whole new level.</p>"},{"location":"concepts/run/task.html#safe","title":"Safe","text":"<p>Any non-trivial infrastructure project will inevitably be full of credentials and secrets which are possibly too sensitive to be stored even on a work laptop. Tasks allow any operation to be executed remotely, preventing the leak of sensitive data.</p> <p>Spacelift's integration with infra providers like AWS also allows authentication without any credentials whatsoever, which further protects you from the shame and humiliation of having the keys to the kingdom leaked by running the occasional <code>env</code> command, as you do. Actually, let's run it in Spacelift to see what gives:</p> <p></p> <p>Yes, the secrets are masked in the output and won't leak due to an honest mistake.</p> <p>Danger</p> <p>There are limits to the extent we can protect you from a determined attacker with write access to your stack. We don't want to give you a false sense of security where none is warranted. You may want to look into task policies to prevent certain (or even all) tasks from being executed.</p>"},{"location":"concepts/run/task.html#audited","title":"Audited","text":"<p>Unlike arbitrary operations performed on your local machine, tasks are recorded for eternity, so in cases where some archaeology is necessary, it's easy to see what happened and when. Tasks are attributed to individuals (or API keys) that triggered them and the access model ensures that only stack writers can trigger tasks, giving you even more control over your infrastructure.</p>"},{"location":"concepts/run/task.html#performing-a-task","title":"Performing a task","text":"<p>Apart from the common run phases described in the general run documentation, tasks have just one extra state - performing. That's when the arbitrary user-supplied command is executed, wrapped in <code>sh -c</code> to support all the shell goodies we all love to abuse. In particular, you can use as many <code>&amp;&amp;</code> and <code>||</code> as you wish.</p> <p>Performing a task will succeed and the task will transition to the finished state if the exit code of your command is 0 (the Unix standard). Otherwise the task is marked as failed. Performing cannot be stopped since we must assume that it involves state changes.</p> <p>Tip</p> <p>Tasks are not interactive so you may need to add the <code>-force</code> argument to the command.</p>"},{"location":"concepts/run/task.html#skipping-initialization","title":"Skipping initialization","text":"<p>In rare cases it may be useful to perform tasks without initialization - like when the initialization would fail without some changes being introduced. An obvious example here are OpenTofu/Terraform version migrations. This corner case is served by explicitly skipping the initialization. In the GUI (on by default), you will find the toggle to control this behavior:</p> <p></p> <p>Let's execute a task without initialization on an OpenTofu stack:</p> <p></p> <p>Notice how the operation failed because it is expected to be executed on an initialized OpenTofu workspace. But the same operation would easily succeed if we were to run it in the default mode, with initialization:</p> <p></p>"},{"location":"concepts/run/test-case.html","title":"Module test case","text":""},{"location":"concepts/run/test-case.html#module-test-case","title":"Module test case","text":"<p>Module test cases are special types of runs that are executed not on Stacks but on Spacelift-managed OpenTofu/Terraform modules. Note that this article does not cover modules specifically - for that please refer directly to their documentation. The purpose of this article is to explain how modules test cases are executed and how they're different from other types of runs.</p> <p>In a nutshell, module test cases are almost identical to autodeployed tracked runs. But unlike stacks, modules are stateless and do not manage any resources directly, so just after the changes are applied and resources are created, they are immediately destroyed during the destroying phase. Here is what a fully successful module test case looks like:</p> <p></p> <p>Note that the destroying phase will run regardless of whether the applying phase succeeds or fails. This is because the failure could have been partial, and some resources may still have been created.</p> <p>The destroying phase can be skipped without execution by setting the <code>SPACELIFT_SKIP_DESTROYING</code> environment variable to true in the stack's environment variables.</p>"},{"location":"concepts/run/test-case.html#success-criteria","title":"Success criteria","text":"<p>Note</p> <p>Make sure to attach the integration for the cloud provider if your module is for a cloud provider.</p> <p>A module test case will only transition to the successful finished state if all the previous phases succeed. If any of the phases fails, the run will be marked as failed.</p>"},{"location":"concepts/run/tracked.html","title":"Tracked run (deployment)","text":""},{"location":"concepts/run/tracked.html#tracked-run-deployment","title":"Tracked run (deployment)","text":"<p>Tracked runs represent the actual changes to your infrastructure caused by changes to your infrastructure definitions and/or configuration. In that sense, they can be also called deployments. Tracked runs are effectively an extension of proposed runs - instead of stopping at the planning phase, they also allow you to apply the previewed changes.</p> <p>They are presented on the Runs screen, which is the main screen of the Stack view:</p> <p></p> <p>Each of the tracked runs is represented by a separate element containing some information about the attempted deployment:</p> <p></p> <p>Also worth noting is the colorful delta counter present on some runs - as long as the planning phase was successful this visually represents the resources and outputs diff introduced by the change:</p> <p></p>"},{"location":"concepts/run/tracked.html#triggering-tracked-runs","title":"Triggering tracked runs","text":"<p>Tracked runs can be triggered in one of the three ways - manually by the user, by a Git push or by a trigger policy.</p>"},{"location":"concepts/run/tracked.html#triggering-manually","title":"Triggering manually","text":"<p>Any account admin or stack writer can trigger a tracked run on a stack:</p> <p></p> <p>Runs triggered by individuals and machine users are marked accordingly:</p> <p></p>"},{"location":"concepts/run/tracked.html#triggering-runs-with-a-custom-runtime-config","title":"Triggering runs with a custom runtime config","text":"<p>It is possible to trigger runs with a custom runtime configuration. This allows you to tailor the runtime environment to specific needs at the moment of triggering a run.</p> <p></p> <p>The details of runtime config format can be found in the section about runtime YAML reference</p> <p></p> <p>Info</p> <p>Triggering runs with custom runtime config is especially useful when last-mile configuration is needed, for example oftentimes when triggering Ansible runs with custom variables and parameters.</p>"},{"location":"concepts/run/tracked.html#triggering-from-git-events","title":"Triggering from Git events","text":"<p>Tracked runs can also be triggered by Git push and tag events. By default, whenever a push occurs to the tracked branch, a tracked run is started - one for each of the affected stacks. This default behavior can be extensively customized using our push policies.</p> <p>Runs triggered by Git push and/or tag events can are marked accordingly:</p> <p></p>"},{"location":"concepts/run/tracked.html#triggering-from-policies","title":"Triggering from policies","text":"<p>Trigger policies can be used to create sophisticated workflows representing arbitrarily complex processes like staged rollouts or cascading updates. This is an advanced topic, which is described in more detail in its dedicated section. But if you see something like this, be aware of the fact that a trigger policy must have been involved:</p> <p></p>"},{"location":"concepts/run/tracked.html#handling-no-op-changes","title":"Handling no-op changes","text":"<p>If the planning phase detects no changes to the resources and outputs managed by the stack, the tracked run is considered a no-op. In that case it transitions directly from planning to finished state, just like a proposed run. Otherwise, it will go through the approval flow.</p>"},{"location":"concepts/run/tracked.html#approval-flow","title":"Approval flow","text":"<p>If the tracked run detects a change to its managed resources or outputs, it goes through the approval flow. This can be automated or manual.</p> <p>The automated flow involves a direct transition between the planning and applying phase, without an extra human intervention. This is a convenient but not always the safest option.</p> <p>Changes can be automatically applied if both these conditions are met:</p> <ul> <li>autodeploy is turned \"on\" for the Stack;</li> <li>if plan policies are attached, none of them returns any warnings;</li> </ul> <p>Otherwise, the change will go through the manual flow described below.</p>"},{"location":"concepts/run/tracked.html#unconfirmed","title":"Unconfirmed","text":"<p>If a change is detected and human approval is required, a tracked run will transition from the planning state to unconfirmed. At that point the worker node encrypts uploads the entire workspace to a dedicated Amazon S3 location and finishes its involvement with the run.</p> <p>The resulting changes are shown to the user for the final approval:</p> <p></p> <p>Unconfirmed is a passive state meaning no operations are performed while a run is in this state.</p> <p>If the user approves (confirms) the plan, the run transitions to the temporary Confirmed state and waits for a worker node to pick it up. If the user doesn't like the plan and discards it, the run transitions to the terminal Discarded state.</p>"},{"location":"concepts/run/tracked.html#targeted-replan","title":"Targeted replan","text":"<p>When a run is in the Unconfirmed state it's also possible to replan it. When replanning, a user is able to generate a new plan to apply by only picking specific changes from the current plan. This is working similarly to how passing the <code>-target</code> option to a OpenTofu/Terraform plan command does, without giving you the headache of writing the name of each resource you want to add to your targeted run.</p> <p>To get to the replan screen after the run reaches the unconfirmed state, click on the Changes button in the left corner, select the resources you would like to have a targeted plan for, and then, the replan option will pop out, similar to the screenshot below.</p> <p></p>"},{"location":"concepts/run/tracked.html#discarded","title":"Discarded","text":"<p>Discarded state follows Unconfirmed and indicates that the user did not like the changes detected by the Planning phase.</p> <p>Discarded is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it.</p>"},{"location":"concepts/run/tracked.html#confirmed","title":"Confirmed","text":"<p>Confirmed state follows Unconfirmed indicates that a user has accepted the plan generated in the Planning phase and wants to apply it but no worker has picked up the job yet. This state is similar to Queued in a sense that shows only temporarily until one of the workers picks up the associated job and changes the state to Applying. On the other hand, there is no way to stop a run once it's confirmed.</p> <p>Confirmed is a passive state meaning no operations are performed while a run is in this state.</p>"},{"location":"concepts/run/tracked.html#applying","title":"Applying","text":"<p>If the run required a manual approval step, this phase is preceded by another handover (preparing phase) since the run again needs to be yielded to a worker node. This preparing phase is subtly different internally but ultimately serves the same purpose from the user perspective. Here's an example:</p> <p></p> <p>This preparation phase is very unlikely to fail, but if it does (eg. the worker node becomes unavailable during the transition), the run will transition to the terminal failed state. If the handover succeeds, or the run does not go through the manual approval process, the applying phase begins and attempts to deploy the changes. Here's an example:</p> <p></p> <p>This phase can be skipped without execution by setting the <code>SPACELIFT_SKIP_APPLYING</code> environment variable to true in the stack's environment variables.</p> <p></p>"},{"location":"concepts/run/tracked.html#success-criteria","title":"Success criteria","text":"<p>If the run is a no-op or the applying phase succeeds, the run transitions to the finished state. On the other hand, if anything goes wrong, the run is marked as failed.</p>"},{"location":"concepts/run/tracked.html#reporting","title":"Reporting","text":"<p>The results of tracked runs are reported in multiple ways:</p> <ul> <li>as deployments in VCS unless the change is a no-op - please refer to GitHub and GitLab documentation for the exact details;</li> <li>through Slack notifications - if set up;</li> <li>through webhooks - if set up;</li> </ul>"},{"location":"concepts/run/user-provided-metadata.html","title":"User-Provided Metadata","text":""},{"location":"concepts/run/user-provided-metadata.html#user-provided-metadata","title":"User-Provided Metadata","text":"<p>Occasionally you might want to add additional information to your Runs which isn\u2019t handled on a first-class basis by Spacelift. You can attach this kind of information using the run metadata parameter, which is available through spacectl as well as the GraphQL API.</p>"},{"location":"concepts/run/user-provided-metadata.html#usage","title":"Usage","text":"<p>Let\u2019s start with a small example. You\u2019ll need a private worker for this.</p> <p>On the machine where the worker resides, create a simple policy in a file:</p> <pre><code>package spacelift\nsample { true }\n</code></pre> <p>And then start the worker with an additional environment variable:</p> <pre><code>SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY=/path/to/your/policy.rego\n</code></pre> <p>This policy will make our launcher sample each initialization policy evaluation and print it as a log on stderr.</p> <p>We\u2019ll also need a Stack to which this worker is attached.</p> <p>We can now trigger a run and provide an arbitrary metadata string:</p> <pre><code>$ spacectl stack deploy --id testing-spacelift --run-metadata \"deploy-metadata\"\nYou have successfully created a deployment\nThe live run can be visited at http://cube2222.app.spacelift.tf/stack/testing-spacelift/run/01FEKAGP4AYV0DWP4QDFTANRES\n</code></pre> <p>And in the private worker logs we should suitably see (formatted for readability):</p> <pre><code>{\n  \"caller\": \"setup.go:201\",\n  \"level\": \"info\",\n  \"msg\": \"Sample 0/INITIALIZATION/7YGHCNF7W6VMBQ49XQ42MH4JD1/allow\",\n  \"sample\": {\n    \"body\": \"package spacelift\\nsample { true }\\n\",\n    \"input\": {\n      \"docker_image\": \"\",\n      \"run\": {\n        \"based_on_local_workspace\": false,\n        \"changes\": [],\n        \"commit\": {\n          \"author\": \"cube2222\",\n          \"branch\": \"master\",\n          \"created_at\": 1628243895000000000,\n          \"message\": \"Update main.tf\"\n        },\n        \"created_at\": 1630588655754344000,\n        \"id\": \"01FEKAGP4AYV0DWP4QDFTANRES\",\n        \"state\": \"PREPARING\",\n        \"triggered_by\": \"api::01FEGXFB7TWQ2NNF95W7HPRE2E\",\n        \"updated_at\": 1630588656197898500,\n        \"user_provided_metadata\": [\n          \"deploy-metadata\" // (1)\n        ]\n      },\n      \"static_run_environment\": {\n        \"account_name\": \"cube2222\",\n        \"auto_deploy\": false,\n        \"before_apply\": null,\n        \"before_init\": null,\n        \"command\": \"\",\n        \"commit_branch\": \"master\",\n        \"commit_sha\": \"7d629c6c3f3b6da07e28a87727f0586e577d98c1\",\n        \"endpoint_logs\": \"tcp://169.254.0.3:1983\",\n        \"endpoint_registry\": \"registry.spacelift.io\",\n        \"environment_variables\": {},\n        \"project_root\": \"\",\n        \"refresh_state\": true,\n        \"repository_path\": \"cube2222/testing-spacelift\",\n        \"run_type\": \"TRACKED\",\n        \"run_ulid\": \"01FEKAGP4AYV0DWP4QDFTANRES\",\n        \"skip_init\": false,\n        \"stack_labels\": null,\n        \"stack_slug\": \"testing-spacelift\",\n        \"terraform_version\": \"0.14.10\",\n        \"vendor_specific_config\": {\n          \"vendor\": \"terraform\",\n          \"typed_config\": {\n            \"use_terragrunt\": false,\n            \"use_infracost\": false\n          }\n        }\n      },\n      \"worker_version\": \"development\"\n    },\n    \"outcome\": \"allow\",\n    \"results\": {\n      \"deny\": [],\n      \"sample\": true\n    },\n    \"error\": \"\"\n  },\n  \"ts\": \"2021-09-02T13:17:37.785219048Z\"\n}\n</code></pre> <ol> <li>The metadata string</li> </ol> <p>Great!</p> <p>We can now go ahead and confirm this run:</p> <pre><code>$ spacectl stack confirm --id testing-spacelift --run-metadata \"confirm-metadata\" --run 01FEKAGP4AYV0DWP4QDFTANRES\nYou have successfully confirmed a deployment\nThe live run can be visited at http://cube2222.app.spacelift.tf/stack/testing-spacelift/run/01FEKAGP4AYV0DWP4QDFTANRES\n</code></pre> <p>In the policy sample log for the relevant metadata key we\u2019ll see an additional entry, which was added when confirming:</p> <pre><code>\"user_provided_metadata\": [\n  \"deploy-metadata\",\n  \"confirm-metadata\"\n]\n</code></pre> <p>And that's basically it! It's a very flexible building block which lets you build various automation and compliance helper tooling.</p>"},{"location":"concepts/run/user-provided-metadata.html#run-signatures","title":"Run signatures","text":"<p>A standard use case for this feature would be to sign your runs when you\u2019re creating them.</p> <p>You'll have to bring the infrastructure for managing keys and signatures yourself - usually you'll already have something like that internally. But in short you can create a cryptographic signature of the parameters for a run you\u2019re about to create - based on the commit SHA, run type, stack, date, etc. - and then you can pass that signature to Spacelift when creating the run.</p> <p>Later, in the initialization policy you can use the exec function to run your custom binary for verifying that signature. This way - for your most sensitive stacks - you can verify whether runs you are receiving from the Spacelift backend are legit, intentionally created by an employee of your company.</p> <p>Tip</p> <p>We created a reference implementation to demonstrate how to sign runs and verify signatures.</p>"},{"location":"concepts/spaces.html","title":"Spaces","text":""},{"location":"concepts/spaces.html#spaces","title":"Spaces","text":""},{"location":"concepts/spaces.html#introduction","title":"Introduction","text":"<p>With increased usage comes a bigger need for access control and self-service. Having a single team of admins doesn't scale when you start having tens or hundreds of people using Spacelift daily.</p> <p>You may want to delegate partial admin rights to those teams, enable them to manage their own limited environments, but without giving them keys to the whole account and other teams' environments.</p> <p>In Spacelift, this can be achieved by splitting your account into multiple Spaces.</p> <p>Spaces are sets that can be filled with various kinds of Spacelift entities: Stacks, Policies, Contexts, Modules, Worker Pools, and Cloud Integrations.</p> <p>Initially, you start with a <code>root</code> and a <code>legacy</code> space. The <code>root</code> space is the top-level space of your account, while the <code>legacy</code> space exists for backward compatibility with pre-spaces RBAC. You can then create more spaces, ending up with a big tree of segregated environments.</p>"},{"location":"concepts/spaces.html#what-problems-do-spaces-solve","title":"What problems do Spaces solve?","text":"<p>First and foremost, Spaces let you give users limited admin access. This means that, in their space, they can create Stacks, Policies, etc. while not interfering with entities present in other Spaces.</p> <p>Additionally, Spaces bring the ability to share resources between all these isolated environments, which means you can have a single worker pool and a single set of policies that can be reused by the whole organization.</p>"},{"location":"concepts/spaces/access-control.html","title":"Access control","text":""},{"location":"concepts/spaces/access-control.html#access-control","title":"Access control","text":"<p>Spaces provide the organizational structure for Spacelift's RBAC system. Permission management is handled through Login policies or User Management depending on your authorization strategy. All roles are assigned to specific spaces, providing precise control over who can access what resources.</p>"},{"location":"concepts/spaces/access-control.html#roles-and-rbac","title":"Roles and RBAC","text":""},{"location":"concepts/spaces/access-control.html#predefined-roles","title":"Predefined roles","text":"<p>Spacelift provides three predefined roles that can be assigned to users on a space-by-space basis:</p> <ul> <li>Space Reader - View-only access to resources within the space, can add comments to runs for collaboration</li> <li>Space Writer - Space Reader permissions + ability to trigger runs and modify environment variables</li> <li>Space Admin - Space Writer permissions + ability to create and modify stacks and attachable entities</li> </ul> <p>These predefined roles correspond to the legacy system roles (Read/Write/Admin) and provide a simple starting point for organizations new to RBAC.</p>"},{"location":"concepts/spaces/access-control.html#custom-roles","title":"Custom roles","text":"<p>Beyond predefined roles, you can create custom roles with precisely tailored permissions:</p> <ul> <li>Granular Actions: Compose roles from specific actions like <code>run:trigger</code>, <code>stack:manage</code>, <code>context:read</code></li> <li>Business-Aligned: Match roles to your organizational structure and job functions</li> <li>Principle of Least Privilege: Grant exactly the permissions needed, nothing more</li> </ul> <p>Custom Role Example</p> <p>Instead of giving someone full Space Admin access, create a custom \"Infrastructure Developer\" role with just:</p> <ul> <li><code>space:read</code>: View space contents</li> <li><code>stack:read</code>: Understand configurations</li> <li><code>run:trigger</code>: Deploy changes</li> <li><code>run:read</code>: Monitor deployments</li> </ul> <p>A \"Root Space Admin\" is a user given administrative permissions to the <code>root</code> space, which is the top-level space in Spacelift's hierarchy. This gives them special permissions and allows them to manage the entire account, including modifying the space tree and accessing account-wide settings.</p>"},{"location":"concepts/spaces/access-control.html#permission-comparison","title":"Permission comparison","text":"Action\\Role Root Space Admin Space Admin Space Writer Space Reader Setup SSO \u2705 \u274c \u274c \u274c Setup VCS \u2705 \u274c \u274c \u274c Manage Sessions \u2705 \u274c \u274c \u274c Manage Login Policies &amp; User Management Controls \u2705 \u274c \u274c \u274c Manage Audit Trails \u2705 \u274c \u274c \u274c Manage Spaces \u2705 \u2705* \u274c \u274c Manage Stack Config Settings \u2705 \u2705 \u274c \u274c Manage Worker Pools, Contexts \u2705 \u2705 \u274c \u274c Manage Stack Env Vars \u2705 \u2705 \u2705 \u274c Trigger runs \u2705 \u2705 \u2705 \u274c View Stacks \u2705 \u2705 \u2705 \u2705 View Spaces \u2705 \u2705 \u2705 \u2705 View Worker Pools, Contexts \u2705 \u2705 \u2705 \u2705 <p>*Only when assigned to the specific space</p>"},{"location":"concepts/spaces/access-control.html#authorization-methods","title":"Authorization methods","text":""},{"location":"concepts/spaces/access-control.html#user-management","title":"User management","text":"<p>The User Management interface provides a way to assign roles to users, groups, and API keys:</p> <ol> <li>Navigate to Organization Settings \u2192 Identity Management</li> <li>Select Users, IdP group mapping, or API keys</li> <li>Assign predefined or custom roles to specific spaces</li> </ol> <p>See assigning roles to users for detailed instructions.</p>"},{"location":"concepts/spaces/access-control.html#login-policies-policy-as-code","title":"Login policies (policy-as-code)","text":"<p>Login policies enable programmatic role assignment using OPA/Rego:</p>"},{"location":"concepts/spaces/access-control.html#legacy-space-rules-deprecated","title":"Legacy space rules (deprecated)","text":"<p>The legacy space rules are deprecated in favor of RBAC roles:</p> <ul> <li>space_read \u2192 Use RBAC roles with <code>space:read</code> action</li> <li>space_write \u2192 Use RBAC roles with appropriate write actions</li> <li>space_admin \u2192 Use RBAC roles with management actions</li> </ul>"},{"location":"concepts/spaces/access-control.html#rbac-role-assignment","title":"RBAC Role Assignment","text":"<p>Use the <code>roles</code> rule to assign RBAC roles in login policies:</p> <pre><code>package spacelift\n\n# Basic login permissions\nallow { input.session.member }\n\n# Assign RBAC roles using role IDs\nroles[\"development\"][\"developer-role-id\"] {\n    input.session.teams[_] == \"Frontend\"\n}\n\nroles[\"infrastructure\"][\"platform-engineer-role-id\"] {\n    input.session.teams[_] == \"DevOps\"\n}\n\n# Assign admin role for root space\nroles[\"root\"][\"space-admin-role-id\"] {\n    input.session.teams[_] == \"Admin\"\n}\n</code></pre> <p>Getting Role IDs</p> <p>To use custom roles in login policies, copy the role ID from Organization Settings \u2192 Access Control Center \u2192 Roles \u2192 select role \u2192 copy ID.</p> <p>Warning</p> <ul> <li>Please note that Login policies are only allowed to be created in the <code>root</code> space, therefore only <code>root</code> space admins and administrative stacks, as well as <code>legacy</code> space administrative stacks can create or modify them.</li> <li>A logged-in user's access levels only get updated when they log out and in again, so newly added spaces might not be visible to some users. An exception is that the space's creator immediately gets access to it.</li> </ul>"},{"location":"concepts/spaces/access-control.html#inheritance","title":"Inheritance","text":"<p>Inheritance is a toggle that defines whether a space inherits resources from its parent space or not. When set to true, any stack in the child space can use resources such as worker pools or contexts from the parent space. If a space inherits from a parent and its parent inherits from the grandparent, then the space inherits from the grandparent as well.</p> <p>Inheritance also modifies how roles propagate between spaces.</p> <p>In a scenario when inheritance between spaces is turned off, the roles are propagated only down the space tree. On the other hand, when inheritance is enabled, then a user with any role in the child space also gets Read role in their parent.</p> <p>Below is a diagram that demonstrates how this all works in practice. This is a view for a user that was given the following roles by Login policies:</p> <ul> <li>Read in <code>read access space</code></li> <li>Write in <code>write access space</code></li> <li>Admin in <code>admin access space</code></li> </ul> <p>Dashed lines indicate a lack of inheritance, while when it's enabled the lines are solid.</p> <p></p> <p>Let's analyze the tree starting from the left.</p> <p>As mentioned, the user was granted Write access to the <code>write access space</code> space. Because inheritance is enabled, they also received Read access to the <code>access propagates up</code> space and the <code>root</code> space. The reason for that is to allow users to see resources that their space can now use.</p> <p>Next, the user was given Admin access to the <code>admin access space</code> space. Regardless of the inheritance being off, they also received Admin access to the <code>access propagates down</code> space. This makes sense, as we want to allow admins to still manage their spaces subtree even if they want to disable resource sharing between some spaces.</p> <p>Finally, the user was given Read access to the <code>read access space</code> space. Because inheritance is off, they did not receive Read access to the <code>legacy</code> space.</p>"},{"location":"concepts/spaces/access-control.html#related-topics","title":"Related topics","text":"<ul> <li>Authorization &amp; RBAC: Complete guide to Spacelift's authorization system</li> <li>RBAC System: Understanding roles, actions, and actors</li> <li>User Management: GUI-based permission management</li> <li>Login Policies: Policy-as-code authorization</li> </ul>"},{"location":"concepts/spaces/allowing-non-root-admins-to-manage-spaces.html","title":"Managing child spaces","text":""},{"location":"concepts/spaces/allowing-non-root-admins-to-manage-spaces.html#managing-child-spaces","title":"Managing child spaces","text":"<p>By default, only <code>root</code> admins can manage the spaces' tree topology.</p> <p>This limitation stems from the fact that we also grant the <code>root</code> admins the ability to enforce a set of best practices across the whole organization by autoattaching policies.</p> <p>However, if this limitation doesn't align with your organization's needs, you can adjust it in the account settings.</p>"},{"location":"concepts/spaces/allowing-non-root-admins-to-manage-spaces.html#toggling-the-setting","title":"Toggling the setting","text":"<p>The feature is controlled by a toggle found in the account settings. Only <code>root</code> admins can toggle the setting. </p> <p>After enabling this setting, non-root admins will be able to create, manage, and delete child spaces that are children of the spaces they administrate. After disabling the setting, non-root admins will no longer have the ability to modify spaces.</p>"},{"location":"concepts/spaces/allowing-non-root-admins-to-manage-spaces.html#limitations","title":"Limitations","text":"<p>A subspace created by a non-root admin has inheritance enabled by default.</p> <p>Non-root admins cannot create a subspace with disabled inheritance or update the subspace to disable inheritance.</p> <p>Only <code>root</code> admins can create child spaces that have disabled inheritance.</p> <p>Only <code>root</code> admins are eligible to disable inheritance in a space.</p> <p>Only <code>root</code> admins can enable/disable non-root admins to manage sub-spaces.</p>"},{"location":"concepts/spaces/best-practices.html","title":"Spaces Best Practices","text":""},{"location":"concepts/spaces/best-practices.html#spaces-best-practices","title":"Spaces Best Practices","text":"<p>Every stack, context, cloud integration, and other Spacelift element resides inside a \u201cspace.\u201d Think of this as your organizational structure for Spacelift: Who needs access to what in order to do some task?</p>"},{"location":"concepts/spaces/best-practices.html#plan-your-space-structure","title":"Plan your space structure","text":"<p>Spaces are a hierarchical structure, with the <code>root</code> space at the top. You can create as many spaces as you need, and each space can have its own child spaces. Child spaces can \"inherit\" spacelift constructs from their parents, but they can also have their own. This is a powerful tool for organizing your resources, but it can also be a bit overwhelming if you don\u2019t plan it out.</p> <p>When planning your space structure, consider the following:</p> <ul> <li>Who needs access? Spaces are a way to control access to resources. Does Jimmy-Developer need access to production stacks? Should Sally-Security-Engineer approve IAM changes?</li> <li>How do you want to organize your spaces? Spaces are a way to organize your resources. Do you want to organize by team, by environment, by project?</li> <li>How do you want to share resources? Spaces are a way to share resources. Through space inheritance you can share policies, worker pools, and other resources from parent to child spaces.</li> </ul> <p>You will need to balance these considerations to create a space structure that works for your organization. An example of an organization based structure might look like the following.</p> <p></p> <p>Note how the final spaces inside the tree are <code>dev</code> and <code>prod</code>, this is good to keep in mind as you build out your space structure so you can grant production access to a subset of users.</p>"},{"location":"concepts/spaces/best-practices.html#use-spaces-to-control-access","title":"Use spaces to control access","text":"<p>Spaces are a powerful tool for controlling access to resources. By creating spaces for different teams, you can ensure that only the right people have access to the right resources. You should, specifically, ensure production Spacelift constructs are separate from development constructs. This can be done by creating a <code>prod</code> space and a <code>dev</code> space, for example. Additionally, Administrative stacks get the Admin role in the space they belong to. (Administrative stacks in the legacy space get admin access to the root space for backward compatibility reasons.)</p>"},{"location":"concepts/spaces/best-practices.html#idp-groups-for-scalability","title":"IDP Groups for Scalability","text":"<p>Utilize IDP groups to manage permissions efficiently, especially in larger teams. Inside Spacelift login policies, you can grant access to specific Spaces using the teams attribute. Using the above example spaces layout, you can set up the following rule to allow your users to write to the R&amp;D Team space if they are assigned the R&amp;D team in your IDP:</p> <pre><code>space_write[space.id] {\n     space := input.spaces[_]\n     space.name == \"R&amp;D Team\"\n     input.session.teams[_] == \"R&amp;D\"\n}\n</code></pre>"},{"location":"concepts/spaces/best-practices.html#space-inheritance","title":"Space Inheritance","text":"<p>Space inheritance is a tool for sharing resources between spaces. When a space inherits from another space, it gets access to all the resources in the parent space.</p> <p>Inheritance works well with Policy Autoattachment. By creating a policy with an <code>autoattach:*</code> label you enforce the policy on all the stacks in all the spaces that inherit the space where the policy resides.</p> <p>Another good use case for inheritance is keeping VCS integrations and Worker Pools managed in a single space and inherited by all the spaces that need them.</p>"},{"location":"concepts/spaces/creating-a-space.html","title":"Creating a Space","text":""},{"location":"concepts/spaces/creating-a-space.html#creating-a-space","title":"Creating a Space","text":"<p>Creating and modifying spaces takes place in the Spaces tab in the UI.</p> <p></p>"},{"location":"concepts/spaces/creating-a-space.html#spaces-view","title":"Spaces View","text":"<p>The view shows a tree of all the spaces visible to you in your account. The immutable <code>root</code> space is at the top, and from there you can build any tree structure you want.</p> <p>This view behaves a bit differently for users that are admins of the <code>root</code> space.</p> <p>If you are not an admin of the <code>root</code> space, you will only see the spaces that you have access to, additionally, you will see a path from the spaces you have access to, to the <code>root</code> space. Each space card would indicate what access level to that space you have.</p> <p>If you are an admin of the <code>root</code> space you don't see individual access levels, as you are automatically an admin of all spaces.</p> <p></p>"},{"location":"concepts/spaces/creating-a-space.html#creating-a-single-space","title":"Creating a Single Space","text":"<p>You can create a space either by clicking a Create space button in the top right corner of the view, or by clicking the blue addition button at the bottom of a space card.</p> <p>By default, only <code>root</code> admin users can create spaces. This can be overwritten in the account settings.</p> <p></p> <p>Clicking either will open a modal where you can enter the name of the space, its parent and optionally a description and labels.</p> <p>You then can click Create to create the space.</p> <p></p>"},{"location":"concepts/spaces/creating-a-space.html#editing-the-space","title":"Editing the Space","text":"<p>An admin of the <code>root</code> space has the ability to modify spaces. This can be done by clicking on a space card, which opens up a form similar to the one used for creating a space. After performing any changes you can click Save to save them.</p> <p></p>"},{"location":"concepts/spaces/deleting-a-space.html","title":"Deleting a Space","text":""},{"location":"concepts/spaces/deleting-a-space.html#deleting-a-space","title":"Deleting a Space","text":"<p>Click on the Delete space button to delete a space:</p> <p></p> <p>The delete operation is only permitted if the space is empty, and it is a leaf of the space tree.</p> <p>If any of the following conditions is true, the space cannot be deleted:</p> <ul> <li>The space has any child spaces</li> <li>The space contains any stacks or modules</li> <li>The space contains any attachable entities such as worker pools, contexts, cloud integrations, etc.</li> </ul> <p>Info</p> <p>You cannot delete neither the <code>root</code> nor the <code>legacy</code> space.</p> <p>By default, only <code>root</code> admin users can delete spaces. This can be overwritten in the account settings.</p>"},{"location":"concepts/spaces/migrating-out-of-the-legacy-space.html","title":"Migrating out of the Legacy Space","text":""},{"location":"concepts/spaces/migrating-out-of-the-legacy-space.html#migrating-out-of-the-legacy-space","title":"Migrating out of the Legacy Space","text":"<p>After the introduction of Spaces most of your resources will end up in the <code>legacy</code> space. This space is there to provide backward-compatibility with your existing setup, and it's the only place Access policies still work.</p> <p>To get the most out of Spacelift, you'll want to move out of the <code>legacy</code> space into other spaces. For each entity, you can do that by going into the entity's settings and choosing a new space for it. However, you'll have to keep a few things in mind.</p> <p>First, moving entities can't break any relationships between them. For example, if you have a policy attached to a stack, you can't move the policy to a space where it won't be accessible to the stack.</p> <p>Second, you have to move entities one by one, which means that moving stacks with their attachments (policies, contexts, worker pools, etc.) needs to be done in multiple steps.</p> <ol> <li>Create your new space as a child of the <code>root</code> space with inheritance enabled.</li> <li>Move all the attachable entities (policies, contexts, worker pools, etc.) from the <code>legacy</code> space to the <code>root</code> space. This way they're accessible from both the <code>legacy</code> space and your new space.</li> <li>Move the stacks to your new space.</li> <li>Move the attachable entities to your new space.</li> </ol> <p></p> <p>Additionally, when you're ready to stop using access policies in the <code>legacy space</code>, you can start providing users a level of access to the <code>legacy</code> space using the space_admin, space_write, and space_read directives of the Login policy. The moment the Login policy specifies any of these for a user for the <code>legacy</code> space, Access policies will stop being evaluated for this user.</p>"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html","title":"Moving a Space or any Entity","text":""},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-a-space-or-any-entity","title":"Moving a Space or any Entity","text":"<p>If after creating multiple Spaces you conclude that the tree structure could be better, it is possible to restructure the tree completely.</p> <p>Restructuring the tree can be achieved either by creating new spaces and then moving entities (stacks, policies, etc.) to them or by moving (re-parenting) spaces (which includes the entities they contain).</p> <p>Both approaches are valid, but some conditions must be satisfied for the move operations to succeed.</p>"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-a-stack","title":"Moving a Stack","text":"<p>Stacks can be moved to a different space using the Stack Details tab in the Stack's settings.</p> <p></p> <p>For the move operation to succeed, the stack has to maintain access to any attachable resources it uses (worker pools, contexts, cloud integrations, etc.).</p> <p>In other words, the new space the stack will be in must either inherit (directly or indirectly via parental chain) from the spaces that the used attachable resources are in or those resources have to be defined in the new space.</p>"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-an-attachable-entity-worker-pool-context-etc","title":"Moving an Attachable Entity (Worker Pool, Context, etc.)","text":"<p>All the attachable entities can be moved to a different space either:</p> <p></p> <p>Moving entities is possible only if all the stacks that have been using them would still be able to access them after the move in compliance with the inheritance rules.</p>"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-a-space","title":"Moving a Space","text":"<p>Moving a space is essentially changing its parent space.</p> <p>This can be done by choosing a new parent in the space's settings:</p> <p></p> <p>This operation affects the whole subtree, because all the children of the space being moved remain its children, so they change their location in the tree as well.</p> <p>Changing a parent for a space is possible only if after the re-parenting process all the stacks in the whole affected subtree would still be able to access any entities that are attached outside of the subtree, in compliance with the inheritance rules.</p> <p>By default, only <code>root</code> admin users can move spaces. This can be overwritten in the account settings.</p>"},{"location":"concepts/spaces/structuring-your-spaces-tree.html","title":"Structuring your Spaces Tree","text":""},{"location":"concepts/spaces/structuring-your-spaces-tree.html#structuring-your-spaces-tree","title":"Structuring your Spaces Tree","text":"<p>Based on experience with other tools, a first intuition might be to structure your spaces team-first or service-first. Something akin to the following picture:</p> <p></p> <p>However, there are likely Spacelift resources you will want to reuse across all development environment services, not across all environments for a single service. For example, a Worker Pool. That is because resources like Worker Pools are usually shared across security domains, not logical domains.</p> <p>Due to this an architecture akin to the following is more advisable:</p> <p></p> <p>This way you can create your Worker Pools, Contexts and Policies in the <code>dev</code>, <code>preprod</code>, and <code>prod</code> spaces, and then reuse them in all spaces below those.</p> <p>Info</p> <p>Make sure to have inheritance enabled in the space settings if you plan to have, for example, your vcs in a separate space above your stack</p>"},{"location":"concepts/stack.html","title":"Stack","text":""},{"location":"concepts/stack.html#stack","title":"Stack","text":"<p>If you're managing your infrastructure in Spacelift, you're managing them with stacks. A stack is a combination of source code, the current state of the managed infrastructure (e.g. OpenTofu/Terraform state file) and the configuration (environment variables and mounted files). A stack is also an isolated, independent entity.</p> <p>Unless you're only using Spacelift to host and test private OpenTofu/Terraform modules, your account should contain one or more stacks.</p> <p></p>"},{"location":"concepts/stack.html#what-can-i-do-with-stacks","title":"What can I do with stacks?","text":"<ul> <li>Create, delete, and lock stacks.</li> <li>Organize stacks.</li> <li>Configure stack settings.</li> <li>Schedule stack runs.</li> <li>Set up stack dependencies.</li> <li>Detect drift in managed stacks.</li> </ul>"},{"location":"concepts/stack.html#stack-state","title":"Stack state","text":"<p>Similar to runs and tasks, stacks also have states that are generally equal to their most recent tracked run state.</p> <p>The state is set to \"None\" on stacks with no runs:</p> <p></p> <p>Stack states show users the overall health of their infrastructure, and the level of development activity associated with it, at a glance.</p>"},{"location":"concepts/stack/creating-a-stack.html","title":"Create, delete, and lock stacks","text":""},{"location":"concepts/stack/creating-a-stack.html#create-delete-and-lock-stacks","title":"Create, delete, and lock stacks","text":""},{"location":"concepts/stack/creating-a-stack.html#create-a-stack-in-spacelift","title":"Create a stack in Spacelift","text":"<p>Creating a stack involves 9 steps, most of which are optional. Required tasks are marked with an asterisk here:</p> <ol> <li>*Name, describe, and label the stack.</li> <li>*Create a link between your new stack and an existing source code repository.</li> <li>*Choose the backend vendor.</li> <li>Define common behavior of the stack.</li> <li>Create stack hooks.</li> <li>Attach a cloud integration.</li> <li>Attach policies.</li> <li>Attach contexts.</li> <li>*Review the summary and create your stack.</li> </ol> <p>Info</p> <p>You need to be an admin to create a stack. By default, GitHub account owners and admins are automatically given Spacelift admin privileges, but this can be customized using login policies and/or SSO integration.</p> <p>To get started, click Create stack on the Stacks page or Create first stack from the LaunchPad if you haven't set up a stack before.</p> <p></p>"},{"location":"concepts/stack/creating-a-stack.html#1-stack-details","title":"1. Stack details","text":"<p>Fill in required stack details.</p> <p></p> <ol> <li>Name: Enter a unique, descriptive name for your stack.</li> <li>Space: Select the space to create the stack in.</li> <li>Labels (optional): Add labels to help sort and filter your stacks.</li> <li>Description (optional): Enter a (markdown-supported) description of the stack and the resources it manages.</li> <li>Click Continue.</li> </ol>"},{"location":"concepts/stack/creating-a-stack.html#2-connect-to-source-code","title":"2. Connect to source code","text":"<p>Connect your VCS provider and fill in the details.</p> <p></p> <ol> <li>Integration: Verify the VCS integration name is correct.</li> <li>Repository: Select the repository to manage in Spacelift. If you have multiple repositories linked to one VCS, leave blank.</li> <li>Branch: Select the branch of the repository to manage with this stack.</li> <li>Project root (optional): If the entrypoint of the stack is different than the root of the repo, enter its path here.</li> <li>Project globs (optional): Enter additional files and directories that should be managed by the stack.</li> <li>Click Continue.</li> </ol>"},{"location":"concepts/stack/creating-a-stack.html#3-choose-vendor","title":"3. Choose vendor","text":"<p>Select your IaC vendor and fill in the required details, then click Create &amp; continue.</p> <p></p>"},{"location":"concepts/stack/creating-a-stack.html#opentofuterraform","title":"OpenTofu/Terraform","text":"<p>We support Terraform 0.12.0 and above, and all OpenTofu versions. Spacelift also supports full Terraform version management allowing you to preview the impact of upgrading to a newer version.</p> <p>Warning</p> <p>This is the only time you can ask Spacelift to be the state backend for a given OpenTofu/Terraform stack.</p> <ol> <li>Workflow tool: Set to OpenTofu, Terraform (FOSS), or Custom.<ul> <li>With OpenTofu or Terraform (FOSS), select a specific version or enter a version range.</li> </ul> </li> <li>Smart Sanitization (recommended): Choose whether Spacelift attempts to sanitize sensitive resources created by OpenTofu/Terraform.</li> <li>Manage State (recommended): Choose whether Spacelift should handle the OpenTofu/Terraform state.<ol> <li>If disabled: Optionally enter a workspace.</li> <li>If enabled: Configure these options:<ul> <li>External state access: Allow external read-only access for administrative stacks or users with write permissions to the Stack's space.</li> <li>Import existing state file: Enable to import a state file from your previous backend.</li> </ul> </li> </ol> </li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"concepts/stack/creating-a-stack.html#pulumi","title":"Pulumi","text":"<ol> <li>Login URL: Enter the URL to your Pulumi state backend.</li> <li>Stack name: Enter a name for your Pulumi stack. This is separate from the name of the Spacelift stack, but you can give both the same name.</li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"concepts/stack/creating-a-stack.html#aws-cloudformation","title":"AWS CloudFormation","text":"<ol> <li>Region: Enter the AWS region your stack will be located in (e.g. <code>us-east-2</code>).</li> <li>Stack name: Enter the name of the corresponding CloudFormation stack.</li> <li>Entry template file: Enter the path to the template file in your repo describing the root CloudFormation stack.</li> <li>Template bucket: Enter the location of the S3 bucket to store processed CloudFormation templates, so Spacelift can manage the state properly.</li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"concepts/stack/creating-a-stack.html#kubernetes","title":"Kubernetes","text":"<ol> <li>Namespace (optional): Enter the namespace of the Kubernetes cluster you want to run commands on. Leave blank for multi-namespace stacks.</li> <li>Workflow tool: Select the tool used to execute workflow commands.<ul> <li>Kubernetes: Provide the kubectl version the worker will download.</li> <li>Custom: No configuration needed.</li> </ul> </li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"concepts/stack/creating-a-stack.html#terragrunt","title":"Terragrunt","text":"<ol> <li>Terragrunt version: Select a specific OpenTofu/Terraform version or enter a version range.</li> <li>Tool: Select the tool used to make infrastructure changes:<ul> <li>OpenTofu/Terraform (FOSS): Select a specific OpenTofu/Terraform version or enter a version range.</li> <li>Manually provisioned: Outside of Spacelift, ensure the tool is available to the worker via a custom image or hook and set the <code>TERRAGRUNT_TFPATH</code> environment variable to tell Terragrunt where to find it.</li> </ul> </li> <li>Smart Sanitization (recommended): Choose whether Spacelift attempts to sanitize sensitive resources created by OpenTofu/Terraform.</li> <li>Use All Run: Enable to use Terragrunt's run-all feature.</li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"concepts/stack/creating-a-stack.html#ansible","title":"Ansible","text":"<ol> <li>Playbook: Enter the playbook file to run in the stack.</li> <li>Click Create &amp; continue.</li> </ol> <p>Once you've configured your vendor information, click Continue to Define stack behavior.</p> <p></p>"},{"location":"concepts/stack/creating-a-stack.html#4-define-behavior","title":"4. Define behavior","text":"<p>Determine and set additional behaviors for your stack.</p> <ol> <li>Worker pool: Choose which worker pool to use (default is public workers).</li> <li>Runner image: Use a custom runner for your runtime environment.</li> <li>Administrative: Choose whether a stack has privileges to create ohter Spacelift resources via our Terraform provider.</li> <li>Allow run promotion: Allows you to promote a proposed run to a tracked run (i.e. deploy from a feature branch).</li> <li>Autodeploy: Automatically deploy changes to your code.</li> <li>Autoretry: Automatically retry deployment of invalidated proposed runs. For stacks using private workers only.</li> <li>Enable local preview: Preview how code changes will execute with the spacectl CLI feature.</li> <li>Enable secret masking: Automatically redact secret patterns from logs.</li> <li>Protect from deletion (recommended): Protect your stacks from accidental deletion.</li> <li>Transfer sensitive outputs across dependencices: Pass sensitive outputs from this stack to dependent stacks.</li> </ol> <p>Once you've configured your settings, click Save &amp; continue.</p>"},{"location":"concepts/stack/creating-a-stack.html#5-add-hooks","title":"5. Add hooks","text":"<p>You also have the ability to control what happens before and after each runner phase using stack hooks. Define commands that run during the following phases:</p> <ul> <li>Initialization</li> <li>Planning</li> <li>Applying</li> <li>Destroying</li> <li>Performing</li> <li>Finally</li> </ul> <p>Once you've added all hooks, click Save &amp; continue.</p>"},{"location":"concepts/stack/creating-a-stack.html#6-attach-cloud","title":"6. Attach cloud","text":"<p>If desired, attach your cloud provider integration.</p> <ol> <li>Select the cloud provider the stack will use.</li> <li>Attach integration: Choose the name of the integration the stack will use.</li> <li>Read: Use the integration during read phases.</li> <li>Write: Use the integration during write phases.</li> <li>Click Attach.</li> <li>Click Continue.</li> </ol>"},{"location":"concepts/stack/creating-a-stack.html#7-attach-policies","title":"7. Attach policies","text":"<p>If you're just following the LaunchPad steps, you won't have any policies yet. If you did configure policies, you will be able to attach them here:</p> <ul> <li>Approval: Who can approve or reject a run and how a run can be approved.</li> <li>Plan: Which changes can be applied.</li> <li>Push: How Git push events are interpreted.</li> <li>Trigger: What happens when blocking runs terminate.</li> </ul> <p>Click Continue.</p>"},{"location":"concepts/stack/creating-a-stack.html#8-attach-context","title":"8. Attach context","text":"<p>Contexts are sets of environment variables and related configuration, including hooks, that can be shared across multiple stacks. By attaching a context, you ensure your stack has all the necessary configuration elements it needs to operate, without repeating the setup for each stack.</p> <p>If you're just following the LaunchPad steps, you won't have any contexts yet. If you did configure contexts, you will be able to attach them here.</p> <p>Click Continue.</p>"},{"location":"concepts/stack/creating-a-stack.html#9-summary","title":"9. Summary","text":"<p>Review your settings before finalizing your stack, then click Confirm.</p>"},{"location":"concepts/stack/creating-a-stack.html#delete-a-stack-in-spacelift","title":"Delete a stack in Spacelift","text":"<p>If you want to save the state file before deleting a stack, you can retrieve it with a task.</p> <ol> <li>On the Stacks tab, click the three dots next to the stack you want to delete.</li> <li>Click Settings, then click Stack deletion.</li> <li>Choose whether to delete or keep the stack's resources.</li> <li>Type \"delete\" in the box, then click Delete.</li> </ol> <p></p> <p>Info</p> <p>Resource deletion is not currently supported while using the native Terragrunt support.</p>"},{"location":"concepts/stack/creating-a-stack.html#deleting-resources-managed-by-a-stack","title":"Deleting resources managed by a stack","text":"<p>Depending on the backend of your stack, there are different commands you can run as a task before deleting the stack.</p> Backend Command Terraform <code>terraform destroy -auto-approve</code> OpenTofu <code>tofu destroy -auto-approve</code> CloudFormation <code>aws cloudformation delete-stack --stack-name &lt;cloudformation-stack-name&gt;</code> Pulumi <code>pulumi destroy --non-interactive --yes</code> Kubernetes <code>kubectl delete --ignore-not-found -l spacelift-stack=&lt;stack-slug&gt; $(kubectl api-resources --verbs=list,create -o name &amp;#124; paste -s -d, -)</code> <p>Tip</p> <p>For Terraform, you can also run a task through our CLI tool spacectl.</p>"},{"location":"concepts/stack/creating-a-stack.html#scheduled-delete","title":"Scheduled delete","text":"<p>You can use scheduling to delete a stack at a specified time and date.</p>"},{"location":"concepts/stack/creating-a-stack.html#using-the-api","title":"Using the API","text":"<p>You can also use Spacelift's GraphQL API to delete a stack.</p>"},{"location":"concepts/stack/creating-a-stack.html#lock-a-stack-in-spacelift","title":"Lock a stack in Spacelift","text":"<p>Spacelift supports locking a stack for one person's exclusive use. This is useful to prevent someone else's changes to the strack from impacting delicate operations. Every stack writer can lock a stack unless it's already locked.</p> <p>The owner of the lock is the only one who can trigger runs and tasks for the entire duration of the lock. Locks never expire, and only its creator and Spacelift admins can release it.</p> <p></p> <p>Info</p> <p>Note that while a stack is locked, auto deploy is disabled to prevent accidental deployments.</p>"},{"location":"concepts/stack/drift-detection.html","title":"Drift detection","text":""},{"location":"concepts/stack/drift-detection.html#drift-detection","title":"Drift detection","text":""},{"location":"concepts/stack/drift-detection.html#drift-happens","title":"Drift happens","text":"<p>In infrastructure-as-code, the concept of drift represents the difference between the desired state and the actual state of the infrastructure managed by your tool of choice: Terraform, Pulumi, AWS CloudFormation, etc. In practice, there are two sources of drift: changes introduced by external actors, and dependencies on external data sources.</p> <p>1. Changes directly introduced by external actors, either humans or machines (scripts), can cause drift. If an on-call SRE changes your database parameters otherwise controlled by Terraform, you've introduced drift. If an external script updates your Kubernetes cluster in a way that conflicts with its Pulumi definition, it's drift as well.</p> <p>2. The dependency of your resources on external data sources can cause drift. For example, if your load balancer only expects to receive traffic from Cloudflare, you may want to restrict ingress to a predefined range of IPs. However, that range may be dynamic, and your IaC tool queries it every time it runs. If there's any change to the external data source, it's showing up as drift, too.</p> <p>In the first scenario, drift is an unwanted by-product of emergencies or broken processes. In the latter, it's both desired and inevitable proof that your otherwise declarative system responds to external changes. In other words, drift happens.</p>"},{"location":"concepts/stack/drift-detection.html#video-walkthrough","title":"Video Walkthrough","text":""},{"location":"concepts/stack/drift-detection.html#how-spacelift-detects-drift","title":"How Spacelift detects drift","text":"<p>Info</p> <p>Drift detection only works on private workers.</p> <p>Spacelift comes with a built-in mechanism to detect and (optionally) reconcile drift. We do it by periodically executing proposed runs on your stable infrastructure (generally represented by the finished stack state in Spacelift) and checking for any changes.</p> <p>To get started, create a drift detection schedule for your stack. You will be able to add multiple cron rules to define when your detection jobs should be scheduled, as well as decide whether you want your jobs to trigger tracked runs (reconciliation jobs) in response to detected drift:</p> <p></p>"},{"location":"concepts/stack/drift-detection.html#to-reconcile-or-not-to-reconcile","title":"To reconcile, or not to reconcile","text":"<p>We generally suggest enabling reconciliation on your drift detection schedules, as it ensures you get the most out of the feature. Reconciliation jobs are equivalent to manually triggering tracked runs and obey the same rules and constraints. In particular, they respect their stacks' auto-deploy setting and trigger plan policies.</p> <p>However, if you choose not to reconcile changes, you can still see drifted resources in the Resources view, both on the stack and account levels. Also, drift detection jobs trigger webhooks like regular runs, where they're clearly marked as such (<code>driftDetection</code> field).</p> <p></p>"},{"location":"concepts/stack/drift-detection.html#drift-detection-run-limits","title":"Drift Detection Run Limits","text":"<p>Private worker pools support configurable drift detection run limits to prevent resource exhaustion. This allows you to control how many drift detection runs can execute concurrently on a specific worker pool.</p>"},{"location":"concepts/stack/drift-detection.html#configuration","title":"Configuration","text":"<p>Drift detection run limits are configured at the worker pool level through the Spacelift UI:</p> <ul> <li>Enable Drift Detection Run Limits: Use the toggle switch in worker pool settings</li> <li>Set Limit Options:<ul> <li>Disable drift detection entirely: Prevents all drift detection runs on this worker pool</li> <li>Set a numeric limit: Specify the maximum number of concurrent drift detection runs</li> </ul> </li> </ul> <p></p>"},{"location":"concepts/stack/drift-detection.html#behavior","title":"Behavior","text":"<p>When drift detection run limits are configured:</p> <ul> <li>Default Behavior: Without limits, drift detection runs have no concurrency restrictions</li> <li>With Numeric Limit: The job scheduler ensures concurrent drift detection runs don't exceed the configured limit</li> <li>When Disabled: Drift detection runs are prevented, and failed runs are created with explanatory notes for record keeping purposes</li> <li>Limit Exceeded: Additional drift detection runs are skipped until existing ones complete, preventing queue buildup</li> </ul>"},{"location":"concepts/stack/drift-detection.html#use-cases","title":"Use Cases","text":"<p>Drift detection run limits are particularly useful for:</p> <ul> <li>Resource Management: Private worker pools with limited compute capacity</li> <li>Priority Management: Ensuring regular infrastructure changes take precedence over drift detection</li> <li>Maintenance Windows: Temporarily disabling drift detection during scheduled maintenance</li> <li>Cost Control: Reducing resource consumption when drift detection frequency is high</li> </ul> <p>Warning</p> <p>This feature only applies to private worker pools. Public worker pools managed by Spacelift do not support drift detection run limits.</p> <p>Warning</p> <p>If drift detection is disabled on a worker pool, you will not be able to create drift detection schedules for stacks using that worker pool.</p>"},{"location":"concepts/stack/drift-detection.html#drift-detection-in-practice","title":"Drift detection in practice","text":"<p>With drift detection enabled on the stack, proposed runs are quietly executing in the background. If they do not detect any changes, the only way you'd know about them is by viewing all runs in the Account &gt; Runs section and filtering by drift detection parameter.</p> <p></p> <p>If your proposed runs detect drift (and you've enabled reconciliation), Spacelift triggers a regular tracked run. This run is subject to the same rules as a regular tracked run. For example, if you set your stack not to deploy changes automatically, the run will end up in an unconfirmed state, waiting for your decision. The same thing will happen if a plan policy produces a warning using a matched <code>warn</code> rule.</p>"},{"location":"concepts/stack/drift-detection.html#policy-input","title":"Policy input","text":"<p>The only real difference between a drift detection job and one triggered manually is that the run section of your policy input will have the <code>drift_detection</code> field set to <code>true</code>. This applies to both plan and trigger policies. You can use this mechanism to add extra controls to your drift detection strategy.</p> <p>For example, if you're automatically deploying your changes but want a human to look at drift before reconciling it, you can add the following section to your plan policy body:</p> <pre><code>package spacelift\n\nwarn[\"Drift reconciliation requires manual approval\"] {\n  input.spacelift.run.drift_detection\n}\n</code></pre>"},{"location":"concepts/stack/organizing-stacks.html","title":"Organize stacks","text":""},{"location":"concepts/stack/organizing-stacks.html#organize-stacks","title":"Organize stacks","text":"<p>Depending on the complexity of your infrastructure, the size of your team, your particular needs, and your preferred way of working, you may end up managing a lot of stacks. Spacelift offers several options for searching through your stacks, from basic query-based searching to filtering by status and' label-based folders.</p>"},{"location":"concepts/stack/organizing-stacks.html#video-walkthrough","title":"Video Walkthrough","text":""},{"location":"concepts/stack/organizing-stacks.html#customize-your-table-view","title":"Customize your table view","text":"<p>You can customize the table view to suit your needs. Click the Customize list gear icon at the top-right to open a customizing drawer.</p> <p> </p> <ol> <li>Drag and drop columns to rearrange or hide them from view. The name column cannot be hidden.<ul> <li>Alternatively, outside the customize list drawer, hover over the column header and click Hide.    </li> </ul> </li> <li>To reset your settings, click Reset to default at the bottom of the customization drawer.</li> </ol> <p>You can also resize columns in the list by dragging the separator: </p>"},{"location":"concepts/stack/organizing-stacks.html#query-based-search-and-filter","title":"Query-based search and filter","text":"<p>You can search and filter by these stack properties in the search bar:</p> <ul> <li>name</li> <li>ID (slug)</li> <li>any of its labels</li> </ul>"},{"location":"concepts/stack/organizing-stacks.html#filter-by-state","title":"Filter by state","text":"<p>Filtering stacks by state is useful for identifying action items like plans pending confirmation (unconfirmed state) or failed jobs that require fixing. Use the State section on the left-hand sidebar, which displays all states attached to your stacks. Click state checkboxes to filter the list of stacks.</p> <p></p> <p>You can also use our predefined tabs to filter some specific group of states: </p> Predefined Tab Label State Needs Attention unconfirmed Failed failed On Hold none, confirmed, or replan-requested In Progress applying, destroying, initializing, planning, preparing-apply, or preparing-replan Finished finished"},{"location":"concepts/stack/organizing-stacks.html#label-based-folders","title":"Label-based folders","text":"<p>You can group stacks by attaching folder labels to them, which are regular labels prefixed with <code>folder:</code>. In the stacks list, the <code>folder:</code> prefix is replaced width a folder icon.</p> <p></p> <p>Your folder labels will appear in the Folders section of the Filters sidebar menu, allowing you to use the checkboxes to filter the list by folder labels.</p> <p></p>"},{"location":"concepts/stack/organizing-stacks.html#nesting-and-multiple-folder-labels","title":"Nesting and multiple folder labels","text":"<p>Folder labels can be nested, allowing you to create hierarchies or arbitrary classifications of your stacks.</p> <p>A single stack can have any number of folder labels set, in which case it belongs to all the collections. If you use more than one folder label on a single stack, they act more like labels in e-mail inboxes rather than directories in your filesystem.</p>"},{"location":"concepts/stack/organizing-stacks.html#save-filters-in-views","title":"Save filters in views","text":"<p>You can save filter views to easily apply filters to your stacks list.</p> <p></p> <ol> <li>Select all filters you would like to apply.</li> <li>If desired, add a search query or sorting option in the top-right.</li> <li>Click New View, then enter a descriptive name for the filter view.</li> <li>Check the box to set the new view as your default, if desired.<ul> <li>If you want to edit your default view, click Views, click the three dots beside the view name you want to use as default, then click Use as your default view.  </li> </ul> </li> <li>Click Save.</li> </ol> <p>If you set a filter view as your default, the next time you log in or navigate to the Stacks tab, your personal default view will be applied.</p>"},{"location":"concepts/stack/organizing-stacks.html#shared-views","title":"Shared views","text":"<p>Filter views can be private (default for new views) or public to all users of the account. To share a filter view, if you are an admin user, click the three dots beside the view name and click Share within the account. The view will now be visible for all users within the account.</p>"},{"location":"concepts/stack/organizing-stacks.html#reset-to-default-view","title":"Reset to default view","text":"<p>To quickly reset your default view to Spacelift's default (no active filters):</p> <ol> <li>Click Views to see your available filter views.</li> <li>Click the three dots beside the view name you're using as default.</li> <li>Click Reset to Spacelift default view.</li> <li>All sorting, search, and filter parameters will be cleared.</li> </ol> <p></p>"},{"location":"concepts/stack/organizing-stacks.html#manage-view","title":"Manage view","text":"<p>Manage existing views when you click the three dots next to New view.</p> <ul> <li>Click Update to update your current filter view with any new filters, searches, and/or sorting settings.</li> <li>Click Edit name to change the name of the current filter view.</li> <li>Click Delete to remove a private view. If the view has been shared with other users on the account, it cannot be deleted.</li> </ul> <p></p>"},{"location":"concepts/stack/scheduling.html","title":"Scheduling stack actions","text":""},{"location":"concepts/stack/scheduling.html#scheduling-stack-actions","title":"Scheduling stack actions","text":"<p>If you are using private workers, you can schedule stack deletion or other tasks at a specific time or periodically based on cron rules you define. If your stack is using public workers, you can still create the schedules, but they will not trigger until the stack is using private workers.</p> <p></p>"},{"location":"concepts/stack/scheduling.html#schedule-stack-deletion","title":"Schedule stack deletion","text":"<p>You can schedule when a stack should be deleted (and, optionally, its resources).</p> <ol> <li>Click the stack you would like to delete.</li> <li>Click Create schedule, then Stack deletion.</li> <li>Select a date and time for deletion, either in your local timezone or UTC.</li> <li>Choose whether Spacelift retains or deletes the stack's resources.</li> <li>Click Create.</li> </ol> <p>If you chose to delete the stack's resources, a destruction run will trigger on the stack at the specified time, and then the stack will be deleted when that is successful. If you chose to keep the stack's resources, the stack will be deleted at the specified time.</p> <p></p>"},{"location":"concepts/stack/scheduling.html#schedule-drift-detection","title":"Schedule drift detection","text":"<p>You can schedule drift detection on your stack to find differences between the desired and actual states of your infrastructure.</p> <ol> <li>Click the stack you would like to check for drift.</li> <li>Click Create schedule, then Drift detectionn.</li> <li>Set the cron expression(s) for how often you'd like to perform drift detection.</li> <li>Select the desired timezone.</li> <li>Select additional options:<ol> <li>Reconcile: Enable Spacelift to automatically create and trigger reconciliation runs to resolve drift.</li> <li>Ignore state: Enable to allow Spacelift to perform drift detection on stacks regardless of state. If disabled, drift detection will only run on stacks with the Finished state.</li> </ol> </li> <li>Click Create.</li> </ol> <p></p>"},{"location":"concepts/stack/scheduling.html#schedule-task","title":"Schedule task","text":"<p>You can schedule tasks to run commands on a stack at a specified timestamp or periodically based on cron rules you define.</p> <ol> <li>Click the stack you would like to schedule a task on.</li> <li>Click Create schedule, then Task.</li> <li>Enter the task command you would like to run.</li> <li>Choose a specific date and time, or set a cron rule for recurring tasks.</li> <li>Select the desired timezone.</li> <li>Click Create.</li> </ol> <p></p>"},{"location":"concepts/stack/scheduling.html#schedule-run","title":"Schedule run","text":"<p>You can also set up a schedule based on when a tracked run will be created.</p> <ol> <li>Click the stack you would like to schedule a run on.</li> <li>Click Create schedule, then Run.</li> <li>(Optionally) Enter a name for the run schedule.</li> <li>Attach custom runtime config: Click the slider to enable this setting, then enter a custom runtime config.</li> <li>Choose a specific date and time, or set a cron rule for recurring tasks.</li> <li>Select the desired timezone.</li> <li>Click Create.</li> </ol> <p></p>"},{"location":"concepts/stack/stack-dependencies.html","title":"Stack dependencies","text":""},{"location":"concepts/stack/stack-dependencies.html#stack-dependencies","title":"Stack dependencies","text":"<p>Stacks can depend on other stacks, allowing you to run a stack only after another stack have finished running. For example, you might want to deploy a database stack before a stack that uses the database.</p> <p>Info</p> <p>Stack dependencies only respect tracked runs. Proposed runs and tasks are not considered.</p> <p>Stack dependencies help you execute related runs triggered by the same VCS event in the proper order.</p> <p>Stack dependencies do not manage stack lifecycle events such as creating or deleting stacks.</p>"},{"location":"concepts/stack/stack-dependencies.html#defining-stack-dependencies","title":"Defining stack dependencies","text":"<p>Stack dependencies can be defined in the <code>Dependencies</code> tab of the stack.</p> <p></p> <p>To create a dependency between two stacks, you need reader permission to the dependent stack and admin permission to the other stack. See Spaces Access Control for more information.</p>"},{"location":"concepts/stack/stack-dependencies.html#defining-references-between-stacks","title":"Defining references between stacks","text":"<p>You have the option to refer to outputs of other stacks: by default, your stack will be only triggered if the referenced output has been created or changed. If you enable the <code>Trigger always</code> option however, the stack will be triggered regardless of the referenced output.</p> <p>Tip</p> <p>Adding a new reference will always trigger a run. Removing one will not. If you want removing a reference to trigger a run, enable the <code>Trigger always</code> flag.</p> <p></p> <p>You can either choose an existing output value or add one that doesn't exist yet but will be created by the stack. On the receiving end, you need to choose an environment variable (<code>Input name</code>) to store the output value in.</p> <p></p> <p>Tip</p> <p>If you use Terraform, make sure to use the <code>TF_VAR_</code> prefix for environment variable names.</p>"},{"location":"concepts/stack/stack-dependencies.html#enabling-sensitive-outputs-for-references","title":"Enabling sensitive outputs for references","text":"<p>A stack output can be sensitive or non-sensitive. For example, in Terraform you can mark an output <code>sensitive = true</code>. Sensitive outputs are masked in the Spacelift UI and in the logs.</p> <p>For Spacelift to start uploading sensitive outputs to the server you need to explicitly enable it on the stack and worker pool level.</p> <ul> <li>Stack-level: Set the <code>Transfer sensitive outputs across dependencies</code> option to <code>Enabled</code>.   </li> <li>Worker pool-level: If you are using our public worker pool, this setting is enabled automatically. On private worker pools however, it needs to be enabled explicitly by adding the <code>SPACELIFT_SENSITIVE_OUTPUT_UPLOAD_ENABLED=true</code> environment variable to the worker.</li> </ul> <p>In self-hosted, when using CloudFormation to deploy your worker pool, make sure to add <code>export SPACELIFT_SENSITIVE_OUTPUT_UPLOAD_ENABLED=true</code> to your <code>CustomUserDataSecretName</code> secret. You'll find more information about that variable on the worker pool page.</p>"},{"location":"concepts/stack/stack-dependencies.html#stack-dependency-reference-limitations","title":"Stack dependency reference limitations","text":"<p>When a stack has an upstream dependency with a reference, it relies on the existence of the outputs.</p> <p>Spacelift only uses the outputs of a successful run. If the latest run of a stack failed, the dependencies won't be able to use its outputs. A retry also does not trigger dependent stack/runs even if the retry is successful.</p> <pre><code>graph TD;\n    Storage --&gt; |TF_VAR_AWS_S3_BUCKET_ARN|storageColor(StorageService);\n\n    style storageColor fill:#51abcb</code></pre> <p>If you trigger <code>StorageService</code> in the above scenario, <code>Storage</code> needs to have completed a tracked run with an apply phase producing the output for <code>TF_VAR_AWS_S3_BUCKET_ARN</code> already. Otherwise you'll get the following error:</p> <pre><code>job assignment failed: the following inputs are missing: Storage.TF_VAR_AWS_S3_BUCKET_ARN =&gt; TF_VAR_AWS_S3_BUCKET_ARN\n</code></pre> <p>Note</p> <p>We enabled output uploading to our backend on August 21 2023. If your stack produced an output before that date, you will need to rerun it to make the output available for references.</p>"},{"location":"concepts/stack/stack-dependencies.html#imported-outputs","title":"Imported outputs","text":"<p>If you have imported outputs into your stack, they also require an apply phase in Spacelift. Outputs imported into a stack are only uploaded to the backend during this phase to be available for use as shared outputs. Without this step, the outputs will not propagate correctly.</p> <p>You can do it by adding a dummy output to the stack and removing it afterwards:</p> <pre><code>output \"dummy\" {\n  value = \"dummy\"\n}\n</code></pre> <p>Note</p> <p>If you are using CloudFormation, adding an output is considered a no-op. If you are making an output-only change, you will need to make a modification to the Resources section of the CloudFormation template that is recognized as a change by CloudFormation, e.g. by adding a dummy resource like <code>AWS::CloudFormation::WaitConditionHandle</code>.</p>"},{"location":"concepts/stack/stack-dependencies.html#reconciling-failed-runs","title":"Reconciling failed runs","text":"<p>When detecting changes in outputs, we also consider the previous run of the dependent stack. If the previous run wasn't successful, we'll trigger it regardless of the output changes. This is to ensure that the dependent stack has a chance to recover from a potential failure.</p>"},{"location":"concepts/stack/stack-dependencies.html#vendor-limitations","title":"Vendor limitations","text":"<p>Ansible and Kubernetes do not have the concept of outputs, so you cannot reference the outputs of Ansible and Kubernetes stacks. They can be on the receiving end though:</p> <pre><code>graph TD;\n    A[Terraform Stack] --&gt; |VARIABLE_1|B[Kubernetes Stack];\n    A --&gt; |VARIABLE_2|C[Ansible Stack];</code></pre>"},{"location":"concepts/stack/stack-dependencies.html#scenario-1","title":"Scenario 1","text":"<pre><code>graph TD;\n    Infrastructure --&gt; |TF_VAR_VPC_ID|Database;\n    Database --&gt; |TF_VAR_CONNECTION_STRING|PaymentService;</code></pre> <p>If your <code>Infrastructure</code> stack has a <code>VPC_ID</code>, you can set that as an input to your <code>Database</code> stack (e.g. <code>TF_VAR_VPC_ID</code>). When the <code>Infrastructure</code> stack finishes running, the <code>Database</code> stack will be triggered and the <code>TF_VAR_VPC_ID</code> environment variable will be set to the value of the <code>VPC_ID</code> output of the <code>Infrastructure</code> stack.</p> <p>If one or more references is defined, the stack will only be triggered if the referenced output has been created or changed. If they remain the same, the downstream stack will be skipped. You can control this behavior by enabling or disabling the <code>Trigger always</code> option.</p>"},{"location":"concepts/stack/stack-dependencies.html#scenario-2","title":"Scenario 2","text":"<pre><code>graph TD;\n    Infrastructure --&gt; |TF_VAR_VPC_ID|Database;\n    Database --&gt; |TF_VAR_CONNECTION_STRING|PaymentService;\n    Database --&gt; CartService;</code></pre> <p>You can also mix references and referenceless dependencies. In the above case, <code>CartService</code> will be triggered whenever <code>Database</code> finishes running, regardless of the <code>TF_VAR_CONNECTION_STRING</code> output.</p>"},{"location":"concepts/stack/stack-dependencies.html#dependencies-overview","title":"Dependencies overview","text":"<p>In the <code>Dependencies</code> tab of the stack, there is a button called <code>Dependencies graph</code> to view the full dependency graph of the stack.</p> <p></p> <p>Stack dependencies are directed acyclic graphs (DAGs). This means that a stack can both depend on multiple stacks and be depended on by multiple stacks. However, there cannot be loops. You will receive an error if you try to add a stack to a dependency graph that will create a cycle.</p> <p>When a tracked run is created in the stack (either triggered manually or by a VCS event), and the stack is a dependency of other stack(s), those stacks will queue up tracked runs and wait until the current stack's tracked run has finished running.</p> <p>If a run fails in the dependency chain, the chain \"breaks\" and all subsequent runs will be cancelled.</p>"},{"location":"concepts/stack/stack-dependencies.html#examples","title":"Examples","text":""},{"location":"concepts/stack/stack-dependencies.html#scenario-1_1","title":"Scenario 1","text":"<pre><code>graph TD;\n    BaseInfra--&gt;Database;\n    BaseInfra--&gt;networkColor(Network);\n    BaseInfra--&gt;Storage;\n    Database--&gt;paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    Database--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n\n    style networkColor fill:#51cbad\n    style paymentSvcColor fill:#51abcb\n    style cartSvcColor fill:#51abcb</code></pre> <p>In this example, if the <code>Network</code> stack receives a push event to the tracked branch, it will start a run immediately and queue up <code>PaymentService</code> and <code>CartService</code>. When <code>Network</code> finishes running, those two will start running. Since <code>PaymentService</code> and <code>CartService</code> do not depend on each other, they can run in parallel.</p> <p><code>BaseInfra</code> remains untouched, as we never go up in the dependency graph.</p>"},{"location":"concepts/stack/stack-dependencies.html#scenario-2_1","title":"Scenario 2","text":"<pre><code>graph TD;\n    baseInfraColor(BaseInfra)--&gt;databaseColor(Database);\n    baseInfraColor(BaseInfra)--&gt;networkColor(Network);\n    baseInfraColor(BaseInfra)--&gt;storageColor(Storage);\n    databaseColor(Database)--&gt;|TF_VAR_CONNECTION_STRING|paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    databaseColor(Database)--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n\n    style baseInfraColor fill:#51cbad\n    style networkColor fill:#51abcb\n    style paymentSvcColor fill:#51abcb\n    style cartSvcColor fill:#51abcb\n    style storageColor fill:#51abcb\n    style databaseColor fill:#51abcb</code></pre> <p>If <code>BaseInfra</code> receives a push event, it will start running immediately and queue up all of the stacks below it. The order of the runs is:</p> <ol> <li><code>BaseInfra</code></li> <li><code>Database</code>,  <code>Network</code>, and <code>Storage</code> in parallel</li> <li><code>PaymentService</code> and <code>CartService</code> in parallel</li> </ol> <p>Since <code>PaymentService</code> and <code>CartService</code> do not depend on <code>Storage</code>, they will not wait until it finishes running.</p> <p><code>PaymentService</code> references <code>Database</code> with <code>TF_VAR_CONNECTION_STRING</code>. But since it also depends on <code>Network</code> with no references, it'll run regardless of the <code>TF_VAR_CONNECTION_STRING</code> output. If the <code>Database</code> stack does not have the corresponding output, the <code>TF_VAR_CONNECTION_STRING</code> environment variable will not be injected into the run.</p>"},{"location":"concepts/stack/stack-dependencies.html#scenario-3","title":"Scenario 3","text":"<pre><code>graph TD;\n    baseInfraColor(BaseInfra)--&gt;databaseColor(Database);\n    baseInfraColor(BaseInfra)--&gt;networkColor(Network);\n    baseInfraColor(BaseInfra)--&gt;storageColor(Storage);\n    databaseColor(Database)--&gt;paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    databaseColor(Database)--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n\n    style baseInfraColor fill:#51cbad\n    style networkColor fill:#e21316\n    style paymentSvcColor fill:#777777\n    style cartSvcColor fill:#777777\n    style storageColor fill:#51abcb\n    style databaseColor fill:#51abcb</code></pre> <p>In this scenario, <code>BaseInfra</code> received a push, started running, and queued up all of the stacks dependent on it. However, the <code>Network</code> stack has failed which means that the rest of the dependent runs in the chain (<code>PaymentService</code> and <code>CartService</code>) will be skipped.</p> <p>Same-level stacks (<code>Database</code> and <code>Storage</code>) are not affected by the failure.</p>"},{"location":"concepts/stack/stack-dependencies.html#scenario-4","title":"Scenario 4","text":"<pre><code>graph TD;\n    baseInfraColor(BaseInfra)--&gt;databaseColor(Database);\n    baseInfraColor(BaseInfra)--&gt;networkColor(Network);\n    baseInfraColor(BaseInfra)--&gt;storageColor(Storage);\n    databaseColor(Database)--&gt;paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    databaseColor(Database)--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n\n    style baseInfraColor fill:#51cbad\n    style networkColor fill:#51cbad\n    style paymentSvcColor fill:#51abcb\n    style cartSvcColor fill:#51abcb\n    style storageColor fill:#51cbad\n    style databaseColor fill:#51cbad</code></pre> <p>Assume that the infrastructure (<code>BaseInfra</code>, <code>Database</code>, <code>Network</code> and <code>Storage</code>) is a monorepo, and a push event affects all 4 stacks. The situation isn't any different than Scenario 2. The dependencies are still respected and the stacks will run in the proper order:</p> <ol> <li><code>BaseInfra</code></li> <li><code>Database</code>, <code>Network</code>, and <code>Storage</code> in parallel</li> <li><code>PaymentService</code> and <code>CartService</code> in parallel</li> </ol>"},{"location":"concepts/stack/stack-dependencies.html#scenario-5","title":"Scenario 5","text":"<pre><code>graph TD;\n    baseInfraColor(BaseInfra)--&gt;databaseColor(Database);\n    baseInfraColor(BaseInfra)--&gt;networkColor(Network);\n    baseInfraColor(BaseInfra)--&gt;storageColor(Storage);\n    databaseColor(Database)--&gt;paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    databaseColor(Database)--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n    storageColor(Storage)--&gt;|TF_VAR_AWS_S3_BUCKET_ARN|storageSvcColor(StorageService);\n\n    style baseInfraColor fill:#51cbad\n    style networkColor fill:#51abcb\n    style paymentSvcColor fill:#51abcb\n    style cartSvcColor fill:#51abcb\n    style storageColor fill:#51abcb\n    style databaseColor fill:#51cbad\n    style storageSvcColor fill:#777777</code></pre> <p>If <code>BaseInfra</code> and <code>Database</code> are a monorepo and a push event affects both of them, this scenario isn't any different than Scenario 2 and Scenario 4. The order from top to bottom is still the same:</p> <ol> <li><code>BaseInfra</code></li> <li><code>Database</code>, <code>Network</code>, and <code>Storage</code> in parallel</li> <li><code>PaymentService</code> and <code>CartService</code> in parallel</li> </ol> <p>For <code>Storage</code> and <code>StorageService</code>, assume the S3 bucket resource of <code>Storage</code> already exists. This means that the bucket ARN didn't change, so <code>StorageService</code> will be skipped.</p>"},{"location":"concepts/stack/stack-dependencies.html#trigger-policies","title":"Trigger policies","text":"<p>Stack dependencies are a simpler alternative to trigger policies that cover most use cases. If your use case does not fit stack dependencies, consider using a trigger policy.</p> <p>There is no connection between the two features, and the two should not be combined to avoid confusion or even infinite loops in the dependency graph.</p>"},{"location":"concepts/stack/stack-dependencies.html#stack-deletion","title":"Stack deletion","text":"<p>A stack cannot be deleted if it has downstream dependencies (child stacks depending on it). If you want to delete such a stack, you need to delete all of its downstream dependencies first. However, if a stack only has upstream dependencies (parent stacks that it depends on), it can be deleted without any issues.</p>"},{"location":"concepts/stack/stack-dependencies.html#ordered-stack-creation-and-deletion","title":"Ordered Stack creation and deletion","text":"<p>Stack dependencies do not handle the lifecycle of the stacks.</p> <p>Ordering the creation and deletion of stacks in a specific order is not impossible though. If you manage your Spacelift stacks with the Spacelift Terraform Provider, you can easily do it by setting <code>spacelift_stack_destructor</code> resources and setting the <code>depends_on</code> Terraform attribute on them.</p> <p>Here is a simple example of creating a dependency between three stacks and setting up a destructor for them. By setting up a destructor resource with the proper <code>depends_on</code> attribute, deletion of the stacks will happen in the proper order. First child, then parent. This is also an easy way to create short-lived environments.</p> <pre><code># VPC -&gt; Infra -&gt; App\n\nresource \"spacelift_stack\" \"vpc\" {\n  name       = \"VPC\"\n  repository = \"vpc\"\n  branch     = \"main\"\n}\n\nresource \"spacelift_stack\" \"infra\" {\n  name       = \"Infra\"\n  repository = \"infra\"\n  branch     = \"main\"\n}\n\nresource \"spacelift_stack\" \"app\" {\n  name       = \"Application\"\n  repository = \"app\"\n  branch     = \"main\"\n}\n\nresource \"spacelift_stack_dependency\" \"infra_vpc\" {\n  stack_id            = spacelift_stack.infra.id\n  depends_on_stack_id = spacelift_stack.vpc.id\n}\n\nresource \"spacelift_stack_dependency\" \"app_infra\" {\n  stack_id            = spacelift_stack.app.id\n  depends_on_stack_id = spacelift_stack.infra.id\n}\n\nresource \"spacelift_stack_dependency_reference\" \"infra_vpc\" {\n  stack_dependency_id = spacelift_stack_dependency.infra_vpc.id\n  input_name          = \"TF_VAR_vpc_id\"\n  output_name         = \"vpc_id\"\n}\n\nresource \"spacelift_stack_dependency_reference\" \"app_infra\" {\n  stack_dependency_id = spacelift_stack_dependency.app_infra.id\n  input_name          = \"TF_VAR_kms_arn\"\n  output_name         = \"kms_arn\"\n}\n\nresource \"spacelift_stack_destructor\" \"vpc\" {\n  stack_id = spacelift_stack.vpc.id\n}\n\nresource \"spacelift_stack_destructor\" \"infra\" {\n  stack_id = spacelift_stack.infra.id\n\n  depends_on = [spacelift_stack_destructor.vpc]\n}\n\nresource \"spacelift_stack_destructor\" \"app\" {\n  stack_id = spacelift_stack.app.id\n\n  depends_on = [spacelift_stack_destructor.infra]\n}\n</code></pre> <p>During <code>terraform apply</code>, Terraform:</p> <ul> <li>Creates the three stacks</li> <li>Sets up the dependency between them</li> </ul> <p>You might notice the three destructors at the end. They don't do anything during <code>terraform apply</code>. However, during <code>terraform destroy</code>, Terraform will destroy:</p> <ol> <li>Dependencies and dependency references</li> <li>The grandchild stack (<code>app</code>) and its resources</li> <li>The parent stack (<code>infra</code>) and its resources</li> <li>The grandparent stack (<code>vpc</code>) and its resources</li> </ol>"},{"location":"concepts/stack/stack-dependencies.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/stack/stack-dependencies.html#error-job-assignment-failed-the-following-inputs-are-missing","title":"Error: job assignment failed: the following inputs are missing","text":"<p>This error occurs when the outputs were not part of an apply phase in Spacelift, including if the outputs are available in your state file and visible under the Outputs tab. For the output references to be available, the outputs need to have been part of a tracked run with an apply phase.</p>"},{"location":"concepts/stack/stack-dependencies.html#error-argument-list-too-long","title":"Error: argument list too long","text":"<p>If you're adding a large number of data sources to the parent stack and pushing them as an output you might get an error like this:</p> <pre><code>argument list too long\n</code></pre> <p>You cannot have more than 128kB in any given argument. This limit is hard-coded in the kernel and difficult to work around.</p> <p>Depending on your use case it may be possible to work around this issue by using the Spacelift Terraform Provider resources <code>spacelift_mounted_file</code> and <code>spacelift_run</code>.</p>"},{"location":"concepts/stack/stack-settings.html","title":"Stack settings","text":""},{"location":"concepts/stack/stack-settings.html#stack-settings","title":"Stack settings","text":"<p>Many settings can be configured directly on the stack to influence how runs and tasks within a stack are processed. Other factors that influence runs and tasks include:</p> <ul> <li>Environment variables</li> <li>Attached contexts</li> <li>Runtime configuration</li> <li>Integrations</li> </ul>"},{"location":"concepts/stack/stack-settings.html#video-walkthrough","title":"Video Walkthrough","text":""},{"location":"concepts/stack/stack-settings.html#common-settings","title":"Common settings","text":"<p>You can configure these settings when you first create a stack or when it's already created. If you're editing an existing stack's settings:</p> <ol> <li>Navigate to the Stacks page in Spacelift.</li> <li>Click the three dots beside a stack name you want to configure.</li> <li>Click Settings, then click Behavior.</li> <li>Make your adjustments and click Save.</li> </ol>"},{"location":"concepts/stack/stack-settings.html#administrative","title":"Administrative","text":"<p>This setting determines whether a stack has administrative privileges within its space. Administrative stacks receive an API token that grants them elevated access to a subset of the Spacelift API, which is used by the Terraform provider. This allows them to create, update, and destroy Spacelift resources.</p> <p>Info</p> <p>Administrative stacks get the Admin role in the space they belong to.</p> <p>Administrative stacks can declaratively manage other stacks, their environments, contexts, policies, modules, and worker pools. This approach helps avoid manual configuration, often referred to as \"ClickOps.\"</p> <p>You can also export stack outputs as a context to avoid exposing the entire state through Terraform's remote state or external storage mechanisms like AWS Parameter Store or Secrets Manager.</p>"},{"location":"concepts/stack/stack-settings.html#autodeploy","title":"Autodeploy","text":"<p>When Autodeploy is enabled (true), any change to the tracked branch will be automatically applied if the planning phase is successful and there are no plan policy warnings.</p> <p>You might consider enabling Autodeploy if you always perform an automated code review before merging to the tracked branch or if you rely on plan policies to flag potential issues. If every change undergoes a meaningful human review by stack writers, requiring an additional step to confirm deployment may be unnecessary.</p> <p>When Autodeploy is enabled, approval policies are only evaluated during the queued stage, not in the unconfirmed state.</p>"},{"location":"concepts/stack/stack-settings.html#autoretry","title":"Autoretry","text":"<p>This setting determines whether obsolete proposed changes are retried automatically. When Autoretry is enabled (true), any Pull Requests to the tracked branch that conflict with an applied change will be reevaluated based on the updated state.</p> <p>This feature saves you from manually retrying runs on Pull Requests when the state changes. It also provides greater confidence that the proposed changes will match the actual changes after merging the Pull Request.</p> <p>Autoretry is only supported for stacks with a private worker pool attached.</p>"},{"location":"concepts/stack/stack-settings.html#customizing-workflow","title":"Customizing workflow","text":"<p>Spacelift workflows can be customized by adding extra commands to be executed before and after specific phases:</p> <ul> <li>Initialization (<code>before_init</code> and <code>after_init</code>)</li> <li>Planning (<code>before_plan</code> and <code>after_plan</code>)</li> <li>Applying (<code>before_apply</code> and <code>after_apply</code>)</li> <li>Destroying (<code>before_destroy</code> and <code>after_destroy</code>)<ul> <li>Used during module test cases</li> <li>Used by stacks during destruction with corresponding stack_destructor_resource</li> </ul> </li> <li>Performing (<code>before_perform</code> and <code>after_perform</code>)</li> <li>Finally (<code>after_run</code>): Executed after each actively processed run, regardless of its outcome. These hooks have access to an environment variable called <code>TF_VAR_spacelift_final_run_state</code>, which indicates the final state of the run.</li> </ul> <p>All hooks, including <code>after_run</code>, execute on the worker. If the run is terminated outside the worker (e.g., canceled or discarded), or if there is an issue setting up the workspace or starting the worker container, the hooks will not fire.</p> <p>Info</p> <p>If a \"before\" hook fails (non-zero exit code), the corresponding phase will not execute. Similarly, if a phase fails, none of the \"after\" hooks will execute unless the hook uses a semicolon (<code>;</code>).</p> <p>These commands can serve two main purposes: modifying the workspace (such as setting up symlinks or moving files) or running validations using tools like <code>tfsec</code>, <code>tflint</code>, or <code>terraform fmt</code>.</p>"},{"location":"concepts/stack/stack-settings.html#how-to-run-multiple-commands","title":"How to run multiple commands","text":"<p>Avoid using newlines (<code>\\n</code>) in hooks. Spacelift chains commands with double ampersands (<code>&amp;&amp;</code>), and using newlines can hide non-zero exit codes if the last command in the block succeeds. To run multiple commands, either add multiple hooks or use a script as a mounted file and call it in the hook.</p> <p>Additionally, using a semicolon (<code>;</code>) in hooks will cause subsequent commands to run even if the phase fails. Use <code>&amp;&amp;</code> or wrap your hook in parentheses to ensure \"after\" commands only execute if the phase succeeds.</p> <p>Warning</p> <p>When a run resumes after being paused (e.g., for confirmation or approval), the remaining phases run in a new container. Any tools installed in earlier phases will not be available. To avoid this, bake the tools into a custom runner image.</p> <p>The workflow can be customized using either the Terraform provider or the GUI. The GUI allows you to select a phase, add commands before and after it, and reorder them using drag-and-drop functionality. Commands preceding a phase are \"before\" hooks, while those following it are \"after\" hooks:</p> <p></p> <p>Commands run in the same shell session as the phase itself, so the phase will have access to any shell variables exported by preceding scripts. Environment variables are preserved across phases.</p> <p>Info</p> <p>These scripts can be overridden by the runtime configuration specified in the <code>.spacelift/config.yml</code> file.</p>"},{"location":"concepts/stack/stack-settings.html#runtime-commands","title":"Runtime commands","text":"<p>Spacelift can handle special commands to change the workflow behavior. Runtime commands use the echo command in a specific format. You can use those commands in any lifecycle step of the workflow.</p> <p></p> <pre><code>echo \"::command arg1 arg2\"\n</code></pre> <p>Below is a list of supported commands. See the more detailed doc after this table.</p> Command Description <code>::add-mask</code> Adds a set of values that should be masked in log output"},{"location":"concepts/stack/stack-settings.html#add-mask","title":"::add-mask","text":"<p>When you mask a value, it is treated as a secret and will be redacted in the logs output. Each masked word separated by whitespace is replaced with five <code>*</code> characters.</p>"},{"location":"concepts/stack/stack-settings.html#example","title":"Example","text":"<pre><code># Multiple masks can be set with a single command\necho \"::add-mask secret-string another-secret-string\"\n\n# You can pull a secret dynamically, for example here we can mask the account ID\necho \"::add-mask $(aws sts get-caller-identity | jq -r .Account)\"\n</code></pre>"},{"location":"concepts/stack/stack-settings.html#enable-local-preview","title":"Enable local preview","text":"<p>Indicates whether creating proposed Runs based on user-uploaded local workspaces is allowed.</p> <p>If this is enabled, you can use spacectl to create a proposed run based on the directory you're in:</p> <pre><code>spacectl stack local-preview --id &lt;stack-id&gt;\n</code></pre> <p>Warning</p> <p>Use this setting with caution, as it allows anybody with write access to the Stack to execute arbitrary code with access to all the environment variables configured in the Stack.</p>"},{"location":"concepts/stack/stack-settings.html#enable-well-known-secret-masking","title":"Enable well known secret masking","text":"<p>This setting determines if secret patterns will be automatically redacted from logs. If enabled, the following secrets will be masked from logs:</p> <ul> <li>AWS Access Key Id</li> <li>Azure AD Client Secret</li> <li>Azure Connection Strings (<code>AccountKey=...</code>, <code>SharedAccessSignature=...</code>)</li> <li>GitHub PAT</li> <li>GitHub Fine-Grained PAT</li> <li>GitHub App Token</li> <li>GitHub Refresh Token</li> <li>GitHub OAuth Access Token</li> <li>JWT tokens</li> <li>Slack Token</li> <li>PGP Private Key</li> <li>RSA Private Key</li> <li>PEM block with BEGIN PRIVATE KEY header</li> </ul>"},{"location":"concepts/stack/stack-settings.html#name-and-description","title":"Name and description","text":"<p>Stack name and description are pretty self-explanatory. The required name is what you'll see in the stack list on the home screen and menu selection dropdown. Make sure that it's informative enough to be able to immediately communicate the purpose of the stack, but short enough so that it fits nicely in the dropdown, and no important information is cut off.</p> <p>The optional description is completely free-form and it supports Markdown. This is a good place for a thorough explanation of the purpose of the stack and a link or two.</p> <p>Tip</p> <p>Based on the original name, Spacelift generates an immutable slug that serves as a unique identifier of this stack. If the name and the slug diverge significantly, things may become confusing.</p> <p>Even though you can change the stack name at any point, we strongly discourage all non-trivial changes.</p>"},{"location":"concepts/stack/stack-settings.html#labels","title":"Labels","text":"<p>Labels are arbitrary, user-defined tags that can be attached to Stacks. A single Stack can have an arbitrary number of these, but they must be unique. Labels can be used for any purpose, including UI filtering, but one area where they shine most is user-defined policies which can modify their behavior based on the presence (or lack thereof) of a particular label.</p> <p>There are some magic labels that you can add to your stacks. These labels add/remove functionalities based on their presence.</p> <p>List of the most useful labels:</p> <ul> <li>infracost -- Enables Infracost on your stack</li> <li>feature:enable_log_timestamps -- Enables timestamps on run logs.</li> <li>feature:add_plan_pr_comment -- Enables Pull Request Plan Commenting. It is deprecated. Please use Notification policies instead.</li> <li>feature:disable_pr_comments - Disables Pull Request Comments</li> <li>feature:disable_pr_delta_comments - Disables Pull Request Delta Comments</li> <li>feature:disable_resource_sanitization -- Disables resource sanitization</li> <li>feature:enable_git_checkout -- Enables support for downloading source code using standard Git checkout rather than downloading a tarball via API</li> <li>feature:ignore_runtime_config -- Ignores .spacelift/config</li> <li>terragrunt -- Old way of using Terragrunt from the Terraform backend</li> <li>ghenv: Name -- GitHub Deployment environment (defaults to the stack name)</li> <li>ghenv: - -- Disables the creation of GitHub deployment environments</li> <li>autoattach:autoattached_label -- Used for policies/contexts to autoattach the policy/contexts to all stacks containing <code>autoattached_label</code></li> <li>feature:k8s_keep_using_prune_white_list_flag -- sets <code>--prune-whitelist</code> flag instead of <code>--prune-allowlist</code> for the template parameter <code>.PruneWhiteList</code> in the Kubernetes custom workflow.</li> <li>feature:pr_enforce_unique_module_version -- Enforces module version to be unique even for PR checks</li> </ul>"},{"location":"concepts/stack/stack-settings.html#project-root","title":"Project root","text":"<p>Project root points to the directory within the repo where the project should start executing. This is especially useful for monorepos or repositories hosting multiple somewhat independent projects. This setting plays very well with Git push policies, allowing you to easily express generic rules on what it means for the stack to be affected by a code change. In the absence of push policies, any changes made to the project root and any paths specified by project globs will trigger Spacelift runs.</p> <p>Info</p> <p>The project root can be overridden by the runtime configuration specified in the <code>.spacelift/config.yml</code> file.</p>"},{"location":"concepts/stack/stack-settings.html#git-sparse-checkout-paths","title":"Git sparse checkout paths","text":"<p>Git sparse checkout paths allow you to specify a list of directories and files that will be used in sparse checkout, meaning that only the specified directories and files from the list will be cloned. This can help reduce the size of the workspace by only downloading the parts of the repository that are needed for the stack.</p> <p>Only path values are allowed; glob patterns are not supported.</p> <p>Example valid paths:</p> <ul> <li><code>./infrastructure/</code></li> <li><code>infrastructure/</code></li> <li><code>infrastructure</code></li> <li><code>./infrastructure/main.tf</code></li> </ul> <p>Example invalid path (glob pattern):</p> <ul> <li><code>./infrastructure/*</code></li> </ul> <p></p>"},{"location":"concepts/stack/stack-settings.html#project-globs","title":"Project globs","text":"<p>The project globs option allows you to specify files and directories outside of the project root that the stack cares about. In the absence of push policies, any changes made to the project root and any paths specified by project globs will trigger Spacelift runs.</p> <p>Warning</p> <p>Project globs do not mount the files or directories in your project root.  They are used primarily for triggering your stack when, for example, there are changes to a module outside of the project root.</p> <p></p> <p>You aren't required to add any project globs if you don't want to, but you have the option to add as many project globs as you want for a stack.</p> <p>Under the hood, the project globs option takes advantage of the doublestar.Match function to do pattern matching.</p> <p>Example matches:</p> <ul> <li>Any directory or file: <code>**</code></li> <li>A directory and all of its content: <code>dir/*</code></li> <li>A directory path and all of its subdirectories and files: <code>dir/**</code></li> <li>Match all files with a specific extension: <code>dir/*.tf</code></li> <li>Match all files that start with a string, end with another and have a predefined number of chars in the middle -- <code>data-???-report</code> will match three chars between data and report</li> <li>Match all files that start with a string, and finish with any character from a sequence: <code>dir/instance[0-9].tf</code></li> </ul> <p>As you can see in the example matches, these are the regex rules that you are already accustomed to.</p>"},{"location":"concepts/stack/stack-settings.html#vcs-integration-and-repository","title":"VCS integration and repository","text":"<p>We have two types of integrations types: default and Space-level. Default integrations will be always available for all stacks, and Space-level integrations will be available only for stacks that are in the same Space as the integration or have access to it via inheritance. Read more about VCS integrations in the source control page.</p> <p>Repository and branch point to the location of the source code for a stack. The repository must either belong to the GitHub account linked to Spacelift (its choice may further be limited by the way the Spacelift GitHub app has been installed) or to the GitLab server integrated with your Spacelift account. For more information about these integrations, please refer to our GitHub and GitLab documentation.</p> <p>Thanks to the strong integration between GitHub and Spacelift, the link between a stack and a repository can survive the repository being renamed in GitHub. If you're storing your repositories in GitLab, then you need to make sure to manually (or programmatically using Terraform) point the stack to the new location of the source code.</p> <p>Info</p> <p>Spacelift does not support moving repositories between GitHub accounts, since Spacelift accounts are strongly linked to GitHub ones. In that case the best course of action is to take your Terraform state, download it and import it while recreating the stack (or multiple stacks) in a different account. After that, all the stacks pointing to the old repository can be safely deleted.</p> <p>Moving a repository between GitHub and GitLab or the other way around is simple, however. Just change the provider setting on the Spacelift project, and point the stack to the new source code location.</p> <p>Branch signifies the repository branch tracked by the stack. By default, unless a Git push policy explicitly determines otherwise, a commit pushed to the tracked branch triggers a deployment represented by a tracked run. A push to any other branch by default triggers a test represented by a proposed run. Learn more about git push policies, tracked branches, and head commits.</p> <p>Results of both tracked and proposed runs are displayed in the source control provider using their specific APIs. Refer to our GitHub and GitLab documentation to understand how Spacelift feedback is provided for your infrastructure changes.</p> <p>Info</p> <p>A branch must exist before it's pointed to in Spacelift.</p>"},{"location":"concepts/stack/stack-settings.html#runner-image","title":"Runner image","text":"<p>Since every Spacelift job (which we call a run) is executed in a separate Docker container, setting a custom runner image provides a convenient way to prepare the exact runtime environment your infra-as-code flow is designed to use.</p> <p>Additionally, for our Pulumi integration, overriding the default runner image is the canonical way of selecting the exact Pulumi version and its corresponding language SDK.</p> <p>Info</p> <p>Runner image can be overridden by the runtime configuration specified in the <code>.spacelift/config.yml</code> file.</p> <p>On the public worker pool, Docker images can only be pulled from allowed registries. On private workers, images can be stored in any registry, including self-hosted ones.</p>"},{"location":"concepts/stack/stack-settings.html#worker-pool","title":"Worker pool","text":"<p>Use this setting to choose which worker pool to use. The default is public workers.</p>"},{"location":"concepts/stack/stack-settings.html#opentofuterraform-specific-settings","title":"OpenTofu/Terraform-specific settings","text":""},{"location":"concepts/stack/stack-settings.html#opentofu-version","title":"Version","text":"<p>The OpenTofu/Terraform version is set when a stack is created to indicate the version of OpenTofu/Terraform that will be used with this project. However, Spacelift covers the entire Terraform version management story, and applying a change with a newer version will automatically update the version on the stack.</p>"},{"location":"concepts/stack/stack-settings.html#opentofu-workspace","title":"Workspace","text":"<p>OpenTofu workspaces and Terraform workspaces are supported by Spacelift, too, as long as your state backend supports them. If the workspace is set, Spacelift will try to first select, and then (should that fail) automatically create the required OpenTofu/Terraform workspace on the state backend.</p> <p>If you're managing your OpenTofu/Terraform state through Spacelift, the workspace argument is ignored since Spacelift gives each stack a separate workspace by default.</p>"},{"location":"concepts/stack/stack-settings.html#pulumi-specific-settings","title":"Pulumi-specific settings","text":""},{"location":"concepts/stack/stack-settings.html#pulumi-login-url","title":"Login URL","text":"<p>Login URL is the address Pulumi should log into during Run initialization. Since we do not yet provide a full-featured Pulumi state backend, you need to bring your own (e.g. Amazon S3).</p> <p>You can read more about the login process and a general explanation of Pulumi state management and backends.</p>"},{"location":"concepts/stack/stack-settings.html#pulumi-stackname","title":"Stack name","text":"<p>The name of the Pulumi stack, which should be selected for backend operations. Please do not confuse it with the Spacelift stack name. They may be different, but it's useful to keep them identical.</p>"},{"location":"concepts/user-management.html","title":"User management","text":""},{"location":"concepts/user-management.html#user-management","title":"User management","text":"<p>Spacelift is made for collaboration. In order to collaborate, you need collaborators. User Management is an easy way to invite new members to your organization and manage their permissions, together with third-party integrations and group access. If you prefer to write a policy rather than using our UI, please check out Login Policies.</p> <p>Warning</p> <p>User Management doesn't affect GitHub organization or SSO admins and private account owners who always get admin access to their respective Spacelift accounts. This is to avoid a situation where a mistake in User Management locks out everyone from the account.</p>"},{"location":"concepts/user-management.html#user","title":"User","text":"<p>Users are individuals invited through their email and authenticated using your account's Identity Provider. Users can have personal permissions assigned.</p>"},{"location":"concepts/user-management.html#idp-group-mapping","title":"IdP group mapping","text":"<p>Group is a group of users as provided by your Identity Provider. If you assign permissions to a Group, all users that your Identity Provider reports as being members of a given group will have the same access, unless the user's permissions are higher than the ones they would get from being a member of a Group.</p>"},{"location":"concepts/user-management.html#roles-in-user-management","title":"Roles in User Management","text":"<p>User Management leverages Spacelift's RBAC system to assign roles to users, groups for selected Spaces.</p>"},{"location":"concepts/user-management.html#invitation-process","title":"Invitation process","text":"<p>New users can be invited through email by account admins and owners. Detailed instructions can be found on the Admin page.</p> <p>Once a user is invited, they will receive an email from Spacelift that will take them to your identity provider page.</p> <p></p> <p>Once the user authenticates with your identity provider, they will be redirected to the application.</p>"},{"location":"concepts/user-management.html#migrating-from-login-policy","title":"Migrating from Login Policy","text":"<p>If you were previously using Login Policy you can queue invites to User Management for your users while still having Login Policy enabled. Once you switch to the User Management strategy, the invites will be sent to your users' emails and allow them to sign in through your Identity Provider. Remember, that you can always go back if it turns out something was misconfigured.</p>"},{"location":"concepts/user-management.html#related-topics","title":"Related topics","text":"<ul> <li>Authorization &amp; RBAC: Complete guide to Spacelift's authorization system</li> <li>Assigning Roles to Users: Detailed role assignment guide</li> <li>Assigning Roles to Groups: Team-based permission management</li> </ul>"},{"location":"concepts/user-management/admin.html","title":"Admin / Owner","text":""},{"location":"concepts/user-management/admin.html#admin-owner","title":"Admin / Owner","text":"<p>Users with Owner and root Admin roles have access to Organization settings. This means they can manage access for the rest of the collaborators within your Spacelift account. The following article details the configuration options and user invitation procedures available to them.</p>"},{"location":"concepts/user-management/admin.html#access-settings","title":"Access settings","text":"<p>Access settings can be found by clicking the button in the lower-left corner with your avatar and selecting Organization settings.</p>"},{"location":"concepts/user-management/admin.html#select-your-management-strategy","title":"Select your Management Strategy","text":"<p>Account administrators can choose between User management and Login policy strategies in Management strategy tab. Once selected, the rules from the other strategy no longer apply.</p> <p>Warning</p> <p>Strategy selection does not affect GitHub organization or SSO admins and private account owners who always get admin access to their respective Spacelift accounts. This is to avoid a situation where a bad management strategy locks out everyone from the account.</p> <p>Danger</p> <p>Changing your Management Strategy will invalidate all active sessions, except the session making the change.</p>"},{"location":"concepts/user-management/admin.html#users","title":"Users","text":"<p>The user list can be accessed by selecting Users tab in the left drawer.</p> <p>The user list consists of all individuals who have or had access to your account through the User Management access strategy.</p> <p>Below is a longer description of fields that we believe might not be obvious at first glance.</p>"},{"location":"concepts/user-management/admin.html#role","title":"Role","text":"<p>The displayed Role badge is different than the space access role. It describes the user's role within the organization, instead of specific space permissions. This badge can have one of three values:</p> <ul> <li>OWNER - account admin, SSO admin or GitHub organization being the owner of an account.</li> <li>ADMIN - a user who has direct admin permissions to the root space. This badge does not take group or integration permissions into account.</li> <li>USER - users without admin permissions to the root space.</li> </ul>"},{"location":"concepts/user-management/admin.html#space","title":"Space","text":"<p>The number of spaces that the user has at least read access to. This only takes direct user permissions into account, and does not include permissions inherited from groups.</p>"},{"location":"concepts/user-management/admin.html#group","title":"Group","text":"<p>The number of groups that the user was a member of during the last login, as reported by the account's Identity Provider.</p>"},{"location":"concepts/user-management/admin.html#login-method","title":"Login method","text":"<p>The Identity Provider that was used for authenticating the user. It will usually be the same as the account's current Identity Provider, but on a rare occasion that Identity Provider changes, this will allow for auditing old access or transferring permissions to users within the new Identity Provider.</p>"},{"location":"concepts/user-management/admin.html#status","title":"Status","text":"<ul> <li>QUEUED - user invitation was issued and will be sent once Management Strategy is changed from Login Policy to User Management</li> <li>PENDING - user invitation was sent and can be accepted</li> <li>EXPIRED - user invitation was sent, but the user did not accept it before it expired. Spacelift invitation is active for 24h. A new invitation must be issued for a user to be able to access your Spacelift account.</li> <li>ACTIVE - user has accepted an invitation to your Spacelift account and has permissions set in User Management as long as User Management is the selected access strategy.</li> <li>INACTIVE - the user was previously able to access your Spacelift account, but the Identity Provider for your account has changed. The user needs to be invited again and must login through the new Identity Provider to continue using Spacelift.</li> </ul>"},{"location":"concepts/user-management/admin.html#inviting-new-users","title":"Inviting new users","text":"<p>To invite new users to your account, click on the 'Invite user' button located in the top right corner. You will be able to send them an email invitation link and determine their access level during the invitation process.</p>"},{"location":"concepts/user-management/admin.html#resending-user-invitation","title":"Resending user invitation","text":"<p>If a user did not receive an invitation email or their invitation has expired, you can select Resend invite from the three dots menu. Issuing a new invite is not possible if a pending or expired invite for a given email address already exists.</p>"},{"location":"concepts/user-management/admin.html#revoking-user-invitation","title":"Revoking user invitation","text":"<p>At any time you can revoke a user invitation by choosing Revoke invite from the three dots menu. Once the invitation is revoked, it will no longer allow user access to your account. If you wish to invite a user with a given email at a later date, you can issue a new invitation.</p>"},{"location":"concepts/user-management/admin.html#managing-user-access","title":"Managing user access","text":"<p>You can manage access rules for anyone who logs into your account by selecting the Access details option from the three dots menu.</p>"},{"location":"concepts/user-management/admin.html#slack-integration","title":"Slack integration","text":"<p>After setting up the Slack integration, you can provide the user's Slack ID to give them the same permissions when interacting through Slack as they would have when interacting through the Spacelift website.</p>"},{"location":"concepts/user-management/admin.html#idp-group-mapping","title":"IdP group mapping","text":"<p>Groups are reported by your Identity Provider for each user during authentication. You can add permissions to those groups that will be honored inside Spacelift. To do that, go to IdP group mapping tab and click Map IdP group. Then select the appropriate Spaces and Roles the same way you would for a single User.</p> <p>Warning</p> <p>Group permissions will only be applied to the user if the group name in Spacelift exactly matches the group name in your Identity Provider including capital letters and whitespaces.</p>"},{"location":"concepts/user-management/admin.html#slack-integration_1","title":"Slack integration","text":"<p>After setting up the Slack integration you can also grant permissions to entire Slack channels after selecting Integrations tab and clicking Manage access button in Slack card. You can input a human-readable name along the Slack channel ID. You can then add Space permissions the same way you would for Users and Groups.</p>"},{"location":"concepts/user-management/user.html","title":"User","text":""},{"location":"concepts/user-management/user.html#user","title":"User","text":"<p>We consider Users to be all entities that can access Spacelift through the account's Identity Provider after going through the invitation flow, as described on the User Management page.</p>"},{"location":"concepts/user-management/user.html#access-settings","title":"Access settings","text":"<p>Access settings can be found by clicking the button in the lower-left corner with your avatar and selecting Personal settings.</p>"},{"location":"concepts/user-management/user.html#requesting-an-invitation","title":"Requesting an invitation","text":"<p>If a user wants another user to be invited, they can use Collaborate with your team banner, to request an invitation for a specified email address. This request will be delivered to account owners and admins to evaluate the request.</p>"},{"location":"concepts/user-management/user.html#spaces","title":"Spaces","text":"<p>This page displays a list of spaces the user has access to, along with the role user has within that spaces. The list of roles and their descriptions can be found on the User Management page.</p> <p>In addition to displaying existing permissions, the user can request a role change for a space they have access to, either directly or by inheritance, by clicking Request role change button to the left of desired space. You can read more about Spaces and Access Control in linked articles from this documentation.</p>"},{"location":"concepts/user-management/user.html#groups","title":"Groups","text":"<p>A list of Groups reported by the Identity Provider on the most recent login will be displayed in this list.</p>"},{"location":"concepts/worker-pools.html","title":"Worker Pools","text":""},{"location":"concepts/worker-pools.html#worker-pools","title":"Worker Pools","text":"<p>Tip</p> <p>A worker is a logical entity that processes a single run at a time. As a result, your number of workers is equal to your maximum concurrency.</p> <p>Typically, a virtual server (AWS EC2 or Azure/GCP VM) hosts a single worker to keep things simple and avoid coordination and resource management overhead.</p> <p>Containerized workers can share the same virtual server because the management is handled by the orchestrator.</p>"},{"location":"concepts/worker-pools.html#setting-up","title":"Setting up","text":""},{"location":"concepts/worker-pools.html#generate-worker-private-key","title":"Generate Worker Private Key","text":"<p>We use asymmetric encryption to ensure that any temporary run state can only be accessed by workers in a given worker pool. To support this, you need to generate a private key that can be used for this purpose, and use it to create a certificate signing request to give to Spacelift. We'll generate a certificate for you, so that workers can use it to authenticate with the Spacelift backend. The following command will generate the key and CSR:</p> <pre><code>openssl req -new -newkey rsa:4096 -nodes -keyout spacelift.key -out spacelift.csr\n</code></pre> <p>Warning</p> <p>Don't forget to store the <code>spacelift.key</code> file (private key) in a secure location. You\u2019ll need it later, when launching workers in your worker pool.</p> <p>You can set up your worker pool from the Spacelift UI by navigating to Worker Pools section of your account, or you can also create it programmatically using the <code>spacelift_worker_pool</code> resource type within the Spacelift Terraform provider.</p>"},{"location":"concepts/worker-pools.html#navigate-to-worker-pools","title":"Navigate to Worker Pools","text":""},{"location":"concepts/worker-pools.html#add-worker-pool-entity","title":"Add Worker Pool Entity","text":"<p>Give your worker pool a name, and submit the <code>spacelift.csr</code> file in the worker pool creation form. After creation of the worker pool, you\u2019ll receive a Spacelift token. This token contains configuration for your worker pool launchers, as well as the certificate we generated for you based on the certificate signing request.</p> <p>Warning</p> <p>After clicking create, you will receive a token for the worker pool. Don't forget to save your Spacelift token in a secure location as you'll need this later when launching the worker pool.</p> <p></p>"},{"location":"concepts/worker-pools.html#launch-worker-pool","title":"Launch Worker Pool","text":"<p>We have two main ways of running workers: using Docker, or running inside a Kubernetes cluster. Please use one of the following guides to setup your worker pool depending on how you would like to run it:</p> <ul> <li>Docker-based workers.</li> <li>Kubernetes workers.</li> </ul>"},{"location":"concepts/worker-pools.html#configuration-options","title":"Configuration options","text":"<p>A number of configuration variables are available to customize how your launcher behaves. Some of the options only make sense when using Docker-based workers, so the configuration options have been split into multiple sections.</p>"},{"location":"concepts/worker-pools.html#shared-options","title":"Shared Options","text":"<ul> <li><code>SPACELIFT_MASK_ENVS</code>- comma-delimited list of whitelisted environment variables that are passed to the workers but should never appear in the logs.</li> <li><code>SPACELIFT_SENSITIVE_OUTPUT_UPLOAD_ENABLED</code> - If set to <code>true</code>, the launcher will upload sensitive run outputs to the Spacelift backend. This is a requirement if you want to use sensitive outputs for stack dependencies.</li> <li><code>SPACELIFT_RUN_LOGS_ON_STANDARD_OUTPUT_ENABLED</code> - If set to <code>true</code>, the launcher will write run logs to standard output in the same structured format as the rest of the logs. Some useful fields are <code>run_ulid</code>, <code>stack_id</code>, <code>ts</code>, and <code>msg</code>. You can easily manage run logs and ship them anywhere you want.</li> <li><code>SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY</code> - file that contains the run initialization policy that will be parsed/used; If the run initialized policy can not be validated at the startup the worker pool will exit with an appropriate error. Please see the Kubernetes-specific configuration section for more information.</li> <li><code>SPACELIFT_LAUNCHER_LOGS_TIMEOUT</code> - custom timeout (the default is 7 minutes) for killing jobs not producing any logs. This is a duration flag, expecting a duration-formatted value, eg <code>1000s</code>. Please see the Kubernetes-specific configuration section for more information.</li> <li><code>SPACELIFT_LAUNCHER_RUN_TIMEOUT</code> - custom maximum run time - the default is 70 minutes. This is a duration flag, expecting a duration-formatted value, eg. <code>120m</code>. Please see the Kubernetes-specific configuration section for more information.</li> <li><code>SPACELIFT_DEBUG</code>- if set to true, this will output the exact commands spacelift runs to the worker logs.</li> </ul> <p>Warning</p> <p>Server-side initialization policies are being deprecated. <code>SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY</code> shouldn't be confused with that. This policy is a Worker-side initialization policy and it can be set by using the launcher run initialization policy flag.</p> <p>For a limited time period we will be running both types of initialization policy checks but ultimately we're planning to move the pre-flight checks to the worker node, thus allowing customers to block suspicious looking jobs on their end.</p>"},{"location":"concepts/worker-pools.html#docker-only-options","title":"Docker-only options","text":"<ul> <li><code>SPACELIFT_DOCKER_CONFIG_DIR</code> - if set, the value of this variable will point to the directory containing Docker configuration, which includes credentials for private Docker registries. Private workers can populate this directory for example by executing <code>docker login</code> before the launcher process is started.</li> <li><code>SPACELIFT_WHITELIST_ENVS</code> - comma-delimited list of environment variables to pass from the launcher's own environment to the workers' environment. They can be prefixed with <code>ro_</code> to only be included in read only runs or <code>wo_</code> to only be included in write only runs.</li> <li><code>SPACELIFT_WORKER_EXTRA_MOUNTS</code> - additional files or directories to be mounted to the launched worker docker containers during either read or write runs, as a comma-separated list of mounts in the form of <code>/host/path:/container/path</code>.</li> <li><code>SPACELIFT_WORKER_NETWORK</code> - network ID/name to connect the launched worker containers, defaults to <code>bridge</code>.</li> <li><code>SPACELIFT_WORKER_RUNTIME</code> - runtime to use for worker container.</li> <li><code>SPACELIFT_WORKER_RO_EXTRA_MOUNTS</code> - Additional directories to be mounted to the worker docker container during read only runs, as a comma separated list of mounts in the form of <code>/host/path:/container/path</code>.</li> <li><code>SPACELIFT_WORKER_WO_EXTRA_MOUNTS</code> - Additional directories to be mounted to the worker docker container during write only runs, as a comma separated list of mounts in the form of <code>/host/path:/container/path</code>.</li> <li><code>SPACELIFT_DEFAULT_RUNNER_IMAGE</code> if set, this will override the default runner image used for non-ansible runs. The default is <code>public.ecr.aws/spacelift/runner-terraform:latest</code>. Note this will not override a custom runner image on a stack, it will only take effect if no custom image is set.</li> <li><code>SPACELIFT_DEFAULT_ANSIBLE_RUNNER_IMAGE</code> if set, this will override the default runner image used for ansible runs. The default is <code>public.ecr.aws/spacelift/runner-ansible:latest</code>. Note this will not override a custom runner image on a stack, it will only take effect if no custom image is set.</li> </ul>"},{"location":"concepts/worker-pools.html#kubernetes-specific-configuration","title":"Kubernetes-specific configuration","text":"<p>There is more detailed information available in the Kubernetes workers documentation for certain configuration options:</p> <ul> <li>Initialization policies.</li> <li>Timeouts.</li> </ul>"},{"location":"concepts/worker-pools.html#passing-metadata-tags","title":"Passing metadata tags","text":"<p>Info</p> <p>Passing custom metadata tags is currently only supported for Docker-based workers. Kubernetes workers send through some metadata including the name of the Worker resource in Kubernetes along with the version of the controller used to create the worker, but do not support user-provided custom metadata.</p> <p>When the launcher from a worker pool is registering with the mothership, you can send along some tags that will allow you to uniquely identify the process/machine for the purpose of draining or debugging. Any environment variables using <code>SPACELIFT_METADATA_</code> prefix will be passed on. As an example, if you're running Spacelift workers in EC2, you can do the following just before you execute the launcher binary:</p> <pre><code>export SPACELIFT_METADATA_instance_id=$(ec2-metadata --instance-id | cut -d ' ' -f2)\n</code></pre> <p>Doing so will set your EC2 instance ID as <code>instance_id</code> tag in your worker.</p> <p>Please see injecting custom commands during instance startup for information about how to do this when using our CloudFormation template.</p>"},{"location":"concepts/worker-pools.html#vcs-agents","title":"VCS Agents","text":"<p>Tip</p> <p>VCS Agents are intended for version control systems (VCS) that cannot be accessed over the internet from the Spacelift backend.</p> <p>If your VCS can be accessed over the internet, possibly after allowing the Spacelift backend IP addresses, then you do not need to use VCS Agents.</p> <p>When using private workers with a privately accessible version control system, you will need to ensure that your private workers have direct network access to your Version Control System.</p> <p>Additionally, you will need to inform the private workers of the target network address for each of your VCS Agent Pools by setting up the following variables:</p> <ul> <li><code>SPACELIFT_PRIVATEVCS_MAPPING_NAME_&lt;NUMBER&gt;</code>: Name of the VCS Agent Pool.</li> <li><code>SPACELIFT_PRIVATEVCS_MAPPING_BASE_ENDPOINT_&lt;NUMBER&gt;</code>: IP address or hostname, with protocol, for the VCS system.</li> </ul> <p>There can be multiple VCS systems so replace <code>&lt;NUMBER&gt;</code> with an integer. Start from <code>0</code> and increment it by one for each new VCS system.</p> <p>Here is an example that configures access to two VCS systems:</p> <pre><code>export SPACELIFT_PRIVATEVCS_MAPPING_NAME_0=bitbucket_pool\nexport SPACELIFT_PRIVATEVCS_MAPPING_BASE_ENDPOINT_0=http://192.168.2.2\nexport SPACELIFT_PRIVATEVCS_MAPPING_NAME_1=github_pool\nexport SPACELIFT_PRIVATEVCS_MAPPING_BASE_ENDPOINT_1=https://internal-github.net\n</code></pre> <p>When using Kubernetes workers, please see the VCS Agents section in the Kubernetes workers docs for specific information on how to configure this.</p>"},{"location":"concepts/worker-pools.html#network-security","title":"Network Security","text":"<p>Private workers need to be able to make outbound connections in order to communicate with Spacelift, as well as to access any resources required by your runs. If you have policies in place that require you to limit the outbound traffic allowed from your workers, you can use the following lists as a guide.</p>"},{"location":"concepts/worker-pools.html#aws-services","title":"AWS Services","text":"<p>Your worker needs access to the following AWS services in order to function correctly. You can refer to the AWS documentation for their IP address ranges.</p> <ul> <li>Access to the public Elastic Container Registry if using our default runner image.</li> <li>Access to your Self-Hosted server, for example <code>https://spacelift.myorg.com</code>.</li> <li>Access to the AWS IoT Core endpoints in your installation region for worker communication via MQTT.</li> <li>Access to Amazon S3 in your installation region for uploading run logs.</li> </ul>"},{"location":"concepts/worker-pools.html#other","title":"Other","text":"<p>In addition, you will also need to allow access to the following:</p> <ul> <li>Your VCS provider.</li> <li>Access to any custom container registries you use if using custom runner images.</li> <li>Access to any other infrastructure required as part of your runs.</li> </ul>"},{"location":"concepts/worker-pools.html#hardware-recommendations","title":"Hardware recommendations","text":"<p>The hardware requirements for the workers will vary depending on the stack size(How many resources managed, resource type, etc.), but we recommend at least 2GB of memory and 2 vCPUs of compute power.</p> <p>These are the recommended server types for the three main cloud providers:</p> <ul> <li>AWS: t3.small instance type</li> <li>Azure: Standard_A2_V2 virtual machine</li> <li>GCP: e2-medium instance type</li> </ul>"},{"location":"concepts/worker-pools.html#using-worker-pools","title":"Using worker pools","text":"<p>Worker pools must be explicitly attached to stacks and/or modules in order to start processing their workloads. This can be done in the Behavior section of stack and module settings:</p> <p></p> <p></p>"},{"location":"concepts/worker-pools.html#worker-pool-management-views","title":"Worker Pool Management Views","text":"<p>You can view the activity and status of every aspect of your worker pool in the worker pool detail view. You can navigate to the worker pool of your choosing by clicking on the appropriate entry in the worker pools list view. </p>"},{"location":"concepts/worker-pools.html#private-worker-pool","title":"Private Worker Pool","text":"<p>A private worker pool is a worker pool for which you are responsible for managing the workers.</p> <p></p>"},{"location":"concepts/worker-pools.html#workers","title":"Workers","text":"<p>The workers tab lists all workers for this worker pool and their status.</p>"},{"location":"concepts/worker-pools.html#status","title":"Status","text":"<p>A worker can have three possible statuses:</p> <ul> <li><code>DRAINED</code> which indicates that the workers is not accepting new work.</li> <li><code>BUSY</code> which indicates that the worker is currently processing or about to process a run.</li> <li><code>IDLE</code> which indicates that the worker is available to start processing new runs.</li> </ul>"},{"location":"concepts/worker-pools.html#queued","title":"Queued","text":"<p>Queued lists all the run that can be scheduled and are currently in progress. In progress runs will be the first entries in the list when using the view without any filtering.</p> <p>Info</p> <p>Reasons a run might not be shown in this list: a tracked run is waiting on another tracked run, the run has a dependency on other runs.</p>"},{"location":"concepts/worker-pools.html#available-actions","title":"Available Actions","text":""},{"location":"concepts/worker-pools.html#cycle","title":"Cycle","text":"<p>Cycling the worker pool sends a self-destruct signal to all the workers in this pool. The process can take up to 20 seconds to complete. When you click on <code>cycle</code>, you will be prompted to confirm this action as it cannot be undone.</p> <p></p>"},{"location":"concepts/worker-pools.html#reset","title":"Reset","text":"<p>When you reset your worker pool, a new token is generated for your pool. This means that any workers that are using the old token will no-longer be able to connect and you need to update the credentials for the workers connected to that pool. This can be used used for security purposes if your certificate has been leaked.</p> <p>Please follow the steps to generate a new certificate.</p> <p></p>"},{"location":"concepts/worker-pools.html#used-by","title":"Used by","text":"<p>Stacks and/or Modules that are using the private worker pool.</p>"},{"location":"concepts/worker-pools.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/worker-pools.html#locally-the-run-completes-in-10-minutes-but-on-spacelift-it-takes-over-30-minutes-with-no-new-activity-appearing-in-the-logs-for-the-entire-30-minute-duration-if-debug-logging-is-set-to-on-then-only-showing-still-running-in-the-logs-for-that-duration","title":"Locally, the run completes in 10 minutes, but on Spacelift, it takes over 30 minutes with no new activity appearing in the logs for the entire 30-minute duration (if debug logging is set to on then only showing Still running... in the logs for that duration).","text":"<p>The issue might be related to the instance size and its CPU limitations. It's advisable to monitor CPU and memory usage and make adjustments as needed.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html","title":"Docker-based Workers","text":""},{"location":"concepts/worker-pools/docker-based-workers.html#docker-based-workers","title":"Docker-based Workers","text":"<p>Spacelift Docker-based workers consist of two main components: the launcher binary, and the worker binary. The launcher is responsible for downloading the correct version of the worker binary to be able to execute Spacelift runs, and for starting new Docker containers in response to those runs being scheduled.</p> <p>In a Self-Hosted install, the version of the launcher binary that is provided comes with the correct worker binary for your version of Self-Hosted embedded. This means that it doesn't need to download the worker binary separately when executing runs.</p> <p>We suggest using our Terraform module or Cloudformation template to deploy your workers, but you can also follow our instructions on manual setup if you need to deploy workers to an environment not supported by either approach.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#terraform-module","title":"Terraform module","text":"<p>The terraform-aws-spacelift-workerpool-on-ec2 module can be used to deploy an EC2-based worker pool on AWS. The module is originally for our SaaS offering, but it's compatible with self-hosted when providing the <code>selfhosted_configuration</code> variable. For example:</p> <pre><code>module \"my_workerpool\" {\n  source = \"github.com/spacelift-io/terraform-aws-spacelift-workerpool-on-ec2?ref=v5.2.0\"\n\n  secure_env_vars = {\n    SPACELIFT_TOKEN            = var.worker_pool_config\n    SPACELIFT_POOL_PRIVATE_KEY = var.worker_pool_private_key\n  }\n\n  configuration = &lt;&lt;EOF\n    export SPACELIFT_SENSITIVE_OUTPUT_UPLOAD_ENABLED=true\n  EOF\n\n  min_size                 = 1\n  max_size                 = 10\n  worker_pool_id           = var.worker_pool_id\n  security_groups          = var.worker_pool_security_groups\n  vpc_subnets              = var.worker_pool_subnets\n  selfhosted_configuration = {\n    s3_uri = \"s3://spacelift-binaries-123ab/spacelift-launcher\"\n  }\n}\n</code></pre>"},{"location":"concepts/worker-pools/docker-based-workers.html#cloudformation-template","title":"CloudFormation Template","text":"<p>Another way to deploy workers for self-hosting is to deploy the CloudFormation template found in <code>cloudformation/workerpool.yaml</code>.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#pseudorandomsuffix","title":"PseudoRandomSuffix","text":"<p>The CloudFormation stack uses a parameter called <code>PseudoRandomSuffix</code> in order to ensure that certain resources are unique within an AWS account. The value of this parameter does not matter, other than that it is unique per worker pool stack you deploy. You should choose a value that is 6 characters long and made up of letters and numbers, for example <code>ab12cd</code>.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#create-a-secret","title":"Create a secret","text":"<p>First, create a new secret in SecretsManager, and add your token and the base64-encoded value of your private key. Use the key <code>SPACELIFT_TOKEN</code> for your token and <code>SPACELIFT_POOL_PRIVATE_KEY</code> for the private key. It should look something like this:</p> <p></p> <p>Give your secret a name and create it. It doesn't matter what this name is, but you'll need it when deploying the CloudFormation stack.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#get-the-downloads-bucket-name","title":"Get the downloads bucket name","text":"<p>The downloads bucket name is output at the end of the installation process. If you don't have a note of it, you can also get it from the resources of the spacelift-infra-s3 stack in CloudFormation:</p> <p></p>"},{"location":"concepts/worker-pools/docker-based-workers.html#ami","title":"AMI","text":"<p>You can use your own custom-built AMI for your workers, or you can use one of the pre-built images we provide. For a list of the correct AMI to use for the region you want to deploy your worker to, see the spacelift-worker-image releases page.</p> <p>Note: please make sure to choose the <code>x86_64</code> version of the AMI.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#subnets-and-security-group","title":"Subnets and Security Group","text":"<p>You will need to have an existing VPC to deploy your pool into, and will need to provide a list of subnet IDs and security groups to match your requirements.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#using-a-custom-iam-role","title":"Using a custom IAM role","text":"<p>By default we will create the instance role for the EC2 ASG as part of the worker pool stack, but you can also provide your own custom role via the <code>InstanceRoleName</code> parameter. This allows you to grant permissions to additional AWS resources that your workers need access to. A great example of this is allowing access to a private ECR in order to use a custom runner image.</p> <p>At a minimum, your role must fulfil the following requirements:</p> <ul> <li>It must have a trust relationship that allows role assumption by EC2.</li> <li>It needs to have the following managed policies attached:<ul> <li><code>AutoScalingReadOnlyAccess</code>.</li> <li><code>CloudWatchAgentServerPolicy</code>.</li> <li><code>AmazonSSMManagedInstanceCore</code>.</li> </ul> </li> </ul>"},{"location":"concepts/worker-pools/docker-based-workers.html#injecting-custom-commands-during-instance-startup","title":"Injecting custom commands during instance startup","text":"<p>You have the option to inject custom commands into the EC2 user data. This can be useful if you want to install additional software on your workers, or if you want to run a custom script during instance startup, or just add some additional environment variables.</p> <p>The script must be a valid shell script and should be put into Secrets Manager. Then you can provide the name of the secret as <code>CustomUserDataSecretName</code> when deploying the stack.</p> <p>Example:</p> <p></p> <p>In the example above, we used <code>spacelift/userdata</code> as a secret name so the parameter will look like this:</p> <pre><code>  [...]\n  --parameter-overrides \\\n    CustomUserDataSecretName=\"spacelift/userdata\" \\\n  [...]\n</code></pre>"},{"location":"concepts/worker-pools/docker-based-workers.html#granting-access-to-a-private-ecr","title":"Granting access to a private ECR","text":"<p>To allow your worker role to access a private ECR, you can attach a policy similar to the following to your instance role (replacing <code>&lt;repository-arn&gt;</code> with the ARN of your ECR repository):</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:BatchGetImage\",\n                \"ecr:BatchCheckLayerAvailability\"\n            ],\n            \"Resource\": \"&lt;repository-arn&gt;\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:GetAuthorizationToken\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>NOTE: repository ARNs are in the format <code>arn:&lt;partition&gt;:ecr:&lt;region&gt;:&lt;account-id&gt;:repository/&lt;repository-name&gt;</code>.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#proxy-configuration","title":"Proxy Configuration","text":"<p>If you need to use an HTTP proxy for internet access, you can provide the proxy configuration using the following CloudFormation parameters:</p> <ul> <li><code>HttpProxyConfig</code>.</li> <li><code>HttpsProxyConfig</code>.</li> <li><code>NoProxyConfig</code>.</li> </ul> <p>For example, you could use the following command to deploy a worker with a proxy configuration:</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n  --stack-name spacelift-default-worker-pool \\\n  --template-file \"cloudformation/workerpool.yaml\" \\\n  --region \"eu-west-1\" \\\n  --parameter-overrides \\\n    PseudoRandomSuffix=\"ab12cd\" \\\n    BinariesBucket=\"012345678901-spacelift-infra-spacelift-downloads\" \\\n    SecretName=\"spacelift/default-worker-pool-credentials\" \\\n    SecurityGroups=\"sg-0d1e157a19ba2106f\" \\\n    Subnets=\"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\\n    ImageId=\"ami-0ead0234bef4f51b0\" \\\n    HttpProxyConfig=\"http://proxy.example.com:1234\" \\\n    HttpsProxyConfig=\"https://proxy.example.com:4321\" \\\n    NoProxyConfig=\"some.domain,another.domain\" \\\n  --capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre>"},{"location":"concepts/worker-pools/docker-based-workers.html#using-custom-ca-certificates","title":"Using custom CA certificates","text":"<p>If you use a custom certificate authority to issue TLS certs for components that Spacelift will communicate with, for example your VCS system, you need to provide your custom CA certificates to the worker. You do this by creating a secret in SecretsManager containing a base64 encoded JSON string.</p> <p>The format of the JSON object is as follows:</p> <pre><code>{\"caCertificates\": [\"&lt;base64-encoded-cert-1&gt;\", \"&lt;base64-encoded-cert-2&gt;\", \"&lt;base64-encoded-cert-N&gt;\"]}\n</code></pre> <p>For example, if you had a file called ca-certs.json containing the following content:</p> <pre><code>{\n  \"caCertificates\": [\n    \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZzVENDQTVtZ0F3SUJBZ0lVREQvNFZCZkx4NUsvdEFZK1Nja0gwNVRKOGk4d0RRWUpLb1pJaHZjTkFRRUwKQlFBd2FERUxNQWtHQTFVRUJoTUNSMEl4RVRBUEJnTlZCQWdNQ0ZOamIzUnNZVzVrTVJBd0RnWURWUVFIREFkSApiR0Z6WjI5M01Sa3dGd1lEVlFRS0RCQkJaR0Z0SUVNZ1VtOXZkQ0JEUVNBeE1Sa3dGd1lEVlFRRERCQkJaR0Z0CklFTWdVbTl2ZENCRFFTQXhNQjRYRFRJek1ETXhNekV4TXpZeE1Wb1hEVEkxTVRJek1URXhNell4TVZvd2FERUwKTUFrR0ExVUVCaE1DUjBJeEVUQVBCZ05WQkFnTUNGTmpiM1JzWVc1a01SQXdEZ1lEVlFRSERBZEhiR0Z6WjI5MwpNUmt3RndZRFZRUUtEQkJCWkdGdElFTWdVbTl2ZENCRFFTQXhNUmt3RndZRFZRUUREQkJCWkdGdElFTWdVbTl2CmRDQkRRU0F4TUlJQ0lqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FnOEFNSUlDQ2dLQ0FnRUF4anYvK3NJblhpUSsKMkZiK2l0RjhuZGxwbW1ZVW9ad1lONGR4KzJ3cmNiT1ZuZ1R2eTRzRSszM25HQnpINHZ0NHBPaEtUV3dhWVhGSQowQ3pxb0lvYXppOFpsMG1lZHlyd3RJVURaMXBOY1Z1Z2I0S0FGYjlKYnE0MElrM3hHNnQxNm1heFFKR1RpQUcyCi94VnRzdVlkaG5CR3gvLzYxU0ViRXdTcFIxNDUvUWYxY2JhOFJsUlFNejRRVVdOZThYWG8zU1lhWDJreGl3MlYKMU9wK2ZReGcyamYxQXl6UVhYMWNoMWp5RzVSTEVTUFVNRmtCaVF3aTdMT1NDYWF2ZkpFVXp3cWVvT1JnZDdUaQp1eU1WKzRHc2IxWEFuSzdLWFl3aXNHZVA1L1FORlBBQnlmQWRQalIyMHJNWVlIZnhxRUR0aDROYWpqbXUvaXlGClBHazRDb2JSaGl0VHRKWFQvUXhXY3Z0clJ1MUJDVm5lZHlFU015aXlhNFE5ZG4yN3JGampnM1pBUnFXT1poeXEKT1RXSG8ybU8yRnpFSnV4aHZZTmUyaVlWcDJzOHdNVEIwMm5QM3dwV29Zd2plMnlEd2Nqa0lsOHVYS3pFWjlHZgpGQVRKYUNMb084bzVKMkhYc2dPSXFYbHB6VTl0VXRFZXcveFR6WnFYNUEzNG84LytOZ1V0bTBGN2pvV2E1bURDClFCN0w4Y0tmQUN5ZGZwZWtKeC9nRlVHU3kvNXZkZkJ6T2N6YzZCbWg2NnlIUEJSRGNneURGbm54MzRtL1hWUWEKckJ3d0lERGJxdTNzc2NkT2dtOXY4Y3NDSmQwWWxYR2IveDRvQUE2MUlJVG5zTmQ5TkN3MEdKSXF1U0VjWWlDRQpBMFlyUVRLVmZSQVh1aFNaMVZQSXV4WGlGMkszWFRNQ0F3RUFBYU5UTUZFd0hRWURWUjBPQkJZRUZENTVSNG10CjBoTk9KVWdQTDBKQktaQjFqeWJTTUI4R0ExVWRJd1FZTUJhQUZENTVSNG10MGhOT0pVZ1BMMEpCS1pCMWp5YlMKTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3RFFZSktvWklodmNOQVFFTEJRQURnZ0lCQUhlY1ZqTWtsVGtTMlB5NQpYTnBKOWNkekc2Nkd1UER3OGFRWkl1bnJ4cVl1ZDc0Q0ExWTBLMjZreURKa0xuV3pWYTduVCtGMGQ4UW4zdG92CnZGd0kzeHk1bCs0dXBtdVozdTFqRkVNaVNrOEMyRlBvaExEbkRvM3J3RVVDR3ZKNmE0R2FzN1l5SFBHTDNEckoKMGRjdTl3c1g5Y1lCMllKMjdRb3NaNXM2em1tVXZCR1RJMzBKTnZQblNvQzdrenFEM0FyeHZURVc5V2FVcW9KdAo4OGxzTW5uNitwczlBNmV4Yi9mSzkwOVpXYUVKV1JkOWNkTUVUMGZuYTdFaGhrTytDcXo0MTVSZ014bEs3Z2dUCjk3Q3ZranZ2TE5lRlQ1bmFIYnpVQU5xZk1WUlJjVWFQM1BqVEM5ejVjRG85Q2FQYUZqVi8rVXhheDJtQWxBUmsKZnFZeVdvcXZaSDkwY3pwdkZHMWpVbzZQNE5weXhaUzhsYXlKd0QyNHFYK0VPTjQzV1lBcExzbC9qRTJBL0ptUQpNZGdXTmhPeTRIUDhVOCthQU5yMEV2N2dXV05pNlZjUjhUNlBUL3JiQUdqblBtVm1vWjRyYzdDZG9TOFpRWkpoCks4RUxBMTcrcG5NVGdvN3d4ZkFScUwrcCttcWd0VXhSYmlXaXRldjhGMmhVVkIvU3dQOGhwY0dyZGhURU43dGQKcFNXMXlrUGVHSkZLU0JvNVFIYW5xcVBGQ3pxdEZlb0w5RGhZeDUveEU2RnBLTUxnM3ZWY0ZzSHU2Z2xTOGlNVgo0SHZiMmZYdWhYeExUQkNiRDErNWxMUC9iSFhvZ1FLbXAySDZPajBlNldCbVEweHFHb3U0SWw2YmF2c1pDeDJ2CkFEV3ZsdWU1alhkTnU1eFBaZHNOVk5BbHVBbmUKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\"\n  ]\n}\n</code></pre> <p>You could then encode it to base64 using <code>base64 -w0 &lt; ca-certs.json</code> (or <code>base64 -b 0 &lt; ca-certs.json</code> on a Mac), resulting in the following string:</p> <pre><code>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\n</code></pre> <p>You would then create a secret in SecretsManager, and deploy the worker pool using the following command (replacing <code>&lt;ca-cert-secret-name&gt;</code> with the name of your secret):</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n  --stack-name spacelift-default-worker-pool \\\n  --template-file \"cloudformation/workerpool.yaml\" \\\n  --region \"eu-west-1\" \\\n  --parameter-overrides \\\n    PseudoRandomSuffix=\"ab12cd\" \\\n    BinariesBucket=\"012345678901-spacelift-infra-spacelift-downloads\" \\\n    SecretName=\"spacelift/default-worker-pool-credentials\" \\\n    SecurityGroups=\"sg-0d1e157a19ba2106f\" \\\n    Subnets=\"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\\n    ImageId=\"ami-0ead0234bef4f51b0\" \\\n    AdditionalRootCAsSecretName=\"&lt;ca-cert-secret-name&gt;\" \\\n  --capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre>"},{"location":"concepts/worker-pools/docker-based-workers.html#running-the-launcher-as-root","title":"Running the launcher as root","text":"<p>By default, when the EC2 instance starts up, it creates a user called <code>spacelift</code> with a UID of 1983. This user is then used to run the launcher process.</p> <p>If for some reason this causes problems, you can run the launcher as <code>root</code> by setting the <code>RunLauncherAsSpaceliftUser</code> CloudFormation parameter to <code>false</code>.</p> <p>Tip</p> <p>Versions v0.0.7 or older of Self-Hosted always ran the launcher as root. In newer versions this behavior has changed to default to the <code>spacelift</code> user.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#poweroffonerror","title":"PowerOffOnError","text":"<p>By default, the startup script for the EC2 instances automatically terminates the instance if the launcher exits. This is to allow the instance to be automatically removed from the autoscale group and a new one added in the case of errors.</p> <p>Sometimes it can be useful to disable this behavior, for example if instances are repeatedly crashing on startup, preventing you from being able to connect to investigate any issues before they terminate.</p> <p>To do this, set the <code>PowerOffOnError</code> setting to false when deploying your CloudFormation stack.</p> <p>Info</p> <p>Please note, if you update this setting on an existing CloudFormation stack, you will need to restart all the workers in the pool before the updated setting takes effect.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#deploying-the-template","title":"Deploying the Template","text":"<p>To deploy your worker pool stack, you can use the following command:</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n  --stack-name spacelift-default-worker-pool \\\n  --template-file \"cloudformation/workerpool.yaml\" \\\n  --region \"&lt;region&gt;\" \\\n  --parameter-overrides \\\n    PseudoRandomSuffix=\"ab12cd\" \\\n    BinariesBucket=\"&lt;binaries-bucket&gt;\" \\\n    SecretName=\"&lt;secret-name&gt;\" \\\n    SecurityGroups=\"&lt;security-groups&gt;\" \\\n    Subnets=\"&lt;subnets&gt;\" \\\n    ImageId=\"&lt;ami-id&gt;\" \\\n  --capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre> <p>For example, to deploy to <code>eu-west-1</code> you might use something like this:</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n  --stack-name spacelift-default-worker-pool \\\n  --template-file \"cloudformation/workerpool.yaml\" \\\n  --region \"eu-west-1\" \\\n  --parameter-overrides \\\n    PseudoRandomSuffix=\"ab12cd\" \\\n    BinariesBucket=\"012345678901-spacelift-infra-spacelift-downloads\" \\\n    SecretName=\"spacelift/default-worker-pool-credentials\" \\\n    SecurityGroups=\"sg-0d1e157a19ba2106f\" \\\n    Subnets=\"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\\n    ImageId=\"ami-0ead0234bef4f51b0\" \\\n  --capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre> <p>To use a custom instance role, you might use something like this:</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n  --stack-name spacelift-default-worker-pool \\\n  --template-file \"cloudformation/workerpool.yaml\" \\\n  --region \"eu-west-1\" \\\n  --parameter-overrides \\\n    PseudoRandomSuffix=\"ab12cd\" \\\n    BinariesBucket=\"012345678901-spacelift-infra-spacelift-downloads\" \\\n    SecretName=\"spacelift/default-worker-pool-credentials\" \\\n    SecurityGroups=\"sg-0d1e157a19ba2106f\" \\\n    Subnets=\"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\\n    ImageId=\"ami-0ead0234bef4f51b0\" \\\n    InstanceRoleName=\"default-worker-role\" \\\n  --capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre>"},{"location":"concepts/worker-pools/docker-based-workers.html#manual-setup","title":"Manual setup","text":""},{"location":"concepts/worker-pools/docker-based-workers.html#pre-requisites","title":"Pre-requisites","text":"<p>In order to work, the launcher expects to be able to write to the local Docker socket. Please make sure that Docker is installed and running.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#downloading-the-launcher","title":"Downloading the launcher","text":"<p>The Self-Hosted release archive contains a copy of the Spacelift launcher binary built specifically for your version of Self-Hosted. You can find this at <code>bin/spacelift-launcher</code>. This binary is also uploaded to the downloads S3 bucket during the Spacelift installation process. For more information on how to find your bucket name see here.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#running-the-launcher","title":"Running the launcher","text":"<p>You can run the launcher binary using the following commands (replacing <code>&lt;worker-pool-id&gt;</code> with the ID of your pool):</p> <pre><code>export SPACELIFT_TOKEN=$(cat worker-pool-&lt;worker-pool-id&gt;.config)\nexport SPACELIFT_POOL_PRIVATE_KEY=$(cat spacelift.key | base64 -w0)\n./spacelift-launcher\n</code></pre> <p>These commands set the following environment variables and then execute the launcher binary:</p> <ul> <li><code>SPACELIFT_TOKEN</code> - the token you\u2019ve received from Spacelift on worker pool creation.</li> <li><code>SPACELIFT_POOL_PRIVATE_KEY</code> - the contents of the private key file you generated, in base64.</li> </ul> <p>Info</p> <p>You need to encode the entire private key using base-64, making it a single line of text. The simplest approach is to just run <code>cat spacelift.key | base64 -w 0</code> in your command line. For Mac users, the command is <code>cat spacelift.key | base64 -b 0</code>.</p> <p>Congrats! Your launcher should now connect to the Spacelift backend and start handling runs.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#ami-updates-deprecation-policy","title":"AMI updates &amp; deprecation policy","text":"<p>If you run your workers in AWS and use the Spacelift AMIs, make sure to update your worker pool routinely as they receive weekly updates to ensure all system components are up-to-date.</p> <p>Currently, the AWS AMIs are set to be deprecated in 364 days after its release and removed after 420 days. You won't be able to start new instances using a deprecated AMI, but existing instances will continue to run.</p>"},{"location":"concepts/worker-pools/docker-based-workers.html#ec2-spot-instances","title":"EC2 Spot Instances","text":"<p>The AWS Terraform module supports EC2 Spot Instances for up to 90% cost savings.</p> <p>Not Recommended for Production/Critical Workloads</p> <p>Spot instances are NOT recommended for critical or production workloads as they can be interrupted with only 2 minutes notice, potentially causing:</p> <ul> <li>Incomplete or corrupted Terraform state</li> <li>Failed deployments leaving infrastructure in inconsistent state</li> <li>Loss of work-in-progress for long-running operations</li> </ul> <p>Use Spot instances only for development, testing, or fault-tolerant workloads where interruption is acceptable: for example, ephemeral environments, Terraform modules, or operations with guaranteed runtimes under one minute.</p> <pre><code>module \"my_workerpool\" {\n  source = \"github.com/spacelift-io/terraform-aws-spacelift-workerpool-on-ec2\"\n\n  # Other settings are omitted for brevity\n\n  instance_market_options = {\n    market_type = \"spot\"\n  }\n  ec2_instance_type = \"t3.medium\"\n}\n</code></pre> <p>The Spacelift worker includes graceful interruption handling: it monitors for spot interruption notices and allows running jobs to complete when possible. However, if a run doesn't complete within the 2-minute interruption grace period, it will be abruptly terminated and crash.</p> <p>Use the AWS EC2 Spot Instance Advisor to select cost-effective instance types with lower interruption rates. See the spot instances example for more configuration options.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html","title":"Kubernetes Workers","text":""},{"location":"concepts/worker-pools/kubernetes-workers.html#kubernetes-workers","title":"Kubernetes Workers","text":"<p>We provide a Kubernetes operator for managing Spacelift worker pools. The operator also works on OpenShift. This operator allows you to define <code>WorkerPool</code> resources in your cluster, and allows you to scale these pools up and down using standard Kubernetes functionality.</p> <p>Info</p> <p>Previously we provided a Helm chart for deploying worker pools to Kubernetes using Docker-in-Docker. This approach is no-longer recommended, and you should use the Kubernetes operator instead. Please see the section on migrating from Docker-in-Docker for more information.</p> <p>A <code>WorkerPool</code> defines the number of <code>Workers</code> registered with Spacelift via the <code>poolSize</code> parameter. The Spacelift operator will automatically create and register a number of <code>Worker</code> resources in Kubernetes depending on your <code>poolSize</code>.</p> <p>Info</p> <p><code>Worker</code> resources do not use up any cluster resources other than an entry in the Kubernetes API when they are idle. <code>Pods</code> are created on demand for <code>Workers</code> when scheduling messages are received from Spacelift. This means that in an idle state no additional resources are being used in your cluster other than what is required to run the controller component of the Spacelift operator.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#kubernetes-version-compatibility","title":"Kubernetes version compatibility","text":"<p>The spacelift controller is compatible with Kubernetes version v1.26+. The controller may also work with older versions, but we do not guarantee and provide support for unmaintained Kubernetes versions.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#installation","title":"Installation","text":""},{"location":"concepts/worker-pools/kubernetes-workers.html#controller-setup","title":"Controller setup","text":"KubectlHelm <p>To install the worker pool controller along with its CRDs, run the following command:</p> <pre><code>kubectl apply --server-side -f https://downloads.spacelift.io/kube-workerpool-controller/latest/manifests.yaml\n</code></pre> <p>Warning</p> <p>It is important to use the <code>--server-side</code> flag here. The reason is that our CRD definitions contains long field descriptions. This causes the apply to fail, as detailed in this kubebuilder issue, because kubernetes sets the <code>kubectl.kubernetes.io/last-applied-configuration</code> annotation, and the size of the CRD exceed the maximum size of an annotation field.</p> <p>Tip</p> <p>You can download the manifests yourself from https://downloads.spacelift.io/kube-workerpool-controller/latest/manifests.yaml if you would like to inspect them or alter the Deployment configuration for the controller.</p> <p>You can install the controller using the official spacelift-workerpool-controller Helm chart.</p> <pre><code>helm repo add spacelift https://downloads.spacelift.io/helm\nhelm repo update\nhelm upgrade spacelift-workerpool-controller spacelift/spacelift-workerpool-controller --install --namespace spacelift-worker-controller-system --create-namespace\n</code></pre> <p>You can open <code>values.yaml</code> from the helm chart repo for more customization options.</p> <p>Warning</p> <p>Helm has no support at this time for upgrading or deleting crd's so this would need to be done manually through kubernetes. The latest CRD's can be found in this link.</p> <p>Prometheus metrics</p> <p>The controller also has a subchart for our prometheus-exporter project that exposes metrics in OpenMetrics spec. This is useful for scaling workers based on queue length in spacelift (<code>spacelift_worker_pool_runs_pending</code> metric). To install the controller with the prometheus-exporter subchart, use the following command: </p><pre><code>helm upgrade spacelift-workerpool-controller spacelift/spacelift-workerpool-controller --install --namespace spacelift-worker-controller-system --create-namespace \\\n--set spacelift-promex.enabled=true \\\n--set spacelift-promex.apiEndpoint=\"https://{yourAccount}.app.spacelift.io\" \\\n--set spacelift-promex.apiKeyId=\"{yourApiToken}\" \\\n--set spacelift-promex.apiKeySecretName=\"spacelift-api-key\"\n</code></pre> Read more on the exporter on its repository here and see more config options in the <code>values.yaml</code> file for the subchart.<p></p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#openshift","title":"OpenShift","text":"<p>If you are using OpenShift, additional steps are required to allow the controller to run properly.</p> <ol> <li> <p>Get the controllers service account name:</p> <pre><code>kubectl get serviceaccounts -n {namespace_of_controller}\n</code></pre> </li> <li> <p>Add the <code>anyuid</code> security context constraint to the service account:</p> <pre><code>oc adm policy add-scc-to-user anyuid -z {service_account_name} -n {namespace_of_controller} --as system:admin\n</code></pre> </li> <li> <p>Create a Spacelift Admin role in the namespace where your worker pods will run (note this may be different from the namespace you installed the controller into):</p> <pre><code>oc create role spacelift-admin --verb='*' --resource='*' -n {namespace_of_worker_pods}\n</code></pre> </li> <li> <p>Bind the role to the controller's service account:</p> <pre><code>oc create rolebinding spacelift-admin-binding --role=spacelift-admin --serviceaccount={namespace_of_controller}:{service_account_name} -n {namespace_of_worker_pods}\n</code></pre> </li> <li> <p>Ensure the controller can indeed use the kubernetes api in the namespace where your worker pods will run:</p> <pre><code>oc auth can-i '*' '*' --as=system:serviceaccount:{namespace_of_controller}:{service_account_name} -n {namespace_of_worker_pods}\n</code></pre> <p>Note: this should return <code>yes</code> if everything is set up correctly.</p> </li> <li> <p>Restart the worker controller pod to make sure it picks up the new permissions.</p> <pre><code>kubectl rollout restart deployments -n {namespace_of_controller}\n</code></pre> </li> </ol>"},{"location":"concepts/worker-pools/kubernetes-workers.html#create-a-secret","title":"Create a Secret","text":"<p>Next, create a Secret containing the private key and token for your worker pool, generated earlier in this guide.</p> <p>First, export the token and private key as base64 encoded strings:</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#mac","title":"Mac","text":"<pre><code>export SPACELIFT_WP_TOKEN=$(cat ./your-workerpool-config-file.config)\nexport SPACELIFT_WP_PRIVATE_KEY=$(cat ./your-private-key.pem | base64 -b 0)\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#linux","title":"Linux","text":"<pre><code>export SPACELIFT_WP_TOKEN=$(cat ./your-workerpool-config-file.config)\nexport SPACELIFT_WP_PRIVATE_KEY=$(cat ./your-private-key.pem | base64 -w 0)\n</code></pre> <p>Then, create the secret.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: test-workerpool\ntype: Opaque\nstringData:\n  token: ${SPACELIFT_WP_TOKEN}\n  privateKey: ${SPACELIFT_WP_PRIVATE_KEY}\nEOF\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#create-a-workerpool","title":"Create a WorkerPool","text":"<p>Finally, create a WorkerPool resource using the following command:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  name: test-workerpool\nspec:\n  poolSize: 2\n  token:\n    secretKeyRef:\n      name: test-workerpool\n      key: token\n  privateKey:\n    secretKeyRef:\n      name: test-workerpool\n      key: privateKey\nEOF\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#grant-access-to-the-launcher-image","title":"Grant access to the Launcher image","text":"<p>During your Self-Hosted installation process, the Spacelift launcher image should be uploaded to a container repository. For instance, it could be Artifact Registry in GCP or ECR in AWS.</p> <p></p> <p>The launcher image is used during runs on Kubernetes workers to prepare the workspace for the run, and the Kubernetes cluster that you want to run your workers on needs to be able to pull that image for runs to succeed.</p> <p>Some options for this include:</p> <ol> <li>If your Kubernetes cluster is running inside AWS, you can add a policy to your ECR to allow pulls from your cluster nodes. Alternatively, you can use one of the methods listed in the ECR private registry authentication guide.</li> <li>You can copy the image to a registry accessible by your cluster, and then set the <code>spec.pod.launcherImage</code> configuration option on your <code>WorkerPool</code> resource to point at it.</li> </ol> <p>Info</p> <p>You can deploy the controller globally (the default option) to monitor all namespaces, allowing worker pools in multiple namespaces, or restrict it to specific namespaces using the <code>namespaces</code> option in the Helm chart values. The namespace of the controller and workers themselves doesn\u2019t impact functionality.</p> <p>That's it - the workers in your pool should connect to Spacelift, and you should be able to trigger runs!</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#upgrade","title":"Upgrade","text":"<p>Usually, there is nothing special to do for upgrading the controller.</p> <p>Some release of the controller may include backward compatibility breaks, you can find below instructions about how to upgrade for those specials versions.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#upgrading-to-controller-v0017-or-helm-chart-v0330","title":"Upgrading to controller v0.0.17 - or Helm chart v0.33.0","text":"<p>This release changes the way the controller exposes metrics by removing usage of the <code>kube-rbac-proxy</code> container.</p> <p>You can find more context about the reason for this change in the Kubebuilder repository.</p> KubectlHelm <p>If the controller was installed using compiled Kubernetes manifest using <code>kubectl apply -f ...</code>, you should first uninstall the current release before deploying the new one.</p> <p>Warning</p> <p>The command below will remove CRDs and thus also remove your <code>WorkerPool</code> from the cluster. Before running it, make sure that you'll be able to recreate them after the upgrade.</p> <pre><code># Scale down all your workerpools to zero, and make sure there is no remaining Worker resource in the cluster.\n# Otherwise the kubectl delete function below will be stuck and you'll have to remove finalizers by hand on Workers.\nkubectl scale workerpool/${WORKERPOOL_NAME} --replicas 0\n# If your're using v0.0.16, change the version to the one that is currently deployed in your cluster.\nkubectl delete -f https://downloads.spacelift.io/kube-workerpool-controller/v0.0.16/manifests.yaml\n</code></pre> <p>Then you can install the new controller version with the following command.</p> <pre><code>kubectl apply -f https://downloads.spacelift.io/kube-workerpool-controller/v0.0.17/manifests.yaml\n</code></pre> <p>CRDs have been updated in this new version, and Helm does not perform CRDs update for us. So before upgrading to the latest version of the chart, you should execute the following commands to upgrade CRDs.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spacelift-io/spacelift-helm-charts/refs/tags/v0.33.0/spacelift-workerpool-controller/crds/worker-crd.yaml\nkubectl apply -f https://raw.githubusercontent.com/spacelift-io/spacelift-helm-charts/refs/tags/v0.33.0/spacelift-workerpool-controller/crds/workerpool-crd.yaml\n</code></pre> <p>Once done, you can upgrade the chart like usual with <code>helm upgrade</code>.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#run-containers","title":"Run Containers","text":"<p>When a run assigned to a Kubernetes worker is scheduled by Spacelift, the worker pool controller creates a new Pod to process the run. This Pod consists of the following containers:</p> <ul> <li>An init container called <code>init</code>, responsible for populating the workspace for the run.</li> <li>A <code>launcher-grpc</code> container that runs a gRPC server used by the worker for certain tasks like uploading the workspace between run stages, and notifying the worker when a user has requested that the run be stopped.</li> <li>A <code>worker</code> container that executes your run.</li> </ul> <p>The <code>init</code> and <code>launcher-grpc</code> containers use the <code>public.ecr.aws/spacelift/launcher:&lt;version&gt;</code> container image published by Spacelift. By default, the Spacelift backend sends the correct value for <code>&lt;version&gt;</code> through to the controller for each run, guaranteeing that the run is pinned to a specific image version that is compatible with the Spacelift backend.</p> <p>The <code>worker</code> container uses the runner image specified by your Spacelift stack.</p> <p>Warning</p> <p>You can use the <code>spec.pod.launcherImage</code> configuration option to pin the <code>init</code> and <code>launcher-grpc</code> containers to a specific version, but we do not typically recommend doing this because it means that your run Pods could become incompatible with the Spacelift backend as new versions are released.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#resource-usage","title":"Resource Usage","text":""},{"location":"concepts/worker-pools/kubernetes-workers.html#kubernetes-controller","title":"Kubernetes Controller","text":"<p>During normal operations the worker pool controller CPU and memory usage should be fairly stable. The main operation that can be resource intensive is scaling out a worker pool. Scaling up involves generating an RSA keypair for each worker, and is CPU-bound. If you notice performance issues when scaling out, it's worth giving the controller more CPU.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#run-pods","title":"Run Pods","text":"<p>Resource requests and limits for the <code>init</code>, <code>launcher-grpc</code> and <code>worker</code> containers can be set via your <code>WorkerPool</code> definitions, like in the following example:</p> <pre><code>apiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  name: test-pool\nspec:\n  poolSize: 2\n  token:\n    secretKeyRef:\n      name: pool-credentials\n      key: token\n  privateKey:\n    secretKeyRef:\n      name: pool-credentials\n      key: privateKey\n  pod:\n    initContainer:\n      resources:\n        requests:\n          cpu: 500m\n          memory: 200Mi\n        # Please note: we recommend being very cautious when adding resource limits\n        # to your containers. Setting too low a limit on the init container can cause\n        # runs to fail during the preparing phase.\n        # limits:\n        #   cpu: 100m\n        #   memory: 50Mi\n    grpcServerContainer:\n      resources:\n        requests:\n          cpu: 500m\n          memory: 200Mi\n        # Setting too low limits on the grpc server container can cause runs to fail\n        # when moving into the unconfirmed stage, as well as problems like not being\n        # able to stop/cancel runs.\n        # limits:\n        #   cpu: 100m\n        #   memory: 50Mi\n    workerContainer:\n      resources:\n        requests:\n          cpu: 500m\n          memory: 200Mi\n        # Setting too low limits on the worker container can cause problems executing\n        # your IaC tool (e.g. OpenTofu, Terraform, etc), causing runs to fail during\n        # planning, applying or destroying phases.\n        # limits:\n        #   cpu: 500m\n        #   memory: 200Mi\n</code></pre> <p>You can use the values above as a baseline to get started, but the exact values you need for your pool will depend on your individual circumstances. You should use monitoring tools to adjust these to values that make sense.</p> <p>Warning</p> <p>In general, we don't suggest setting very low CPU or memory limits for the <code>init</code>, <code>grpc</code> or <code>worker</code> containers since doing so could affect the performance of runs, or even cause runs to fail if they are set too low. And in particular, the worker container resource usage will very much depend on your workloads. For example stacks with large numbers of Terraform resources may use more memory than smaller stacks.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#volumes","title":"Volumes","text":"<p>There are two volumes that are always attached to your run Pods:</p> <ul> <li>The workspace volume.</li> <li>The binaries cache volume.</li> </ul> <p>Both of these volumes default to using <code>emptyDir</code> storage with no size limit. Spacelift workers will function correctly without using a custom configuration for these volumes, but there may be situations where you wish to change this default, for example:</p> <ul> <li>To prevent Kubernetes evicting your run Pods due to disk pressure (and therefore causing runs to fail).</li> <li>To support caching tool binaries (for example Terraform or OpenTofu) between runs.</li> </ul>"},{"location":"concepts/worker-pools/kubernetes-workers.html#workspace-volume","title":"Workspace Volume","text":"<p>The workspace volume is used to store the temporary workspace data needed for processing a run. This includes metadata about the run, along with your source code. The workspace volume does not need to be shared or persisted between runs, and for that reason we recommend using an Ephemeral Volume so that the volume is bound to the lifetime of the run, and will be destroyed when the run Pod is deleted.</p> <p>The workspace volume can be configured via the <code>spec.pod.workspaceVolume</code> property, which accepts a standard Kubernetes volume definition. Here's an example of using an ephemeral AWS GP2 volume for storage:</p> <pre><code>apiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  name: worker-pool\nspec:\n  poolSize: 1\n  privateKey:\n    secretKeyRef:\n      key: privateKey\n      name: pool-credentials\n  token:\n    secretKeyRef:\n      key: token\n      name: pool-credentials\n  pod:\n    securityContext:\n      # The fsGroup may or may not be required depending on your volume type. The reason for\n      # specifying it is because the containers in the run pods run as the Spacelift (UID 1983)\n      # user. Depending on the volume type in use, you may experience permission errors during\n      # runs if the fsGroup is not specified.\n      fsGroup: 1983\n\n    # The workspaceVolume property is used to specify the volume to use for the run's workspace.\n    workspaceVolume:\n      name: workspace\n      ephemeral:\n        volumeClaimTemplate:\n          spec:\n            accessModes:\n            - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n            storageClassName: gp2\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#binaries-cache-volume","title":"Binaries Cache Volume","text":"<p>The binaries cache volume is used to cache binaries (e.g. <code>terraform</code> and <code>kubectl</code>) across multiple runs. You can use an ephemeral volume for the binaries cache like with the workspace volume, but doing so will not result in any caching benefits. To be able to share the binaries cache with multiple run pods, you need to use a volume type that supports <code>ReadWriteMany</code>, for example AWS EFS.</p> <p>To configure the binaries cache volume, you can use exactly the same approach as with the workspace volume, the only difference is that you should use the <code>spec.pod.binariesCacheVolume</code> property instead of <code>spec.pod.workspaceVolume</code>.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#custom-volumes","title":"Custom Volumes","text":"<p>See the section on configuration for more details on how to configure these two volumes along with any additional volumes you require.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#configuration","title":"Configuration","text":"<p>The following example shows all the configurable options for a WorkerPool:</p> <pre><code>apiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  # name defines the name of the pool in Kubernetes - does not need to match the name in Spacelift.\n  name: test-workerpool\nspec:\n  # poolSize specifies the current number of Workers that belong to the pool.\n  # Optional, defaults to 1 if not provided.\n  poolSize: 2\n\n  # token points at a Kubernetes Secret key containing the worker pool token.\n  # Required\n  token:\n    secretKeyRef:\n      name: test-workerpool\n      key: token\n\n  # privateKey points at a Kubernetes Secret key containing the worker pool private key.\n  # Required\n  privateKey:\n    secretKeyRef:\n      name: test-workerpool\n      key: privateKey\n\n  # allowedRunnerImageHosts defines the hostnames of registries that are valid to use stack\n  # runner images from. If no specified images from any registries are allowed.\n  # Optional\n  allowedRunnerImageHosts:\n    - docker.io\n    - some.private.registry\n\n  # Pod history management configuration\n  # These settings control how many completed pods are retained and for how long\n\n  # successfulPodsHistoryLimit specifies the number of successful Pods to keep for inspection purposes.\n  # When set to a positive number, only the N most recent successful Pods are kept per worker.\n  # When set to 0, all successful Pods are removed immediately.\n  # When unset and successfulPodsHistoryTTL is also unset, defaults to 0 (remove all).\n  # When unset but successfulPodsHistoryTTL is set, count-based cleanup is disabled (TTL-only).\n  # Optional\n  successfulPodsHistoryLimit: 0\n\n  # failedPodsHistoryLimit specifies the number of failed Pods to keep for debugging purposes.\n  # When set to a positive number, only the N most recent failed Pods are kept per worker.\n  # When set to 0, all failed Pods are removed immediately.\n  # When unset and failedPodsHistoryTTL is also unset, defaults to 5 (keep 5 most recent).\n  # When unset but failedPodsHistoryTTL is set, count-based cleanup is disabled (TTL-only).\n  # Optional\n  failedPodsHistoryLimit: 5\n\n  # successfulPodsHistoryTTL specifies the duration to keep successful Pods after they are created.\n  # When set, successful Pods that have been created for longer than this duration are removed.\n  # The TTL timer starts from Pod creation time for consistency with history limit ordering.\n  # Running pods are never affected by this TTL.\n  # When unset (nil), no time-based cleanup is performed for successful Pods.\n  # This works in combination with successfulPodsHistoryLimit - pods are removed if they exceed EITHER limit.\n  # Optional\n  successfulPodsHistoryTTL: \"24h\"\n\n  # failedPodsHistoryTTL specifies the duration to keep failed Pods after they are created.\n  # When set, failed Pods that have been created for longer than this duration are removed.\n  # The TTL timer starts from Pod creation time for consistency with history limit ordering.\n  # Running pods are never affected by this TTL.\n  # When unset (nil), no time-based cleanup is performed for failed Pods.\n  # This works in combination with failedPodsHistoryLimit - pods are removed if they exceed EITHER limit.\n  # Optional\n  failedPodsHistoryTTL: \"72h\"\n\n  # pod contains the spec of Pods that will be created to process Spacelift runs. This allows\n  # you to set things like custom resource requests and limits, volumes, and service accounts.\n  # Most of these settings are just standard Kubernetes Pod settings and are not explicitly\n  # explained below unless they are particularly important or link directly to a Spacelift\n  # concept.\n  # Optional\n  pod:\n    # activeDeadlineSeconds defines the length of time in seconds before which the Pod will\n    # be marked as failed. This can be used to set a deadline for your runs. The default is\n    # 70 minutes.\n    activeDeadlineSeconds: 4200\n\n    terminationGracePeriodSeconds: 30\n\n    # volumes allows additional volumes to be attached to the run Pod. This is an array of\n    # standard Kubernetes volume definitions.\n    volumes: []\n\n    # binariesCacheVolume is a special volume used to cache binaries like tool downloads (e.g.\n    # terraform, kubectl, etc). These binaries can be reused by multiple runs, and potentially\n    # by multiple workers in your pool. To support this you need to use a volume type that\n    # can be read and written to by multiple Pods at the same time.\n    # It's always mounted in the same path: /opt/spacelift/binaries_cache\n    binariesCacheVolume: null\n\n    # workspaceVolume Special volume shared between init containers and the worker container.\n    # Used to populate the workspace with the repository content.\n    # It's always mounted in the same path: /opt/spacelift/workspace\n    # IMPORTANT: when using a custom value for this volume bear in mind that data stored in it is sensitive.\n    # We recommend that you make sure this volume is ephemeral and is not shared with other pods.\n    workspaceVolume: null\n\n    # DefaultAnsibleRunnerImage overrides the default runner image for Ansible runs.\n    # When set, this image will be used instead of the backend-provided runner image\n    # for Ansible stacks.\n    # Default: public.ecr.aws/spacelift/runner-ansible:latest\n    defaultAnsibleRunnerImage: \"my-custom-ansible-image:latest\"\n\n\n    # DefaultRunnerImage overrides the default runner image for non-Ansible runs.\n    # When set, this image will be used instead of the backend-provided runner image\n    # for Terraform and other non-Ansible stacks.\n    # Default: public.ecr.aws/spacelift/runner-terraform:latest\n    defaultRunnerImage: \"my-custom-image:latest\"\n\n    serviceAccountName: \"custom-service-account\"\n    automountServiceAccountToken: true\n    securityContext: {}\n    imagePullSecrets: []\n    nodeSelector: {}\n    nodeName: \"\"\n    affinity: {}\n    schedulerName: \"\"\n    tolerations: []\n    hostAliases: []\n    dnsConfig: {}\n    runtimeClassName: \"\"\n    topologySpreadConstraints: []\n    labels: {}\n    annotations: {}\n\n    # customBinariesPath allows you to add additional directories to the start of the path used\n    # by the worker. This allows you to do things like use a custom tool version provided on the\n    # runner image instead of the version downloaded by Spacelift.\n    customBinariesPath: \"\"\n\n    # customInitContainers allow you to define a list of custom init containers to be run before the builtin init one.\n    customInitContainers: []\n\n    # launcherImage allows you to customize the container image used by the init and gRPC server\n    # containers. NOTE that by default the correct image is sent through to the controller\n    # from the Spacelift backend, ensuring that the image used is compatible with the current\n    # version of Spacelift.\n    #\n    # You can use this setting if you want to use an image stored in a container registry that\n    # you control, but please note that doing so may cause incompatibilities between run containers\n    # and the Spacelift backend, and we do not recommend this.\n    launcherImage: \"\"\n\n    # initContainer defines the configuration for the container responsible for preparing the\n    # workspace for the worker. This includes downloading source code, performing role assumption,\n    # and ensuring that the correct tools are available for your stack amongst other things.\n    # The container name is \"init\".\n    initContainer:\n      envFrom: []\n      env: []\n      volumeMounts: []\n      resources:\n        requests:\n          # Standard resource requests\n        limits:\n          # Standard request limits\n        claims: []\n      # SecurityContext defines the security options the container should be run with.\n      # \u26a0\ufe0f Overriding this field may cause unexpected behaviors and should be avoided as much as possible.\n      # The operator is configured to run in a least-privileged context using UID/GID 1983. Running it as root may\n      # lead to unexpected behavior. Use at your own risk.\n      securityContext: {}\n\n    # grpcServerContainer defines the configuration for the side-car container used by the\n    # worker container for certain actions like uploading the current workspace, and being\n    # notified of stop requests.\n    # The container name is \"launcher-grpc\".\n    grpcServerContainer:\n      envFrom: []\n      env: []\n      volumeMounts: []\n      resources:\n        requests:\n          # Standard resource requests\n        limits:\n          # Standard request limits\n        claims: []\n      # SecurityContext defines the security options the container should be run with.\n      # \u26a0\ufe0f Overriding this field may cause unexpected behaviors and should be avoided as much as possible.\n      # The operator is configured to run in a least-privileged context using UID/GID 1983. Running it as root may\n      # lead to unexpected behavior. Use at your own risk.\n      securityContext: {}\n\n    # workerContainer defines the configuration for the container that processes the workflow\n    # for your run. This container uses the runner image defined by your stack.\n    workerContainer:\n      envFrom: []\n      env: []\n      volumeMounts: []\n      resources:\n        requests:\n          # Standard resource requests\n        limits:\n          # Standard request limits\n        claims: []\n      # SecurityContext defines the security options the container should be run with.\n      # \u26a0\ufe0f Overriding this field may cause unexpected behaviors and should be avoided as much as possible.\n      # The operator is configured to run in a least-privileged context using UID/GID 1983. Running it as root may\n      # lead to unexpected behavior. Use at your own risk.\n      securityContext: {}\n\n    # additionalSidecarContainers allows you to add any custom container to the pod.\n    # If an additional container is running a long-running process like a database or a daemon,\n    # it will be terminated when the spacelift run succeed.\n    additionalSidecarContainers:\n      # Every entry of this array needs to follow the kubernetes container spec.\n      - name: redis\n        image: redis\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#pod-history-management","title":"Pod History Management","text":"<p>The Kubernetes operator provides flexible pod history management to control how long completed run pods are retained. This allows you to balance between debugging capabilities and resource usage.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#overview","title":"Overview","text":"<p>The pod history management system supports both count-based and time-based cleanup strategies that work together:</p> <ul> <li>Count-based limits: Control how many completed pods to keep per worker</li> <li>Time-based cleanup (TTL): Automatically remove pods after a specified duration</li> <li>Combined strategy: Pods are removed when they exceed either the count limit or the time limit</li> </ul>"},{"location":"concepts/worker-pools/kubernetes-workers.html#default-behavior","title":"Default Behavior","text":"<ul> <li>Successful pods: Removed immediately (limit: 0)</li> <li>Failed pods: Keep 5 most recent (limit: 5)</li> <li>Time limits: No automatic TTL cleanup (unless explicitly configured)</li> </ul>"},{"location":"concepts/worker-pools/kubernetes-workers.html#configuration-options","title":"Configuration Options","text":""},{"location":"concepts/worker-pools/kubernetes-workers.html#count-based-limits","title":"Count-Based Limits","text":"<pre><code>spec:\n  # Keep 3 most recent successful pods per individual worker\n  successfulPodsHistoryLimit: 3\n\n  # Keep 10 most recent failed pods per individual worker\n  failedPodsHistoryLimit: 10\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#time-based-cleanup-ttl","title":"Time-Based Cleanup (TTL)","text":"<pre><code>spec:\n  # Remove successful pods older than 24 hours\n  successfulPodsHistoryTTL: \"24h\"\n\n  # Remove failed pods older than 72 hours\n  failedPodsHistoryTTL: \"72h\"\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#special-modes","title":"Special Modes","text":"<p>Delete-All Mode: Set limit to 0 to remove all pods immediately</p> <pre><code>spec:\n  successfulPodsHistoryLimit: 0  # Remove all successful pods immediately\n  failedPodsHistoryLimit: 0      # Remove all failed pods immediately\n</code></pre> <p>TTL-Only Mode: Set TTL without limit for time-based cleanup only</p> <pre><code>spec:\n  # Only time-based cleanup, no count limits\n  successfulPodsHistoryTTL: \"48h\"\n  # successfulPodsHistoryLimit is intentionally unset\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#behavior-details","title":"Behavior Details","text":"<ul> <li>Pod selection: Uses creation time for consistent ordering (oldest removed first)</li> <li>Running pods: Never affected by cleanup, only applies to completed pods</li> <li>WorkerPool scope: Cleanup is managed centrally at the WorkerPool level across all workers in the pool</li> <li>Deletion safety: Pods already being deleted are excluded from counts</li> </ul> <p>Migration from keepSuccessfulPods</p> <p>The <code>keepSuccessfulPods</code> field has been deprecated since controller version v0.0.25 and Helm chart version 0.46.0, and has been removed in favor of the new pod history management system. If you previously used <code>keepSuccessfulPods: true</code>, set <code>successfulPodsHistoryLimit</code> to a positive value instead.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#configure-a-docker-daemon-as-a-sidecar-container","title":"Configure a docker daemon as a sidecar container","text":"<p>If for some reason you need to have a docker daemon running as a sidecar, you can follow the example below.</p> <pre><code>apiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  name: test-workerpool\nspec:\n  poolSize: 2\n  pod:\n    workerContainer:\n      env:\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n    additionalSidecarContainers:\n      - image: docker:dind\n        name: docker\n        securityContext:\n          privileged: true\n        command:\n          - docker-init\n          - \"--\"\n          - dockerd\n          - \"--host\"\n          - tcp://127.0.0.1:2375\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#timeouts","title":"Timeouts","text":"<p>There are two types of timeouts that you can set</p> <ul> <li>The run timeout: this causes the run to fail if its duration exceeds a defined duration.</li> <li>The log output timeout: this causes the run to fail if no logs has been generated for a defined duration.</li> </ul> <p>To configure the run timeout you need to configure two items - the <code>activeDeadlineSeconds</code> for the Pod, as well as the <code>SPACELIFT_LAUNCHER_RUN_TIMEOUT</code> for the worker container:</p> <pre><code>apiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  name: test-workerpool\nspec:\n  pod:\n    activeDeadlineSeconds: 3600\n    workerContainer:\n      env:\n        - name: SPACELIFT_LAUNCHER_RUN_TIMEOUT\n          value: 3600s # This is using the golang duration format, more info here https://pkg.go.dev/time#ParseDuration\n</code></pre> <p>To configure the logs timeout you just need to add a single environment variable to the worker container:</p> <pre><code>apiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  name: test-workerpool\nspec:\n  pod:\n    workerContainer:\n      env:\n        - name: SPACELIFT_LAUNCHER_LOGS_TIMEOUT\n          value: 3600s # This is using the golang duration format, more info here https://pkg.go.dev/time#ParseDuration\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#network-configuration","title":"Network Configuration","text":"<p>Your cluster configuration needs to be set up to allow the controller and the scheduled pods to reach the internet. This is required to listen for new jobs from the Spacelift backend and report back status and run logs.</p> <p>You can find the necessary endpoints to allow in the Network Security section.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#initialization-policies","title":"Initialization Policies","text":"<p>Using an initialization policy is simple and requires three steps:</p> <ul> <li>Create a <code>ConfigMap</code> containing your policy.</li> <li>Attach the <code>ConfigMap</code> as a volume in the <code>pod</code> specification for your pool.</li> <li>Add an environment variable to the init container, telling it where to read the policy from.</li> </ul> <p>First, create your policy:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: test-workerpool-initialization-policy\ndata:\n  initialization-policy.rego: |\n    package spacelift\n\n    deny[\"you shall not pass\"] {\n        false\n    }\n</code></pre> <p>Next, create a <code>WorkerPool</code> definition, configuring the <code>ConfigMap</code> as a volume, and setting the custom env var:</p> <pre><code>apiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  labels:\n    app.kubernetes.io/name: test-workerpool\n  name: test-workerpool\nspec:\n  poolSize: 2\n  token:\n    secretKeyRef:\n      name: test-workerpool\n      key: token\n  privateKey:\n    secretKeyRef:\n      name: test-workerpool\n      key: privateKey\n  pod:\n    volumes:\n      # Here's where you attach the policy to the Pod as a volume\n      - name: initialization-policy\n        configMap:\n          name: test-workerpool-initialization-policy\n    initContainer:\n      volumeMounts:\n        # Here's where you mount it into the init container\n        - name: initialization-policy\n          mountPath: \"/opt/spacelift/policies/initialization\"\n          readOnly: true\n      env:\n        # And here's where you specify the path to the policy\n        - name: \"SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY\"\n          value: \"/opt/spacelift/policies/initialization/initialization-policy.rego\"\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#using-vcs-agents-with-kubernetes-workers","title":"Using VCS Agents with Kubernetes Workers","text":"<p>Using VCS Agents with Kubernetes workers is simple, and uses exactly the same approach outlined in the VCS Agents section. To configure your VCS Agent environment variables in a Kubernetes WorkerPool, add them to the <code>spec.pod.initContainer.env</code> section, like in the following example:</p> <pre><code>apiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  name: test-pool\nspec:\n  poolSize: 2\n  token:\n    secretKeyRef:\n      name: test-pool\n      key: token\n  privateKey:\n    secretKeyRef:\n      name: test-pool\n      key: privateKey\n  pod:\n    initContainer:\n      env:\n        - name: \"SPACELIFT_PRIVATEVCS_MAPPING_NAME_0\"\n          value: \"gitlab-pool\"\n        - name: \"SPACELIFT_PRIVATEVCS_MAPPING_BASE_ENDPOINT_0\"\n          value: \"https://gitlab.myorg.com\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#controller-metrics","title":"Controller metrics","text":"<p>The workerpool controller does not expose any metrics by default.</p> <p>You can set <code>--metrics-bind-address=:8443</code> flag to enable them and activate the Prometheus endpoint. By default, the controller exposes metrics using HTTPS and a self-signed certificate. This endpoint is also protected using RBAC. If you use the helm chart to deploy the controller, you can use the built-in metrics reader role to grant access.</p> <p>You may also want to use a valid certificate for production workloads. You can mount your cert in the container to the following paths:</p> <pre><code>/tmp/k8s-metrics-server/serving-certs/tls.crt\n/tmp/k8s-metrics-server/serving-certs/tls.key\n</code></pre> <p>It's also possible to fully disable TLS on the metrics endpoint and ask the controller to export metrics using http. You need to set --metrics-secure=false flag for that.</p> <p>More information about metrics authentication and TLS config can be found on the kubebuilder docs.</p> <p>More information about exposed metrics can be found by scrapping the metrics endpoint, see and example below</p> <pre><code># HELP spacelift_workerpool_controller_worker_creation_duration_seconds Time in seconds needed to create a new worker\n# TYPE spacelift_workerpool_controller_worker_creation_duration_seconds histogram\nspacelift_workerpool_controller_worker_creation_duration_seconds_bucket{le=\"0.5\"} 0\nspacelift_workerpool_controller_worker_creation_duration_seconds_bucket{le=\"1\"} 0\nspacelift_workerpool_controller_worker_creation_duration_seconds_bucket{le=\"2\"} 0\nspacelift_workerpool_controller_worker_creation_duration_seconds_bucket{le=\"4\"} 0\nspacelift_workerpool_controller_worker_creation_duration_seconds_bucket{le=\"10\"} 0\nspacelift_workerpool_controller_worker_creation_duration_seconds_bucket{le=\"+Inf\"} 0\nspacelift_workerpool_controller_worker_creation_duration_seconds_sum 0\nspacelift_workerpool_controller_worker_creation_duration_seconds_count 0\n# HELP spacelift_workerpool_controller_worker_creation_errors_total Total number of worker creation errors\n# TYPE spacelift_workerpool_controller_worker_creation_errors_total counter\nspacelift_workerpool_controller_worker_creation_errors_total 0\n# HELP spacelift_workerpool_controller_worker_idle_total Number of idle worker\n# TYPE spacelift_workerpool_controller_worker_idle_total gauge\nspacelift_workerpool_controller_worker_idle_total{pool_ulid=\"01JHFXXPDC6J8XM2VB0M9CS338\"} 0\n# HELP spacelift_workerpool_controller_worker_total Total number of workers\n# TYPE spacelift_workerpool_controller_worker_total gauge\nspacelift_workerpool_controller_worker_total{pool_ulid=\"01JHFXXPDC6J8XM2VB0M9CS338\"} 2\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#helm","title":"Helm","text":"<p>If you are using our Helm chart to deploy the controller, you can configure metrics by switching some boolean flags in <code>values.yml</code>.</p> <p>You can check the links in the comments below about how to secure your metrics endpoint.</p> <pre><code># The metric service will expose a metrics endpoint that can be scraped by a prometheus instance.\n# This is disabled by default, enable this if you want to enable controller observability.\nmetricsService:\n  enabled: false\n  # Enabling secure will also create ClusterRole to enable authn/authz to the metrics endpoint through RBAC.\n  # More details here https://book.kubebuilder.io/reference/metrics#by-using-authnauthz-enabled-by-default\n  # Secure is enabled by default to be consistent with Kubebuilder defaults.\n  #\n  # If you want to avoid cluster roles, you can keep this set to false and configure a NetworkPolicu instead.\n  # An example can be found in Kubebuilder docs here https://github.com/kubernetes-sigs/kubebuilder/blob/d063d5af162a772379a761fae5aaea8c91b877d4/docs/book/src/getting-started/testdata/project/config/network-policy/allow-metrics-traffic.yaml#L2\n  secure: true\n  enableHTTP2: false\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#custom-binaries-path","title":"Custom binaries path","text":"<p>Kubernetes workers download the <code>spacelift-worker</code> binary, along with any tools needed for your runs and mount them into a directory called <code>/opt/spacelift/binaries</code> in the worker. To ensure that these tools are used, this directory is added to the start of the worker's path.</p> <p>In some situations you may wish to use your own version of tools that are bundled with the runner image used for your stack. To support this, we provide a <code>spec.pod.customBinariesPath</code> option to allow you to customize this.</p> <p>The following example shows how to configure this:</p> <pre><code>apiVersion: workers.spacelift.io/v1beta1\nkind: WorkerPool\nmetadata:\n  name: test-workerpool\nspec:\n  poolSize: 2\n  token:\n    secretKeyRef:\n      name: test-workerpool\n      key: token\n  privateKey:\n    secretKeyRef:\n      name: test-workerpool\n      key: privateKey\n  pod:\n    customBinariesPath: \"/bin\" # This will result in \"/bin:/opt/spacelift/binaries\" being added to the start of the worker's path.\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#autoscaling","title":"Autoscaling","text":"<p>Careful consideration should be given when scheduling the controller in the cluster. If the worker pool controller is evicted due to autoscaling or other reasons, it may miss MQTT messages and cause temporary run failures.</p> <p>Therefore, it is strongly recommended to deploy the controller on nodes with high stability and availability.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#eks","title":"EKS","text":"<p>For EKS Auto cluster you can set the following Karpenter annotation on the controller pod.</p> KubectlHelm <pre><code>karpenter.sh/do-not-disrupt: \"true\"\n</code></pre> <pre><code>--set-string controllerManager.podAnnotations.\"karpenter\\.sh/do-not-disrupt\"=\"true\"\n</code></pre> <p>For Standard clusters:</p> KubectlHelm <pre><code>cluster-autoscaler.kubernetes.io/safe-to-evict: \"false\"\n</code></pre> <pre><code>--set-string controllerManager.podAnnotations.\"cluster-autoscaler\\.kubernetes\\.io/safe-to-evict\"=\"false\"\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#gke","title":"GKE","text":"<p>For autopilot cluster you can set the following annotation on the controller pod.</p> KubectlHelm <pre><code># Bear in mind that this will not 100% prevent autopilot from evicting pods.\n# Please refer to autopilot documentation for more details.\nautopilot.gke.io/priority: high\n</code></pre> <pre><code>--set-string controllerManager.podAnnotations.\"autopilot\\.gke\\.io/priority\"=\"high\"\n</code></pre> <p>For Standard clusters:</p> KubectlHelm <pre><code>cluster-autoscaler.kubernetes.io/safe-to-evict: \"false\"\n</code></pre> <pre><code>--set-string controllerManager.podAnnotations.\"cluster-autoscaler\\.kubernetes\\.io/safe-to-evict\"=\"false\"\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#aks","title":"AKS","text":"<p>For Azure cluster you can set the following annotation on the controller pod.</p> KubectlHelm <pre><code>cluster-autoscaler.kubernetes.io/safe-to-evict: \"false\"\n</code></pre> <pre><code>--set-string controllerManager.podAnnotations.\"cluster-autoscaler\\.kubernetes\\.io/safe-to-evict\"=\"false\"\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#fips","title":"FIPS","text":"<p>With Go 1.24, the Go runtime has added support for FIPS mode. This allows you to run your Spacelift workerpool-controller in a FIPS 140-3-compliant manner.</p> <p>Note</p> <p>Note that the above Go documentation mentions that FIPS mode is best effort based and doesn't guarantee compliance with all requirements.</p> <p>If you'd like to have the workerpool-controller run in FIPS mode, turn the <code>controllerManager.enforceFips140</code> flag to <code>true</code> in the Helm chart values. We introduced this in the v0.42.0 release of the Helm chart.</p> HelmKubernetes <p>You can set this in your <code>values.yaml</code> file like so: </p><pre><code>controllerManager:\n  enforceFips140: true\n</code></pre><p></p> <p>Or pass it as a parameter: <code>--set controllerManager.enforceFips140=true</code>.</p> <p>When deployed without helm, you'll need to set the <code>GODEBUG=fips140=only</code> environment variable manually on the controller container. The command to do this is:</p> <pre><code># List all deployments to get the name of the controller deployment:\nkubectl get deployments --all-namespaces\n\n# Edit the deployment to add the environment variable:\nkubectl edit deployment/&lt;deployment-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>During the controller's startup, you should see the <code>FIPS 140 mode {\"enabled\": true}</code> message in the logs.</p> <p>Note</p> <p>This will only make the controller run in FIPS mode. The Spacelift worker pods are not affected by this setting - they are not compliant with FIPS 140-3 yet.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#scaling-a-pool","title":"Scaling a pool","text":"<p>To scale your WorkerPool, you can either edit the resource in Kubernetes, or use the <code>kubectl scale</code> command:</p> <pre><code>kubectl scale workerpools my-worker-pool --replicas=5\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#billing-for-kubernetes-workers","title":"Billing for Kubernetes Workers","text":"<p>Kubernetes workers are billed based on the number of provisioned workers that you have, exactly the same as for any of our other ways of running workers. What this means in practice is that you will be billed based on the number of workers defined by the <code>poolSize</code> of your WorkerPool, even when those workers are idle and not processing any runs.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#migrating-from-docker-in-docker","title":"Migrating from Docker-in-Docker","text":"<p>If you currently use our Docker-in-Docker Helm chart to run your worker pools, we recommend that you switch to our worker pool operator. For full details of how to install the operator and setup a worker pool, please see the installation section.</p> <p>The rest of this section provides useful information to be aware of when switching over from the Docker-in-Docker approach to the operator.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#why-migrate","title":"Why migrate","text":"<p>There are a number of improvements with the Kubernetes operator over the previous Docker-in-Docker approach, including:</p> <ul> <li>The operator does not require privileged pods unlike the Docker-in-Docker approach.</li> <li>The operator creates standard Kubernetes pods to handle runs. This provides advantages including Kubernetes being aware of the run workloads that are executing as well as the ability to use built-in Kubernetes functionality like service accounts and affinity.</li> <li>The operator only creates pods when runs are scheduled. This means that while your workers are idle, they are not running pods that are using up resources in your cluster.</li> <li>The operator can safely handle scaling down the number of workers in a pool while making sure that in-progress runs are not killed.</li> </ul>"},{"location":"concepts/worker-pools/kubernetes-workers.html#deploying-workers","title":"Deploying workers","text":"<p>One major difference between the Docker-in-Docker Helm chart and the new operator is that the new chart only deploys the operator, and not any workers. To deploy workers you need to create WorkerPool resources after the operator has been deployed. See the section on creating a worker pool for more details.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#testing-both-alongside-each-other","title":"Testing both alongside each other","text":"<p>You can run both the new operator as well as your existing Docker-in-Docker workers. In fact you can even connect both to the same Spacelift worker pool. This allows you to test the operator to make sure everything is working before switching over.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#customizing-timeouts","title":"Customizing timeouts","text":"<p>If you are currently using <code>SPACELIFT_LAUNCHER_RUN_TIMEOUT</code> or <code>SPACELIFT_LAUNCHER_LOGS_TIMEOUT</code>, please see the section on timeouts to find out how to achieve this with the operator.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#storage-configuration","title":"Storage configuration","text":"<p>If you are using custom storage volumes, you can configure these via the <code>spec.pod</code> section of the WorkerPool resource. Please see the section on volumes for more information.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#pool-size","title":"Pool size","text":"<p>In the Docker-in-Docker approach, the number of workers is controlled by the <code>replicaCount</code> value of the Chart which controls the number of replicas in the Deployment. In the operator approach, the pool size is configured by the <code>spec.poolSize</code> property. Please see the section on scaling for information about how to scale your pool up or down.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/worker-pools/kubernetes-workers.html#listing-workerpools-and-workers","title":"Listing WorkerPools and Workers","text":"<p>To list all of your WorkerPools, you can use the following command:</p> <pre><code>kubectl get workerpools\n</code></pre> <p>To list all of your Workers, use the following command:</p> <pre><code>kubectl get workers\n</code></pre> <p>To list the Workers for a specific pool, use the following command (replace <code>&lt;worker-pool-id&gt;</code> with the ID of the pool from Spacelift):</p> <pre><code>kubectl get workers -l \"workers.spacelift.io/workerpool=&lt;worker-pool-id&gt;\"\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#listing-run-pods","title":"Listing run pods","text":"<p>When a run is scheduled, a new pod is created to process that run. It's important to note that a single worker can only process a single run at a time, making it easy to find pods by run or worker IDs.</p> <p>To list the pod for a specific run, use the following command (replacing <code>&lt;run-id&gt;</code> with the ID of the run):</p> <pre><code>kubectl get pods -l \"workers.spacelift.io/run-id=&lt;run-id&gt;\"\n</code></pre> <p>To find the pod for a particular worker, use the following command (replacing <code>&lt;worker-id&gt;</code> with the ID of the worker):</p> <pre><code>kubectl get pods -l \"workers.spacelift.io/worker=&lt;worker-id&gt;\"\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#workers-not-connecting-to-spacelift","title":"Workers not connecting to Spacelift","text":"<p>If you have created a WorkerPool in Kubernetes but no workers have shown up in Spacelift, use <code>kubectl get workerpools</code> to view your pool:</p> <pre><code>kubectl get workerpools\nNAME         DESIRED POOL SIZE   ACTUAL POOL SIZE\nlocal-pool   2\n</code></pre> <p>If the actual pool size for your pool is not populated, it typically indicates an issue with your pool credentials. The first thing to do is to use <code>kubectl describe</code> to inspect your pool and check for any events indicating errors:</p> <pre><code>kubectl describe workerpool local-pool\nName:         local-pool\nNamespace:    default\nLabels:       app.kubernetes.io/name=local-pool\n              workers.spacelift.io/ulid=01HPS9HDSWCQ73RPDTVAK0KK0A\nAnnotations:  &lt;none&gt;\nAPI Version:  workers.spacelift.io/v1beta1\nKind:         WorkerPool\n\n...\n\nEvents:\n  Type     Reason                    Age              From                   Message\n  ----     ------                    ----             ----                   -------\n  Warning  WorkerPoolCannotRegister  7s (x2 over 7s)  workerpool-controller  Unable to register worker pool: cannot retrieve workerpool token: unable to base64 decode privateKey: illegal base64 data at input byte 4364\n</code></pre> <p>In the example above, we can see that the private key for the pool is invalid.</p> <p>If the WorkerPool events don't provide any useful information, another option is to take a look at the logs for the controller pod using <code>kubectl logs</code>, for example:</p> <pre><code>kubectl logs -n spacelift-worker-controller-system spacelift-workerpool-controller-controller-manager-bd9bcb46fjdt\n</code></pre> <p>For example, if your token is invalid, you may find a log entry similar to the following:</p> <pre><code>cannot retrieve workerpool token: unable to base64 decode token: illegal base64 data at input byte 2580\n</code></pre> <p>Another common reason that can cause workers to fail to connect with Spacelift is network or firewall rules blocking connections to AWS IoT Core. Please see our network security section for more details on the networking requirements for workers.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#run-not-starting","title":"Run not starting","text":"<p>If a run is scheduled to a worker but it gets stuck in the preparing phase for a long time, it may be caused by various issues like CPU or memory limits that are too low, or not being able to pull the stack's runner image. The best option in this scenario is to find the run pod and describe it to find out what's happening.</p> <p>For example, in the following scenario, we can use <code>kubectl get pods</code> to discover that the run pod is stuck in <code>ImagePullBackOff</code>, meaning that it is unable to pull one of its container images:</p> <pre><code>$ kubectl get pods -l \"workers.spacelift.io/run-id=01HPS6XB76J1JB3EHSK4AWE5AB\"\nNAME                                     READY   STATUS             RESTARTS   AGE\n01hps6xb76j1jb3ehsk4awe5ab-preparing-2   1/2     ImagePullBackOff   0          3m2s\n</code></pre> <p>If we describe that pod, we can get more details about the failure:</p> <pre><code>$ kubectl describe pods -l \"workers.spacelift.io/run-id=01HPS6XB76J1JB3EHSK4AWE5AB\"\nName:             01hps6xb76j1jb3ehsk4awe5ab-preparing-2\nNamespace:        default\nPriority:         0\nService Account:  default\nNode:             kind-control-plane/172.18.0.2\nStart Time:       Fri, 16 Feb 2024 15:00:18 +0000\nLabels:           workers.spacelift.io/run-id=01HPS6XB76J1JB3EHSK4AWE5AB\n                  workers.spacelift.io/worker=01HPS6K4BNB7BPHCDHDWFAMJNV\n\n...\n\nEvents:\n  Type     Reason     Age                    From               Message\n  ----     ------     ----                   ----               -------\n  Normal   Scheduled  4m23s                  default-scheduler  Successfully assigned default/01hps6xb76j1jb3ehsk4awe5ab-preparing-2 to kind-control-plane\n  Normal   Pulled     4m23s                  kubelet            Container image \"public.ecr.aws/spacelift/launcher:d0a81de1085a7cc4f4561a776ab74a43d4497f6c\" already present on machine\n  Normal   Created    4m23s                  kubelet            Created container init\n  Normal   Started    4m23s                  kubelet            Started container init\n  Normal   Pulled     4m15s                  kubelet            Container image \"public.ecr.aws/spacelift/launcher:d0a81de1085a7cc4f4561a776ab74a43d4497f6c\" already present on machine\n  Normal   Created    4m15s                  kubelet            Created container launcher-grpc\n  Normal   Started    4m15s                  kubelet            Started container launcher-grpc\n  Normal   Pulling    3m36s (x3 over 4m15s)  kubelet            Pulling image \"someone/non-existent-image:1234\"\n  Warning  Failed     3m35s (x3 over 4m14s)  kubelet            Failed to pull image \"someone/non-existent-image:1234\": rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/someone/non-existent-image:1234\": failed to resolve reference \"docker.io/someone/non-existent-image:1234\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed\n  Warning  Failed     3m35s (x3 over 4m14s)  kubelet            Error: ErrImagePull\n  Normal   BackOff    2m57s (x5 over 4m13s)  kubelet            Back-off pulling image \"someone/non-existent-image:1234\"\n  Warning  Failed     2m57s (x5 over 4m13s)  kubelet            Error: ImagePullBackOff\n</code></pre> <p>In this case, we can see that the problem is that the <code>someone/non-existent-image:1234</code> container image cannot be pulled, meaning that the run can't start. In this situation the fix would be to add the correct authentication to allow your Kubernetes cluster to pull the image, or to adjust your stack settings to refer to the correct image if it is wrong.</p> <p>Similarly, if you specify too low memory limits for one of the containers in the run pod Kubernetes may end up killing it. You can find this out in exactly the same way:</p> <pre><code>$ kubectl get pods -l \"workers.spacelift.io/run-id=01HPS85J6SRG37DG6FGNRZGHMM\"\nNAME                                     READY   STATUS           RESTARTS   AGE\n01hps85j6srg37dg6fgnrzghmm-preparing-2   0/2     Init:OOMKilled   0          24s\n\n$ kubectl describe pods -l \"workers.spacelift.io/run-id=01HPS85J6SRG37DG6FGNRZGHMM\"\nName:             01hps85j6srg37dg6fgnrzghmm-preparing-2\nNamespace:        default\nPriority:         0\nService Account:  default\nNode:             kind-control-plane/172.18.0.2\nStart Time:       Fri, 16 Feb 2024 15:22:17 +0000\nLabels:           workers.spacelift.io/run-id=01HPS85J6SRG37DG6FGNRZGHMM\n                  workers.spacelift.io/worker=01HPS7FRV3JJWWVJ1P9RQ7JN2N\nAnnotations:      &lt;none&gt;\nStatus:           Failed\nIP:               10.244.0.14\nIPs:\n  IP:           10.244.0.14\nControlled By:  Worker/local-pool-01hps7frv3jjwwvj1p9rq7jn2n\nInit Containers:\n  init:\n    Container ID:  containerd://567f505a638e0b42e23d275a5a1b75f40ac6b706490ada9ea7901219b54e43c8\n    Image:         public.ecr.aws/spacelift-dev/launcher:2ff3b7ad1d532ca51b5b2c54ded40ad19669d379\n    Image ID:      public.ecr.aws/spacelift-dev/launcher@sha256:baa99ca405f5c42cc16b5e93b5faa9467c8431c048f814e9623bdfee0bef8c4d\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Command:\n      /usr/bin/spacelift-launcher\n    Args:\n      init\n    State:          Terminated\n      Reason:       OOMKilled\n      Exit Code:    137\n      Started:      Fri, 16 Feb 2024 15:22:17 +0000\n      Finished:     Fri, 16 Feb 2024 15:22:17 +0000\n\n...\n</code></pre>"},{"location":"concepts/worker-pools/kubernetes-workers.html#getting-help-with-run-issues","title":"Getting help with run issues","text":"<p>If you're having trouble understanding why a run isn't starting, is failing, or is hanging, and want to reach out for support, please include the output of the following commands (replacing the relevant IDs/names as well as specifying the namespace of your worker pool):</p> <ul> <li><code>kubectl get pods  --namespace &lt;worker-pool-namespace&gt; -l \"workers.spacelift.io/run-id=&lt;run-id&gt;\"</code></li> <li><code>kubectl describe pods --namespace &lt;worker-pool-namespace&gt; -l \"workers.spacelift.io/run-id=&lt;run-id&gt;\"</code></li> <li><code>kubectl logs --namespace &lt;worker-pool-namespace&gt; -l \"workers.spacelift.io/run-id=&lt;run-id&gt;\" --all-containers --prefix --timestamps</code></li> <li><code>kubectl events --namespace &lt;worker-pool-namespace&gt; workers/&lt;worker-name&gt; -o json</code></li> </ul> <p>Please also include your controller logs from 10 minutes before the run started. You can do this using the <code>--since-time</code> flag, like in the following example:</p> <ul> <li><code>kubectl logs -n spacelift-worker-controller-system spacelift-worker-controllercontroller-manager-6f974d9b6d-kx566 --since-time=\"2024-04-02T09:00:00Z\" --all-containers --prefix --timestamps</code></li> </ul>"},{"location":"concepts/worker-pools/kubernetes-workers.html#custom-runner-images","title":"Custom runner images","text":"<p>Please note that if you are using a custom runner image for your stack, it must include a Spacelift user with a UID of 1983. If your image does not include this user, it can cause permission issues during runs, for example while trying to write out configuration files while preparing the run.</p> <p>Please see our instructions on customizing the runner image for more information.</p>"},{"location":"concepts/worker-pools/kubernetes-workers.html#networking-issues-caused-by-pod-identity","title":"Networking issues caused by Pod identity","text":"<p>When a run is assigned to a worker, the controller creates a new Pod to process that run. The Pod has labels indicating the worker and run ID, and looks something like this:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    workers.spacelift.io/run-id: 01HN37WC3MCNE3CY9HAHWRF06K\n    workers.spacelift.io/worker: 01HN356WGGNGTXA8PHYRRKEEZ5\n  name: 01hn37wc3mcne3cy9hahwrf06k-preparing-2\n  namespace: default\nspec:\n  ... rest of the pod spec\n</code></pre> <p>Because the set of labels are unique for each run being processed, this can cause problems with systems like Cilium that use Pod labels to determine the identity of each Pod, leading to your runs having networking issues. If you are using a system like this, you may want to exclude the <code>workers.spacelift.io/*</code> labels from being used to determine network identity.</p>"},{"location":"faq.html","title":"\u270b FAQ","text":""},{"location":"faq.html#faq","title":"FAQ","text":"<p>Spacelift has many features and hidden nuggets so it is easy to overlook some of them but we have you covered with this list of frequently asked questions.</p> <p>If you still cannot find the answer to your question below, please reach out to our support team.</p>"},{"location":"faq.html#product","title":"Product","text":""},{"location":"faq.html#how-to-submit-a-feature-request","title":"How to submit a feature request?","text":"<p>You can submit a feature request by visiting our dashboard, where you can also review other feature requests. To proceed, you'll need to sign up in the tool. This allows us to ask follow-up questions and keep you informed about status changes.</p> <p>More details on the feature request process can be found here.</p>"},{"location":"faq.html#providing-admin-consent-for-microsoft-login","title":"Providing Admin consent for Microsoft login","text":"<p>In order to sign up for Spacelift using an Azure AD account via our Microsoft login option, the Spacelift application needs to be installed into your Azure AD directory. To do this you either need to be an Azure AD administrator, or your Azure AD configuration needs to allow users to install applications.</p> <p>If you don't have permission, you will receive the following message when attempting to sign up:</p> <p></p> <p>If this happens, it means that you need ask an Azure AD admin to provide Admin consent, as described in the Microsoft documentation.</p> <p>To do this, your Azure AD admin can use a URL like the following to grant permission to Spacelift:</p> <pre><code>https://login.microsoftonline.com/&lt;tenant-id&gt;/adminconsent?client_id=&lt;client-id&gt;\n</code></pre> <p>Info</p> <p>NOTE: make sure to replace <code>&lt;tenant-id&gt;</code> with your Azure AD Tenant ID!</p> <p>Info</p> <p>NOTE: ensure you replace <code>&lt;client-id&gt;</code> based on your environment.</p> <ul> <li>for spacelift.io, use <code>fba648b0-4b78-4224-b510-d96ff51eeef9</code>.</li> <li>for us.spacelift.io, use <code>823de581-43fe-42cd-8ace-a90c2ba36f01</code>.</li> </ul> <p>After granting admin consent, your administrator will be redirected to Spacelift and receive the following message:</p> <p></p> <p>After admin consent has been provided, you should be able to sign-up for Spacelift using your Microsoft account.</p>"},{"location":"faq.html#platforms","title":"Platforms","text":""},{"location":"faq.html#terraformopentofu","title":"Terraform/OpenTofu","text":""},{"location":"faq.html#how-do-i-import-the-state-for-my-stack","title":"How do I import the state for my stack?","text":"<p>The state file can be imported during the creation of a stack.</p>"},{"location":"faq.html#how-do-i-export-the-state-for-my-stack","title":"How do I export the state for my stack?","text":"<p>The state file can be pulled and then exported using a Task.</p> <p>For example, to export the state to an Amazon S3 bucket, you would run the following command or equivalent as a Task:</p> <pre><code>terraform state pull &gt; state.json &amp;&amp; aws s3 cp state.json s3://&lt;PATH&gt;\n</code></pre> <p>Warning</p> <p>For that example to work, the stack needs to have write access to the AWS S3 bucket, possibly via an AWS Integration.</p>"},{"location":"faq.html#how-do-i-switch-from-spacelift-managing-the-state-to-me-managing-it","title":"How do I switch from Spacelift managing the state to me managing it?","text":"<p>You would first need to export the state file to a suitable location.</p> <p>The state management setting can not be changed once a stack has been created so you will need to recreate the stack and make sure that the \"Manage state\" setting is disabled.</p>"},{"location":"faq.html#how-do-i-manipulate-the-state-file","title":"How do I manipulate the state file?","text":"<p>You can manipulate the state by running a command such as <code>terraform state &lt;SUBCOMMAND&gt;</code> commands in a Task.</p> <p>This applies whether you or Spacelift manages the state file.</p>"},{"location":"faq.html#how-do-i-import-existing-resources-into-a-stack","title":"How do I import existing resources into a stack?","text":"<p>Just run the <code>terraform import \u2026</code> or equivalent in a Task.</p> <p>This applies whether you or Spacelift manages the state file.</p>"},{"location":"faq.html#can-spacelift-modules-be-used-outside-of-spacelift","title":"Can Spacelift modules be used outside of Spacelift?","text":"<p>Yes, modules in the private registry can be used outside of Spacelift with proper credential management. More information can be found here.</p>"},{"location":"faq.html#can-i-trigger-a-run-when-theres-a-modules-update","title":"Can I trigger a run when theres a modules update?","text":"<p>Modules track the consumers of each of their versions. When a new module version is released, the consumers of the previously newest version are assumed to be potential consumers of the newly released one. Hence, the trigger policy for a module can be used to trigger a run on all of these stacks. More information can be found here.</p>"},{"location":"faq.html#policies","title":"Policies","text":""},{"location":"faq.html#my-policy-works-fine-in-the-workbench-but-not-on-my-stackmodule","title":"My policy works fine in the workbench but not on my stack/module","text":"<p>Except for the Login policies, all policies must be attached to stacks or modules to be evaluated so let's first confirm this by verifying that the stack or module is listed in the \"Used by\" section on the policy page. If it does not show up there, you will need to attach the policy.</p> <p>If your policy is attached to your stack/module and you still do not see the expected behavior from that policy, you should make sure that sampling is enabled for that policy, and then review the recorded samples in the Policy Workbench. That should give you valuable insight.</p> <p>If you do not see any sampled events despite sampling being enabled and having performed events that should have triggered events, make sure that the appropriate type was selected when the policy was created.</p>"},{"location":"faq.html#i-do-not-see-some-samples-for-my-login-policy","title":"I do not see some samples for my Login policy","text":"<p>Login policies are not evaluated for account creators and SSO admins who always get admin access to their respective Spacelift accounts. This is to avoid a situation where a bad Login policy locks out everyone from the account.</p> <p>The side-effect is that you will not see samples for these users.</p>"},{"location":"faq.html#are-approval-policies-and-run-confirmation-the-same-thing","title":"Are Approval policies and run confirmation the same thing?","text":"<p>Approval policies and run confirmation are related but different concepts.</p> <p>Just think about how GitHub's Pull Requests work - you can approve a PR before merging it in a separate step. Just like a PR approval means \"I'm OK with this being merged\", a run approval means \"I'm OK with that action being executed\" but nothing will happen until someone clicks on the \"Merge\" or \"Confirm\" button, respectively.</p>"},{"location":"faq.html#can-i-use-multiple-policies-of-the-same-type","title":"Can I use multiple policies of the same type?","text":"<p>When there are multiple policies of the same type, they are evaluated independently and then the decisions are merged. With Login policy, denies will take precedence over allows. That is why we recommend having a single Login policy. It is easier to reason about it.</p> <p>Otherwise, policies could look perfectly fine but block each other with deny rules. This is also true for the Push policy type but Plan policies, for example, are fine because they usually don\u2019t conflict.</p>"},{"location":"faq.html#can-i-attach-my-policy-to-multiple-stacks-modules","title":"Can I attach my policy to multiple stacks / modules?","text":"<p>This can be done with the <code>autoattach</code> label, you can read more about that here.</p>"},{"location":"faq.html#billing","title":"Billing","text":""},{"location":"faq.html#what-counts-as-a-user","title":"What counts as a user?","text":"<p>Everyone who logged in to the Services in a given month is counts as a user.</p> <p>API keys are virtual users and are billed like regular users, too. Thus, each API key used during any billing cycle counts against the total number of users.</p> <p>When setting up SSO, future logins will appear as new users since Spacelift cannot map those without your assistance. New users will count against your quota, and you may run out of seats. If you run into this problem, you can contact us.</p>"},{"location":"faq.html#stack","title":"Stack","text":""},{"location":"faq.html#my-stack-is-not-being-triggered","title":"My Stack is not being triggered.","text":"<p>The main culprits are usually a push policy or the wrong branch being tracked.</p> <p>If you do not have a push policy in place, you can attach a push policy with the default push policy and enable sampling to then review the inputs in the policy workbench to confirm that Spacelift has received the push event. (If you are using GitLab, you need to set up webhooks for every project)</p> <p>You can review the branch you are tracking in your stack settings.</p> <p>We also recommend checking your VCS provider is not currently experiencing any issues.</p>"},{"location":"faq.html#does-spacelift-support-monorepos","title":"Does Spacelift support monorepos?","text":"<p>Spacelift does support monorepos. You can set a project root in your stack settings. Our default push policy will trigger runs on changes within the project root or the project globs. You can review our policy example repo and see how you can customize this further.</p>"},{"location":"faq.html#run","title":"Run","text":""},{"location":"faq.html#how-do-i-trigger-a-run-locally","title":"How do I trigger a run locally?","text":"<p>You can use spacectl and the command <code>spacectl stack local-previews</code> then packs the content of the local folder, uploads it to a worker, which will then execute a run.</p> <p>That run will include the init and plan commands but they will not run locally. They will run on the worker.</p>"},{"location":"faq.html#how-can-the-workflow-be-customized","title":"How can the workflow be customized?","text":"<p>You can use hooks to customize your workflow. Hooks refer to extra commands that can be added to customize the workflow at various stages of a process. These hooks are essentially scripts or commands that are executed before and after certain phases in the Spacelift workflow</p>"},{"location":"getting-started/create-stack.html","title":"Create a stack in Spacelift","text":""},{"location":"getting-started/create-stack.html#create-a-stack-in-spacelift","title":"Create a stack in Spacelift","text":"<p>Creating a stack involves 9 steps, most of which are optional. Required tasks are marked with an asterisk here:</p> <ol> <li>*Name, describe, and label the stack.</li> <li>*Create a link between your new stack and an existing source code repository.</li> <li>*Choose the backend vendor.</li> <li>Define common behavior of the stack.</li> <li>Create stack hooks.</li> <li>Attach a cloud integration.</li> <li>Attach policies.</li> <li>Attach contexts.</li> <li>*Review the summary and create your stack.</li> </ol> <p>To get started, click Create stack on the Stacks page or Create first stack from the LaunchPad.</p> <p></p>"},{"location":"getting-started/create-stack.html#1-stack-details","title":"1. Stack details","text":"<p>Fill in required stack details.</p> <p></p> <ol> <li>Name: Enter a unique, descriptive name for your stack.</li> <li>Space: Select the space to create the stack in.</li> <li>Labels (optional): Add labels to help sort and filter your stacks.</li> <li>Description (optional): Enter a (markdown-supported) description of the stack and the resources it manages.</li> <li>Click Continue.</li> </ol>"},{"location":"getting-started/create-stack.html#2-connect-to-source-code","title":"2. Connect to source code","text":"<p>Connect your VCS provider as configured during LaunchPad step 1 and fill in the details.</p> <p></p> <ol> <li>Integration: Verify the VCS integration name is correct.</li> <li>Repository: Select the repository to manage in Spacelift. If you have multiple repositories linked to one VCS, leave blank.</li> <li>Branch: Select the branch of the repository to manage with this stack.</li> <li>Project root (optional): If the entrypoint of the stack is different than the root of the repo, enter its path here.</li> <li>Project globs (optional): Enter additional files and directories that should be managed by the stack.</li> <li>Click Continue.</li> </ol>"},{"location":"getting-started/create-stack.html#3-choose-vendor","title":"3. Choose vendor","text":"<p>Select your IaC vendor and fill in the required details, then click Create &amp; continue.</p> <p></p>"},{"location":"getting-started/create-stack.html#opentofuterraform","title":"OpenTofu/Terraform","text":"<ol> <li>Workflow tool: Set to OpenTofu, Terraform (FOSS), or Custom.<ul> <li>With OpenTofu or Terraform (FOSS), select a specific version or enter a version range.</li> </ul> </li> <li>Smart Sanitization (recommended): Choose whether Spacelift attempts to sanitize sensitive resources created by OpenTofu/Terraform.</li> <li>Manage State (recommended): Choose whether Spacelift should handle the OpenTofu/Terraform state.<ol> <li>If disabled: Optionally enter a workspace.</li> <li>If enabled: Configure these options:<ul> <li>External state access: Allow external read-only access for administrative stacks or users with write permissions to the Stack's space.</li> <li>Import existing state file: Enable to import a state file from your previous backend.</li> </ul> </li> </ol> </li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"getting-started/create-stack.html#pulumi","title":"Pulumi","text":"<ol> <li>Login URL: Enter the URL to your Pulumi state backend.</li> <li>Stack name: Enter a name for your Pulumi stack. This is separate from the name of the Spacelift stack, but you can give both the same name.</li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"getting-started/create-stack.html#aws-cloudformation","title":"AWS CloudFormation","text":"<ol> <li>Region: Enter the AWS region your stack will be located in (e.g. <code>us-east-2</code>).</li> <li>Stack name: Enter the name of the corresponding CloudFormation stack.</li> <li>Entry template file: Enter the path to the template file in your repo describing the root CloudFormation stack.</li> <li>Template bucket: Enter the location of the S3 bucket to store processed CloudFormation templates, so Spacelift can manage the state properly.</li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"getting-started/create-stack.html#kubernetes","title":"Kubernetes","text":"<ol> <li>Namespace (optional): Enter the namespace of the Kubernetes cluster you want to run commands on. Leave blank for multi-namespace stacks.</li> <li>Workflow tool: Select the tool used to execute workflow commands.<ul> <li>Kubernetes: Provide the kubectl version the worker will download.</li> <li>Custom: No configuration needed.</li> </ul> </li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"getting-started/create-stack.html#terragrunt","title":"Terragrunt","text":"<ol> <li>Terragrunt version: Select a specific Terraform version or enter a version range.</li> <li>Tool: Select the tool used to make infrastructure changes:<ul> <li>OpenTofu/Terraform (FOSS): Select a specific Terraform version or enter a version range.</li> <li>Manually provisioned: Outside of Spacelift, ensure the tool is available to the worker via a custom image or hook and set the <code>TERRAGRUNT_TFPATH</code> environment variable to tell Terragrunt where to find it.</li> </ul> </li> <li>Smart Sanitization (recommended): Choose whether Spacelift attempts to sanitize sensitive resources created by OpenTofu/Terraform.</li> <li>Use All Run: Enable to use Terragrunt's run-all feature.</li> <li>Click Create &amp; continue.</li> </ol>"},{"location":"getting-started/create-stack.html#ansible","title":"Ansible","text":"<ol> <li>Playbook: Enter the playbook file to run in the stack.</li> <li>Click Create &amp; continue.</li> </ol> <p>Once you've configured your vendor information, click Continue to Define stack behavior.</p> <p></p>"},{"location":"getting-started/create-stack.html#4-define-behavior","title":"4. Define behavior","text":"<p>Determine and set additional behaviors for your stack.</p> <ol> <li>Worker pool: Choose which worker pool to use (default is public workers).</li> <li>Runner image: Use a custom runner for your runtime environment.</li> <li>Administrative: Choose whether a stack has privileges to create other Spacelift resources via our Terraform provider.</li> <li>Allow run promotion: Allows you to promote a proposed run to a tracked run (i.e. deploy from a feature branch).</li> <li>Autodeploy: Automatically deploy changes to your code.</li> <li>Autoretry: Automatically retry deployment of invalidated proposed runs. For stacks using private workers only.</li> <li>Enable local preview: Preview how code changes will execute with the spacectl CLI feature.</li> <li>Enable secret masking: Automatically redact secret patterns from logs.</li> <li>Protect from deletion (recommended): Protect your stacks from accidental deletion.</li> <li>Transfer sensitive outputs across dependencices: Pass sensitive outputs from this stack to dependent stacks.</li> </ol> <p>Once you've configured your settings, click Save &amp; continue.</p>"},{"location":"getting-started/create-stack.html#5-add-hooks","title":"5. Add hooks","text":"<p>You also have the ability to control what happens before and after each runner phase using stack hooks. Define commands that run during the following phases:</p> <ul> <li>Initialization</li> <li>Planning</li> <li>Applying</li> <li>Destroying</li> <li>Performing</li> <li>Finally</li> </ul> <p>Once you've added all hooks, click Save &amp; continue.</p>"},{"location":"getting-started/create-stack.html#6-attach-cloud","title":"6. Attach cloud","text":"<p>If desired, attach the cloud provider integration configured in LaunchPad step 2.</p> <ol> <li>Select the cloud provider the stack will use.</li> <li>Attach integration: Choose the name of the integration the stack will use.</li> <li>Read: Use the integration during read phases.</li> <li>Write: Use the integration during write phases.</li> <li>Click Attach.</li> <li>Click Continue.</li> </ol>"},{"location":"getting-started/create-stack.html#7-attach-policies","title":"7. Attach policies","text":"<p>If you're just following the LaunchPad steps, you won't have any policies yet. If you did configure policies, you will be able to attach them here:</p> <ul> <li>Approval: Who can approve or reject a run and how a run can be approved.</li> <li>Plan: Which changes can be applied.</li> <li>Push: How Git push events are interpreted.</li> <li>Trigger: What happens when blocking runs terminate.</li> </ul> <p>Click Continue.</p>"},{"location":"getting-started/create-stack.html#8-attach-context","title":"8. Attach context","text":"<p>Contexts are sets of environment variables and related configuration, including hooks, that can be shared across multiple stacks. By attaching a context, you ensure your stack has all the necessary configuration elements it needs to operate, without repeating the setup for each stack.</p> <p>If you're just following the LaunchPad steps, you won't have any contexts yet. If you did configure contexts, you will be able to attach them here.</p> <p>Click Continue.</p>"},{"location":"getting-started/create-stack.html#9-summary","title":"9. Summary","text":"<p>Review your settings before finalizing your stack, then click Confirm.</p> <p>\u2705 Step 3 of the LaunchPad is complete! Now you can invite teammates.</p> <p></p>"},{"location":"getting-started/integrate-cloud.html","title":"Integrate cloud providers with Spacelift","text":""},{"location":"getting-started/integrate-cloud.html#integrate-cloud-providers-with-spacelift","title":"Integrate cloud providers with Spacelift","text":"<p>Infrastructure-as-code automation tools such as Terraform, AWS CloudFormation, or Pulumi require powerful credentials to execute. Typically, you'd provide static credentials (such as AWS credentials, GCP service keys, etc.), which goes against security best practices. Spacelift's cloud integrations manage your resources without the need for long-lived static credentials, dynamically generating short-lived access tokens to connect cloud providers with IaC providers.</p> <p>Spacelift currently supports AWS natively. A generic OpenID Connect integration is also available to work with any compatible service provider.</p> <p>Public vs private workers</p> <p>This feature is designed for customers using the shared public worker pool. When hosting Spacelift workers on your own infrastructure, you can use your cloud providers' ambient credentials (e.g. EC2 instance role or EKS worker role on AWS).</p>"},{"location":"getting-started/integrate-cloud.html#set-up-your-cloud-provider-integration","title":"Set up your cloud provider integration","text":"<ul> <li>Configure Amazon Web Services (AWS).</li> </ul> <p>You can also use OIDC for available cloud providers.</p>"},{"location":"getting-started/integrate-cloud/AWS.html","title":"Amazon Web Services","text":""},{"location":"getting-started/integrate-cloud/AWS.html#integrate-spacelift-with-amazon-web-services-aws","title":"Integrate Spacelift with Amazon Web Services (AWS)","text":"<p>The AWS integration allows Spacelift runs or tasks to automatically assume an IAM role in your AWS account, and in the process, generate a set of temporary credentials. These credentials are then exposed as computed environment variables during the run/task that takes place on the Spacelift stack where the integration is attached.</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_SECURITY_TOKEN</code></li> <li><code>AWS_SESSION_TOKEN</code></li> </ul> <p>These temporary credentials are enough for both the AWS Terraform provider and the Amazon S3 state backend to generate a fully authenticated AWS session without further configuration.</p> <p>To use the AWS integration, you need to set it up and attach it to any stacks that need it.</p>"},{"location":"getting-started/integrate-cloud/AWS.html#set-up-the-aws-iam-role","title":"Set up the AWS IAM role","text":"<p>Prerequisites</p> <p>To set up the AWS integration, you need:</p> <ul> <li>The ability to create IAM roles for your AWS account.</li> <li>Administrator access to your Spacelift account.</li> </ul>"},{"location":"getting-started/integrate-cloud/AWS.html#step-1-create-a-role-in-aws","title":"Step 1: Create a role in AWS","text":"<p>Before creating the Spacelift AWS integration, you need an AWS IAM Role within your AWS account.</p> <p>Multiple AWS accounts</p> <p>You can extend this role to have cross-account permissions to the target accounts to allow Spacelift to access multiple AWS accounts. See AWS documentation for more details.</p> <ol> <li>Within your AWS account, navigate to AWS IAM.</li> <li> <p>Select the Roles section and click Create role.</p> <p></p> </li> </ol>"},{"location":"getting-started/integrate-cloud/AWS.html#step-2-configure-trust-policy","title":"Step 2: Configure trust policy","text":"<p>You will need to configure a custom trust policy for the IAM role in AWS to allow Spacelift to assume the role and generate temporary credentials. When completing the role assumption, Spacelift will pass extra information in the <code>ExternalId</code> attribute, allowing you to add additional layers of security to your role.</p> <p>External ID Format: <code>&lt;spacelift-account-name&gt;@&lt;integration-id&gt;@&lt;stack-slug&gt;@&lt;read|write&gt;</code></p> <ul> <li><code>&lt;spacelift-account-name&gt;</code>: The name of the Spacelift account, found in the lower left-hand side of the Spacelift platform UI.</li> <li><code>&lt;integration-id&gt;</code>: The ID of the AWS Cloud Integration.</li> <li><code>&lt;stack-slug&gt;</code>: The slug of the stack that the AWS Cloud Integration is attached to.</li> <li><code>&lt;read|write&gt;</code>: Set to either <code>read</code> or <code>write</code> based upon the event that initiated the role assumption. The Planning phase uses <code>read</code> while the Applying phase uses <code>write</code>.</li> </ul> <p>Given the format of the External ID passed by Spacelift, you can further secure your IAM Role trust policies for more granular security. For example, you may wish to lock down an IAM Role so that it can only be used by a specific stack.</p>"},{"location":"getting-started/integrate-cloud/AWS.html#example-trust-policies","title":"Example trust policies","text":"Any stack can use roleSpecific stack can use role <p>Here's an example trust policy statement that allows any stack within your Spacelift account to use the IAM Role.</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n        {\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n            \"StringLike\": {\n                \"sts:ExternalId\": \"yourSpaceliftAccountName@*\"\n            }\n            },\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n            \"AWS\": \"&lt;principal&gt;\"\n            }\n        }\n]\n}\n</code></pre> <p>Here's an example trust policy that locks down an IAM Role so it can only be used by the stack <code>stack-a</code>.</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n        {\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n            \"StringLike\": {\n                \"sts:ExternalId\": \"yourSpaceliftAccountName@*@stack-a@*\"\n            }\n            },\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n            \"AWS\": \"&lt;principal&gt;\"\n            }\n        }\n]\n}\n</code></pre> <ol> <li>In AWS' Create Role wizard, select Custom trust policy.</li> <li>Paste the configured trust policy, then click Next.</li> </ol>"},{"location":"getting-started/integrate-cloud/AWS.html#step-3-configure-role-permissions","title":"Step 3: Configure role permissions","text":"<ol> <li>Check the boxes to attach at least one permissions policy to your IAM role.<ul> <li>Ensure it has sufficient permissions to deploy any resources your IaC code defines.</li> </ul> </li> <li>Click Next.</li> </ol> <p>Info for Terraform users</p> <p>For Terraform users managing their own state file, give your role sufficient permissions to access your state. Terraform documents the permissions required for S3-managed state and for DynamoDB state locking.</p>"},{"location":"getting-started/integrate-cloud/AWS.html#step-4-create-iam-role","title":"Step 4: Create IAM role","text":"<ol> <li>Enter a role name and description, then review your configuration.</li> <li>Click Create role.</li> <li>Once the role is created, click View role or its name in the list.</li> <li> <p>Copy the IAM role ARN to set up the integration in Spacelift.</p> <p></p> </li> </ol>"},{"location":"getting-started/integrate-cloud/AWS.html#create-the-cloud-integration-in-spacelift","title":"Create the cloud integration in Spacelift","text":"<ol> <li> <p>On the Integrations screen, find the AWS card and click View, then Set up integration.</p> <p></p> </li> <li> <p>Fill in the integration details:</p> <ol> <li>Name: Enter a name for the cloud integration.</li> <li>Space: Select the space that can access the integration.</li> <li>Role ARN: Paste the ARN copied from the IAM role you created in AWS.</li> <li>Assume role on worker: If enabled, role assumption will be performed on your private worker rather than on Spacelift's end. You can also specify a custom External ID to use during role assumption.</li> <li>Duration (optional): Select how long the role session will last, from 15 minutes to 1 hour (default).</li> <li>Region (optional): Set the AWS regional endpoint to use (such as us-east-2).</li> <li>Labels (optional): Enter a label or labels to help sort your integrations if needed.</li> </ol> </li> <li>Click Set up.</li> </ol> <p>Warning</p> <p>If you receive an error message when trying to set up the integration in Spacelift, see Troubleshoot trust relationship issues.</p> <p>\u2705 Step 2 of the LaunchPad is complete! Now you can create your first stack.</p> <p></p>"},{"location":"getting-started/integrate-cloud/AWS.html#troubleshoot-trust-relationship-issues","title":"Troubleshoot trust relationship issues","text":"<p>If you get the error <code>you need to configure trust relationship section in your AWS account</code> when attaching a cloud integration to a stack:</p> <p></p> <p>There are a couple of common causes to check.</p>"},{"location":"getting-started/integrate-cloud/AWS.html#incorrect-or-missing-trust-relationship-policy","title":"Incorrect or missing trust relationship policy","text":"<p>The error message in the UI includes a tailored trust relationship policy example. This policy allows Spacelift to assume the IAM role and must be added to the Trust relationships section of your role in AWS IAM. See Step 2: Configure trust policy for more information.</p>"},{"location":"getting-started/integrate-cloud/AWS.html#sts-security-token-service-not-enabled","title":"STS (Security Token Service) not enabled","text":"<p>This error can be caused by STS not being enabled in the AWS region where your Spacelift instance is deployed. Check your region settings and ensure STS is active. Learn more about AWS STS here.</p>"},{"location":"getting-started/integrate-cloud/Azure.html","title":"Integrate Spacelift with Microsoft Azure","text":""},{"location":"getting-started/integrate-cloud/Azure.html#integrate-spacelift-with-microsoft-azure","title":"Integrate Spacelift with Microsoft Azure","text":"<p>Spacelift provides support for managing Azure resources via the Terraform Azure Provider. The documentation for the Azure Provider outlines the different authentication methods it supports, and it should always be considered the ultimate source of truth.</p> <p>This page explains how to configure the Spacelift-managed integration, the simplest way to get Azure up and running as your cloud provider in Spacelift. The Spacelift-managed integration handles automatic secret creation and rotation.</p> <p>Other authentication methods</p> <p>See our Azure cloud provider documentation to set up the integration through static credentials or managed service identities.</p>"},{"location":"getting-started/integrate-cloud/Azure.html#set-up-the-microsoft-azure-cloud-integration","title":"Set up the Microsoft Azure cloud integration","text":"<p>Tip</p> <p>This guide explains how to configure the Azure provider using environment variables. Although you can add these environment variables directly to individual stacks, it may be worth creating a Spacelift context to store your Azure credentials. This allows you to easily add the same credentials to any stack that requires them.</p> <p>The Spacelift-managed integration is great when you want to get up and running quickly, and when you want to ensure the credentials for accessing your Azure account will be automatically rotated and stored securely. If you are not comfortable with Spacelift managing your Azure credentials, we would suggest that you use a private worker configured with a managed identity for the most control and security.</p>"},{"location":"getting-started/integrate-cloud/Azure.html#credential-storage-and-rotation","title":"Credential storage and rotation","text":"<p>When an Azure integration is created, an associated Microsoft Entra Application is created within Azure. We automatically create a client secret for that application, and rotate it roughly once every 24 hours. The secret is stored securely, encrypted using AWS Key Management Service.</p>"},{"location":"getting-started/integrate-cloud/Azure.html#step-1-find-your-active-directory-tenant-id","title":"Step 1: Find your Active Directory Tenant ID","text":"<p>Before creating the Spacelift Azure integration, you need your Tenant ID.</p> <ol> <li>Within your Azure portal, navigate to either:<ul> <li>The Azure Active Directory section of the Azure portal.</li> <li>The Subscriptions section of the Azure portal.</li> </ul> </li> <li>Locate and copy your Tenant ID.</li> </ol>"},{"location":"getting-started/integrate-cloud/Azure.html#step-2-create-the-cloud-integration-in-spacelift","title":"Step 2: Create the cloud integration in Spacelift","text":"<ol> <li> <p>On the Integrations screen, find the Azure card and click View, then Set up integration.</p> </li> <li> <p>Fill in the integration details:     </p> <ol> <li>Name: Enter a name for the cloud integration.</li> <li>Space: Select the space that can access the integration.</li> <li>Tenant ID: Paste the Tenant ID copied from the Azure portal.</li> <li>Default subscription ID (optional): If you want to attach your Azure cloud integration to multiple stacks all using the same Azure subscription, specify a default subscription ID here.</li> <li>Labels (optional): Enter a label or labels to help sort your integrations if needed.</li> </ol> </li> <li>Click Set up.</li> </ol>"},{"location":"getting-started/integrate-cloud/Azure.html#step-3-provide-admin-consent","title":"Step 3: Provide admin consent","text":"<p>Once your integration has been created successfully, you will be taken to the integration details page to provide the required admin consent. The consent process requires Spacelift to request at least one permission. Although Spacelift requests the \u201cSign in and read user profile\u201d permission, it never signs in as any users in your account or accesses their information.</p> <p></p> <ol> <li>Click Consent to install the Microsoft Entra application for your Spacelift integration into your Azure account.</li> <li>Log in to your Azure account, if needed.</li> <li>Click Accept on the permissions screen to complete the admin consent process.     </li> <li>You will be redirected back to Spacelift's Azure integration settings.</li> </ol> <p>Possible error message</p> <p>Microsoft Entra uses eventual consistency to replicate new Azure applications globally. Because of this, you might see the following error message if you try to grant admin consent too quickly after the integration is created:</p> <p></p> <p>This isn\u2019t a problem. Just wait a few minutes and try again.</p>"},{"location":"getting-started/integrate-cloud/Azure.html#step-4-configure-azure-permissions","title":"Step 4: Configure Azure permissions","text":"<p>Now that you have granted admin consent, a new Enterprise Application will be created for your integration in the Enterprise Applications section of Azure Active Directory.</p> <p></p> <p>You will need to add a new role assignment for the Spacelift integration.</p> <ol> <li>Navigate to the Access Control (IAM) section of the Azure subscription or resource group you're integrating with Spacelift.</li> <li>Click Add &gt; Add role assignment.</li> <li>Role: Select Contributor.</li> <li>Assign access to: Select User, group, or service principal.</li> <li>Select: Select the Spacelift Enterprise Application name from the list.</li> <li>Click Review + assign.</li> </ol> <p></p> <p>Info</p> <p>The Spacelift integration has no access to any of your Azure infrastructure unless you explicitly grant it the appropriate permissions.</p> <p>\u2705 Step 2 of the LaunchPad is complete! Now you can create your first stack.</p> <p></p>"},{"location":"getting-started/integrate-cloud/GCP.html","title":"Integrate Spacelift with Google Cloud Platform","text":""},{"location":"getting-started/integrate-cloud/GCP.html#integrate-spacelift-with-google-cloud-platform","title":"Integrate Spacelift with Google Cloud Platform","text":"<p>Spacelift's GCP integration via OIDC allows Spacelift to manage your Google Cloud resources without the need for long-lived static credentials by creating a service account inside the project dedicated to your Stack.</p> <p>With the service account already created, Spacelift generates temporary OAuth token for the service account as a <code>GOOGLE_OAUTH_ACCESS_TOKEN</code> variable in the environment of your runs and tasks. This is one of the configuration options for the Google Terraform provider, so you can define it like this:</p> <pre><code>provider \"google\" {}\n</code></pre> <p>Many GCP resources require the <code>project</code> identifier too, so if you don't specify a default in your provider, you will need to pass it to each individual resource that requires it.</p>"},{"location":"getting-started/integrate-cloud/GCP.html#set-up-the-google-cloud-platform-integration","title":"Set up the Google Cloud Platform integration","text":"<p>In order to enable Spacelift runs to access GCP resources, you need to set up Spacelift as a valid identity provider for your account within GCP.</p>"},{"location":"getting-started/integrate-cloud/GCP.html#step-1-set-spacelift-as-a-valid-identity-provider","title":"Step 1: Set Spacelift as a valid identity provider","text":"<ol> <li>Navigate to the GCP console and select the IAM &amp; Admin service.</li> <li>Click Workload Identity Federation in the left-hand menu.</li> <li>If this is your first time creating a Workload Identity Pool, click Get Started, then Create Pool.     <ul> <li>If you have already created a Workload Identity Pool before, click Create Pool.    </li> </ul> </li> <li>Enter a name for your new identity pool and optionally set a description.</li> <li>Fill in the identity provider details:     <ol> <li>Select a provider: Select OpenID Connect (OIDC).</li> <li>Provider name: Enter the email address linked to your Spacelift account.</li> <li>Issuer (URL): The URL of your Spacelift account, including the scheme. Ensure you add <code>iss</code> to the URL.</li> <li>Audiences: Select Allowed audiences, then enter the hostname of your Spacelift account (e.g. <code>demo.app.spacelift.io</code>). Ensure you add <code>aud</code> to the hostname.</li> </ol> </li> <li>Fill in the provider attributes to configure mappings between Spacelift token claims (assertions) and Google attributes:     <ol> <li>Google 1: This is filled in automatically with <code>google.subject</code>.</li> <li>OIDC 1: Enter <code>assertion.sub</code>.</li> <li>Google 2: Enter <code>attribute.space</code>.</li> <li>OIDC 2: Enter <code>assertion.spaceId</code>. Custom claims like this can be mapped to custom attributes, which need to start with the <code>attribute.</code> prefix.</li> </ol> </li> <li>Attribute conditions: Specify extra conditions using Google's Common Expression Language to restrict which identities can authenticate using your workload identity pool.</li> <li>Finish creating the workload identity pool.</li> </ol> <p>Warning</p> <p>If your Stack ID is too long, it may exceed the threshold set by Google for the <code>google.subject</code> mapping. In that case, you can use a different custom claim to create the mapping.</p>"},{"location":"getting-started/integrate-cloud/GCP.html#step-2-grant-access-to-service-account","title":"Step 2: Grant access to service account","text":"<p>Once the workload identity pool has been created, you need to grant it access impersonate the service account we will be using.</p> <ol> <li>Ensure you have a Spacelift service account ready to use.</li> <li>In the workload identity pool details, click Grant access.</li> <li>Service account: Select the Spacelift service account from the list.</li> <li>Select principals: Select space in the attribute name dropdown, then enter the full SpaceId (from Spacelift) in the text box.</li> <li>Click Save.</li> </ol> <p>In this example, any token claiming to originate from our Spacelift account's <code>prod</code> space can impersonate the service account: </p>"},{"location":"getting-started/integrate-cloud/GCP.html#step-3-download-the-configuration-file","title":"Step 3: Download the configuration file","text":"<p>After you give the workload identity pool access to impersonate the service account, you will be able to Configure your application.</p> <ol> <li>Provider: Select your Spacelift service account name in the dropdown.</li> <li>OIDC ID token path: Enter <code>/mnt/workspace/spacelift.oidc</code>.</li> <li>Format type: Select json.</li> <li>Subject token field name: Leave as <code>access_token</code>.</li> <li>Click Download config.</li> </ol> <p></p> <p>The downloaded file will include the format type in <code>credential_source</code>. You can remove this so your <code>credential_source</code> section only contains:</p> <pre><code> \"credential_source\": {\n    \"file\": \"/mnt/workspace/spacelift.oidc\"\n  }\n</code></pre>"},{"location":"getting-started/integrate-cloud/GCP.html#internal-only-load-balancer-configuration","title":"Internal-only load balancer configuration","text":"<p>GCP needs information about the Spacelift OIDC provider to enable trust between Spacelift and GCP. Generally, GCP will gather this information itself via a JWKS endpoint hosted by Spacelift.</p> <p>However, if you're using an internal only load balancer, GCP will not have access to that endpoint and you will need to provide the JWKS details manually.</p> <ol> <li>Download your JWKS from <code>https://{your-spacelift-url}/.well-known/jwks</code></li> <li>Follow this guide on GCP to upload the JWKS to GCP manually.</li> </ol> <p>Once the JWKS is uploaded, OIDC between Spacelift and GCP should work as expected.</p>"},{"location":"getting-started/integrate-cloud/GCP.html#step-4-connect-with-specific-iac-providers","title":"Step 4: Connect with specific IaC providers","text":""},{"location":"getting-started/integrate-cloud/GCP.html#opentofu-terraform-and-pulumi","title":"OpenTofu, Terraform, and Pulumi","text":"<p>Once the Spacelift-GCP OIDC integration is set up, the Google Cloud Terraform provider and Pulumi GCP provider can be configured without the need for any static credentials.</p> <p>You will need to provide a configuration file telling the provider how to authenticate. The configuration file can be created manually or generated by the <code>gcloud</code> utility and looks like this:</p> <pre><code>{\n  \"type\": \"external_account\",\n  \"audience\": \"//iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${WORKER_POOL_ID}/providers/${IDENTITY_PROVIDER_ID}\",\n  \"subject_token_type\": \"urn:ietf:params:oauth:token-type:jwt\",\n  \"token_url\": \"https://sts.googleapis.com/v1/token\",\n  \"credential_source\": {\n    \"file\": \"/mnt/workspace/spacelift.oidc\"\n  },\n  \"service_account_impersonation_url\": \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/${SERVICE_ACCOUNT_EMAIL}:generateAccessToken\",\n  \"service_account_impersonation\": {\n    \"token_lifetime_seconds\": 3600\n  }\n}\n</code></pre> <p>Your Spacelift run needs to have access to this file, so check it in, then mount it on a stack directly or in a context that is attached to the stack.</p> <p>You will also need to tell the provider how to find this configuration file. Create a <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable, and set its value as the path to your credentials file.</p> <p>Here is an example of using a Spacelift context to mount the file and configure the provider to be attached to an arbitrary number of stacks:</p> <p></p> <p>For more information about configuring the OpenTofu/Terraform provider, please see the Google Cloud Terraform provider docs. The Pulumi configuration follows the same steps as OpenTofu/Terraform.</p> <p>\u2705 Step 2 of the LaunchPad is complete! Now you can create your first stack.</p> <p></p>"},{"location":"getting-started/integrate-source-code.html","title":"Integrate source code with Spacelift","text":""},{"location":"getting-started/integrate-source-code.html#integrate-source-code-with-spacelift","title":"Integrate source code with Spacelift","text":"<p>For everything but raw Git, integrate your source code with Spacelift on the Source code tab.</p> <p></p> <ol> <li>Click Set up integration.</li> <li>Select your VCS from the dropdown.</li> <li>Follow the wizard to configure the integration.</li> </ol> <p>Your source code can be stored on any of the supported version control systems (VCS):</p> <ul> <li>GitHub</li> <li>GitLab</li> <li>Bitbucket<ul> <li>Cloud</li> <li>Data Center/Server</li> </ul> </li> <li>Azure DevOps</li> <li>Raw Git</li> </ul>"},{"location":"getting-started/integrate-source-code.html#example-starter-repository","title":"Example starter repository","text":"<p>We provide a Terraform Starter repository you can fork to test Spacelift's capabilities right away.</p> <p>Tip</p> <p>If you are using the Terraform starter repository, and you did not sign up for your Spacelift account with GitHub, you may need to add the environment variable <code>TF_VAR_github_app_namespace</code> with the value as your organization name or GitHub handle. You can do this under the <code>Environment</code> tab in the stack.</p>"},{"location":"getting-started/integrate-source-code/Azure-DevOps.html","title":"Azure DevOps","text":""},{"location":"getting-started/integrate-source-code/Azure-DevOps.html#use-azure-devops-as-your-source-code-provider","title":"Use Azure DevOps as your source code provider","text":"<p>Spacelift supports Azure DevOps as the code source for your stacks and modules.</p> <p>You can set up multiple Space-level and one default Azure DevOps integration per account.</p>"},{"location":"getting-started/integrate-source-code/Azure-DevOps.html#create-the-azure-devops-integration","title":"Create the Azure DevOps integration","text":"<p>Azure DevOps integration details</p> <p>Learn more about setting up and using the Azure DevOps integration on the Azure DevOps source control page.</p>"},{"location":"getting-started/integrate-source-code/Azure-DevOps.html#initial-setup","title":"Initial setup","text":"<ol> <li>On the Integrations screen, click View on the Azure DevOps card, then Set up Azure DevOps.     </li> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol>"},{"location":"getting-started/integrate-source-code/Azure-DevOps.html#find-your-organization-url","title":"Find your organization URL","text":"<p>You will need your Azure DevOps organization URL, which usually follows this format: <code>https://dev.azure.com/{my-organization-name}</code>.</p> <p>Tip</p> <p>Depending on when your Azure DevOps organization was created, it may use a different format, for example: <code>https://{my-organization-name}.visualstudio.com</code>.</p> <p></p> <ol> <li>Navigate to your main organization page in Azure DevOps.</li> <li>Copy the Azure DevOps organization URL.</li> </ol>"},{"location":"getting-started/integrate-source-code/Azure-DevOps.html#create-a-personal-access-token","title":"Create a personal access token","text":"<p>You need to create a personal access token in Azure DevOps to create the integration with Spacelift.</p> <ol> <li> <p>Navigate to User settings &gt; Personal access tokens in the top-right section of the Azure DevOps page.</p> <p></p> </li> <li> <p>Click New Token.</p> </li> <li>Fill in the details to create a new personal access token:     <ol> <li>Name: Enter a descriptive name for the token.</li> <li>Organization: Select the organization to connect to Spacelift.</li> <li>Expiration: Select an expiration date for the token.</li> <li>Scopes: Select Custom defined, then check Read &amp; write in the Code section.</li> </ol> </li> <li>Click Create.</li> <li>Copy the token details to finish the integration in Spacelift.     </li> </ol>"},{"location":"getting-started/integrate-source-code/Azure-DevOps.html#copy-details-into-spacelift","title":"Copy details into Spacelift","text":"<p>Now that your personal access token has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>Organization URL: Paste your Azure DevOps organization URL.</li> <li>User facing host URL: Enter the URL that will be shown to the user and displayed in the Spacelift UI. This will be the same as the API host URL unless you are using VCS Agents, in which case it should be <code>private://&lt;vcs-agent-pool-name&gt;/&lt;azure-organization-name&gt;</code>.</li> <li>Personal access token: Paste the personal access token that Spacelift will use to access your Azure DevOps organization.</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Click Set up to save your integration details.</li> </ol>"},{"location":"getting-started/integrate-source-code/Azure-DevOps.html#set-up-webhooks","title":"Set up webhooks","text":"<p>For every Azure DevOps repository being used in Spacelift stacks or modules, you will need to set up a webhook to notify Spacelift about project changes.</p> <p>Note</p> <p>Default integrations are visible to all users of the account, but only root Space admins can see their details.</p> <p>Space-level integrations will be listed to users with read access to the integration Space. Integration details, however, contain sensitive information (such as the webhook secret) and are only visible to those with admin access.</p> <ol> <li>On the Integrations &gt; Azure DevOps page, click the three dots next to the integration name.</li> <li>Click See details to find the webhook endpoint and webhook secret.     </li> </ol>"},{"location":"getting-started/integrate-source-code/Azure-DevOps.html#configure-webhooks-in-azure-devops","title":"Configure webhooks in Azure DevOps","text":"<ol> <li>In Azure DevOps, select the project you are connecting to Spacelift.</li> <li>Navigate to Project settings &gt; Service hooks.</li> <li>Click Create subscription, then select Web Hooks and click Next.     </li> <li>On the Trigger page of the New Service Hooks Subscription window:<ol> <li>Trigger on this type of event: Select Code pushed, then click Next. </li> </ol> </li> <li>In the Settings section of the Action page:     <ol> <li>URL: Enter the webhook endpoint from Spacelift.</li> <li>Basic authentication username: Leave blank.</li> <li>Basic authentication password: Enter the webhook secret from Spacelift.</li> </ol> </li> <li>Click Finish.</li> <li>Repeat steps 3 through 6 for the following event triggers:<ul> <li>Pull request created.</li> <li>Pull request merge attempted.</li> <li>Pull request updated.</li> <li>Pull request commented on.</li> </ul> </li> </ol> <p>Once all hooks are created, you should see them on the Service Hooks page.</p> <p></p> <p>\u2705 Step 1 of the LaunchPad is complete! Now you can connect your cloud account.</p> <p></p>"},{"location":"getting-started/integrate-source-code/Bitbucket-Cloud.html","title":"Bitbucket Cloud","text":""},{"location":"getting-started/integrate-source-code/Bitbucket-Cloud.html#use-bitbucket-cloud-as-your-source-code-provider","title":"Use Bitbucket Cloud as your source code provider","text":"<p>Spacelift supports Bitbucket Cloud as the code source for your stacks and modules.</p> <p>You can set up multiple Space-level and one default Bitbucket Cloud integration per account.</p>"},{"location":"getting-started/integrate-source-code/Bitbucket-Cloud.html#create-the-bitbucket-cloud-integration","title":"Create the Bitbucket Cloud integration","text":"<p>Bitbucket Cloud integration details</p> <p>Learn more about setting up and using the Bitbucket integration on the Bitbucket Cloud source control page.</p>"},{"location":"getting-started/integrate-source-code/Bitbucket-Cloud.html#initial-setup","title":"Initial setup","text":"<ol> <li>On the Integrations screen, click View on the Bitbucket Cloud card, then Set up Bitbucket Cloud.     </li> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol> <p>Migration from App Passwords to API Tokens</p> <p>Previously, this integration used Bitbucket App Passwords for authentication. Atlassian has deprecated App Passwords and replaced them with API tokens.</p>"},{"location":"getting-started/integrate-source-code/Bitbucket-Cloud.html#create-your-api-token","title":"Create your API token","text":"<p>You will need to create an API token for this integration on the Bitbucket Cloud site.</p> <ol> <li>Navigate to Atlassian account settings &gt; Security &gt; Create API token with scopes.</li> <li> <p>Fill in the details to create a new API token:</p> <ol> <li>Choose a name for your API token.</li> <li>Set an expiration date.</li> <li> <p>Select Bitbucket as the app.</p> <p></p> </li> <li> <p>Select read permissions for:</p> <ul> <li>Repository</li> <li>Pull requests</li> </ul> <p></p> </li> </ol> </li> <li> <p>Click Create.</p> </li> <li>Copy the API token details to finish the integration in Spacelift.</li> </ol>"},{"location":"getting-started/integrate-source-code/Bitbucket-Cloud.html#copy-details-into-spacelift","title":"Copy details into Spacelift","text":"<p>Now that your API token has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>Email: Enter your Bitbucket Cloud email address.</li> <li>API token: Paste the API token that Spacelift will use to access your Bitbucket Cloud repository.</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Click Set up to save your integration details.     </li> </ol>"},{"location":"getting-started/integrate-source-code/Bitbucket-Cloud.html#set-up-webhooks","title":"Set up webhooks","text":"<p>For every Bitbucket Cloud repository being used in Spacelift stacks or modules, you will need to set up a webhook to notify Spacelift about project changes.</p> <p>Note</p> <p>Default integrations are visible to all users of the account, but only root Space admins can see their details.</p> <p>Space-level integrations will be listed to users with read access to the integration Space. Integration details, however, contain sensitive information (such as the webhook secret) and are only visible to those with admin access.</p> <ol> <li>On the Integrations &gt; Bitbucket Cloud page, click the three dots next to the integration name.</li> <li>Click See details to find the webhook endpoint and webhook secret.     </li> </ol>"},{"location":"getting-started/integrate-source-code/Bitbucket-Cloud.html#configure-webhooks-in-bitbucket-cloud","title":"Configure webhooks in Bitbucket Cloud","text":"<p>For each repository you want to use with Spacelift, you now need to add webhooks in Bitbucket Cloud.</p> <ol> <li>In Bitbucket Cloud, select the repository you are connecting to Spacelift.</li> <li>Navigate to Repository settings &gt; Webhooks.</li> <li>Click Add webhook.     </li> <li>Title: Enter a name for the webhook.</li> <li>URL: Paste the webhook endpoint from Spacelift.</li> <li>Secret: Paste the webhook secret from Spacelift.</li> <li>Status: Check Active.</li> <li>Triggers:<ol> <li>Under Repository, check Push.</li> <li>Under Pull Request, check:<ul> <li>Created</li> <li>Updated</li> <li>Approved</li> <li>Approval removed</li> <li>Merged</li> <li>Comment created</li> </ul> </li> </ol> </li> <li>Click Save.</li> </ol>"},{"location":"getting-started/integrate-source-code/Bitbucket-Cloud.html#install-pull-request-commit-links-app","title":"Install Pull Request Commit Links app","text":"<p>Finally, you should install the Pull Request Commit Links app to be able to use this API. The app is installed automatically when you go to the commit's details and click Pull requests.</p> <p></p> <p>\u2705 Step 1 of the LaunchPad is complete! Now you can connect your cloud account.</p> <p></p>"},{"location":"getting-started/integrate-source-code/Bitbucket-DataCenter.html","title":"Bitbucket Data Center","text":""},{"location":"getting-started/integrate-source-code/Bitbucket-DataCenter.html#use-bitbucket-data-center-as-your-source-code-provider","title":"Use Bitbucket Data Center as your source code provider","text":"<p>Spacelift supports Bitbucket Data Center (on-premise) as the code source for your stacks and modules.</p> <p>You can set up multiple Space-level and one default Bitbucket Data Center integration per account.</p>"},{"location":"getting-started/integrate-source-code/Bitbucket-DataCenter.html#create-the-bitbucket-data-center-integration","title":"Create the Bitbucket Data Center integration","text":"<p>Bitbucket Data Center integration details</p> <p>Learn more about setting up and using the Bitbucket integration on the Bitbucket Data Center source control page.</p>"},{"location":"getting-started/integrate-source-code/Bitbucket-DataCenter.html#initial-setup","title":"Initial setup","text":"<ol> <li>On the Integrations screen, click View on the Bitbucket Data Center card, then Set up Bitbucket Data Center.     </li> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol>"},{"location":"getting-started/integrate-source-code/Bitbucket-DataCenter.html#create-an-access-token","title":"Create an access token","text":"<p>You will need to create an access token in Bitbucket to use with Spacelift. The token requires the following access:</p> <ul> <li>Read access to any projects Spacelift needs to be able to access.</li> <li>Read access to the repositories within those projects.</li> </ul> <p></p> <ol> <li>Navigate to Manage account &gt; Personal access tokens.</li> <li>Click Create.</li> <li>Name: Enter a descriptive name for the token.</li> <li>Permissions &gt; Projects: Select Read.</li> <li>Permissions &gt; Repositories: Select Read.</li> <li>Automated expiry: Select No.</li> <li>Click Create.     </li> <li>Copy the token details to finish the integration in Spacelift.</li> </ol>"},{"location":"getting-started/integrate-source-code/Bitbucket-DataCenter.html#copy-details-into-spacelift","title":"Copy details into Spacelift","text":"<p>Now that your Bitbucket Data Center access token has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>API host URL: Enter the URL of your Bitbucket server. This will likely use a format like: <code>https://bitbucket.&lt;myorganization&gt;.com</code></li> <li>User facing host URL: Enter the URL that will be shown to the user and displayed in the Spacelift UI. This will be the same as the API host URL unless you are using VCS Agents, in which case it should be <code>private://&lt;vcs-agent-pool-name&gt;</code>.</li> <li>Username: Enter the username for the Bitbucket account where you created the access token.</li> <li>Access token: Enter the access token that Spacelift will use to access your Bitbucket.</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Click Set up to save your integration settings.</li> </ol> <p></p>"},{"location":"getting-started/integrate-source-code/Bitbucket-DataCenter.html#set-up-webhooks","title":"Set up webhooks","text":"<p>For every Bitbucket Data Center repository being used in Spacelift stacks or modules, you will need to set up a webhook to notify Spacelift about project changes.</p> <p>Note</p> <p>Default integrations are visible to all users of the account, but only root Space admins can see their details.</p> <p>Space-level integrations will be listed to users with read access to the integration Space. Integration details, however, contain sensitive information (such as the webhook secret) and are only visible to those with admin access.</p> <ol> <li>On the Integrations &gt; Bitbucket Data Center page, click the three dots next to the integration name.</li> <li>Click See details to find the webhook endpoint and webhook secret.     </li> </ol>"},{"location":"getting-started/integrate-source-code/Bitbucket-DataCenter.html#configure-webhooks-in-bitbucket-data-center","title":"Configure webhooks in Bitbucket Data Center","text":"<p>For each repository you want to use with Spacelift, you now need to add webhooks in Bitbucket Data Center.</p> <ol> <li>In Bitbucket Data Center, select the repository you are connecting to Spacelift.</li> <li>Navigate to Repository settings &gt; Webhooks.</li> <li>Click Add webhook.     </li> <li>Title: Enter a name for the webhook.</li> <li>URL: Paste the webhook endpoint from Spacelift.</li> <li>Secret: Paste the webhook secret from Spacelift.</li> <li>Status: Check Active.</li> <li>Triggers:<ol> <li>Under Repository, check Push.</li> <li>Under Pull Request, check:<ul> <li>Opened</li> <li>Source branch updated</li> <li>Modified</li> <li>Approved</li> <li>Unapproved</li> <li>Merged</li> <li>Comment added</li> </ul> </li> </ol> </li> <li>Click Save.</li> </ol> <p>Warning</p> <p>Don't forget to enter a secret when configuring your webhook. Bitbucket will allow you to create your webhook with no secret specified, but any webhook requests to Spacelift will fail without one configured.</p>"},{"location":"getting-started/integrate-source-code/Bitbucket-DataCenter.html#install-pull-request-commit-links-app","title":"Install Pull Request Commit Links app","text":"<p>Finally, you should install the Pull Request Commit Links app to be able to use this API. The app is installed automatically when you go to the commit's details and click Pull requests.</p> <p></p> <p>\u2705 Step 1 of the LaunchPad is complete! Now you can connect your cloud account.</p> <p></p>"},{"location":"getting-started/integrate-source-code/GitHub.html","title":"GitHub","text":""},{"location":"getting-started/integrate-source-code/GitHub.html#use-github-as-your-source-code-provider","title":"Use GitHub as your source code provider","text":"<p>Spacelift is deeply integrated with GitHub, providing organizations a simple way to manage IaC versioned in GitHub.</p> <p>You can set up multiple Space-level and one default GitHub integration per account.</p> <p>Using multiple GitHub accounts</p> <p>If you want to use multiple GitHub accounts or organizations, or connect Spacelift to your GitHub Enterprise instance, you will need to set up a custom GitHub integration via a GitHub App.</p>"},{"location":"getting-started/integrate-source-code/GitHub.html#create-and-link-a-custom-application","title":"Create and link a custom application","text":"<p>You will need a GitHub Enterprise account to create a custom GitHub application to link it to Spacelift.</p>"},{"location":"getting-started/integrate-source-code/GitHub.html#create-the-github-application","title":"Create the GitHub application","text":"<ol> <li>On the Integrations screen, click View on the GitHub card, then Set up GitHub.</li> <li>Click Set up via wizard (recommended) or Set up manually.</li> </ol> <p>Warning</p> <p>Manual application setup is more prone to errors and should only be used if other methods will not work.</p>"},{"location":"getting-started/integrate-source-code/GitHub.html#set-up-via-wizard","title":"Set up via wizard","text":"<ol> <li>Select whether you're integrating with GitHub.com or a self-hosted installation, then click Continue.</li> <li>Select whether the GitHub integration should be owned by a personal or organization account, then click Continue.</li> <li>Click Continue to create the application on GitHub.com.<ol> <li>Enter a name for your integration. This can be changed later.</li> <li>Click Create GitHub app. You will be redirected back to Spacelift.</li> </ol> </li> <li>Fill in the additional information:     <ol> <li>Integration name: Must be unique, and cannot be changed after app creation because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> <li>VCS checks: Individual checks (one per stack) or aggregated checks (summarized checks across all affected stacks).</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> </ol> </li> <li>Click Set up. Once the application is created, you will automatically be redirected to install it in GitHub.</li> </ol>"},{"location":"getting-started/integrate-source-code/GitHub.html#set-up-manually","title":"Set up manually","text":"<p>After selecting the option to enter your details manually, you should see the following form:</p> <p></p> <ol> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol> <p>Once the integration name and the type are chosen, a webhook endpoint and a webhook secret will be generated for the GitHub app in the middle of the form.</p>"},{"location":"getting-started/integrate-source-code/GitHub.html#create-app-in-github","title":"Create app in GitHub","text":""},{"location":"getting-started/integrate-source-code/GitHub.html#initial-setup","title":"Initial setup","text":"<ol> <li>Open GitHub, navigate to the GitHub Apps page in the Developer Settings for your account/organization, and click New GitHub App.</li> <li>You can either create the App in an individual user account or within an organization account:    </li> <li>Give your app a name and homepage URL (these are only used for informational purposes within GitHub):    </li> <li>Paste your Webhook URL and secret from Spacelift:    </li> <li> <p>Set the following Repository permissions:</p> Permission Access Checks Read &amp; write Commit statuses Read &amp; write Contents Read-only Deployments Read &amp; write Metadata Read-only Pull requests Read &amp; write Webhooks Read &amp; write </li> <li> <p>Set the following Organization permissions:</p> Permission Access Members Read-only </li> <li> <p>Subscribe to the following events:</p> <ul> <li>Organization</li> <li>Pull request</li> <li>Pull request review</li> <li>Push</li> <li>Repository</li> </ul> </li> <li>Choose whether you want to allow the App to be installed on any account or only the current account, then click Create GitHub App:     </li> </ol>"},{"location":"getting-started/integrate-source-code/GitHub.html#generate-key","title":"Generate key","text":"<ol> <li>Copy the App ID in the About section:     </li> <li>Scroll down to the Private keys section of the page and click Generate a private key:          This will download the private key file for your GitHub app named <code>&lt;app-name&gt;.&lt;date&gt;.private-key.pem</code> (for example: <code>spacelift.2025-05-11.private-key.pem</code>).</li> </ol>"},{"location":"getting-started/integrate-source-code/GitHub.html#copy-api-details-into-spacelift","title":"Copy API details into Spacelift","text":"<p>Now that your GitHub App has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>API host URL: Enter the URL to your GitHub server, which should be https://api.github.com.</li> <li>User facing host URL: Enter the URL that will be shown to the user and displayed in the Spacelift UI. This will be the same as the API host URL unless you are using VCS Agents, in which case it should be <code>private://&lt;vcs-agent-pool-name&gt;</code>.</li> <li>App ID: Enter the App ID you copied before generating the private key.</li> <li>Private key: Paste the contents of your private key file.     </li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Click Set up to save your integration settings.</li> </ol>"},{"location":"getting-started/integrate-source-code/GitHub.html#install-the-github-application","title":"Install the GitHub application","text":"<p>Once your GitHub app has been created and configured in Spacelift, you can install it on one or more accounts or organizations you have access to.</p> Via Spacelift UIVia GitHub UI <ol> <li> <p>On the Integrations &gt; Github page, click Install the app:</p> <p></p> </li> <li> <p>On GitHub, click Install.</p> </li> <li> <p>Choose whether you want to allow Spacelift access to all repositories or only specific ones in the account:</p> <p></p> </li> <li> <p>Click Install to link your GitHub account to Spacelift.</p> </li> </ol> <ol> <li>Find your Spacelift app on the GitHub Apps page in your account settings, and click Edit:     </li> <li>In the Install App section, click Install next to the account you want Spacelift to access:     </li> <li> <p>Choose whether you want to allow Spacelift access to all repositories or only specific ones in the account:</p> <p></p> </li> <li> <p>Click Install to link your GitHub account to Spacelift.</p> </li> </ol> <p>\u2705 Step 1 of the LaunchPad is complete! Now you can connect your cloud account.</p> <p></p>"},{"location":"getting-started/integrate-source-code/GitLab.html","title":"GitLab","text":""},{"location":"getting-started/integrate-source-code/GitLab.html#use-gitlab-as-your-source-code-provider","title":"Use GitLab as your source code provider","text":"<p>Spacelift supports GitLab as the code source for your stacks and modules.</p> <p>You can set up multiple Space-level and one default GitLab integration per account.</p> <p>Using multiple GitLab accounts</p> <p>If you want to use multiple GitLab accounts, teams, or groups, or connect Spacelift to your GitLab Enterprise instance, you will need to set up separate GitLab integrations (with different access tokens) for each different team or group in GitLab.</p>"},{"location":"getting-started/integrate-source-code/GitLab.html#create-the-gitlab-integration","title":"Create the GitLab integration","text":"<p>GitLab integration details</p> <p>Learn more about setting up and using the GitLab integration on the GitLab source control page.</p>"},{"location":"getting-started/integrate-source-code/GitLab.html#initial-setup","title":"Initial setup","text":"<ol> <li>On the Integrations screen, click View on the GitLab card, then Set up GitLab.     </li> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol>"},{"location":"getting-started/integrate-source-code/GitLab.html#create-an-access-token","title":"Create an access token","text":"<p>Assuming you don't already have an access token at the ready, navigate to your GitLab server (we'll just use <code>gitlab.com</code>) to create one from the Access Tokens section of your User Settings page:</p> <p></p> <ol> <li>Name: Enter a descriptive name for the token.</li> <li>Expires at: We recommend leaving this blank. If set and the token expires before being replaced, Spacelift won't be able to access your GitLab environment.</li> <li>Scopes: Check the <code>api</code> box. While Spacelift will only write commit statuses, merge request comments, and environment deployments, GitLab's permissions require us to take write access on everything.</li> <li>Create the token and copy its details to finish the integration in Spacelift.</li> </ol> <p>Required user access level</p> <p>When creating tokens bound to a GitLab user, the user is required to have \"Maintainer\" level access to any projects you require Spacelift to access.</p>"},{"location":"getting-started/integrate-source-code/GitLab.html#copy-details-into-spacelift","title":"Copy details into Spacelift","text":"<p>Now that your GitLab access token has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>API host URL: Enter the URL of your GitLab server. For SaaS GitLab, this is <code>https://gitlab.com</code>.</li> <li>User facing host URL: Enter the URL that will be shown to the user and displayed in the Spacelift UI. This will be the same as the API host URL unless you are using VCS Agents, in which case it should be <code>private://&lt;vcs-agent-pool-name&gt;</code>.</li> <li>API token: Enter the access token that Spacelift will use to access your GitLab.</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Click Set up to save your integration settings.</li> </ol> <p>Warning</p> <p>Unlike GitHub credentials (which are organization-specific), the GitLab integration uses personal credentials, which makes it more fragile in situations where an individual leaves the organization and deletes the access token. This is a general concern across your environment, not one specific to Spacelift.</p> <p>We recommend you create \"virtual\" (machine) users in GitLab as a source of more stable credentials.</p>"},{"location":"getting-started/integrate-source-code/GitLab.html#set-up-webhooks","title":"Set up webhooks","text":"<p>For every GitLab project being used in Spacelift stacks or modules, you will need to set up a webhook to notify Spacelift about the project changes.</p> <p>Note</p> <p>Default integrations are visible to all users of the account, but only root Space admins can see their details.</p> <p>Space-level integrations will be listed to users with read access to the integration Space. Integration details, however, contain sensitive information (such as the webhook secret) and are only visible to those with admin access.</p> <ol> <li>On the Integrations &gt; GitLab page, click the three dots next to the integration name.</li> <li>Click See details to find the webhook endpoint and webhook secret.     </li> <li>In GitLab, navigate to Settings &gt; Webhooks to create a new webhook.     <ol> <li>URL: Enter the webhook endpoint from Spacelift.</li> <li>Secret Token: Enter the webhook secret from Spacelift.</li> <li>Trigger: Check the Push events, Tag push events, and Merge request events boxes.</li> </ol> </li> <li>Complete the webhook setup in GitLab.</li> </ol> <p>Warning</p> <p>You only need to set up one hook for each repository used by Spacelift, regardless of how many stacks use it. Setting up multiple hooks for a single repo may lead to unintended behavior.</p> <p>\u2705 Step 1 of the LaunchPad is complete! Now you can connect your cloud account.</p> <p></p>"},{"location":"getting-started/invite-teammates.html","title":"Invite teammates to your Spacelift instance","text":""},{"location":"getting-started/invite-teammates.html#invite-teammates-to-your-spacelift-instance","title":"Invite teammates to your Spacelift instance","text":"<p>You have two options for inviting people to your Spacelift account:</p> <ul> <li>Add single users.</li> <li>Add users via policies.</li> </ul> <p>Warning</p> <p>Granting access to individuals is more risky than granting access to only teams and account members. In the latter case, when an account member loses access to your organization, they automatically lose access to Spacelift. But when allowlisting individuals and not restricting access to members only, you'll need to explicitly remove the individuals from your Spacelift login policy.</p>"},{"location":"getting-started/invite-teammates.html#add-single-users","title":"Add single users","text":"<ol> <li>From the LaunchPad, click Invite teammates.<ul> <li>Alternatively, click your name in the bottom left, then Organization settings.</li> </ul> </li> <li>In the \"Collaborate with your team\" section:<ol> <li>Email: Enter the email address of the user to add.</li> <li>Role: Select the user's role, admin or user.</li> <li>Click Send invite.</li> </ol> </li> </ol>"},{"location":"getting-started/invite-teammates.html#add-users-via-policies","title":"Add users via policies","text":"<ol> <li>Click your name in the bottom left, then Organization settings, then Management strategy.</li> <li>Beside login policy, click Enable, then Enable in the pop-up window.</li> <li>Click the Login policy tab, then click Create policy.</li> <li>Name: Enter a name for your policy. Choose a name that explains who or what the policy grants access to.</li> <li>Labels: Organize policies by assigning labels to them.</li> <li>Click Continue.</li> <li>Fill in the policy code through one of these options:<ul> <li>Review the provided policy code and remove comments from pieces you want to use.</li> <li>Copy and paste (and edit) one of the examples provided that matches the identity provider you used to sign up for the Spacelift account.</li> </ul> </li> <li>Click Create.</li> </ol>"},{"location":"getting-started/invite-teammates.html#policy-examples","title":"Policy examples","text":"GitHubGitLab, Google, Microsoft <p>This example uses GitHub usernames to grant access to Spacelift.</p> <pre><code>package spacelift\n\nadmins  := { \"alice\" }\nallowed := { \"bob\", \"charlie\", \"danny\" }\nlogin   := input.session.login\n\nadmin { admins[login] }\nallow { allowed[login] }\ndeny  { not admin; not allow }\n</code></pre> <p>Tip</p> <p>GitHub organization admins are automatically Spacelift admins. There is no need to grant them permissions in the Login policy.</p> <p>This example uses email addresses to grant access to Spacelift.</p> <pre><code>package spacelift\n\nadmins  := { \"alice@example.com\" }\nallowed := { \"bob@example.com\" }\nlogin   := input.session.login\n\nadmin { admins[login] }\nallow { allowed[login] }\n# allow { endswith(input.session.login, \"@example.com\") } Alternatively, grant access to every user with an @example.com email address\ndeny  { not admin; not allow }\n</code></pre> <p>Now your colleagues can access your Spacelift account as well.</p> <p>\u2705 Step 4 of the LaunchPad is complete! Now you can explore and configure Spacelift as needed. Consider triggering your first stack run, or creating a policy or a context.</p> <p></p>"},{"location":"installing-spacelift/changelog.html","title":"Changelog","text":""},{"location":"installing-spacelift/changelog.html#changelog","title":"Changelog","text":""},{"location":"installing-spacelift/changelog.html#changes-between-v360-and-v350","title":"Changes between v3.6.0 and v3.5.0","text":""},{"location":"installing-spacelift/changelog.html#features","title":"Features","text":"<ul> <li>VCS Integrations: The \\\"Use Git checkout\\\" field is now visible by default in all VCS integration details pages. This field indicates whether the integration uses git checkout to download source code (required for sparse checkout functionality).</li> <li>Dashboard: The Dashboard is now accessible to all users, not just admins. Non-admin users can view most dashboard widgets, with the Launch Pad and User Activity widgets remaining admin-only.</li> <li>Filters: Enhanced filtering interface with improved selection states, dropdown functionality, and visual styling for better user experience</li> <li>Personal Settings: You can now find new \"Spaces\" view under your personal settings. This view lets you see the permissions you have for each space, making it easier to understand your access across Spacelift.</li> <li>Added SSO SAML attribute mapping support. See the custom attribute mapping documentation for more information.</li> <li>Errors and panics are now logged to stdout in a structured JSON format, providing better observability.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v350-and-v340","title":"Changes between v3.5.0 and v3.4.0","text":""},{"location":"installing-spacelift/changelog.html#features_1","title":"Features","text":"<ul> <li>Run details now display both the API key ID and name when triggered by an API key. Previously only the key ID was shown, which made it difficult to identify which key was used. The key ID is shown in shortened format. For example: <code>api::01K56PK::DevopsSpaceKey</code>.</li> <li>Added Samples section in policy view, which offers powerful filtering options to help quickly locate relevant samples</li> <li>Role Based Access Control is now available for self-hosted distributions</li> <li>CloudFormation Drift Detection: Added native drift detection for CloudFormation stacks using AWS's DetectStackDrift API. Shows field-level differences for modified resources and flags deleted resources across root and nested stacks. Note: Auto-reconciliation is not supported for CloudFormation as AWS does not provide reconciliation capabilities through their API.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v340-and-v330","title":"Changes between v3.4.0 and v3.3.0","text":""},{"location":"installing-spacelift/changelog.html#features_2","title":"Features","text":"<ul> <li>Support for VCS agent pools for self-hosted distributions: installation instruction</li> <li>OIDC configuration options are now configurable (<code>issuer</code> and <code>jwks_uri</code>)</li> <li>Exposed a new configuration value for the message queue:<ul> <li><code>MESSAGE_PROCESSING_TIMEOUT_SECONDS</code> environment variable controls how long messages can be processed before timing out (default: 900 seconds). Reducing this value (e.g., to 300 seconds) causes long-running messages to return to the queue faster, allowing other messages to be processed.</li> </ul> </li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v330-and-v320","title":"Changes between v3.3.0 and v3.2.0","text":""},{"location":"installing-spacelift/changelog.html#features_3","title":"Features","text":"<ul> <li> <p>Allow machine users to discard runs</p> </li> <li> <p>Allow filtering by stack name in resources view</p> </li> <li> <p>Added a filter to view enabled/disabled stacks in the stack list view.</p> </li> <li> <p>Upgraded to AWS STS v2 and we now use regional endpoints for role assumption. Default region is us-east-1, configurable via integration settings.</p> </li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes","title":"Fixes","text":"<ul> <li>Fixed an issue where a user who locked a stack needed admin access to unlock it.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v320-and-v310","title":"Changes between v3.2.0 and v3.1.0","text":""},{"location":"installing-spacelift/changelog.html#features_4","title":"Features","text":"<ul> <li> <p>Add ability to reset policy flags. It add a comprehensive policy flag reset mechanism with a multi-owner security model. Policies can now reset flags they have set, providing fine-grained control over flag management while preventing malicious policies from hijacking flags set by other policies.</p> </li> <li> <p>Add audit trail for user management login failures.</p> </li> <li> <p>Add Ready as a valid stack state for filtering.</p> </li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_1","title":"Fixes","text":"<ul> <li>Load runtime configs only for the triggered stacks.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v310-and-v300","title":"Changes between v3.1.0 and v3.0.0","text":""},{"location":"installing-spacelift/changelog.html#features_5","title":"Features","text":"<ul> <li>To be able to transfer sensitive outputs between stacks, a new stack-level setting needs to be explicitly enabled</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_2","title":"Fixes","text":"<ul> <li>Fixed an issue where in certain cases (5000+ tasks) Ansible tracked runs could fail at the planning phase.</li> </ul>"},{"location":"installing-spacelift/changelog.html#infrastructure","title":"Infrastructure","text":"<ul> <li>Configurable database backup retention period via the <code>.database.backup_retention_period_days</code> variable in the JSON config file for Cloudformation-based installations.</li> </ul> <pre><code>{\n    [...]\n    \"database\": {\n        \"backup_retention_period_days\": 7\n    }\n}\n</code></pre>"},{"location":"installing-spacelift/changelog.html#changes-between-v261-and-v300","title":"Changes between v2.6.1 and v3.0.0","text":"<p>We\u2019re excited to announce Spacelift Self-Hosted v3! This version represents a significant milestone in Spacelift\u2019s evolution, delivering enhanced flexibility and control to our users.</p>"},{"location":"installing-spacelift/changelog.html#key-highlights","title":"Key Highlights:","text":"<ul> <li>Cloud-agnostic deployment: Spacelift is no longer dependent on AWS services. We've introduced a new Reference Architecture concept that defines the basic requirements for a Self-Hosted installation. This enables Spacelift to run on AWS, Google Cloud Platform, Azure, or even on-premises infrastructure, as long as the infrastructure requirements are met.<ul> <li>While you have the freedom to create your own infrastructure, we've developed four Terraform modules to simplify deployment to AWS ECS, AWS EKS, GCP GKE, and Azure AKS. Comprehensive deployment guides for these modules are available in the guides section.</li> </ul> </li> <li>Simplified infrastructure: The infrastructure footprint is now smaller and more streamlined. External dependencies like IoT brokers and queueing systems are no longer required, as they're now built directly into the Spacelift application.</li> <li>Enhanced first-time setup: The new installation model features a streamlined onboarding experience. When you first access Spacelift, you'll be greeted by a setup wizard that guides you through configuring your account name. You can log into this page with the temporary admin user that also serves as break-glass access in case of SSO integration issues.</li> <li>Greater infrastructure flexibility: Without predefined CloudFormation templates, you have complete freedom to customize the infrastructure according to your specific requirements.</li> <li>Customizable deployment process: You can either customize the deployment process to fit your needs or leverage Spacelift's predefined Terraform modules and Helm charts for standardized deployments.</li> <li>Enhanced observability: Improved monitoring capabilities with added support for Datadog and OpenTelemetry.</li> </ul>"},{"location":"installing-spacelift/changelog.html#licensing-system","title":"Licensing system","text":"<p>Self-Hosted v3 introduces a new licensing method. As we no longer rely on AWS License Manager, we've transitioned to a new JWT-based license token system. A Spacelift representative will provide you with a license token to activate your v3 instance. The license token has a defined validity period and requires renewal upon expiration.</p>"},{"location":"installing-spacelift/changelog.html#usage-reporting","title":"Usage reporting","text":"<p>We support two ways of usage reporting: automatic and manual. You can enable automatic reporting by setting the <code>SPACELIFT_PUBLIC_API</code> variable to <code>https://app.spacelift.io</code>. In case your network policies prevent Spacelift from reaching our public API, you can still use the manual reporting method. More information about usage reporting is available in the general configuration reference.</p>"},{"location":"installing-spacelift/changelog.html#migration-from-cloudformation-to-opentofuterraform","title":"Migration from CloudFormation to OpenTofu/Terraform","text":"<p>We've developed a comprehensive migration toolkit to facilitate the transition from the CloudFormation-based deployment to the OpenTofu/Terraform-based process. A detailed overview of the migration process is available in the migration guide.</p> <p>Prerequisites: The migration toolkit requires a working installation of <code>v2.6.0</code> or later. You can also upgrade to <code>v3.0.0</code> with CloudFormation and later switch to the new Terraform modules.</p>"},{"location":"installing-spacelift/changelog.html#artifacts-and-installation-package","title":"Artifacts and installation package","text":"<p>Currently, the installation artifacts remain unchanged. The package continues to include CloudFormation templates and shell scripts, allowing you to deploy Spacelift Self-Hosted v3 using existing methods. However, we recommend migrating to the new Terraform modules, as they offer greater flexibility and customization options for your deployment process.</p> <p>Note: We will continue to support the CloudFormation approach and will provide customers with notice well in advance before discontinuing support for this installation method.</p>"},{"location":"installing-spacelift/changelog.html#changes-between-v260-and-v261","title":"Changes between v2.6.0 and v2.6.1","text":"<p>This release fixes an issue that could cause certain upgrades of existing Self-Hosted installations to fail. The issue was caused by one of our data migrations attempting to access the <code>allow_non_root_admin_space_creation</code> column on the <code>accounts</code> table before it existed.</p> <p>This failure could happen if the following scenario was true:</p> <ul> <li>An upgrade was being performed on a self-hosted instance running v2.3.0 of Self-Hosted or older.</li> <li>The version being upgraded to was v2.4.0 or higher.</li> <li>An audit trail webhook was enabled.</li> </ul> <p>In this situation, the upgrade would fail with the following error messages:</p> <p>could not run DB migrations and tasks: could not run migrations: could not migrate the database: failed to update encryption slug for webhook 1: a database constraint for which we have no friendly error message has been violated, we've been notified, reach out to us or wait until we add an error message if you want to know more</p> <p>And:</p> <p>unexpected database error: ERROR: column \\\"allow_non_root_admin_space_creation\\\" of relation \\\"accounts\\\" does not exist (SQLSTATE 42703): ERROR: column \\\"allow_non_root_admin_space_creation\\\" of relation \\\"accounts\\\" does not exist (SQLSTATE 42703)</p> <p>In this scenario, running the v2.6.1 installer should resolve the problem.</p>"},{"location":"installing-spacelift/changelog.html#changes-between-v250-and-v260","title":"Changes between v2.5.0 and v2.6.0","text":""},{"location":"installing-spacelift/changelog.html#features_6","title":"Features","text":"<ul> <li>Ignored run warnings: ever wondered why didn't your stack trigger? We've added a new tab on the stack page - Ignored runs - that shows you all the ignored runs for the last 7 days. Take a look at the documentation for more details.</li> <li>Aggregated VCS checks: if you have multiple stacks tracking the same repository, you can enable the Aggregate VCS checks feature in the VCS integration's settings (Source code menu) which will allow you to group all the checks from the same commit into a predefined set of checks, making it easier to see the overall status of the commit. Documentation: Azure DevOps, GitHub, GitLab, Bitbucket Datacenter, Bitbucket Cloud.</li> <li>Added native ServiceNow integration, allowing teams to provision and manage infrastructure directly from ServiceNow using Spacelift Blueprints. This integration enables automated stack creation via ServiceNow catalog items, while maintaining governance through Business Rules and REST Messages. The feature is still in beta.</li> <li>Space management improvements: added a new account-level toggle that allows non-root admins to manage child spaces within their scope. When enabled, non-root admins can create and manage subspaces (with inheritance enforced), while root admins retain control over inheritance settings and overall topology.</li> </ul>"},{"location":"installing-spacelift/changelog.html#infrastructure_1","title":"Infrastructure","text":"<ul> <li>Scheduler service: cron jobs are now triggered via a new ECS service called scheduler. This has no impact on the app functionality, but those who use custom VPCs will need to provide a new config value under <code>vpc_config</code> called <code>scheduler_security_group_id</code>. Important: the database security group must be updated as well since the scheduler service needs to access the database. So for custom VPC installations, the required updates are the following:<ul> <li>creating a new security group for the scheduler service<ul> <li>with no ingress</li> <li>an egress record to the database security group</li> </ul> </li> <li>updating the database security group ingress to allow connections from the scheduler security group</li> <li>please see the advanced installations page for code examples</li> </ul> </li> <li>Disable XRay: if you wish to disable telemetry in the backend, you can do so by setting the <code>tracing_enabled</code> configuration value to <code>false</code> in the install script's config file.</li> </ul> <pre><code>{\n    [...]\n    \"account_name\": \"&lt;account-name&gt;\",\n    \"aws_region\": \"&lt;region&gt;\",\n    \"tracing_enabled\": false\n    [...]\n}\n</code></pre> <ul> <li>Enabled AZ rebalancing in the ECS services: availability zone (AZ) rebalancing in Amazon ECS helps maintain high availability by automatically redistributing tasks across multiple AZs when an imbalance is detected, launching tasks in underutilized zones and stopping them in overloaded ones, all while minimizing manual intervention.</li> <li>Cloudwatch log driver mode is now in set to <code>non-blocking</code> with 25MB buffer size: in the default blocking mode, if CloudWatch Logs is unreachable, container logging can block stdout/stderr and potentially halt the app. To improve resilience, we're opting for non-blocking mode, where logs are buffered in memory (up to max-buffer-size) instead of blocking app execution. If the buffer fills up, logs are dropped, but the app stays available.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v240-and-v250","title":"Changes between v2.4.0 and v2.5.0","text":""},{"location":"installing-spacelift/changelog.html#features_7","title":"Features","text":"<ul> <li>Built-in Audit Trails - this aims to provide a faster, easier way to identify and fix issues without needing to reach out for support.</li> </ul>"},{"location":"installing-spacelift/changelog.html#internal","title":"Internal","text":"<ul> <li>We've increased the default Postgres version from 13.7 to 13.16 because AWS has deprecated 13.7. This is a minor update that takes around 5 minutes and doesn't cause any outages.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v230-and-v240","title":"Changes between v2.3.0 and v2.4.0","text":""},{"location":"installing-spacelift/changelog.html#features_8","title":"Features","text":"<ul> <li>MFA - you can now enforce MFA for all users in your organization</li> <li>Stack list redesign - we've revamped the stack list view to make it easier to navigate and manage your stacks</li> <li>Policy templates: quickly create and customize policies using predefined templates</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_3","title":"Fixes","text":"<ul> <li>Fixed an issue where certain API calls failed for newer Bitbucket Datacenter versions (8.19+)</li> <li>Added an option to configure the <code>limit</code> query parameter of the compare commits Bitbucket Datacenter API call. It is set by the <code>BITBUCKET_AFFECTED_CHANGES_PER_PAGE_FILES</code> environment variable.</li> </ul>"},{"location":"installing-spacelift/changelog.html#internal_1","title":"Internal","text":"<ul> <li>The DB migrations will run during the application startup, instead of the installation script initiating it</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v220-and-v230","title":"Changes between v2.2.0 and v2.3.0","text":""},{"location":"installing-spacelift/changelog.html#features_9","title":"Features","text":"<ul> <li>Ansible reloaded! We've shipped the following improvements for our Ansible integration:<ul> <li>We've added the ability to specify a custom runtime config in YAML format. Click the dropdown button next to the <code>Trigger</code> button and choose <code>Trigger with custom runtime config</code>.</li> <li>The final runtime configuration (including all custom configurations) can be retrieved for all runs. Use the 3 dots in the top right corner of the run details view and choose <code>Runtime config details</code>.</li> <li>In the run changes views, ansible tasks are listed as separate resources.</li> <li>We've added a new <code>Configuration Management</code> tab to the stack view. This view allows you to display the last status of each item in your Ansible inventory (i.e. the outcome of the last run).</li> <li>We've added logs of task executions to the task details tab.</li> </ul> </li> <li>Dashboard - Spacelift's new default view is the dashboard, which provides a high-level overview of your stacks, runs, drift detection schedules, as well as some basic metrics (stack failures, run duration etc.).</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v211-and-v220","title":"Changes between v2.1.1 and v2.2.0","text":""},{"location":"installing-spacelift/changelog.html#features_10","title":"Features","text":"<ul> <li>Our bulk actions feature has been reworked, making it simpler to perform actions on multiple items at once.</li> <li>We've added support for OIDC Based API Keys to provide a more secure way of accessing our API without requiring static credentials.</li> <li>We've added the ability to add custom headers to your audit trail webhooks, making it easier than ever to integrate with external systems.</li> <li>We've added an option to <code>config.json</code> to set <code>load_balancer.subnet_placement</code> to either public (default) or private. If the option is omitted, the default remains public.</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_4","title":"Fixes","text":"<ul> <li>We've added a new check that automatically fails the run if a Kubernetes worker pod exits without properly marking the run as finished or failed.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v210-and-v211","title":"Changes between v2.1.0 and v2.1.1","text":""},{"location":"installing-spacelift/changelog.html#fixes_5","title":"Fixes","text":"<ul> <li>Use <code>boostMatches</code> when trying to find a branch in Bitbucket Datacenter. This fixes an issue where we could fail to find the stack's tracked branch if there were too many other branches in the repo with a similar name (for example <code>development</code>, <code>development-123</code>, etc). The exact number of similar branches depends on your Bitbucket configuration, but defaults to 25.</li> <li>Fixed an issue with the Raw Git integration that caused a <code>server does not support exact SHA1 refspec</code> error to be returned when using a Git server that doesn't support cloning with an exact SHA1 refspec.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v200-and-v210","title":"Changes between v2.0.0 and v2.1.0","text":"<p>Warning</p> <p>You must upgrade to v2.0.0 before installing v2.1.0. If you attempt to upgrade an existing installation running a version older than v2.0.0, the installer will report an error and exit without making any changes.</p> <p>If you are currently on a version older than v2.0.0 and don't have access to the v2.0.0 installer anymore, please reach out to our support team for new download links.</p>"},{"location":"installing-spacelift/changelog.html#features_11","title":"Features","text":"<ul> <li>The stack and module settings have been revamped, making it simpler and more efficient to manage your stacks and modules. One of the standout additions is the new Scheduling and Policies tabs, which have moved from the stack settings to the main stack view. Your schedules and policies are now more easily discoverable, and can also be viewed by team members who don\u2019t have access to stack settings.</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_6","title":"Fixes","text":"<ul> <li>We've fixed an issue with our initial Disaster Recovery support that meant that IoT policies would not be added to your secondary region when resetting an existing worker pool that was created before DR was configured. The impact of this would have been that workers from the affected pools would not be able to connect to your IoT broker after failing over. No user intervention is required to resolve this - the v2.1.0 installation process will fix any affected worker pools.</li> <li>We've fixed an issue affecting Bitbucket Data Center that prevented branches being retrieved correctly if the repository contained more than 25 branches that contained your stack's branch name as part of their name (for example if a stack has a tracked branch called <code>dev</code>, and other branches exist in the repository like <code>development</code>, <code>dev-1</code>, <code>dev-2</code>, etc). This could prevent the stack settings for an affected stack from being saved.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v130-and-v200","title":"Changes between v1.3.0 and v2.0.0","text":"<p>Warning</p> <p>This release of Self-Hosted involves mandatory downtime during the installation process. We expect that downtime to be between 5 and 10 minutes, but it's important that you do not start the installation at a time you cannot afford Spacelift to be unavailable.</p> <p>v2.0.0 introduces multi-region failover support to Self-Hosted to help as part of a disaster recovery process. One of these changes involves converting the single-region KMS key used to encrypt sensitive data like stack and context secrets to a multi-region key. In order to do that, the installation includes a migration to convert the data from one key to another.</p> <p>The migration is performed inside a transaction, and in the case of any errors the changes will be rolled back to avoid a situation where data is encrypted using both old and new keys.</p> <p>Although we have safeguards in place to ensure the migration is successful, we recommend taking a snapshot of your RDS cluster before performing the installation in case anything goes wrong.</p>"},{"location":"installing-spacelift/changelog.html#features_12","title":"Features","text":"<ul> <li>Added the ability to provide a custom database connection string during install/upgrade. This allows you to take full control over the database used by Spacelift.</li> <li>Added multi-region disaster recovery support.</li> <li>Added support for OpenTofu 1.8.0.</li> <li>Various other small features and improvements.</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_7","title":"Fixes","text":"<ul> <li>Fixed a misconfiguration that was causing the server logs to be filled with messages containing <code>failed to record HTTP transaction</code>.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v121-and-v130","title":"Changes between v1.2.1 and v1.3.0","text":""},{"location":"installing-spacelift/changelog.html#features_13","title":"Features","text":"<ul> <li>Added ability to view spaces as a list view, you can now switch between Diagram and List view using the toggle in the page header</li> <li>Added Account details drawer (under user menu) with self hosted version, license information and identity provider data.</li> <li>Updated documentation links to use the currently used self hosted version immediately</li> <li>Added list view customization</li> <li>Added stack settings scheduling and policies tabs</li> <li>Added module list and form redesign</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_8","title":"Fixes","text":"<ul> <li>Fix: Raw Git does not work with terraform modules</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v120-and-v121","title":"Changes between v1.2.0 and v1.2.1","text":""},{"location":"installing-spacelift/changelog.html#fixes_9","title":"Fixes","text":"<ul> <li>Fix for an issue where a commit to Bitbucket Datacenter could trigger more stacks than necessary</li> <li>Fix for an issue where crashed workers left runs in a hanging state</li> <li>If custom certificates are defined, Spacelift's internal HTTP client will use those for AWS-related requests as well</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v110-hotfix1-and-v120","title":"Changes between v1.1.0-hotfix.1 and v1.2.0","text":""},{"location":"installing-spacelift/changelog.html#features_14","title":"Features","text":"<ul> <li>Added OpenTofu support for Terragrunt<ul> <li>Important note: in order to use this new feature, you need to recycle your worker pools. This is because new launcher versions are downloaded during the instance startup, and the old launchers do not support this new feature. Note: we recommend recycling the worker pools after each release anyway. The native Kubernetes workers are an exception to this rule since each run starts a new container running the latest launcher image for your Self-Hosted instance.</li> </ul> </li> <li>Added <code>Trigger always</code> flag to Stack Dependencies</li> <li>Disabled the rate limiting for policy sampling</li> <li>Added LaunchPad, a dashboard for new Spacelift users that provides a guided tour of the platform</li> <li>Added support for OPA v0.64</li> <li>Support for moved and imported Terraform resources</li> <li>Installation script:<ul> <li>We added support for defining custom retention periods for all of the S3 buckets. If you don't specify it, they remain untouched.</li> </ul> </li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_10","title":"Fixes","text":"<ul> <li>Fixed a bug where some of the runs weren't scheduled because we attempted to checkout the same license from License Manager at the same time in parallel. Now license checkouts are serialized to avoid this issue.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v110-and-v110-hotfix1","title":"Changes between v1.1.0 and v1.1.0-hotfix.1","text":""},{"location":"installing-spacelift/changelog.html#fixes_11","title":"Fixes","text":"<ul> <li>Fixed an issue where license check-out could fail when multiple runs were scheduled at the same time</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v100-and-v110","title":"Changes between v1.0.0 and v1.1.0","text":""},{"location":"installing-spacelift/changelog.html#features_15","title":"Features","text":"<ul> <li>Beta Terragrunt support</li> <li>Enhanced VCS integrations</li> <li>OpenTofu v1.6.2 support</li> <li>New run history view</li> <li>Redesigned stack creation view</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_12","title":"Fixes","text":"<ul> <li>Various backend and frontend fixes and improvements</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v0012-and-v100","title":"Changes between v0.0.12 and v1.0.0","text":""},{"location":"installing-spacelift/changelog.html#features_16","title":"Features","text":"<ul> <li>User Management</li> <li>Terraform Provider Registry</li> <li>The settings page is now split into Organization and Personal settings</li> <li>OpenTofu v1.6.1 support</li> <li>PR stack locking</li> <li>Support for deploying workers via the Kubernetes operator</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_13","title":"Fixes","text":"<ul> <li>Improved license check-out logic</li> <li>Fix stale logs display for targeted replans</li> <li>Allow to persist roles and collections installed during run initialization for Ansible stacks automatically</li> <li>Various other backend and frontend fixes and improvements</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v0011-and-v0012","title":"Changes between v0.0.11 and v0.0.12","text":""},{"location":"installing-spacelift/changelog.html#features_17","title":"Features","text":"<ul> <li>OpenTofu v1.6.0 support</li> <li>PRs as notification targets</li> <li>Run prioritization through Push Policy (<code>prioritize</code> keyword)</li> <li>Add state size (in bytes) to <code>ManagedStateVersion</code> type in GraphQL</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_14","title":"Fixes","text":"<ul> <li>Various backend and frontend fixes and improvements</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v0010-and-v0011","title":"Changes between v0.0.10 and v0.0.11","text":""},{"location":"installing-spacelift/changelog.html#features_18","title":"Features","text":"<ul> <li>New stack creation view</li> <li>Auto Attaching Contexts</li> <li>Context Hooks</li> <li>Additional project globs</li> <li>Pull request default behaviour change<ul> <li>Spacelift will start handling pull request events and creating proposed runs if no push policy is set as the default behaviour</li> </ul> </li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_15","title":"Fixes","text":"<ul> <li>Various backend and frontend fixes and improvements</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v009-and-v0010","title":"Changes between v0.0.9 and v0.0.10","text":""},{"location":"installing-spacelift/changelog.html#features_19","title":"Features","text":"<ul> <li>Stack Dependencies with output/input references.</li> <li>Ready run state.</li> <li>Targeted replan support.</li> <li>New detailed terraform changes view.</li> <li>Worker Pool Management views.</li> <li>Add OpenTofu and custom workflows support for terraform.</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_16","title":"Fixes","text":"<ul> <li>Do not re-create SAML certificate during each install</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v008-and-v009","title":"Changes between v0.0.8 and v0.0.9","text":""},{"location":"installing-spacelift/changelog.html#features_20","title":"Features","text":"<ul> <li>Increase worker default disk size to 40GB.</li> <li>Adding support for Terraform versions up to v1.5.7.</li> <li>Update frontend and backend to the latest versions.</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_17","title":"Fixes","text":"<ul> <li>Enforce bucket policy to prevent objects getting fetched not using HTTPS.</li> <li>Updated no account ID message to indicate that it is caused by missing AWS credentials in the install script.</li> </ul>"},{"location":"installing-spacelift/changelog.html#changes-between-v007-and-v008","title":"Changes between v0.0.7 and v0.0.8","text":""},{"location":"installing-spacelift/changelog.html#features_21","title":"Features","text":"<ul> <li>Update CloudFormation worker pool template to allow a custom instance role to be provided.</li> <li>Update CloudFormation worker pool template to allow poweroff on crash to be disabled to aid debugging.</li> <li>Update CloudFormation worker pool template to allow custom user data to be provided.</li> <li>Update frontend and backend to the latest versions.</li> <li>Adding support for Terraform versions up to v1.5.4 and kubectl up to v1.27.4.</li> <li>Added support for External Dependencies.</li> <li>Added support for Raw Git source code provider.</li> </ul>"},{"location":"installing-spacelift/changelog.html#removals","title":"Removals","text":"<ul> <li>Remove the unused <code>ecs-state-handler</code> Lambda.</li> </ul>"},{"location":"installing-spacelift/changelog.html#fixes_18","title":"Fixes","text":"<ul> <li>Improve warning message during installation when changeset contains no changes.</li> <li>Fix role assumption and automatic ECR login in GovCloud regions.</li> <li>Don't incorrectly attempt to report errors to Bugsnag in Self-Hosting (errors were never reported, but this could cause some misleading log entries).</li> <li>Fix crash on run startup if the runner image was missing the <code>ps</code> command.</li> <li>Increase default worker pool size to <code>t3.medium</code>.</li> <li>Increase minimum drain instances to 3 to provide more resilience.</li> </ul>"},{"location":"installing-spacelift/install-methods.html","title":"Install Methods","text":""},{"location":"installing-spacelift/install-methods.html#install-methods","title":"Install Methods","text":"<p>We currently have two supported methods of installing Spacelift:</p> <ul> <li>Using our reference architecture approach (recommended) .</li> <li>Using our CloudFormation approach.</li> </ul> <p>For new installations we recommend following the reference architecture approach. This provides much more flexibility to customers, and also supports installation in environments outside of AWS. Customers who are currently using the CloudFormation approach can continue to do so.</p> <p>If you wish to switch from the Cloudformation-based installation to the reference architecture, check out the migration guide.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html","title":"Advanced Installations","text":""},{"location":"installing-spacelift/cloudformation/advanced-installations.html#advanced-installations","title":"Advanced Installations","text":""},{"location":"installing-spacelift/cloudformation/advanced-installations.html#custom-vpc","title":"Custom VPC","text":"<p>In certain situations you may want to have full control of the network that Spacelift runs in, and the default VPC and security groups created by Spacelift don't fit your circumstances. In these situations, you can create your own networking components and supply them during the installation process.</p> <p>If you choose to do this, you will need to create the following resources:</p> <ul> <li>A VPC.</li> <li>A set of private subnets.</li> <li>A set of public subnets (the same subnets IDs can be used if you don't need separate private and public subnets).</li> <li>Security groups for the various Spacelift components.</li> </ul> <p>The following sections explain the requirements for each component.</p> <p>Also, see the section on HTTP Proxies if you need to use a proxy to allow the Spacelift components to make HTTP requests.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#vpc","title":"VPC","text":"<p>The VPC needs to have a CIDR block large enough to run the Spacelift server, drain and scheduler instances along with the database and a few other networking components. We would recommend using a minimum network prefix of <code>/27</code>.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#private-subnets","title":"Private Subnets","text":"<p>The number of private subnets depends on the number of availability zones you want to deploy Spacelift to.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#public-subnets","title":"Public Subnets","text":"<p>The public subnets are used to place the load balancer for the Spacelift server. If you don't need separate public and private subnets you can use the same subnets for both. Just use the same subnet IDs to populate both the private and public subnet configuration options.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#security-groups","title":"Security Groups","text":"<p>Security groups need to be created for the following components:</p> <ul> <li>Drain.</li> <li>Server.</li> <li>Scheduler.</li> <li>Load Balancer.</li> <li>Installation Task.</li> <li>Database.</li> </ul> <p>The next sections explain the requirements of each group.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#drain","title":"Drain","text":"<p>Needs to be able to access the following components:</p> <ul> <li>Your VCS system (e.g. GitHub, GitLab, etc).</li> <li>Various AWS APIs.</li> <li>The Spacelift database.</li> </ul> <p>Our default CloudFormation template for the drain security group looks like the following, and allows unrestricted egress (for accessing VCS systems):</p> <pre><code>DrainSecurityGroup:\n  Type: AWS::EC2::SecurityGroup\n  Properties:\n    GroupName: \"drain_sg\"\n    GroupDescription: \"The security group for the Spacelift async-processing service\"\n    SecurityGroupEgress:\n      - Description: \"Unrestricted egress\"\n        FromPort: 0\n        ToPort: 0\n        IpProtocol: \"-1\"\n        CidrIp: \"0.0.0.0/0\"\n    VpcId: {Ref: VPC}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#scheduler","title":"Scheduler","text":"<p>Needs to be able to access the following components:</p> <ul> <li>The AWS SQS API.</li> <li>The Spacelift database.</li> </ul> <p>Our default CloudFormation template for the scheduler security group looks like the following, and allows unrestricted egress:</p> <pre><code>SchedulerSecurityGroup:\n  Type: AWS::EC2::SecurityGroup\n  Properties:\n    GroupName: \"scheduler_sg\"\n    GroupDescription: \"The security group for the Spacelift scheduler service\"\n    SecurityGroupEgress:\n    - Description: Unrestricted egress\n      FromPort: 0\n      ToPort: 0\n      IpProtocol: \"-1\"\n      CidrIp: \"0.0.0.0/0\"\n    VpcId:\n      Ref: {Ref: VPC}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#server","title":"Server","text":"<p>Needs to be able to access the following components:</p> <ul> <li>Your VCS system (e.g. GitHub, GitLab, etc).</li> <li>Your identity provider for SSO.</li> <li>Various AWS APIs.</li> <li>The Spacelift database.</li> </ul> <p>The server also needs to allow ingress from its load balancer.</p> <p>Our default CloudFormation template for the server security group looks like the following, and allows unrestricted egress along with ingress via the load balancer:</p> <pre><code>ServerSecurityGroup:\n  Type: AWS::EC2::SecurityGroup\n  Properties:\n    GroupName: \"server_sg\"\n    GroupDescription: \"The security group for the Spacelift HTTP server\"\n    SecurityGroupEgress:\n      - Description: \"Unrestricted egress\"\n        FromPort: 0\n        ToPort: 0\n        IpProtocol: \"-1\"\n        CidrIp: \"0.0.0.0/0\"\n    SecurityGroupIngress:\n      - Description: \"Only accept HTTP connections on port 1983 from the load balancer\"\n        FromPort: 1983\n        ToPort: 1983\n        IpProtocol: \"tcp\"\n        SourceSecurityGroupId: {Ref: LoadBalancerSecurityGroup}\n    VpcId: {Ref: VPC}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#load-balancer","title":"Load Balancer","text":"<p>The load balancer needs to be able to accept traffic from any clients (e.g. users logging into Spacelift via a browser or using <code>spacectl</code>), and also needs to accept incoming webhooks from your VCS system. In addition it needs to be able to access the server container.</p> <p>Our default CloudFormation template for the load balancer security group looks like the following:</p> <pre><code>LoadBalancerSecurityGroup:\n  Type: AWS::EC2::SecurityGroup\n  Properties:\n    GroupName: \"load_balancer_sg\"\n    GroupDescription: \"The security group for the load balancer sitting in front of the Spacelift HTTP server\"\n    SecurityGroupIngress:\n      - Description: \"Accept HTTPS connections on port 443\"\n        FromPort: 443\n        ToPort: 443\n        IpProtocol: \"tcp\"\n        CidrIp: \"0.0.0.0/0\"\n    VpcId: {Ref: VPC}\n\nLoadBalancerToServerEgress:\n  Type: AWS::EC2::SecurityGroupEgress\n  Properties:\n    Description: \"Allow the server load balancer to access server app containers\"\n    DestinationSecurityGroupId: {Ref: ServerSecurityGroup}\n    FromPort: 1983\n    ToPort: 1983\n    IpProtocol: \"tcp\"\n    GroupId: {Ref: LoadBalancerSecurityGroup}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#installation-task","title":"Installation Task","text":"<p>The installation task security group allows one-off tasks that are run during the installation process to access the Spacelift database. Our default CloudFormation template looks like the following:</p> <pre><code>InstallationTaskSecurityGroup:\n  Type: AWS::EC2::SecurityGroup\n  Properties:\n    GroupName: \"installation_task_sg\"\n    GroupDescription: \"The security group for tasks that run as part of the installation process\"\n    SecurityGroupEgress:\n      - Description: \"Unrestricted egress\"\n        FromPort: 0\n        ToPort: 0\n        IpProtocol: \"-1\"\n        CidrIp: \"0.0.0.0/0\"\n    VpcId: {Ref: VPC}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#database","title":"Database","text":"<p>The database security group needs to allow inbound access from the server, drain, scheduler and installation tasks. Our default CloudFormation template looks like the following:</p> <pre><code>DatabaseSecurityGroup:\n  Type: AWS::EC2::SecurityGroup\n  Properties:\n    GroupName: \"database_sg\"\n    GroupDescription: \"The security group defining what services can access the Spacelift database\"\n    SecurityGroupIngress:\n      - Description: \"Only accept TCP connections on appropriate port from the drain\"\n        FromPort: 5432\n        ToPort: 5432\n        IpProtocol: \"tcp\"\n        SourceSecurityGroupId: {Ref: DrainSecurityGroup}\n      - Description: \"Only accept TCP connections on appropriate port from the server\"\n        FromPort: 5432\n        ToPort: 5432\n        IpProtocol: \"tcp\"\n        SourceSecurityGroupId: {Ref: ServerSecurityGroup}\n      - Description: \"Only accept TCP connections on appropriate port from the scheduler\"\n        FromPort: 5432\n        ToPort: 5432\n        IpProtocol: \"tcp\"\n        SourceSecurityGroupId: {Ref: SchedulerSecurityGroup}\n      - Description: \"Only accept TCP connections on appropriate port from the installation tasks\"\n        FromPort: 5432\n        ToPort: 5432\n        IpProtocol: \"tcp\"\n        SourceSecurityGroupId: {Ref: InstallationTaskSecurityGroup}\n    VpcId: {Ref: VPC}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#performing-a-custom-vpc-installation","title":"Performing a custom VPC installation","text":"<p>To install Spacelift into a custom VPC, create your VPC along with all the other required components like security groups, then edit the <code>vpc_config</code> section of your config.json file, making sure to set <code>use_custom_vpc</code> to <code>true</code>. A correctly populated <code>vpc_config</code> will look like this:</p> <pre><code>{\n    \"vpc_config\": {\n        \"use_custom_vpc\": true,\n        \"vpc_id\": \"vpc-091e6f4d35908e7c1\",\n        \"private_subnet_ids\": \"subnet-01a25f47c5a7e94fc,subnet-035169be40fbfbbbf,subnet-09c72e8ab5499eed1\",\n        \"public_subnet_ids\": \"subnet-08aa756ab626d690f,subnet-0d03bb49f32922d93,subnet-0d85c4a80db226099\",\n        \"drain_security_group_id\": \"sg-045061f7120343acd\",\n        \"load_balancer_security_group_id\": \"sg-086a38a75894c4fc5\",\n        \"server_security_group_id\": \"sg-0cb943fd285fc5c85\",\n        \"scheduler_security_group_id\": \"sg-0ef464fd834fc5a43\",\n        \"installation_task_security_group_id\": \"sg-03b9b0e17cce91d3a\",\n        \"database_security_group_id\": \"sg-0b67dd8ad00e237fd\",\n        \"availability_zones\": \"eu-west-1a,eu-west-1b,eu-west-1c\"\n    }\n}\n</code></pre> <p>Once you have populated your configuration, just run the installer as described in the installation guide.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#http-proxies","title":"HTTP Proxies","text":"<p>If you need to use an HTTP proxy to allow the Spacelift components to access the internet, you can specify this via the config.json file:</p> <pre><code>{\n    \"proxy_config\": {\n        \"http_proxy\": \"\",\n        \"https_proxy\": \"\",\n        \"no_proxy\": \"\"\n    }\n}\n</code></pre> <p>These three settings correspond to the <code>HTTP_PROXY</code>, <code>HTTPS_PROXY</code> and <code>NO_PROXY</code> environment variables, respectively. These environment variables are automatically added to the Spacelift ECS containers during installation when values are specified in the configuration file.</p> <p>Any variable that isn't populated will not be added. For example, if you use the following configuration all three environment variables will be added to the containers:</p> <pre><code>{\n    \"proxy_config\": {\n        \"http_proxy\": \"http://my.http.proxy\",\n        \"https_proxy\": \"https://my.https.proxy\",\n        \"no_proxy\": \"safe.domain\"\n    }\n}\n</code></pre> <p>However if you only need to specify the <code>HTTPS_PROXY</code> environment variable you can use the following configuration:</p> <pre><code>{\n    \"proxy_config\": {\n        \"http_proxy\": \"\",\n        \"https_proxy\": \"https://my.https.proxy\",\n        \"no_proxy\": \"\"\n    }\n}\n</code></pre> <p>NOTE: you must include the protocol with your proxy URL (e.g. <code>http://</code> or <code>https://</code>), otherwise the proxy configuration can fail to parse and prevent the Spacelift ECS services from starting correctly.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#using-a-tls-connection-between-the-application-load-balancer-and-spacelift-server","title":"Using a TLS connection between the Application Load Balancer and Spacelift Server","text":"<p>The Spacelift server is served over TLS by default. This is achieved by using an AWS Application Load Balancer (ALB) to terminate the TLS connection and forward the request to the Spacelift server running in an ECS container. That is through HTTP.</p> <pre><code>client -&gt; (https) -&gt; Load Balancer -&gt; (http) -&gt; ECS\n</code></pre> <p>If you want the server to use HTTPS as well, you can do so by specifying the TLS certificate in <code>config.json</code>. <code>tls_config.server_certificate_secrets_manager_arn</code> is the ARN of the SecretsManager secret that holds both the private and public keys of your TLS certificate in the following format:</p> <pre><code>{\"privateKey\": \"&lt;base64-encoded-private-key&gt;\", \"publicKey\": \"&lt;base64-encoded-public-key&gt;\"}\n</code></pre> <p>Example <code>config.json</code>:</p> <pre><code>{\n    \"tls_config\": {\n        \"server_certificate_secrets_manager_arn\": \"arn:aws:secretsmanager:eu-west-1:123456789012:secret:spacelift-server-tls-cert-123456\",\n        \"ca_certificates\": []\n    }\n}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#using-custom-ca-certificates","title":"Using custom CA certificates","text":"<p>If you use a custom certificate authority to issue TLS certs for components that Spacelift will communicate with, for example your VCS system, you need to provide your custom CA certificates. You can provide these via the <code>config.json</code> file via the <code>tls_config.ca_certificates</code> property:</p> <pre><code>{\n    \"tls_config\": {\n        \"server_certificate_secrets_manager_arn\": \"\",\n        \"ca_certificates\": [\n          \"&lt;base64-encoded certificate 1&gt;\",\n          \"&lt;base64-encoded certificate 2&gt;\"\n        ]\n    }\n}\n</code></pre> <p>Please note that each certificate should be base64-encoded and formatted onto a single line. For example, if we had the following certificate:</p> <pre><code>-----BEGIN CERTIFICATE-----\nMIIFsTCCA5mgAwIBAgIUDD/4VBfLx5K/tAY+SckH05TJ8i8wDQYJKoZIhvcNAQEL\nBQAwaDELMAkGA1UEBhMCR0IxETAPBgNVBAgMCFNjb3RsYW5kMRAwDgYDVQQHDAdH\nbGFzZ293MRkwFwYDVQQKDBBBZGFtIEMgUm9vdCBDQSAxMRkwFwYDVQQDDBBBZGFt\nIEMgUm9vdCBDQSAxMB4XDTIzMDMxMzExMzYxMVoXDTI1MTIzMTExMzYxMVowaDEL\nMAkGA1UEBhMCR0IxETAPBgNVBAgMCFNjb3RsYW5kMRAwDgYDVQQHDAdHbGFzZ293\nMRkwFwYDVQQKDBBBZGFtIEMgUm9vdCBDQSAxMRkwFwYDVQQDDBBBZGFtIEMgUm9v\ndCBDQSAxMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAxjv/+sInXiQ+\n2Fb+itF8ndlpmmYUoZwYN4dx+2wrcbOVngTvy4sE+33nGBzH4vt4pOhKTWwaYXFI\n0CzqoIoazi8Zl0medyrwtIUDZ1pNcVugb4KAFb9Jbq40Ik3xG6t16maxQJGTiAG2\n/xVtsuYdhnBGx//61SEbEwSpR145/Qf1cba8RlRQMz4QUWNe8XXo3SYaX2kxiw2V\n1Op+fQxg2jf1AyzQXX1ch1jyG5RLESPUMFkBiQwi7LOSCaavfJEUzwqeoORgd7Ti\nuyMV+4Gsb1XAnK7KXYwisGeP5/QNFPAByfAdPjR20rMYYHfxqEDth4Najjmu/iyF\nPGk4CobRhitTtJXT/QxWcvtrRu1BCVnedyESMyiya4Q9dn27rFjjg3ZARqWOZhyq\nOTWHo2mO2FzEJuxhvYNe2iYVp2s8wMTB02nP3wpWoYwje2yDwcjkIl8uXKzEZ9Gf\nFATJaCLoO8o5J2HXsgOIqXlpzU9tUtEew/xTzZqX5A34o8/+NgUtm0F7joWa5mDC\nQB7L8cKfACydfpekJx/gFUGSy/5vdfBzOczc6Bmh66yHPBRDcgyDFnnx34m/XVQa\nrBwwIDDbqu3sscdOgm9v8csCJd0YlXGb/x4oAA61IITnsNd9NCw0GJIquSEcYiCE\nA0YrQTKVfRAXuhSZ1VPIuxXiF2K3XTMCAwEAAaNTMFEwHQYDVR0OBBYEFD55R4mt\n0hNOJUgPL0JBKZB1jybSMB8GA1UdIwQYMBaAFD55R4mt0hNOJUgPL0JBKZB1jybS\nMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggIBAHecVjMklTkS2Py5\nXNpJ9cdzG66GuPDw8aQZIunrxqYud74CA1Y0K26kyDJkLnWzVa7nT+F0d8Qn3tov\nvFwI3xy5l+4upmuZ3u1jFEMiSk8C2FPohLDnDo3rwEUCGvJ6a4Gas7YyHPGL3DrJ\n0dcu9wsX9cYB2YJ27QosZ5s6zmmUvBGTI30JNvPnSoC7kzqD3ArxvTEW9WaUqoJt\n88lsMnn6+ps9A6exb/fK909ZWaEJWRd9cdMET0fna7EhhkO+Cqz415RgMxlK7ggT\n97CvkjvvLNeFT5naHbzUANqfMVRRcUaP3PjTC9z5cDo9CaPaFjV/+Uxax2mAlARk\nfqYyWoqvZH90czpvFG1jUo6P4NpyxZS8layJwD24qX+EON43WYApLsl/jE2A/JmQ\nMdgWNhOy4HP8U8+aANr0Ev7gWWNi6VcR8T6PT/rbAGjnPmVmoZ4rc7CdoS8ZQZJh\nK8ELA17+pnMTgo7wxfARqL+p+mqgtUxRbiWitev8F2hUVB/SwP8hpcGrdhTEN7td\npSW1ykPeGJFKSBo5QHanqqPFCzqtFeoL9DhYx5/xE6FpKMLg3vVcFsHu6glS8iMV\n4Hvb2fXuhXxLTBCbD1+5lLP/bHXogQKmp2H6Oj0e6WBmQ0xqGou4Il6bavsZCx2v\nADWvlue5jXdNu5xPZdsNVNAluAne\n-----END CERTIFICATE-----\n</code></pre> <p>We would base64-encode it and then use a tls_config.json file that looks something like the following:</p> <pre><code>{\n    \"ServerCertificateSecretsManagerArn\": \"\",\n    \"CACertificates\": [\n      \"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\"\n    ]\n}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#installing-into-a-private-vpc","title":"Installing into a Private VPC","text":"<p>If you want to install Spacelift into a private VPC that does not have any internet access, you need to setup VPC endpoints for the following AWS services:</p> <ul> <li>ECR and ECR Docker endpoint.</li> <li>IoT.</li> <li>KMS.</li> <li>License manager.</li> <li>Logs.</li> <li>Monitoring.</li> <li>S3.</li> <li>Secrets manager.</li> <li>SQS.</li> <li>Xray.</li> </ul> <p>In addition, if you want to deploy a worker pool into a VPC with no internet access and are using our CloudFormation template to deploy the pool, you need to create a VPC endpoint for the following service:</p> <ul> <li>EC2 autoscaling.</li> </ul> <p>The following sections contain example CloudFormation definitions describing each endpoint that needs to be created.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#proxy-configuration","title":"Proxy Configuration","text":"<p>As well as setting up VPC endpoints, you will also need to configure an HTTP Proxy. This is required because AWS does not currently provide an endpoint for the IoT control plane, meaning that these requests cannot use a private VPC endpoint.</p> <p>When using VPC endpoints, you should include the following in your <code>NO_PROXY</code> environment variable to ensure that requests to the required AWS services are routed via your VPC endpoints rather than via the proxy (making sure to substitute <code>&lt;region&gt;</code> with your install region):</p> <pre><code>s3.&lt;region&gt;.amazonaws.com,license-manager.&lt;region&gt;.amazonaws.com,a3mducvsqca9re-ats.iot.&lt;region&gt;.amazonaws.com,logs.&lt;region&gt;.amazonaws.com,monitoring.&lt;region&gt;.amazonaws.com,sqs.&lt;region&gt;.amazonaws.com,xray.&lt;region&gt;.amazonaws.com,secretsmanager.&lt;region&gt;.amazonaws.com,kms.&lt;region&gt;.amazonaws.com,ecr.&lt;region&gt;.amazonaws.com,api.ecr.&lt;region&gt;.amazonaws.com\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#vpc-endpoint-security-group","title":"VPC Endpoint Security Group","text":"<p>You need to provide a security group for the VPC endpoints to use. This security group should allow inbound access on port 443. For example you could use something like the following:</p> <pre><code>VPCEndpointSecurityGroup:\n  Type: AWS::EC2::SecurityGroup\n  Properties:\n    GroupName: \"vpc_endpoint_sg\"\n    GroupDescription: \"The sg to use for VPC endpoints\"\n    SecurityGroupIngress:\n      - Description: \"Allow inbound HTTPS access to VPC endpoints from VPC\"\n        FromPort: 443\n        ToPort: 443\n        IpProtocol: \"tcp\"\n        CidrIp: \"&lt;replace-with-your-own-address-range&gt;\"\n    VpcId: {Ref: VPC}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#ecr-and-ecr-docker-endpoint","title":"ECR and ECR Docker endpoint","text":"<p>The following VPC endpoints need to be created:</p> <pre><code>ECRInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.ecr.api\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n\nECRDockerInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.ecr.dkr\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#iot","title":"IoT","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>IoTInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.iot.data\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1}\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: false\n</code></pre> <p>Note that the <code>PrivateDnsEnabled</code> option is set to false. For the IoT data endpoint you need to manually create the correct DNS entry to allow Spacelift workers to connect to the IoT broker for your account.</p> <p>First, run the following command to find the correct IoT endpoint for your region:</p> <pre><code>aws iot describe-endpoint --endpoint-type iot:Data-ATS --region &lt;region&gt; --no-cli-pager --output json\n</code></pre> <p>This should output something like the following:</p> <pre><code>{\n    \"endpointAddress\": \"b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com\"\n}\n</code></pre> <p>Next, go to the Route53 console, and create a private hosted zone for your endpoint address. In the example above, this would be <code>b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com</code>.</p> <p>Finally, create an A record for your endpoint address (for example <code>b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com</code>), and use an alias to point it at your IoT VPC endpoint.</p> <p>NOTE: make sure that you create your private hosted zone for your full endpoint address, and not for <code>iot.&lt;region&gt;.amazonaws.com</code>. If you create a hosted zone for <code>iot.&lt;region&gt;.amazonaws.com</code> it will prevent the Spacelift server and drain processes from being able to access the IoT control plane.</p>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#kms","title":"KMS","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>KMSInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.kms\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#license-manager","title":"License manager","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>LicenseManagerInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.license-manager\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#logs","title":"Logs","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>LogsInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.logs\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#monitoring","title":"Monitoring","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>MonitoringInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.monitoring\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#s3","title":"S3","text":"<p>The following VPC endpoint needs to be created. Also note that each of your Spacelift subnets will also need a route table attached that can be referenced in the endpoint definition:</p> <pre><code>S3GatewayEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.s3\"}\n    VpcEndpointType: Gateway\n    VpcId: {Ref: VPC}\n    RouteTableIds:\n      # Attach a route table corresponding to each of the subnets being used for Spacelift\n      - {Ref: PrivateSubnet1RouteTable}\n      - {Ref: PrivateSubnet2RouteTable}\n      - {Ref: PrivateSubnet3RouteTable}\n    PolicyDocument:\n      Version: 2012-10-17\n      Statement:\n        - Effect: Allow\n          Principal: '*'\n          Action: '*'\n          Resource: '*'\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#secrets-manager","title":"Secrets manager","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>SecretsManagerInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.secretsmanager\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#sqs","title":"SQS","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>SQSInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.sqs\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#xray","title":"Xray","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>XrayInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.xray\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n</code></pre>"},{"location":"installing-spacelift/cloudformation/advanced-installations.html#ec2-autoscaling","title":"EC2 Autoscaling","text":"<p>The following optional VPC endpoint can be created if using our CloudFormation worker pool template:</p> <pre><code>AutoscalingInterfaceEndpoint:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcEndpointType: Interface\n    ServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.autoscaling\"}\n    VpcId: {Ref: VPC}\n    SubnetIds:\n      - {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\n    SecurityGroupIds:\n      - {Ref: VPCEndpointSecurityGroup}\n    PrivateDnsEnabled: true\n</code></pre>"},{"location":"installing-spacelift/cloudformation/cloudformation-to-opentofu-terraform-migration.html","title":"CloudFormation to OpenTofu/Terraform migration","text":""},{"location":"installing-spacelift/cloudformation/cloudformation-to-opentofu-terraform-migration.html#cloudformation-to-opentofuterraform-migration","title":"CloudFormation to OpenTofu/Terraform migration","text":"<p>If you seek to migrate from the Cloudformation-based installation to the reference architecture, we developed a small migration toolkit to help you with the process.</p> <p>The end result will be similar to the ECS guide with the following differences:</p> <ul> <li>the message queue will remain SQS as opposed to Postgres</li> <li>the MQTT server will remain IoT Core as opposed to the built-in server</li> </ul> <p>The migration process does not require any downtime. It pulls up a new application (ECS) environment in parallel to the existing one so that you can test it before switching over.</p> <p>Note</p> <p>The most important resources are the persistence layer: the S3 buckets and the RDS cluster. Both of those resources are protected by default. S3 buckets cannot be removed as long as they have objects in them and the RDS cluster has deletion protection enabled by default.</p> <p>In short, the migration process looks like this:</p> <ul> <li>the script ensures the prerequisites are met (you're at least on Self-Hosted v2.6.0 version)</li> <li>the Python script will read your Cloudformation stacks and generate an equivalent Terraform project, which will include various of import statements to import your Cloudformation-managed infrastructure into Terraform</li> <li>the Terraform code will also create a brand new load balancer and an ECS cluster which will connect to the exact same AWS resources as the old one</li> <li>once you have confirmed the new ECS cluster's services are looking good, you can point your website's DNS to the new load balancer</li> <li>you can manually scale down the services' desired count to 0 in the old ECS cluster</li> <li>ideally, you should test this new environment for a few days before deleting the old one</li> <li>once you are happy with the new environment, you can delete the old one by running the <code>delete_cf_stacks.py</code> Python script which will make sure to retain the required resources and delete the rest</li> <li>there are a few leftover resources that you can delete manually</li> </ul> <p>Warning</p> <p>Be extra careful when applying the Terraform changes. Look out for resource deletion and replacements. Note that we have some minor differences in the new Terraform modules compared to the old Cloudformation templates (different wording in the ECR lifecycle policy, slightly different S3 retention policy, different tags for the gateways etc.) but those are completely safe. On the other hand, you definitely don't want to see RDS instance or subnet replacements.</p>"},{"location":"installing-spacelift/cloudformation/cloudformation-to-opentofu-terraform-migration.html#cloudformation-stack-deletion-approach","title":"CloudFormation stack deletion approach","text":"<p>One of the reasons the Cloudformation stack deletion requires such a complex Python script is a Cloudformation quirk. Cloudformation can retain resources when deleting a stack, but only if the stack is in the <code>DELETE_FAILED</code> state. We're using a workaround to achieve this: first, attempt to delete the stack with a \"weak\" IAM role that doesn't have permissions to delete the resources. This will put the stack in the <code>DELETE_FAILED</code> state. Afterwards, we can delete the stack with an administrator IAM role that has permissions to delete the resources. At this point, we can specify the retained resources. The script will delete both temporary IAM roles (weak, admin) after it is done.</p> <p>The exact steps can be found in the README of the repository.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html","title":"Disaster Recovery","text":""},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#disaster-recovery","title":"Disaster Recovery","text":"<p>Spacelift Self-Hosted installations support multi-region disaster recovery (DR). This allows you to cope with an outage of an entire AWS region by failing over to a secondary region.</p> <p>At a high level, the architecture of a multi-region DR installation of Spacelift looks like the following:</p> <p></p> <p>The main points to be aware of are:</p> <ul> <li>Two instances of Self-Hosted are deployed to two different AWS regions.</li> <li>S3 replication between the primary and secondary regions is used to automatically replicate things like state files, run logs and module definitions. The replication is bidirectional.</li> <li>Both regions connect to the same database instance (although only one instance needs write access at any one point in time).</li> <li>In the case of a failover, you can either update DNS records to switch from pointing at the primary to the secondary region, or you can use something like CloudFront to switch over without relying on DNS.</li> </ul> <p>The rest of this page explains how to install a DR instance of Self-Hosted, how to convert an existing installation to be capable of failover, and an example runbook explaining the failover and failback processes.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#installation","title":"Installation","text":""},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#prerequisites","title":"Prerequisites","text":"<p>To install a DR-capable version of Self-Hosted, you must meet the following requirements:</p> <ul> <li>A Postgres 13.7 or higher database capable of being used in multiple regions, and the Self-Hosted instance configured to use a custom DB connection string.<ul> <li>NOTE: the database does not need to be active-active. It just needs to be capable of being used by the failover instance in the case of a regional failover (for example, Aurora Global).</li> </ul> </li> <li>Both instances of Self-Hosted need to be able to access your Postgres database. Because of this you will probably want to follow the Advanced Installation guide when setting up your Self-Hosted instance to allow you to configure and have full control over the VPCs that your Self-Hosted instances use.</li> <li>A custom IoT broker domain configuration along with the relevant DNS records configured to allow you to failover to your secondary region.</li> </ul>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#steps","title":"Steps","text":"<p>To setup a DR instance of Spacelift to failover to in the case of an AWS outage in your primary region you can use the following steps:</p> <ol> <li>Deploy your primary region.</li> <li>Deploy your secondary (DR) region.</li> <li>Configure S3 replication in your primary region.</li> </ol> <p>The following sections explain each step in detail, describing how to setup a Self-Hosted installation with a primary region in <code>eu-west-1</code> and a secondary (DR) region in <code>eu-west-3</code>.</p> <p>Warning</p> <p>Please note that the instructions deliberately leave out details like how to setup your DNS records for Self-Hosted because this does not differ from a standard Self-Hosted installation.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#deploying-primary-region","title":"Deploying primary region","text":"<p>Deploy a copy of Self-Hosted as you normally would into your primary region, making sure to configure the following settings:</p> <ul> <li><code>database.connection_string_ssm_arn</code> and <code>database.connection_string_ssm_kms_arn</code> to use a self-managed database capable of being used from multiple regions.</li> <li><code>iot_broker_endpoint</code> pointing at your custom IoT broker domain (for example <code>worker-iot.spacelift.myorg.com</code>).</li> </ul> <p>Once the installation has completed, it will output some information about your instance:</p> <pre><code>Installation info:\n\n  * Load balancer DNS: spacelift-server-1234567890.eu-west-1.elb.amazonaws.com\n  * Launcher container image: 012345678901.dkr.ecr.eu-west-1.amazonaws.com/spacelift-launcher:v2.0.0\n  * Downloads bucket: 012345678901-spacelift-downloads-601776\n  * Encryption primary key ARN: arn:aws:kms:eu-west-1:012345678901:key/mrk-abc92048ad9283944ab23cc22342b43f\n\nS3 bucket replication info:\n\n  * KMS key ARN: arn:aws:kms:eu-west-1:012345678901:key/77218af9-085a-42d7-95b4-3a79be1fb08f\n  * States: arn:aws:s3:::012345678901-spacelift-states-ca9bf8\n  * Run logs: arn:aws:s3:::012345678901-spacelift-run-logs-ca9bf8\n  * Modules: arn:aws:s3:::012345678901-spacelift-modules-ca9bf8\n  * Policy inputs: arn:aws:s3:::012345678901-spacelift-policy-inputs-ca9bf8\n  * Workspaces: arn:aws:s3:::012345678901-spacelift-workspaces-ca9bf8\n</code></pre> <p>The key pieces of information you need for the next step are the Encryption primary key ARN and the information under the S3 bucket replication info section. Take a note of these values.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#deploying-secondary-region","title":"Deploying secondary region","text":"<p>Deploying your secondary region is very similar to deploying your primary region. The main differences are that you will provide some DR-specific config information, and you will also provide different values for the following properties:</p> <ul> <li><code>aws_region</code> - set to the name of your secondary region.</li> <li><code>database.db_cluster_identifier</code> - this is optional, and only required if you want the Spacelift CloudWatch dashboard to be deployed.</li> <li><code>database.connection_string_ssm_arn</code> - points at a secret in your secondary region containing the database connection string.</li> <li><code>database.connection_string_ssm_kms_arn</code> - a key available in your secondary region used to encrypt the database connection string secret.</li> <li><code>load_balancer.certificate_arn</code> - a certificate manager TLS cert available in the secondary region.</li> <li><code>disable_services</code> - set to <code>true</code> in the DR region.</li> </ul> <p>The <code>disaster_recovery</code> section should look something like this:</p> <pre><code>\"disaster_recovery\": {\n  \"is_dr_instance\": true,\n  \"replica_region\": \"eu-west-1\",\n  \"encryption_primary_key_arn\": \"arn:aws:kms:eu-west-1:012345678901:key/mrk-abc92048ad9283944ab23cc22342b43f\",\n  \"s3_bucket_replication\": {\n    \"enabled\": true,\n    \"replica_kms_key_arn\": \"arn:aws:kms:eu-west-1:012345678901:key/77218af9-085a-42d7-95b4-3a79be1fb08f\",\n    \"states_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-states-ca9bf8\",\n    \"run_logs_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-run-logs-ca9bf8\",\n    \"modules_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-modules-ca9bf8\",\n    \"policy_inputs_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-policy-inputs-ca9bf8\",\n    \"workspaces_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-workspaces-ca9bf8\"\n  }\n},\n</code></pre> <ul> <li><code>disaster_recovery.encryption_primary_key_arn</code> is the Encryption primary key ARN output from installing Self-Hosted in your primary region.</li> <li>Make sure that <code>s3_bucket_replication.enabled</code> is set to <code>true</code>, set <code>replica_region</code> to the region that your primary instance is installed into, and populate the rest of the values using the S3 bucket replication info installer output.</li> </ul> <p>So for example, your config file may look similar to this:</p> <pre><code>{\n    \"account_name\": \"test-org\",\n    \"aws_region\": \"eu-west-3\",\n    \"disable_services\": true,\n    \"database\": {\n        \"connection_string_ssm_arn\": \"arn:aws:secretsmanager:eu-west-3:012345678901:secret:spacelift/database-secondary-BAD0vA\",\n        \"connection_string_ssm_kms_arn\": \"arn:aws:kms:eu-west-3:012345678901:key/1a5a0be1-b083-43dc-babf-384766b4446e\"\n    },\n    \"load_balancer\": {\n        \"certificate_arn\": \"arn:aws:acm:eu-west-3:012345678901:certificate/e2f6cfce-4925-4e85-8dcd-dbb2871583cd\",\n        \"ssl_policy\": \"ELBSecurityPolicy-TLS-1-2-2017-01\",\n        \"scheme\": \"internet-facing\"\n    },\n    \"disaster_recovery\": {\n        \"is_dr_instance\": true,\n        \"replica_region\": \"eu-west-1\",\n        \"encryption_primary_key_arn\": \"arn:aws:kms:eu-west-1:012345678901:key/mrk-abc92048ad9283944ab23cc22342b43f\",\n        \"s3_bucket_replication\": {\n          \"enabled\": true,\n          \"replica_kms_key_arn\": \"arn:aws:kms:eu-west-1:012345678901:key/77218af9-085a-42d7-95b4-3a79be1fb08f\",\n          \"states_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-states-ca9bf8\",\n          \"run_logs_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-run-logs-ca9bf8\",\n          \"modules_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-modules-ca9bf8\",\n          \"policy_inputs_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-policy-inputs-ca9bf8\",\n          \"workspaces_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-workspaces-ca9bf8\"\n        }\n    },\n    \"hostname\": \"spacelift.myorg.com\",\n    \"iot_broker_endpoint\": \"worker-iot.spacelift.myorg.com\",\n    ... other settings\n}\n</code></pre> <p>Once you\u2019re ready, run the installer just like you would for the primary region. Just point at the config file for your DR region instead:</p> <pre><code>./install.sh -c config.dr.json\n</code></pre> <p>Once the installer completes, it will output some information that you\u2019ll need to configure S3 replication in your primary instance:</p> <pre><code>Installation info:\n\n  * Load balancer DNS: spacelift-server-987654321.eu-west-3.elb.amazonaws.com\n  * Launcher container image: 012345678901.dkr.ecr.eu-west-3.amazonaws.com/spacelift-launcher:v0.0.1-preview1479\n  * Downloads bucket: 012345678901-spacelift-downloads-24a213\n\nS3 bucket replication info:\n\n  * KMS key ARN: arn:aws:kms:eu-west-3:012345678901:key/583756c0-8666-4676-beae-a0099f7dbd5b\n  * States: arn:aws:s3:::012345678901-spacelift-states-24a213\n  * Run logs: arn:aws:s3:::012345678901-spacelift-run-logs-24a213\n  * Modules: arn:aws:s3:::012345678901-spacelift-modules-24a213\n  * Policy inputs: arn:aws:s3:::012345678901-spacelift-policy-inputs-24a213\n  * Workspaces: arn:aws:s3:::012345678901-spacelift-workspaces-24a213\n\n[2024-07-12T08:00:12+0000] INFO: Spacelift version v2.0.0 has been successfully installed!\n</code></pre> <p>The information you need is all under the S3 bucket replication info section.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#enabling-s3-replication-from-primary-to-secondary","title":"Enabling S3 replication from primary to secondary","text":"<p>At this point, the last remaining thing to configure is S3 replication. This ensures that things like modules, run logs and state files are replicated from your primary to secondary region so that they are available in the case you need to failover.</p> <p>To do this, edit the config file for your primary region and populate the <code>.disaster_recovery.s3_bucket_replication</code> section:</p> <pre><code>{\n    \"account_name\": \"test-org\",\n    \"aws_region\": \"eu-west-1\",\n    \"disaster_recovery\": {\n        \"is_dr_instance\": false,\n        \"replica_region\": \"eu-west-3\",\n        \"s3_bucket_replication\": {\n            \"enabled\": true,\n            \"replica_kms_key_arn\": \"arn:aws:kms:eu-west-3:012345678901:key/583756c0-8666-4676-beae-a0099f7dbd5b\",\n            \"states_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-states-24a213\",\n            \"run_logs_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-run-logs-24a213\",\n            \"modules_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-modules-24a213\",\n            \"policy_inputs_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-policy-inputs-24a213\",\n            \"workspaces_bucket_arn\": \"arn:aws:s3:::012345678901-spacelift-workspaces-24a213\"\n        }\n    },\n    ... other settings\n}\n</code></pre> <p>Once you\u2019ve done this, you can run the installer again to complete your DR setup:</p> <pre><code>./install.sh -c config.json\n</code></pre>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#migrating-from-an-existing-installation-to-one-with-dr","title":"Migrating from an existing installation to one with DR","text":"<p>This section explains how to take an existing Self-Hosted installation, and enable multi-region failover for disaster recovery purposes. For the most part this follows the same process as if you were configuring a DR installation from scratch, but some additional steps will be required to make the database available in multiple regions, and also replicate existing S3 objects.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#install-the-latest-version-of-self-hosted","title":"Install the latest version of Self-Hosted","text":"<p>In case your Self-Hosted installtion predates <code>v2.0.0</code> (eg. <code>v1.x.x</code>), the first step is to install <code>v2.0.0</code>. As part of the installation process any encrypted secret data in your database will be migrated from a single-region to a multi-region KMS key to allow for failover. To do this, just follow the standard installation process for any Self-Hosted version, but with the following caveats:</p> <ol> <li>We strongly advise that you take a snapshot of your RDS cluster before starting the installation. The KMS migration is designed so that if the conversion of any of your secrets fail the entire process will be rolled back, but we still recommend making sure you have an up to date snapshot before starting the process.</li> <li>The upgrade to v2.0.0 involves downtime. This is to ensure that no secrets are accidentally encrypted with the existing single-region key while the migration is taking place. The downtime should be relatively brief (less than 5 minutes).</li> </ol> <p>When you have finished upgrading to the latest version of Self-Hosted, note the Encryption primary key ARN in the Installation info output from the script as well as the information in the S3 bucket replication info section. You will need this information when configuring your DR instance later in this guide.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#migrating-the-database","title":"Migrating the database","text":"<p>Next, migrate your database to a setup capable of being used in multiple regions, and update your Self-Hosted installation to use a custom connection string to access the database.</p> <p>Info</p> <p>We don't provide instructions on how to migrate your database because the exact process would depend on what you want to migrate to. As an example however, you could use an Aurora Global database and convert your existing database cluster into a global cluster capable of failover.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#configure-your-secondary-instance-and-setup-s3-replication","title":"Configure your secondary instance and setup S3 replication","text":"<p>Use the following sections of the installation guide to setup your secondary region as well as S3 replication:</p> <ul> <li>Deploying secondary region.</li> <li>Enabling S3 replication</li> </ul>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#perform-s3-batch-replication","title":"Perform S3 batch replication","text":"<p>At this point, your disaster recovery setup is fully configured, but because S3 replication rules don\u2019t replicate existing objects that were created before the rules were put in place, you need to run some one-time manual batch jobs to copy the existing objects.</p> <p>The following S3 buckets have replication enabled, so you will need to run a batch copy job for each:</p> <ul> <li><code>spacelift-states</code> - contains any Spacelift-managed state files.</li> <li><code>spacelift-run-logs</code> - contains the logs shown for runs in the Spacelift UI.</li> <li><code>spacelift-modules</code> - contains the code for any modules uploaded to your Terraform module registry.</li> <li><code>policy-inputs</code> - contains any policy samples.</li> <li><code>spacelift-workspaces</code> - used to store the temporary data used by in-progress Spacelift runs.</li> </ul>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#creating-a-batch-operations-role","title":"Creating a batch operations role","text":"<p>To perform the batch replication jobs you need to define an IAM role that has permissions on both your source and destination buckets. You can use a policy like the following (although you can also adjust the policy to restrict it to the specific buckets you need to replicate):</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:InitiateReplication\",\n                \"s3:GetReplicationConfiguration\",\n                \"s3:PutInventoryConfiguration\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*\",\n                \"arn:aws:s3:::*/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::myorg-spacelift-replication-reports-bucket\",\n                \"arn:aws:s3:::myorg-spacelift-replication-reports-bucket/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Info</p> <p>Please note that the previous policy allows access to a bucket called <code>myorg-spacelift-replication-reports-bucket</code>. This is to allow batch operations to store a completion report for the job in case you need to investigate any failures. If you want to make use of this functionality you will need to also create a bucket for the reports and replace <code>myorg-spacelift-replication-reports-bucket</code> in the policy above with the name of the bucket you want to use.</p> <p>Create a role with your policy attached, and use the following policy for the role\u2019s trust relationship to allow S3 batch operations to assume the role:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"batchoperations.s3.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#running-batch-replication-for-a-bucket","title":"Running batch replication for a bucket","text":"<p>You will need to perform these steps for each bucket with replication enabled.</p> <p>First, go to Amazon S3 \u2192 Batch Operations in your AWS console and click on Create job:</p> <p></p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#choose-manifest","title":"Choose manifest","text":"<p>On the choose manifest screen, select the Create manifest using S3 Replication configuration option, and then choose the bucket you want to replicate:</p> <p></p> <p>Leave all the other options at their defaults, and click the Next button.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#choose-operation","title":"Choose operation","text":"<p>The only option available on this page is Replicate. Choose it and click Next:</p> <p></p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#configure-additional-options","title":"Configure additional options","text":"<p>On this page, enter a description for your job, for example <code>2024-07-12 - Replicate states bucket</code>.</p> <p>You may also want to enable a completion report. This can be particularly useful in case any objects fail to replicate. If you want to do this, please see the AWS documentation for details on how to configure the correct IAM roles and a bucket for the report.</p> <p>In the permissions section, choose the S3 replication role you created earlier.</p> <p></p> <p>Click Next to review your job configuration.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#review","title":"Review","text":"<p>If everything looks good, click Create job to create the job.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#run-batch-operation","title":"Run batch operation","text":"<p>Once the job is in the Awaiting your confirmation to run status, select it in the list of jobs, and click on Run job:</p> <p></p> <p>On the next screen, review the job configuration, and click on the Run job button at the bottom of the screen.</p> <p>If all goes well, your job should end up in the Completed state once all the objects have successfully been replicated:</p> <p></p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#failover-runbook","title":"Failover runbook","text":"<p>The following two sections explain how to failover to and failback from your secondary region.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#failing-over","title":"Failing over","text":"<p>To failover to your secondary region, use the following steps:</p> <ol> <li>Make sure your database is available for your secondary region to use in read-write mode. For example, failover to your secondary region so that the secondary DB connection is writable.</li> <li>Run the following command to start the services in your secondary region: <code>./start-stop-services.sh -e true -c config.dr.json</code> (where config.dr.json contains the configuration for your secondary region).<ol> <li>Note: it may take a few minutes for the services to start fully.</li> </ol> </li> <li>Update your server DNS to point at the load balancer for your secondary region.</li> <li>Update your IoT broker custom domain DNS to point at the IoT broker endpoint in your secondary region.</li> <li>Restart your workers to allow them to reconnect to the IoT broker in your secondary region. If using Kubernetes workers, restart the controller pod. One way to restart the workers is to click the Cycle button in the UI. Another way is to manually shut down the virtual machines in the cloud provider. Since they're in an auto-scaling group, new ones will be started automatically.</li> <li>Run the following command to stop the services in your primary region: <code>./start-stop-services.sh -e false -c config.json</code> (where config.json contains the configuration for your secondary region).<ol> <li>Note that while you could leave the services running in your primary region, they will begin to fail and go into a crash loop because they will no-longer be able to access the Spacelift database after it is failed over to the secondary region.</li> </ol> </li> </ol> <p>Warning</p> <p>After failing over to the secondary region, you will need to clear your cookies before being able to login to Spacelift.</p>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#failing-back","title":"Failing back","text":"<ol> <li>Make sure your database is available for your primary region to use in read-write mode. For example, failover to your primary region so that the primary DB connection is writable.</li> <li>Run the following command to start the services in your primary region: <code>./start-stop-services.sh -e true -c config.json</code> (where config.json contains the configuration for your primary region).</li> <li>Update your DNS to point at the load balancer for your primary region.</li> <li>Update your IoT broker custom domain DNS to point at the IoT broker endpoint in your primary region.</li> <li>Restart your workers to allow them to reconnect to the IoT broker in your secondary region. If using Kubernetes workers, restart the controller pod.</li> <li>Run the following command to stop the services in your secondary region: <code>./start-stop-services.sh -e false -c config.dr.json</code> (where config.dr.json contains the configuration for your secondary region).<ol> <li>Note that while you could leave the services running in your secondary region, they will begin to fail and go into a crash loop because they will no-longer be able to access the Spacelift database after it is failed back to the primary region.</li> </ol> </li> </ol>"},{"location":"installing-spacelift/cloudformation/disaster-recovery.html#considerations-after-failing-over","title":"Considerations after failing over","text":"<ul> <li>If you need to run the install.sh script against your secondary instance after failing over, make sure to adjust the <code>disable_services</code> property in the config file to false . Otherwise running the installer will stop your Spacelift services.</li> <li>If you plan on permanently switching to the new region, you should also change the <code>is_dr_instance</code> property to <code>false</code> (and set it to <code>true</code> in your other region). This is important so that things like database migrations run successfully when new versions are installed.</li> </ul>"},{"location":"installing-spacelift/cloudformation/install.html","title":"Installation Guide","text":""},{"location":"installing-spacelift/cloudformation/install.html#installation-guide","title":"Installation Guide","text":"<p>This guide contains instructions on installing a self-hosted copy of Spacelift in an AWS account you control.</p>"},{"location":"installing-spacelift/cloudformation/install.html#pre-requisites","title":"Pre-requisites","text":"<p>Before proceeding with the installation, you need to satisfy the following pre-requisites:</p> <ul> <li>You need access to an AWS account you wish to install Spacelift into.</li> <li>You need to choose a hostname that you wish to use for your Spacelift installation, for example <code>spacelift.example.com</code>. This needs to be on a domain that you control and can add DNS records to.</li> <li>You need to create an ACM certificate for your chosen domain in the same account that you want to install Spacelift in.</li> </ul>"},{"location":"installing-spacelift/cloudformation/install.html#requirements","title":"Requirements","text":"<p>The installation process requires the following tools:</p> <ul> <li>A Mac or Linux machine to run the installation script from.</li> <li>A copy of the AWS CLI v2, configured to access the account you wish to install Spacelift into.</li> <li>jq version 1.6.</li> <li>Standard unix utilities including bash, base64, cat, read, openssl.</li> <li>Docker.</li> </ul>"},{"location":"installing-spacelift/cloudformation/install.html#spacelift-infrastructure","title":"Spacelift infrastructure","text":""},{"location":"installing-spacelift/cloudformation/install.html#server-and-drain","title":"Server and drain","text":"<p>The Spacelift infrastructure has two core parts: the server and the drain. The server is a web application that provides a GraphQL API that serves frontend and every HTTPS request in general. The drain is a worker that processes all asynchronous tasks such as VCS webhooks, run scheduling, worker handling, etc. The drain consumes messages from several SQS queues. The server serves requests through a load balancer.</p> <p>As of today, both the server and the drain are ECS services running on Fargate. We have autoscaling set up for the server so ideally you don't need to worry about scaling it.</p> <p></p>"},{"location":"installing-spacelift/cloudformation/install.html#scheduler","title":"Scheduler","text":"<p>Scheduler is a service that is responsible for scheduling cron jobs and other periodic tasks (e.g. drift detection triggering). It uses the database as a basis for scheduling and puts messages into an SQS queue for the drain to consume.</p>"},{"location":"installing-spacelift/cloudformation/install.html#worker-pool-infrastructure","title":"Worker pool infrastructure","text":"<p>A worker pool is a group of workers that can be used to run workloads. During the startup the worker will attempt to connect to the regional AWS IoT Core broker endpoint and register itself. The drain and the server will then be able to communicate with the worker via AWS IoT Core, specifically via MQTT.</p> <p></p>"},{"location":"installing-spacelift/cloudformation/install.html#running-costs","title":"Running Costs","text":"<p>We estimate the baseline running costs of a Spacelift Self-Hosted instance to be around $15 per day (roughly $450 per month). Note that this is with no activity in your account, so your costs may be higher after factoring in things like bandwidth.</p> <p>These baseline costs include all of the resources deployed as part of your Spacelift install, for example Aurora RDS, Fargate cluster, and KMS keys.</p>"},{"location":"installing-spacelift/cloudformation/install.html#installation","title":"Installation","text":"<p>This section explains the installation process for Spacelift. You may also be interested in the following pages that explain how to configure the Slack integration as well as advanced installations and disaster recovery:</p> <ul> <li>Slack integration setup - explains how to configure the Slack integration for your Spacelift instance.</li> <li>Advanced installations - explains how to configure advanced options like providing a custom VPC configuration, or specifying HTTP proxy settings.</li> <li>Disaster recovery - explains how to configure your installation for multi-region failover.</li> </ul>"},{"location":"installing-spacelift/cloudformation/install.html#aws-requirements","title":"AWS Requirements","text":"<p>Before you start the installation process, make sure the following requirements are met:</p> <ul> <li>The region that you wish to install self-hosting in has at least 3 EIPs available. The default quota per region in an AWS account only allows 5 EIPs to be created, so you may need to choose another region or ask for an increase. These EIPs are used as part of NAT Gateways to allow outbound traffic from Spacelift. If you want full control over your networking setup, please see advanced installations.</li> </ul>"},{"location":"installing-spacelift/cloudformation/install.html#accepting-your-license","title":"Accepting your License","text":"<p>When you sign up for self-hosting, a license will be issued to your AWS account via AWS License Manager. Before you can use your license, you need to accept it.</p> <p>Navigate to AWS License Manager in your AWS Console, and go to Granted licenses:</p> <p></p> <p>Note: if this is your first time accessing License Manager, you may need to grant permissions to AWS before you can use it. If this is the case you will automatically be prompted to grant permission.</p> <p>Click on your license ID, and then choose the Accept &amp; activate license option on the license page:</p> <p></p> <p>Follow the instructions on the popup that appears to activate your license:</p> <p></p> <p>That\u2019s it - you\u2019re now ready to proceed with the rest of the installation!</p>"},{"location":"installing-spacelift/cloudformation/install.html#release-archive","title":"Release Archive","text":"<p>Spacelift self-hosted is distributed as an archive containing everything needed to install Spacelift into your AWS account. The archive has the following structure:</p> <ul> <li><code>config.json</code> - the configuration file containing all necessary configuration options. Some options come prepopulated with default values.</li> <li><code>bin</code> - contains binaries including a copy of the launcher built for self-hosting.</li> <li><code>cloudformation</code> - contains CloudFormation templates used to deploy Spacelift.</li> <li><code>container-images</code> - contains container images for running the Spacelift backend as well as a launcher image.</li> <li><code>install.sh</code> - the installation script.</li> <li><code>uninstall.sh</code> - the uninstallation script.</li> <li><code>scripts</code> - contains other scripts called by the installation script.</li> <li><code>version</code> - contains the version number.</li> </ul>"},{"location":"installing-spacelift/cloudformation/install.html#signature-validation","title":"Signature Validation","text":"<p>Along with the release archive, we also provide a SHA-256 checksum of the archive as well as a GPG signature. The fingerprint of our GPG key is <code>380BD7699053035B71D027B173EBA0CF3B3F4A46</code>, and you can import it using the following command:</p> <pre><code>gpg --keyserver hkps://keys.openpgp.org --recv-keys 380BD7699053035B71D027B173EBA0CF3B3F4A46\n</code></pre> <p>You can verify the integrity of the release archive using the following command:</p> <pre><code>sha256sum -c self-hosted-&lt;version&gt;.tar.gz_SHA256SUMS\n</code></pre> <p>And you can verify the authenticity using the following command:</p> <pre><code>gpg --verify self-hosted-&lt;version&gt;.tar.gz_SHA256SUMS.sig\n</code></pre>"},{"location":"installing-spacelift/cloudformation/install.html#extraction","title":"Extraction","text":"<p>First, extract the release artifacts and move to the extracted directory (replacing <code>&lt;version&gt;</code> with the version you are installing):</p> <pre><code>tar -zxf self-hosted-&lt;version&gt;.tar.gz\ncd self-hosted-&lt;version&gt;\n</code></pre>"},{"location":"installing-spacelift/cloudformation/install.html#configuration","title":"Configuration","text":"<p>The included <code>config.json</code> file provides an easy way to provide additional and required configuration for the resources created during the deployment.</p> <p>The mandatory fields are:</p> <ul> <li><code>account_name</code> - the name of your Spacelift account. Note: the URL of your Spacelift installation doesn't necessarily need to match this name but the account name affects the URL of the module registry.</li> <li><code>aws_region</code> - the AWS region you wish to install Spacelift into.</li> <li><code>load_balancer.certificate_arn</code> - the ARN of the ACM certificate you created in the pre-requisites.</li> <li><code>spacelift_hostname</code> - the hostname you wish to use for your Spacelift installation without the protocol or trailing slash, for example <code>spacelift.mycorp.com</code>.</li> <li><code>sso_config.admin_login</code> - the email address of the user you wish to use as the initial admin user for your Spacelift installation.</li> <li><code>sso_config.sso_type</code> - the type of SSO you wish to use. Valid values are <code>OIDC</code> and <code>SAML</code>.</li> <li><code>sso_config.oidc_args</code> - if <code>sso_type</code> is <code>OIDC</code>, all fields are mandatory:<ul> <li><code>client_id</code> - the OIDC client ID.</li> <li><code>client_credentials</code> - the OIDC client secret.</li> <li><code>identity_provider_host</code> - the OIDC identity provider host with protocol. Example: <code>\"https://mycorp.okta.com\"</code>.</li> </ul> </li> <li><code>sso_config.saml_args</code> - if <code>sso_type</code> is <code>SAML</code>, <code>dynamic</code> and <code>metadata</code> fields are mandatory:<ul> <li><code>dynamic</code> - <code>true</code> or <code>false</code>. if <code>true</code> then <code>metadata</code> must be a URL to the SAML IDP metadata. If <code>false</code> then <code>metadata</code> must be the SAML IDP metadata.</li> <li><code>metadata</code> - either the full SAML IDP metadata or the URL to the SAML IDP metadata.</li> <li><code>name_id_format</code> - SAML name identifier format, can be left empty. Valid values are <code>TRANSIENT</code>, <code>EMAIL_ADDRESS</code> or <code>PERMANENT</code>. Defaults to <code>TRANSIENT</code>.</li> </ul> </li> </ul> <p>NOTE: we recommend that you store your config.json file so that you can reuse it when upgrading to newer Spacelift versions.</p>"},{"location":"installing-spacelift/cloudformation/install.html#optional-configuration-options","title":"Optional configuration options","text":""},{"location":"installing-spacelift/cloudformation/install.html#webhooks-url","title":"Webhooks URL","text":"<p>By default Spacelift if configured to receive webhooks on the URL specified by the <code>spacelift_hostname</code> configuration option. If you want to receive webhooks via a different URL, you can specify the <code>webhooks_endpoint</code> property:</p> <pre><code>{\n  \"spacelift_hostname\": \"spacelift.myorg.com\",\n  \"webhooks_endpoint\": \"webhooks.spacelift.myorg.com\",\n  ... other settings\n}\n</code></pre> <p>Warning</p> <p>Please note, this URL is used by Spacelift to build the webhook endpoints displayed within the Spacelift user interface. This setting does not affect how traffic is routed to your Spacelift instance, and you are responsible for configuring DNS and any other infrastructure required to route the webhooks traffic to your Spacelift server instance.</p>"},{"location":"installing-spacelift/cloudformation/install.html#iot-broker-endpoint","title":"IoT Broker Endpoint","text":"<p>Spacelift uses AWS IoT Core to communicate with workers in order to schedule runs. By default Spacelift will use the IoT broker endpoint for the installation region of your AWS account, but you can also use your own custom IoT broker endpoint, for example <code>worker-iot.spacelift.myorg.com</code>.</p> <p>To configure this, use the following steps:</p> <ol> <li>Follow the AWS documentation to configure a custom IoT broker endpoint in your Spacelift region, and to setup DNS to point your custom domain name at the broker endpoint.</li> <li>Specify the <code>iot_broker_endpoint</code> setting in your config.json file.</li> </ol> <p>Your config.json file should look something like this:</p> <pre><code>{\n  \"spacelift_hostname\": \"spacelift.myorg.com\",\n  \"iot_broker_endpoint\": \"worker-iot.spacelift.myorg.com\",\n  ... other settings\n}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/install.html#load-balancer","title":"Load balancer","text":"<p>Load balancer configuration has a mix of required and optional fields alongside some which should already be prefilled. The object itself looks like this:</p> <pre><code>\"load_balancer\": {\n    \"certificate_arn\": \"\",\n    \"scheme\": \"internet-facing\",\n    \"ssl_policy\": \"ELBSecurityPolicy-TLS-1-2-2017-01\",\n    \"subnet_placement\": \"public\",\n    \"tag\": {\n        \"key\": \"\",\n        \"value\": \"\"\n    }\n}\n</code></pre> <p>The prefilled fields are valid defaults and can be left unchanged, while the <code>certificate_arn</code> field is required and must be set, and the <code>tag</code> object is optional and can be used to set a custom tag against the load balancer resource.</p> <p>The <code>scheme</code> can be either <code>internet-facing</code> or <code>internal</code> and defaults to <code>internet-facing</code>: internal load balancers can only route requests from clients with access to the VPC for the load balancer, while internet-facing load balancers can route requests from clients over the internet.</p> <p><code>ssl_policy</code> is the name of the security policy that defines ciphers and protocols. The default value is <code>ELBSecurityPolicy-TLS-1-2-2017-01</code> which is the most recent security policy that supports TLS 1.2. For more information, see Security policies.</p> <p><code>subnet_placement</code> defines in which subnets a load balancer is launched. It can be set to either <code>public</code>(default) or <code>private</code>.</p>"},{"location":"installing-spacelift/cloudformation/install.html#database","title":"Database","text":"<p>There are two ways to configure the database: either let Spacelift create the RDS cluster for you or you can create and manage it yourself, and just provide the SecretsManager secret ARN to the installer.</p> <p>The possible configuration options are:</p> <ul> <li><code>database.delete_protection_enabled</code> - only for Spacelift-managed databases. Whether to enable deletion protection for the database (defaults to <code>true</code>). Note: <code>uninstall.sh</code> script will disable this option before deleting the database. Leave it empty for self-managed databases.</li> <li><code>database.instance_class</code> - only for Spacelift-managed databases. The instance class of the database (defaults to <code>db.t4g.large</code>). Leave it empty for self-managed databases.</li> <li><code>database.connection_string_ssm_arn</code> - only for self-managed databases. The ARN of the SSM parameter that stores the connection string to the database. Leave it empty for Spacelift-managed databases.</li> <li><code>database.connection_string_ssm_kms_arn</code> - only for self-managed databases. The ARN of the KMS key used to encrypt the SSM parameter. Leave it empty for Spacelift-managed databases.</li> </ul>"},{"location":"installing-spacelift/cloudformation/install.html#self-managed-database","title":"Self-managed database","text":"<p>In case you want to use a self-managed database, your SSM secret needs to have the following format:</p> <pre><code>{\"DATABASE_URL\":\"postgres://&lt;username&gt;:&lt;password&gt;@&lt;rds-url&gt;/&lt;db-name&gt;?statement_cache_capacity=0\"}\n</code></pre> <p>For example:</p> <pre><code>{\"DATABASE_URL\":\"postgres://spacelift:pw@spacelift.cluster-abc123.eu-west-3.rds.amazonaws.com:5432/spacelift?statement_cache_capacity=0\"}\n</code></pre> <p>When choosing an encryption key for the secret, we recommend using the Spacelift Master KMS key. The key is being created by <code>spacelift-infra-kms</code> CloudFormation stack. The ECS tasks will read this secret so the execution role of the ECS task needs to have permissions to decrypt the secret - by default the execution roles have permission to decrypt secrets encrypted with the Spacelift Master KMS key.</p> <p>Note</p> <p>Make sure the Postgres version is the same as the one in the Cloudformation template. In general, there shouldn't be issues with newer versions but it's the safest to use the same major version at least.</p>"},{"location":"installing-spacelift/cloudformation/install.html#going-from-spacelift-managed-to-self-managed-database","title":"Going from Spacelift-managed to self-managed database","text":"<p>The transition from a Spacelift-managed database to a self-managed one is seamless. When using a Spacelift-managed database, the installer saves the connection string into a secret named <code>spacelift/database</code>. However, when migrating to a self-managed one, the installer will remove it.</p> <p>Warning</p> <p>The <code>spacelift/database</code> secret will be deleted during the next installation after <code>connection_string_ssm_arn</code> is populated. Please make sure to create a completely new secret, rather than passing the ARN of the existing <code>spacelift/database</code> secret into your config file, otherwise the upgrade will fail.</p> <p>You must copy the connection string from the existing secret and create your own secret with the same value. Finally, populate <code>connection_string_ssm_arn</code> and <code>connection_string_ssm_kms_arn</code> in the configuration file while making sure that <code>delete_protection_enabled</code> and <code>instance_class</code> are empty (since they are disregarded for self-managed databases):</p> <pre><code>\"database\": {\n    \"delete_protection_enabled\": \"\",\n    \"instance_class\": \"\",\n    \"connection_string_ssm_arn\": \"arn:aws:secretsmanager:eu-west-3:123456789:secret:spacelift/custom-connectionstring-jYx1BH\",\n    \"connection_string_ssm_kms_arn\": \"arn:aws:kms:eu-west-3:123456789:key/12345678-1234-1234-1234-123456789012\"\n}\n</code></pre> <p>The installer detects that <code>connection_string_ssm_arn</code> is set and will stop deploying both database-related Cloudformation stacks (<code>spacelift-infra-db</code>, <code>spacelift-infra-db-secrets</code>). In fact, you can take control of those two stacks and update them as you see fit, the installer will not touch them anymore.</p>"},{"location":"installing-spacelift/cloudformation/install.html#monitoring-alerting","title":"Monitoring &amp; Alerting","text":"<p>As part of the self-hosting installation, we deploy a monitoring dashboard to help you monitor the health of your Spacelift installation. It is accessible via the CloudWatch UI.</p> <p>Additionally, we can also create a few preconfigured alerts for you. You can configure the following options for the CloudWatch alarms:</p> <pre><code>\"alerting\": {\n    \"sns_topic_arn\": \"\"\n}\n</code></pre> <p>If an SNS topic ARN is configured, we'll create an SNS subscription for each alarm. If left empty, we won't create any alarms, only the monitoring dashboard.</p> <p>Important! Your SNS topic's access policy must allow the <code>cloudwatch.amazonaws.com</code> service principal to publish to the topic. An example access policy:</p> <pre><code>{\n  \"Sid\": \"Allow_Publish_Alarms\",\n  \"Effect\": \"Allow\",\n  \"Principal\": {\n    \"Service\": \"cloudwatch.amazonaws.com\"\n  },\n  \"Action\": \"sns:Publish\",\n  \"Resource\": \"&lt;sns-topic-arn&gt;\"\n}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/install.html#global-tags","title":"Global tags","text":"<p>You can add additional tags to all the resources created by the installer by adding your desired tags to the <code>global_resources_tags</code> array in the config.json:</p> <pre><code>\"global_resource_tags\": [\n    {\n        \"key\": \"selfhost\",\n        \"value\": \"spacelift\"\n    }\n]\n</code></pre>"},{"location":"installing-spacelift/cloudformation/install.html#s3-config","title":"S3 Config","text":"<p>You can configure the following options for the S3 buckets, they are all required, but have prefilled values in the config.</p> Bucket name Description <code>run_logs</code> This is where we store the logs of a run. <code>deliveries_bucket</code> Contains webhook and audit trail deliveries. <code>large_queue_messages_bucket</code> SQS has a limitation of message size (256 KiB), we use an S3 bucket to work around it. <code>metadata_bucket</code> Contains metadata for run initialization. <code>policy_inputs_bucket</code> We store policy inputs here - this is used for Policy Sampling. <code>uploads_bucket</code> Used for uploading stack states during stack creation. <code>user_uploaded_workspaces_bucket</code> Used for storing code for the local preview feature. <code>workspaces_bucket</code> The workspaces are stored here for paused runs (eg.: waiting for confirmation). <code>access_logs_bucket</code> Access logs for the load balancer. <pre><code>    \"s3_config\": {\n        \"run_logs_expiration_days\": 60,\n        \"deliveries_bucket_expiration_days\": 1,\n        \"large_queue_messages_bucket_expiration_days\": 2,\n        \"metadata_bucket_expiration_days\": 2,\n        \"policy_inputs_bucket_expiration_days\": 2,\n        \"uploads_bucket_expiration_days\": 1,\n        \"user_uploaded_workspaces_bucket_expiration_days\": 1,\n        \"workspaces_bucket_expiration_days\": 7,\n        \"access_logs_bucket_expiration_days\": 7\n    }\n</code></pre>"},{"location":"installing-spacelift/cloudformation/install.html#usage-data","title":"Usage data","text":"<p>Usage data only involves data related to service operation such as Spacelift object creation and modifications. It does not involve any customer data such as object names, policy bodies, or free text fields.</p> <p>You can decide whether you automatically share usage data via our metrics endpoint, or manually by exporting it via Web UI. To enable automatic usage data reporting simply set the <code>automatically_report_usage_data</code> variable to <code>true</code>.</p> <pre><code>\"automatically_report_usage_data\": true,\n... other settings\n</code></pre>"},{"location":"installing-spacelift/cloudformation/install.html#identity-provider","title":"Identity Provider","text":"<p>Note: When using OIDC, the identity provider must support the <code>email</code> scope. Identity providers that do not support the <code>email</code> scope are unsupported (Ex. GitHub). For more information, check out our docs on SSO.</p>"},{"location":"installing-spacelift/cloudformation/install.html#urls","title":"URLs","text":"<p>You may need certain URLs when configuring an application in your identity provider. For SAML, use the following URLs:</p> <ul> <li>Single Sign-On URL: <code>https://&lt;spacelift-hostname&gt;/saml/acs</code></li> <li>Entity ID (audience): <code>https://&lt;spacelift-hostname&gt;/saml/metadata</code></li> </ul> <p>For OIDC, use the following URL:</p> <ul> <li>Authorized redirect URL: <code>https://&lt;spacelift-hostname&gt;/oidc/exchange</code></li> </ul> <p>NOTE: please make sure to substitute <code>&lt;spacelift-hostname&gt;</code> with the hostname you plan to use for your Spacelift install.</p>"},{"location":"installing-spacelift/cloudformation/install.html#saml-metadata","title":"SAML Metadata","text":"<p>If you are using non-dynamic SAML metadata rather than using a dynamic metadata URL, you need to ensure that the metadata provided is a valid JSON-escaped string. One way to do this is to use <code>jq</code> to escape your metadata. For example, if your metadata was contained in a file called metadata.xml, you could run the following command:</p> <pre><code>cat metadata.xml| jq -R -s '.'\n</code></pre> <p>You would then enter the escaped string into your config.json file:</p> <pre><code>\"saml_args\": {\n    \"metadata\": \"&lt;?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?&gt;&lt;EntityDescriptor ID=\\\"_90756ab2...\",\n    \"dynamic\": false,\n    \"name_id_format\": \"EMAIL_ADDRESS\"\n}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/install.html#updating-identity-provider-during-installs-and-upgrades","title":"Updating identity provider during installs and upgrades","text":"<p>The installer will set up the identity provider the first time you install Spacelift. Subsequent upgrades won't change it even if you have changed the configuration file. This is to prevent accidental changes to the identity provider configuration.</p> <p>There are two way to update an existing identity provider:</p> <ul> <li>Run the <code>update-sso-settings.sh</code> script that you can find below</li> <li>Set <code>sso_config.update_on_install</code> to <code>true</code> in the configuration file. This will force the installer to update the identity provider configuration even during upgrades.</li> </ul>"},{"location":"installing-spacelift/cloudformation/install.html#running-the-installer","title":"Running the installer","text":"<p>This section covers simple installations using a Spacelift-created VPC and a public facing HTTP load balancer. For information about using an existing VPC please see advanced installations.</p> <p>To run the installer, pass in the path of the configuration file:</p> <pre><code>./install.sh [-c \"&lt;configuration file&gt;\"]\n</code></pre> <p>The <code>-c</code> flag is optional, it defaults to <code>config.json</code> if not specified.</p> <p>When the installer starts, it will check it can connect to your AWS account, and will ask for confirmation to continue. Please check the AWS account ID is correct, and if so enter <code>yes</code>:</p> <pre><code>./install.sh\n[2023-01-24T12:17:52+0000] INFO: installing version v0.0.6 of Spacelift into AWS account 123456789012\n\nAre you sure you want to continue? Only 'yes' will be accepted: yes\n</code></pre> <p>Please note, the installation process can take between 10 and 20 minutes to create all of the required infrastructure for Spacelift.</p>"},{"location":"installing-spacelift/cloudformation/install.html#troubleshooting","title":"Troubleshooting","text":"<p>If you see the following error message indicating that the account could not be found, please check the credentials for your AWS CLI are correctly configured:</p> <pre><code>ERROR: could not find AWS account ID. Cannot continue with Spacelift install. Please check that your AWS CLI credentials are correct and have not expired.\n</code></pre> <p>This error message can also be displayed if your AWS credentials are connected to a GovCloud account but a non-GovCloud region has been specified in the <code>aws_region</code> configuration option.</p>"},{"location":"installing-spacelift/cloudformation/install.html#setting-up-dns-entries","title":"Setting up DNS entries","text":"<p>Once the installer has completed, you should see output similar to the following, providing you with the DNS address of the load balancer along with the Launcher container image URL:</p> <pre><code>Installation info:\n\n  * Load balancer DNS: spacelift-server-1234567890.eu-west-1.elb.amazonaws.com\n  * Launcher container image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/spacelift-launcher:v0.0.6\n\n[2023-01-24T11:30:59+0000] INFO: Spacelift version v0.0.6 has been successfully installed!\n</code></pre> <p>Please use the Load balancer DNS to setup a CNAME entry or an A record using an alias. This entry should point from the hostname you want to use for Spacelift (e.g. <code>spacelift.saturnhead.io</code>) to the Spacelift Load Balancer.</p> <p>Once your DNS changes propagate, you should be able to login to your instance by navigating to its hostname (for example <code>https://spacelift.saturnhead.io</code>). Assuming everything has been successful, you should see a welcome screen similar to the following:</p> <p></p>"},{"location":"installing-spacelift/cloudformation/install.html#creating-your-first-worker-pool","title":"Creating your first worker pool","text":"<p>Before you can create stacks or trigger any runs, you need a worker pool. For more information please see our worker pools page.</p>"},{"location":"installing-spacelift/cloudformation/install.html#updating-existing-sso-configuration","title":"Updating existing SSO configuration","text":"<p>If you already have an existing SSO configuration and want to update it, you need to update the <code>sso_config</code> section of the configuration file and run the update script:</p> <pre><code>./scripts/update-sso-settings.sh [-c \"&lt;configuration file&gt;\"]\n</code></pre> <p>It will run the ECS task that will update the SSO configuration.</p>"},{"location":"installing-spacelift/cloudformation/install.html#upgrading","title":"Upgrading","text":"<p>To upgrade to the latest version of self-hosting, follow these steps:</p> <ol> <li>Make sure your config.json file is fully configured to match your existing installation. Ideally you should use the config file from your previous installation.</li> <li>Run <code>./install.sh</code>.</li> <li>Deploy the latest version of the CloudFormation worker pool template, and restart any workers connected to your Spacelift installation to make sure they're running the latest version.</li> </ol>"},{"location":"installing-spacelift/cloudformation/install.html#uninstalling","title":"Uninstalling","text":"<p>If you want to completely uninstall Spacelift, you can use the <code>uninstall.sh</code> script. By default, the script will retain S3 buckets, database and KMS keys so that they can be restored later.</p> <p>To run the uninstall script, use the following command, specifying your AWS region:</p> <pre><code>./uninstall.sh [-c &lt;config-file&gt;] [-f | -n | -h]\n</code></pre> <p>Flags:</p> <ul> <li><code>-c &lt;config-file&gt;</code>: path to the config file (default: <code>config.json</code>)</li> <li><code>-f</code>: force uninstallation, do not prompt for confirmation</li> <li><code>-n</code>: do not retain S3 buckets, database or KMS keys. complete uninstallation.</li> <li><code>-h</code>: show help</li> </ul> <p>For example:</p> <pre><code>./uninstall.sh\n</code></pre>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html","title":"Slack integration setup","text":""},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#slack-integration-setup","title":"Slack integration setup","text":"<p>If you want to use the Slack integration in your self-hosting instance, you need to create your own Slack app and add its details to the config.json file before running the self-hosting installer. This section explains how to do that, along with describing limitations of the Slack integration in self-hosting.</p>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#known-limitations","title":"Known Limitations","text":"<p>The Slack integration relies on Slack being able to communicate with Spacelift in order to provide support for Slash commands. This means that Slack must be able to access your Spacelift load balancer in order for it to be able to make requests to <code>https://&lt;your-spacelift-domain&gt;/webhooks/slack</code>.</p> <p>This means that if you are using an internal load balancer for Spacelift, Slack will not be able to access this endpoint.</p>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#creating-your-slack-app","title":"Creating your Slack app","text":"<p>First, create a new Slack app in your workspace by navigating to https://api.slack.com/apps and following these instructions:</p> <ul> <li>Click \"Create New App\"</li> <li>Choose \"From an app manifest\"</li> <li>Select your workspace</li> <li>Paste following manifest, replacing <code>&lt;your-domain&gt;</code> with the domain you want to host the self-hosted Spacelift instance on.</li> </ul> <pre><code>{\n    \"display_information\": {\n        \"name\": \"Spacelift\",\n        \"description\": \"Taking your infra-as-code to the next level\",\n        \"background_color\": \"#131417\",\n        \"long_description\": \"Spacelift is a sophisticated and compliant infrastructure delivery platform for Terraform (including Terragrunt), Pulumi, CloudFormation, Ansible, and Kubernetes.\\r\\n\\r\\n\u2022 No lock-in. Under the hood, Spacelift uses your choice of Infrastructure as Code providers: open-source projects with vibrant ecosystems and a multitude of existing providers, modules, and tutorials.\\r\\n\\r\\n\u2022 Works with your Git flow. Spacelift integrates with GitHub (and other VCSes) to provide feedback on commits and Pull Requests, allowing you and your team to preview the changes before they are applied.\\r\\n\\r\\n\u2022 Drift detection. Spacelift natively detects drift, and can optionally revert it, to provide visibility and awareness to those \\\"changes\\\" that will inevitably happen.\\r\\n\\r\\n\u2022 Policy as a Code. With Open Policy Agent (OPA) Rego, you can programmatically define policies, approval flows, and various decision points within your Infrastructure as Code flow.\\r\\n\\r\\n\u2022 Customize your runtime. Spacelift uses Docker to run its workflows, which allows you to fully control your execution environment.\\r\\n\\r\\n\u2022 Share config using contexts. Spacelift contexts are collections of configuration files and environment variables that can be attached to multiple stacks.\\r\\n\\r\\n\u2022 Look ma, no credentials. Spacelift integrates with identity management systems from major cloud providers; AWS, Azure, and Google Cloud, allowing you to set up limited temporary access to your resources without the need to supply powerful static credentials.\\r\\n\\r\\n\u2022 Manage programmatically. With the Terraform provider, you can manage Spacelift resources as code.\\r\\n\\r\\n\u2022 Protect your state. Spacelift supports a sophisticated state backend and can optionally manage the state on your behalf.\"\n    },\n    \"features\": {\n        \"bot_user\": {\n            \"display_name\": \"Spacelift\",\n            \"always_online\": true\n        },\n        \"slash_commands\": [\n            {\n                \"command\": \"/spacelift\",\n                \"url\": \"https://&lt;your-domain&gt;/webhooks/slack\",\n                \"description\": \"Get notified about Spacelift events\",\n                \"usage_hint\": \"subscribe, unsubscribe or help\",\n                \"should_escape\": false\n            }\n        ]\n    },\n    \"oauth_config\": {\n        \"redirect_urls\": [\n            \"https://&lt;your-domain&gt;/slack_oauth\"\n        ],\n        \"scopes\": {\n            \"bot\": [\n                \"channels:read\",\n                \"chat:write\",\n                \"chat:write.public\",\n                \"commands\",\n                \"links:write\",\n                \"team:read\",\n                \"users:read\"\n            ]\n        }\n    },\n    \"settings\": {\n        \"event_subscriptions\": {\n            \"request_url\": \"https://&lt;your-domain&gt;/webhooks/slack\",\n            \"bot_events\": [\n                \"app_uninstalled\"\n            ]\n        },\n        \"interactivity\": {\n            \"is_enabled\": true,\n            \"request_url\": \"https://&lt;your-domain&gt;/webhooks/slack\"\n        },\n        \"org_deploy_enabled\": false,\n        \"socket_mode_enabled\": false,\n        \"token_rotation_enabled\": false\n    }\n}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#configuring-the-spacelift-installer","title":"Configuring the Spacelift installer","text":"<p>Once it's done, you just need to copy the relevant information from the \"Basic Information\" tab of your Slack App to the configuration file:</p> <pre><code>{\n    \"slack_config\": {\n        \"enabled\": true,\n        \"client_id\": \"&lt;client id here&gt;\",\n        \"client_secret\": \"&lt;client secret here&gt;\",\n        \"signing_secret\": \"&lt;signing secret here&gt;\"\n    }\n}\n</code></pre> <p>Once you have populated your configuration, just run the installer as described in the installation guide.</p> <p>After the installation script finishes. You can now go to your self-hosted Spacelift instance, to Settings -&gt; Slack and install the Slack app into your workspace.</p>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#updating-the-slack-integration","title":"Updating the Slack integration","text":"<p>To update the Slack integration for your self-hosted Spacelift instance:</p>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#1-modify-the-installer-config","title":"1. Modify the Installer Config","text":"<p>Update the config.json file with the new Slack app details:</p> <pre><code>{\n    \"slack_config\": {\n        \"enabled\": true,\n        \"client_id\": \"&lt;new client id here&gt;\",\n        \"client_secret\": \"&lt;new client secret here&gt;\",\n        \"signing_secret\": \"&lt;new signing secret here&gt;\"\n    }\n}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#2-restart-the-server","title":"2. Restart the Server","text":"<p>After updating the configuration, re-run the install.sh script to apply the changes. Once the installer finishes, please restart the server ECS service as the configuration is loaded during startup.</p> <p>Note: If you are using Slack notification policies, ensure the updated channel IDs are reflected in the policies.</p>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#removing-slack-integration","title":"Removing Slack Integration","text":"<p>To remove the Slack integration from your self-hosted Spacelift instance:</p>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#1-uninstall-the-slack-app","title":"1. Uninstall the Slack App","text":"<p>Remove the Slack app from your workspace by following Slack's guide</p>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#2-update-the-installer-config","title":"2. Update the Installer Config","text":"<p>Set the Slack configuration in the config.json file to enabled: false and clear the other fields:</p> <pre><code>{\n    \"slack_config\": {\n        \"enabled\": false,\n        \"client_id\": \"\",\n        \"client_secret\": \"\",\n        \"signing_secret\": \"\"\n    }\n}\n</code></pre>"},{"location":"installing-spacelift/cloudformation/slack-integration-setup.html#3-restart-the-server","title":"3. Restart the Server","text":"<p>Re-run the install.sh script to apply the updated configuration. Restart the ECS service to ensure the changes are reflected.</p>"},{"location":"installing-spacelift/reference-architecture.html","title":"Introduction","text":""},{"location":"installing-spacelift/reference-architecture.html#introduction","title":"Introduction","text":"<p>This page provides an overview of how to deploy Spacelift into your own environment. Our recommended approach is to deploy Spacelift into a Kubernetes cluster, however it is also possible to run Spacelift in other containerised environments like AWS ECS.</p>"},{"location":"installing-spacelift/reference-architecture.html#requirements","title":"Requirements","text":"<p>Tip</p> <p>We recommend reading the external dependencies page below before proceeding with the installation process and checking specific configurations. The objective is to help you develop a clear understanding of how you intend to run Spacelift, essentially allowing you to customize your setup according to your needs.</p> <p>Please see the external dependencies page for the set of requirements for running Spacelift along with its external dependencies.</p>"},{"location":"installing-spacelift/reference-architecture.html#quick-start","title":"Quick Start","text":"<p>If you just want to get up and running quickly, please check out one of our quick start guides:</p> <ul> <li>Deploying to AKS.</li> <li>Deploying to ECS.</li> <li>Deploying to EKS.</li> <li>Deploying to GKE.</li> <li>Deploying to an on-prem Kubernetes cluster.</li> </ul> <p>Otherwise the rest of this page will provide an overview of the installation process for Spacelift.</p>"},{"location":"installing-spacelift/reference-architecture.html#configuration-reference","title":"Configuration reference","text":"<p>For information about all of the available configuration options, please see the configuration reference.</p>"},{"location":"installing-spacelift/reference-architecture.html#installation-process","title":"Installation process","text":"<p>The main steps required to deploy Spacelift are:</p> <ol> <li>Get your license key and installation materials.</li> <li>Prepare your environment for installation.</li> <li>Deploy Spacelift.</li> <li>Perform first setup.</li> </ol> <p>The following sections explain each of these steps in detail.</p>"},{"location":"installing-spacelift/reference-architecture.html#getting-your-license-key-and-installation-materials","title":"Getting your license key and installation materials","text":"<p>In order to install Spacelift you need a valid license key along with the installation materials containing resources like the required container images. Please reach out to your sales representative for more information.</p>"},{"location":"installing-spacelift/reference-architecture.html#preparing-your-environment","title":"Preparing your environment","text":"<p>Before you can install Spacelift, you need an environment to install it into. Please see the environment requirements page for more details.</p>"},{"location":"installing-spacelift/reference-architecture.html#deploying-spacelift","title":"Deploying Spacelift","text":"<p>There are multiple ways you can deploy Spacelift - we'll ship the Docker images to you and you can choose to deploy them in a way that suits your environment. We also have a number of quick start guides available to get you up and running quickly.</p>"},{"location":"installing-spacelift/reference-architecture.html#first-setup","title":"First setup","text":"<p>Once Spacelift has been deployed to your environment, follow our first setup guide to get your account up and running.</p>"},{"location":"installing-spacelift/reference-architecture.html#upgrading","title":"Upgrading","text":"<p>Spacelift supports zero downtime upgrades unless otherwise mentioned in the changelog. The process for upgrading to a new release is as follows:</p> <ol> <li>Make any required infrastructure adjustments.</li> <li>Push the latest container images to your registry.</li> <li>Deploy the latest Spacelift backend components.</li> <li>Redeploy your workers (note this is not required when using our Kubernetes operator).</li> </ol>"},{"location":"installing-spacelift/reference-architecture/environment-requirements.html","title":"Environment Requirements","text":""},{"location":"installing-spacelift/reference-architecture/environment-requirements.html#environment-requirements","title":"Environment Requirements","text":"<p>Spacelift consists of a number of containerised applications along with certain external dependencies. You are responsible for creating and maintaining the environment that Spacelift runs in.</p> <p>Take the following steps to get your environment ready for running Spacelift:</p> <ol> <li>Choose your hostnames.</li> <li>Create an encryption key for encrypting sensitive data and signing tokens.</li> <li>Configure networking.</li> <li>Deploy Postgres.</li> <li>Deploy object storage.</li> <li>Deploy a runtime environment for Spacelift.</li> </ol>"},{"location":"installing-spacelift/reference-architecture/environment-requirements.html#choose-your-hostnames","title":"Choose your hostnames","text":"<p>Spacelift requires two hostnames to operate:</p> <ol> <li>The hostname to use for accessing the Spacelift UI and API (for example, <code>spacelift.example.com</code>).</li> <li>The hostname for the MQTT broker used for worker communication (for example <code>workers.spacelift.example.com</code>).</li> </ol> <p>Info</p> <p>The only requirement for the MQTT broker hostname is that it is accessible from your workers. This can be useful when deploying Spacelift to Kubernetes clusters because you can use the internal Kubernetes DNS name for the service that exposes the MQTT broker endpoint, for example <code>spacelift-mqtt.spacelift.svc.cluster.local</code>.</p>"},{"location":"installing-spacelift/reference-architecture/environment-requirements.html#create-an-encryption-key","title":"Create an encryption key","text":"<p>Spacelift requires an encryption key to store sensitive information and to sign things like session tokens. Depending on the environment you are deploying into, there are different options available for configuring this. Please see the encryption reference documentation for more details.</p>"},{"location":"installing-spacelift/reference-architecture/environment-requirements.html#configure-networking","title":"Configure networking","text":"<p>Spacelift needs to be able to access certain external dependencies like a Postgres database, object storage, and your VCS system. For a full breakdown of the Spacelift services along with their ingress and egress requirements please see our networking reference documentation.</p>"},{"location":"installing-spacelift/reference-architecture/environment-requirements.html#deploy-postgres","title":"Deploy Postgres","text":"<p>Spacelift requires a Postgres database running version 14 or later. Other than the version there are no specific requirements.</p>"},{"location":"installing-spacelift/reference-architecture/environment-requirements.html#deploy-object-storage","title":"Deploy object storage","text":"<p>Spacelift requires access to an object storage system like AWS S3 or Google Cloud Storage to function. For the full configuration requirements see our object storage reference documentation.</p>"},{"location":"installing-spacelift/reference-architecture/environment-requirements.html#setup-container-registries","title":"Setup container registries","text":"<p>Spacelift relies on two container images to operate:</p> <ul> <li><code>spacelift-backend</code> - the container image containing the backend services.</li> <li><code>spacelift-launcher</code> - the container image required for running workers in Kubernetes. This is optional when choosing another approach for running workers.</li> </ul> <p>You will need to have access to at least one container registry to push these images to when installing Spacelift. This registry will need to be accessible from the runtime environment you are using to host Spacelift.</p>"},{"location":"installing-spacelift/reference-architecture/environment-requirements.html#deploy-a-runtime-environment","title":"Deploy a runtime environment","text":"<p>Spacelift is a containerised application and requires an orchestration platform capable of running containers to operate. We recommend that you deploy Spacelift to a Kubernetes cluster, but it is also possible to run Spacelift in other environments like AWS ECS.</p>"},{"location":"installing-spacelift/reference-architecture/external-dependencies.html","title":"External Dependencies","text":""},{"location":"installing-spacelift/reference-architecture/external-dependencies.html#external-dependencies","title":"External Dependencies","text":"<p>This document provides a comprehensive overview of all external components necessary to operate Spacelift. It is useful for those planning to deploy a self-hosted version of Spacelift and seeking to ensure their infrastructure is appropriately prepared.</p> <p>The configuration of these components is flexible. For instance, you may opt to use Google Object Storage alongside AWS SQS for queuing.</p> <p>This modular approach allows you to tailor the setup to suit your specific needs, giving you the freedom to design and deploy Spacelift in a manner that aligns with your infrastructure preferences.</p> <p></p> <p>For convenience, we provide implementations of certain requirements built into Spacelift, allowing you to avoid reliance on separate external services.</p> <p>Therefore, using an external service for the following components is optional:</p> <ul> <li>Message queues.</li> <li>Encryption.</li> <li>MQTT Broker.</li> </ul> <p>On the other hand, the components below are mandatory and must be provided:</p> <ul> <li>PostgreSQL database.</li> <li>Object storage backend.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/external-dependencies.html#database","title":"Database","text":"<p>Spacelift only supports PostgreSQL.</p> <p>Warning</p> <pre><code>Creating a database in Kubernetes, while possible, is not recommended. If you choose to do so, set up a volume for continuity of operations in the event of a power outage.\n</code></pre> <p>While PostgreSQL versions higher than 16 may work with Spacelift, please note that these versions have not been officially tested or supported.</p> <p>As such, we cannot guarantee compatibility or provide assistance for any issues that may arise. We recommend using these versions at your own discretion and risk.</p> <p>The recommended approach is to use the database service provided by your cloud provider for running PostgreSQL.</p> <p>However, if necessary, you may choose to manage and operate your own PostgreSQL instance independently. There are no obligations to use a cloud provider\u2019s service.</p> <p>If you already have an existing PostgreSQL instance running, you can reuse it. Spacelift simply requires a dedicated database with full administrative permissions.</p> Supported PostgreSQL \u2264 13 \u274c PostgreSQL 14.x \u2705 PostgreSQL 15.x \u2705 PostgreSQL 16.x \u2705 PostgreSQL \u2265 17 \u26a0\ufe0f not officially supported"},{"location":"installing-spacelift/reference-architecture/external-dependencies.html#object-storage-backend","title":"Object storage backend","text":"<p>An object storage backend is essential for Spacelift to store various files, including run logs, Terraform state files, and other necessary data.</p> <p>This storage ensures the proper management and persistence of critical information needed for the operation of Spacelift.</p> Supported AWS S3 \u2705 Google Cloud Storage \u2705 Azure Blob Storage \u2705 MinIO \u2705 <p>\ud83d\udd17 Object storage reference</p>"},{"location":"installing-spacelift/reference-architecture/external-dependencies.html#encryption","title":"Encryption","text":"<p>Danger</p> <p>Once an encryption method is selected, transitioning to an alternative method will not be possible, as it would require the migration of all encrypted database entries, which is not supported. Therefore, it is essential to make a well-informed decision when selecting the encryption method.</p> <p>Spacelift employs encryption to protect sensitive data stored in the database, ensuring that such information is not stored in plaintext.</p> <p>When using the built-in encryption mechanism, a private RSA key is required, which must be passed to the Spacelift backend via an environment variable. This approach introduces a potential risk, as the secret could be accessed if not properly secured. It is important to carefully consider how this secret will be managed and protected within your infrastructure.</p> <p>Spacelift also requires the use of an asymmetric key pair for performing the signing and verification of JWT (JSON Web Tokens).</p> <p>The asymmetric key pair consists of a private key, which is used to sign the token, and a corresponding public key, which is used to verify its authenticity. This process ensures that the integrity of the tokens is maintained, and that they can be trusted by the recipient, as the signature is uniquely tied to the private key and cannot be altered without detection.</p> Supported AWS KMS \u2705 Built-in RSA + AES256 \u2705 <p>\ud83d\udd17 Encryption reference</p>"},{"location":"installing-spacelift/reference-architecture/external-dependencies.html#message-queues","title":"Message queues","text":"<p>Spacelift uses message queues to dispatch events for asynchronous processing. We support two types of message queue: AWS SQS, or a built-in message queue that uses the Postgres database. We recommend using the built-in message queue for any new installations, but SQS support is provided for backwards-compatibility with existing installations.</p> Supported AWS SQS \u2705 Built-in \u2705 <p>\ud83d\udd17 Message queues reference</p>"},{"location":"installing-spacelift/reference-architecture/external-dependencies.html#mqtt-broker","title":"MQTT broker","text":"<p>A MQTT broker is required for communication with workers.</p> <p>Spacelift can run its own MQTT broker embedded with the Spacelift Server.</p> <p>Info</p> <p>If you want to use IoT Core as the MQTT broker, you must use AWS SQS for message queues.</p> Supported AWS IoT Core \u2705 Built-in \u2705 <p>\ud83d\udd17 MQTT broker reference</p>"},{"location":"installing-spacelift/reference-architecture/external-dependencies.html#observability","title":"Observability","text":""},{"location":"installing-spacelift/reference-architecture/external-dependencies.html#logging","title":"Logging","text":"<p>Spacelift components will output logs in JSON to the standard output (<code>/dev/stdout</code>).</p> <p>Since log collection is dependent on the observability system you use and the infrastructure you deploy Spacelift to, it is out of scope for this document.</p>"},{"location":"installing-spacelift/reference-architecture/external-dependencies.html#telemetry","title":"Telemetry","text":"<p>Spacelift also support tracing for in depth debugging. That is fully optional and can be disabled.</p> Vendor Supported Datadog \u2705 AWS X-Ray \u2705 OpenTelemetry \u2705 <p>\ud83d\udd17 Telemetry reference</p>"},{"location":"installing-spacelift/reference-architecture/guides.html","title":"Guides","text":""},{"location":"installing-spacelift/reference-architecture/guides.html#guides","title":"Guides","text":""},{"location":"installing-spacelift/reference-architecture/guides.html#quick-start-guides","title":"Quick start guides","text":"<p>If you just want to get up and running quickly, please check out one of our quick start guides:</p> <ul> <li>Deploying to AKS.</li> <li>Deploying to ECS.</li> <li>Deploying to EKS.</li> <li>Deploying to GKE.</li> <li>Deploying to an on-prem Kubernetes cluster.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides.html#first-time-setup","title":"First time setup","text":"<p>Once you have Spacelift up and running, take a look at our first-time setup guide for your next steps.</p>"},{"location":"installing-spacelift/reference-architecture/guides.html#operations","title":"Operations","text":"<p>The following guides provide information about managing a Spacelift installation:</p> <ul> <li>Observability.</li> <li>Deploying to air-gapped environments.</li> <li>Disaster recovery.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides/air-gapped.html","title":"Deploying to air-gapped environments","text":""},{"location":"installing-spacelift/reference-architecture/guides/air-gapped.html#deploying-to-air-gapped-environments","title":"Deploying to air-gapped environments","text":"<p>An air-gapped environment is a network security measure that physically isolates computers or networks from unsecured networks, such as the public internet.</p> <p>Air-gapped environments can vary significantly in their level of isolation and implementation:</p> <ul> <li>Complete isolation: Systems have absolutely no network connectivity to external networks</li> <li>Partial air gaps: Certain systems or network segments have limited, controlled access to external networks through secure gateways, proxies, or one-way data flows</li> </ul> <p>When deploying Spacelift in air-gapped environments, you'll need to consider which type of air gap you're working with, as this will determine the specific configuration requirements and constraints for your deployment.</p> <p>This guide outlines the key considerations and implementation strategies for deploying Spacelift in networks isolated from the internet.</p>"},{"location":"installing-spacelift/reference-architecture/guides/air-gapped.html#vcs-provider","title":"VCS provider","text":"<p>If your Spacelift network has connectivity to your VCS providers, you can skip this section.</p> <p>If it doesn't, it's recommended to set up connectivity between the two networks, or alternatively use VCS Agents to connect Spacelift to your VCS provider.</p>"},{"location":"installing-spacelift/reference-architecture/guides/air-gapped.html#opentofuterraform","title":"OpenTofu/Terraform","text":"<p>Running OpenTofu/Terraform in an internet-isolated network requires careful consideration of provider management and the binary distribution (tofu or terraform CLI).</p> <p>Multiple approaches exist to address this challenge. This guide focuses on caching providers and the OpenTofu/Terraform binary directly within the Docker runner image used by your stacks. One of the advantages of this approach is that you'll have great control of your supply chain and thus mitigate potential attacks.</p> <p>Note</p> <p>Ensure your Spacelift installation can reach the endpoints required by your configured providers.</p>"},{"location":"installing-spacelift/reference-architecture/guides/air-gapped.html#installing-the-vendor-binary","title":"Installing the vendor binary","text":"<p>By default, Spacelift attempts to download the OpenTofu/Terraform version specified in your stack configuration when using an OpenTofu/Terraform vendor.</p> <p>The first step is to configure a custom workflow tool instead of the default vendor.</p> <p></p> <p>Next, create the following file in your repository at <code>.spacelift/workflow.yml</code>:</p> <pre><code>init: tofu init -input=false\nworkspaceSelect: tofu workspace select \"{{ .WorkspaceName }}\"\nworkspaceNew: tofu workspace new \"{{ .WorkspaceName }}\"\nplan: tofu plan -input=false -lock={{ .Lock }} {{ if not .Refresh }}-refresh=false {{ end }}-out={{ .PlanFileName }} {{ range .Targets }}-target='{{ . }}' {{ end }}\nshowState: tofu show -json\nshowPlan: tofu show -json \"{{ .PlanFileName }}\"\ngetOutputs: tofu output -json\napply: tofu apply -auto-approve -input=false \"{{ .PlanFileName }}\"\ndestroy: tofu destroy -auto-approve -input=false\n</code></pre> <p>This configuration prevents Spacelift from downloading OpenTofu/Terraform from the internet. However, this requires providing a Docker image with OpenTofu/Terraform pre-installed.</p> <p>Use the following Dockerfile as a reference for building compatible images. Modify it as needed to install OpenTofu/Terraform or other custom binaries for your workflow:</p> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform AS builder\n\nARG VERSION=1.9.1\nARG PLATFORM=linux_amd64\n\nUSER root\n\nRUN apk add --update gnupg &amp;&amp; \\\n    wget https://github.com/opentofu/opentofu/releases/download/v${VERSION}/tofu_${VERSION}_${PLATFORM}.zip &amp;&amp;\\\n    wget https://github.com/opentofu/opentofu/releases/download/v${VERSION}/tofu_${VERSION}_${PLATFORM}.zip.gpgsig &amp;&amp;\\\n    wget -qO- https://get.opentofu.org/opentofu.gpg | gpg --import &amp;&amp; \\\n    gpg --verify tofu_${VERSION}_${PLATFORM}.zip.gpgsig tofu_${VERSION}_${PLATFORM}.zip &amp;&amp; \\\n    unzip tofu_${VERSION}_${PLATFORM}.zip -d /tmp\n\nFROM public.ecr.aws/spacelift/runner-terraform\n\nCOPY --from=builder /tmp/tofu /bin/tofu\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/air-gapped.html#installing-providers","title":"Installing providers","text":"<p>Download OpenTofu/Terraform providers offline and cache them in your Docker image, enabling OpenTofu/Terraform to retrieve them from disk rather than attempting internet downloads.</p> <p>The <code>providers mirror</code> subcommand provides an efficient solution for that.</p> <pre><code># Run this command in the directory containing your OpenTofu/Terraform code.\n# It will read your .terraform.lock.hcl file and download the required providers\n$ tofu providers mirror -platform=linux_amd64 ../providers\n- Mirroring hashicorp/random...\n  - Selected v3.7.2 to match dependency lock file\n  - Downloading package for linux_amd64...\n  - Package authenticated: signed\n</code></pre> <p>Note</p> <p>Ensure you specify the correct <code>-platform</code> flag to match your runner platform. For example, running this command from a macOS workstation without the platform flag may download incompatible providers.</p> <p>This command downloads all required providers to a local <code>./providers</code> directory. Add this directory to your Docker image:</p> <pre><code># OpenTofu/Terraform binary installation from previous step goes here\nFROM public.ecr.aws/spacelift/runner-terraform\n\n# Add providers\nADD providers /providers\n\nCOPY --from=builder /tmp/tofu /bin/tofu\n</code></pre> <p>Push the image to a registry accessible by your Spacelift installation and configure your stacks to use it.</p> <p></p> <p>Configure OpenTofu/Terraform to retrieve providers from the local filesystem by creating a <code>.tofurc</code> (or <code>.terraformrc</code>) file in your repository root and setting the <code>TF_CLI_CONFIG_FILE</code> environment variable.</p> <pre><code># Add the following to a .tofurc or .terraformrc file in the root of your repository\nprovider_installation {\n  filesystem_mirror {\n    path    = \"/providers\"\n  }\n}\n</code></pre> <p>Configure your stack to use this configuration by setting <code>TF_CLI_CONFIG_FILE=/mnt/workspace/source/.tofurc</code>:</p> <p></p> <p>The initialization step should now function completely offline.</p> <p></p> <p>Warning</p> <p>Precise control over OpenTofu/Terraform provider versions is essential for offline operation. Ensure your repository contains a <code>.terraform.lock.hcl</code> file.</p> <p>Verify that the lock file includes all providers for resources in your state. Missing provider entries may cause OpenTofu/Terraform to attempt downloading additional providers.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html","title":"Deploying to AKS","text":""},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#deploying-to-aks","title":"Deploying to AKS","text":"<p>This guide provides a way to quickly get Spacelift up and running on an Azure Kubernetes Service (AKS) cluster. In this guide we show a relatively simple networking setup where Spacelift is accessible via a public load balancer.</p> <p>To deploy Spacelift on AKS you need to take the following steps:</p> <ol> <li>Deploy your cluster and other infrastructure components.</li> <li>Push the Spacelift images to your container registry.</li> <li>Deploy the Spacelift backend services using our Helm chart.</li> </ol>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#overview","title":"Overview","text":"<p>The illustration below shows what the infrastructure looks like when running Spacelift in AKS.</p> <p></p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#networking","title":"Networking","text":"<p>Info</p> <p>More details regarding networking requirements for Spacelift can be found on this page.</p> <p>This section will solely focus on how the Azure infrastructure will be configured to meet Spacelift's requirements.</p> <p>In this guide we'll create a new Virtual Network and subnetwork to allocate IPs for nodes, pods and services running in the cluster.</p> <p>The database will allocate a private IP in the VPC, and we'll connect directly to it from pods running in the cluster.</p> <p>Incoming HTTPS traffic will be handled by an nginx ingress controller. It will bind to a reserved static IPv4 address that you can add to your DNS zone.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#object-storage","title":"Object Storage","text":"<p>The Spacelift instance needs an object storage backend to store Terraform state files, run logs, and other things. Several Azure Storage containers will be created in this guide. This is a hard requirement for running Spacelift.</p> <p>Spacelift supports flexible authentication methods for Azure Blob Storage, including environment variables and Workload Identity. More details about object storage requirements and authentication can be found here.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#database","title":"Database","text":"<p>Spacelift requires a PostgreSQL database to operate. In this guide we'll create a new dedicated Postgresql Flexible Server instance.</p> <p>More details about database requirements for Spacelift can be found here.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#aks","title":"AKS","text":"<p>In this guide, we'll create a new AKS cluster to run Spacelift. The Spacelift application can be deployed to that cluster using a Helm chart.</p> <p>The chart will deploy 3 main components:</p> <ul> <li>The scheduler.</li> <li>The drain.</li> <li>The server.</li> </ul> <p>The scheduler is the component that handles recurring tasks. It creates new entries in a message queue when a new task needs to be performed.</p> <p>The drain is an async background processing component that picks up items from message queues and processes events.</p> <p>The server hosts the Spacelift GraphQL API, REST API and serves the embedded frontend assets. It also contains the MQTT server to handle interactions with workers. The server is exposed to the outside world using <code>Ingress</code> resources. There is also a MQTT <code>Service</code> to expose the broker to workers.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#workers","title":"Workers","text":"<p>In this guide Spacelift workers will be also deployed in AKS. That means that your Spacelift runs will be executed in the same environment as the app itself (we recommend using a separate K8s namespace).</p> <p>We recommend running your Spacelift workers in a separate namespace inside the same cluster as your Spacelift installation unless you have a specific requirement to deploy your workers elsewhere. This approach simplifies your infrastructure deployment, and reduces the surface area of your installation.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#requirements","title":"Requirements","text":"<p>Before proceeding with the next steps, the following tools must be installed on your computer.</p> <ul> <li>Azure CLI.</li> <li>Docker.</li> <li>Helm.</li> <li>OpenTofu or Terraform.</li> </ul> <p>Info</p> <p>In the following sections of the guide, OpenTofu will be used to deploy the infrastructure needed for Spacelift. If you are using Terraform, simply swap <code>tofu</code> for <code>terraform</code>.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#generate-encryption-key","title":"Generate encryption key","text":"<p>Spacelift requires an RSA key to encrypt sensitive information stored in the Postgres database. Please follow the instructions in the RSA Encryption section of our reference documentation to generate a new key.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#deploy-infrastructure","title":"Deploy infrastructure","text":"<p>Warning</p> <p>Before attempting to apply the following Terraform module, make sure you have the correct permissions on your Azure subscription. You can create a least-privileges role based on the architecture overview above, or use Azure's built-in Owner role. The Contributor role does not provide sufficient permissions because the Terraform module makes IAM role assignments.</p> <p>We provide a Terraform module to help you deploy Spacelift's infrastructure requirements.</p> <p>Before you start, set a few environment variables that will be used by the Spacelift modules:</p> <pre><code># Extract this from your archive: self-hosted-v3.0.0.tar.gz\nexport TF_VAR_spacelift_version=\"v3.0.0\"\n\n# Configure a default temporary admin account that could be used to setup the instance.\nexport TF_VAR_admin_username=\"admin\"\nexport TF_VAR_admin_password=\"&lt;password-here&gt;\"\n\n# Configure the Spacelift license\nexport TF_VAR_license_token=\"&lt;license-received-from-Spacelift&gt;\"\n\n# Set this to the base64-encoded RSA private key that you generated earlier in the \"Generate encryption key\" section of this guide.\nexport TF_VAR_encryption_rsa_private_key=\"&lt;base64-encoded-private-key&gt;\"\n\n# Adjust the following variable to the Azure subscription ID where you want to deploy your infrastructure.\nexport TF_VAR_subscription_id=\"&lt;subscription-id&gt;\"\n\n# Set this to the Azure location you wish to deploy your Spacelift infrastructure to. For example, polandcentral.\nexport TF_VAR_location=\"&lt;azure-location&gt;\"\n\n# Adjust the following variable if you want to use a different Azure resource group name for your infrastructure.\nexport TF_VAR_resource_group_name=\"spacelift\"\n\n# Set this to the domain name you want to access Spacelift from, for example \"spacelift.example.com\". Do not prefix this\n# with the protocol (e.g. https://).\nexport TF_VAR_website_domain=\"&lt;domain-name&gt;\"\n\n# Uncomment the following line to enable automatically sharing usage data via our metrics endpoint.\n# If you don't enable this, you can still export the usage data via the Web UI.\n# export TF_VAR_spacelift_public_api=\"https://app.spacelift.io\"\n</code></pre> <p>Note</p> <p>The admin login/password combination is only used for the very first login to the Spacelift instance. It can be removed after the initial setup. More information can be found in the initial setup section.</p> <p>Below is a small example of how to use this module:</p> <pre><code>variable \"spacelift_version\" {\n  type = string\n}\n\nvariable \"license_token\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"encryption_rsa_private_key\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"k8s_namespace\" {\n  type    = string\n  default = \"spacelift\"\n}\n\nvariable \"website_domain\" {\n  type = string\n}\n\nvariable \"admin_username\" {\n  type = string\n}\n\nvariable \"admin_password\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"subscription_id\" {\n  type = string\n}\n\nvariable \"location\" {\n  type = string\n}\n\nvariable \"resource_group_name\" {\n  type = string\n}\n\nprovider \"azurerm\" {\n  features {}\n  subscription_id = var.subscription_id\n}\n\nmodule \"spacelift\" {\n  source = \"github.com/spacelift-io/terraform-azure-spacelift-selfhosted?ref=v1.0.0\"\n\n  app_domain                 = var.website_domain\n  spacelift_version          = var.spacelift_version\n  license_token              = var.license_token\n  encryption_rsa_private_key = var.encryption_rsa_private_key\n  k8s_namespace              = \"spacelift\"\n  admin_username             = var.admin_username\n  admin_password             = var.admin_password\n  location                   = var.location\n  resource_group_name        = var.resource_group_name\n}\n\noutput \"shell\" {\n  value     = module.spacelift.shell\n  sensitive = true\n}\n\noutput \"kubernetes_secrets\" {\n  sensitive = true\n  value     = module.spacelift.kubernetes_secrets\n}\n\noutput \"helm_values\" {\n  value     = module.spacelift.helm_values\n}\n</code></pre> <p>Feel free to take a look at the documentation for the terraform-azure-spacelift-selfhosted module before applying your infrastructure in case there are any settings that you wish to adjust. Once you are ready, apply your changes:</p> <pre><code>tofu apply\n</code></pre> <p>Once applied, grab all the variables that need to be exported in your shell for the rest of this guide. We expose a <code>shell</code> output in terraform that you can source directly for convenience:</p> <pre><code># Source in your shell all the required env vars to continue the installation process\n$(tofu output -raw shell)\n</code></pre> <p>Info</p> <p>During this guide you'll export shell variables that will be useful in future steps. So please keep the same shell open for the entire guide.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#configure-your-dns-zone","title":"Configure your DNS zone","text":"<p>Configure the following record in your DNS zone. The <code>${PUBLIC_IP_ADDRESS}</code> environment variable should be available in your shell from the previous step.</p> <pre><code>spacelift.mycompany.com       3600 IN  A     ${PUBLIC_IP_ADDRESS}\n</code></pre> <p>Info</p> <p>It is useful to configure this entry as early as possible since it will be used by the Let's Encrypt handshake later. So it's better to do this right now, and continue the setup while records are being propagated.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#push-images-to-container-registry","title":"Push images to Container Registry","text":"<p>From the previous terraform apply step, you need to grab the URL of the registry from the output and push our docker images to it.</p> <pre><code># Login to Azure Container Registry\n\naz acr login --name ${PRIVATE_CONTAINER_REGISTRY_NAME}\naz acr login --name ${PUBLIC_CONTAINER_REGISTRY_NAME}\n</code></pre> <pre><code>tar -xzf self-hosted-${SPACELIFT_VERSION}.tar.gz -C .\n\ndocker image load --input=\"self-hosted-${SPACELIFT_VERSION}/container-images/spacelift-launcher.tar\"\ndocker tag \"spacelift-launcher:${SPACELIFT_VERSION}\" \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\ndocker push \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\n\ndocker image load --input=\"self-hosted-${SPACELIFT_VERSION}/container-images/spacelift-backend.tar\"\ndocker tag \"spacelift-backend:${SPACELIFT_VERSION}\" \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\ndocker push \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#deploy-spacelift","title":"Deploy Spacelift","text":"<p>First, we need to configure Kubernetes credentials to interact with the AKS cluster.</p> <pre><code># We set a kubeconfig so we do not mess up with any existing config.\nexport KUBECONFIG=${HOME}/.kube/config_spacelift\naz aks get-credentials --resource-group ${AZURE_RESOURCE_GROUP_NAME} --name ${AKS_CLUSTER_NAME} --overwrite-existing\n</code></pre> <p>Warning</p> <p>Make sure the above <code>KUBECONFIG</code> environment variable is present when running following helm commands.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#nginx-controller","title":"NGINX controller","text":"<pre><code>helm upgrade --install ingress-nginx ingress-nginx \\\n  --repo https://kubernetes.github.io/ingress-nginx \\\n  --namespace ingress-nginx \\\n  --create-namespace \\\n  --version v4.12.0 \\\n  --set rbac.create=true \\\n  --set controller.service.externalTrafficPolicy=Local \\\n  --set controller.service.loadBalancerIP=${PUBLIC_IP_ADDRESS} \\\n  --set crds.enabled=true\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#cert-manager","title":"Cert manager","text":"<p>Spacelift should run under valid HTTPS endpoints, so you need to provide valid certificates to Ingress resources deployed by Spacelift. One simple way to achieve that is to use cert-manager to generate Let's Encrypt certificates.</p> <p>If you already have cert-manager running in your cluster and know how to configure Certificates on Ingress, you can skip this step.</p> <pre><code>helm repo add jetstack https://charts.jetstack.io --force-update\n\nhelm upgrade \\\n    --install \\\n    cert-manager jetstack/cert-manager \\\n    --namespace cert-manager \\\n    --create-namespace \\\n    --version v1.16.2 \\\n    --set crds.enabled=true\n</code></pre> <p>Info</p> <p>Note that this command can take a few minutes to finish.</p> <p>Next, we will configure an issuer to tell cert-manager how to generate certificates. In this guide we'll use ACME with Let's Encrypt and HTTP01 challenges.</p> <p>Note</p> <p>It is highly recommended testing against the Let's Encrypt staging environment before using the production environment. This will allow you to get things right before issuing trusted certificates and reduce the chance of hitting rate limits. Note that the staging root CA is untrusted by browsers, and Spacelift workers won't be able to connect to the server endpoint either.</p> ProductionStaging (optional) <pre><code>export ACME_EMAIL=\"your email\"\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: $ACME_EMAIL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: prod-issuer-account-key\n    solvers:\n      - http01:\n          ingress:\n            ingressClassName: nginx\nEOF\n</code></pre> <pre><code>export ACME_EMAIL=\"your email\"\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: $ACME_EMAIL\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: staging-issuer-account-key\n    solvers:\n      - http01:\n          ingress:\n            ingressClassName: nginx\nEOF\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#install-spacelift","title":"Install Spacelift","text":""},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#create-kubernetes-namespace","title":"Create Kubernetes namespace","text":"<pre><code>kubectl create namespace $K8S_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#create-secrets","title":"Create secrets","text":"<p>The Spacelift services need various environment variables to be configured in order to function correctly. In this guide we will create three Spacelift secrets to pass these variables to the Spacelift backend services:</p> <ul> <li><code>spacelift-shared</code> - contains variables used by all services.</li> <li><code>spacelift-server</code> - contains variables specific to the Spacelift server.</li> <li><code>spacelift-drain</code> - contains variables specific to the Spacelift drain.</li> </ul> <p>For convenience, the <code>terraform-azure-spacelift-selfhosted</code> Terraform module provides a kubernetes_secrets output that you can pass to kubectl apply to create the secrets:</p> <pre><code>tofu output -raw kubernetes_secrets &gt; secrets.yaml\nkubectl apply -f secrets.yaml\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#deploy-application","title":"Deploy application","text":"<p>You need to provide a number of configuration options to Helm when deploying Spacelift to configure it correctly for your environment. You can generate a Helm values.yaml file to use via the <code>helm_values</code> output variable of the <code>terraform-azure-spacelift-selfhosted</code> Terraform module:</p> <pre><code>tofu output -raw helm_values &gt; spacelift-values.yaml\n</code></pre> <p>Feel free to take a look at this file to understand what is being configured. Once you're happy, run the following command to deploy Spacelift:</p> <pre><code>helm upgrade \\\n  --repo https://downloads.spacelift.io/helm \\\n  spacelift \\\n  spacelift-self-hosted \\\n  --install --wait --timeout 20m \\\n  --namespace \"$K8S_NAMESPACE\" \\\n  --values \"spacelift-values.yaml\"\n</code></pre> <p>Tip</p> <p>You can follow the deployment progress with: <code>kubectl logs -n ${K8S_NAMESPACE} deployments/spacelift-server</code></p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#next-steps","title":"Next steps","text":"<p>Now that your Spacelift installation is up and running, take a look at the initial installation section for the next steps to take.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#create-a-worker-pool","title":"Create a worker pool","text":"<p>We recommend that you deploy workers in a dedicated namespace.</p> <pre><code># Choose a namespace to deploy the workers to\nexport K8S_WORKER_POOL_NAMESPACE=\"spacelift-workers\"\nkubectl create namespace $K8S_WORKER_POOL_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>Warning</p> <p>When creating your <code>WorkerPool</code>, make sure to configure resources. This is highly recommended because otherwise very high resources requests can be set automatically by your admission controller.</p> <p>Also make sure to deploy the WorkerPool and its secrets into the correct namespace we just created by adding <code>-n ${K8S_WORKER_POOL_NAMESPACE}</code> to the commands in the guide below.</p> <p>\u27a1\ufe0f You need to follow this guide for configuring Kubernetes Workers.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-aks.html#deletion-uninstall","title":"Deletion / uninstall","text":"<p>Before running <code>tofu destroy</code> on the infrastructure, we recommended that you do a proper cleanup in the K8s cluster. That's because the Spacelift helm chart links a couple of Azure resources (such as an external IP) that are not managed by Terraform. If you do not remove them from K8s, <code>tofu destroy</code> will complain because some resources like networks cannot be removed if not empty.</p> <pre><code>helm uninstall -n $K8S_NAMESPACE spacelift\nhelm uninstall -n cert-manager cert-manager\nhelm uninstall -n ingress-nginx ingress-nginx\nkubectl delete namespace $K8S_WORKER_POOL_NAMESPACE\nkubectl delete namespace $K8S_NAMESPACE\nkubectl delete namespace cert-manager\n\ntofu destroy\n</code></pre> <p>Note</p> <p>Namespace deletions in Kubernetes can take a while or even get stuck. If that happens, you need to remove the finalizers from the stuck resources.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html","title":"Deploying to ECS","text":""},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#deploying-to-ecs","title":"Deploying to ECS","text":"<p>This guide provides a way to quickly get Spacelift up and running on an Elastic Compute Service (ECS) Fargate cluster. In this guide we show a relatively simple networking setup where Spacelift is accessible via a public load balancer, but you can adjust this as long as you meet the basic networking requirements for Spacelift.</p> <p>To deploy Spacelift on ECS you need to take the following steps:</p> <ol> <li>Deploy your basic infrastructure components.</li> <li>Push the Spacelift images to your Elastic Container Registry.</li> <li>Deploy the Spacelift backend services using our ECS Terraform module.</li> </ol>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#overview","title":"Overview","text":"<p>The illustration below shows what the infrastructure looks like when running Spacelift in ECS.</p> <p></p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#networking","title":"Networking","text":"<p>Info</p> <p>More details regarding networking requirements for Spacelift can be found on this page.</p> <p>This section will solely focus on how the ECS infrastructure will be configured to meet Spacelift's requirements.</p> <p>In this guide we'll create a new VPC with public and private subnets. The public subnets will contain the following items to allow communication between Spacelift and the external internet:</p> <ul> <li>An Application Load Balancer to allow inbound HTTPS traffic to reach the Spacelift server instances.</li> <li>A Network Load Balancer to allow inbound MQTT traffic to reach the Spacelift server instances.</li> <li>An Internet Gateway to allow inbound access to those load balancers.</li> <li>A NAT Gateway to allow egress traffic from the Spacelift services.</li> </ul> <p>The private subnets contain the Spacelift RDS Postgres database, along with the Spacelift ECS services and are not directly accessible via the internet.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#object-storage","title":"Object Storage","text":"<p>The Spacelift instance needs an object storage backend to store Terraform state files, run logs, and other things. Several S3 buckets will be created in this guide. This is a hard requirement for running Spacelift.</p> <p>Spacelift uses the AWS SDK default credential provider chain for S3 authentication, supporting environment variables, shared credential files and IAM roles for ECS. More details about object storage requirements and authentication can be found here.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#database","title":"Database","text":"<p>Spacelift requires a PostgreSQL database to operate. In this guide we'll create a new Aurora Serverless RDS instance. You can also reuse an existing instance and create a new database in it. In that case you'll have to adjust the database URL and other settings across the guide. It's also up to you to configure appropriate networking to expose this database to Spacelift's VPC.</p> <p>You can switch the <code>create_database</code> option to false in the terraform module to disable creating an RDS instance.</p> <p>More details about database requirements for Spacelift can be found here.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#ecs","title":"ECS","text":"<p>In this guide, we'll deploy a new ECS Fargate cluster to run Spacelift. The Spacelift application can be deployed using the terraform-aws-ecs-spacelift-selfhosted Terraform module.</p> <p>The Terraform module will deploy the ECS cluster, associated resources like IAM roles, and the following Spacelift services:</p> <ul> <li>The scheduler.</li> <li>The drain.</li> <li>The server.</li> </ul> <p>The scheduler is the component that handles recurring tasks. It creates new entries in a message queue when a new task needs to be performed.</p> <p>The drain is an async background processing component that picks up items from the queue and processes events.</p> <p>The server hosts the Spacelift GraphQL API, REST API and serves the embedded frontend assets. It also contains the MQTT server to handle interactions with workers. The server is exposed to the outside world using an Application Load Balancer for HTTP traffic, and a Network Load Balancer for MQTT traffic.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#workers","title":"Workers","text":"<p>In this guide Spacelift workers will be deployed as an EC2 autoscaling group, using the terraform-aws-spacelift-workerpool-on-ec2 Terraform module.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#requirements","title":"Requirements","text":"<p>Before proceeding with the next steps, the following tools must be installed on your computer.</p> <ul> <li>AWS CLI v2.</li> <li>Docker.</li> <li>OpenTofu or Terraform.</li> </ul> <p>Info</p> <p>In the following sections of the guide, OpenTofu will be used to deploy the infrastructure needed for Spacelift. If you are using Terraform, simply swap <code>tofu</code> for <code>terraform</code>.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#server-certificate","title":"Server certificate","text":"<p>To be able to reach your Spacelift installation using HTTPS, we need a valid certificate to serve it under secure endpoints. Before jumping into deploying Spacelift, we first need to create a valid ACM certificate.</p> <p>We are going to use DNS validation to make sure the issued certificate is valid, so you need to have access to your DNS zone to add to it the ACM certificate validation CNAME entries.</p> <p>The Spacelift infrastructure module will take as an input an ACM certificate ARN. You can either create one yourself manually or use the OpenTofu code snippets below to help you create one.</p> OpenTofu / TerraformManually <p>You can apply the following snippet of OpenTofu code with a credentials set allowed to configure your route53 zone.</p> <pre><code>resource \"aws_acm_certificate\" \"server-certificate\" {\n  # Update this entry to match the domain of the Spacelift installation\n  domain_name       = \"spacelift.example.com\"\n  validation_method = \"DNS\"\n}\n\ndata \"aws_route53_zone\" \"spacelift-zone\" {\n  # Update this entry to match your desired DNS zone\n  name = \"example.com.\"\n}\n\nresource \"aws_route53_record\" \"server-certificate\" {\n  for_each = {\n    for dvo in aws_acm_certificate.server-certificate.domain_validation_options : dvo.domain_name =&gt; {\n      name   = dvo.resource_record_name\n      record = dvo.resource_record_value\n      type   = dvo.resource_record_type\n    }\n  }\n\n  allow_overwrite = true\n  name            = each.value.name\n  records         = [each.value.record]\n  ttl             = 60\n  type            = each.value.type\n  zone_id         = data.aws_route53_zone.spacelift-zone.zone_id\n}\n\nresource \"aws_acm_certificate_validation\" \"server-certificate\" {\n  certificate_arn         = aws_acm_certificate.server-certificate.arn\n  validation_record_fqdns = [for record in aws_route53_record.server-certificate : record.fqdn]\n}\n</code></pre> <p>You can apply the following snippet of OpenTofu code and then just go to your Route53 console to create the DNS entries specified in the output.</p> <pre><code>resource \"aws_acm_certificate\" \"server-certificate\" {\n  # Update this entry to match the domain of the Spacelift installation\n  domain_name       = \"spacelift.example.com\"\n  validation_method = \"DNS\"\n}\n\noutput \"certificate-verification-records\" {\n  value = &lt;&lt;EOT\n  \u2139\ufe0f Please create the following entries in your DNS zone\n\n  %{ for dvo in aws_acm_certificate.server-certificate.domain_validation_options ~}\n  ${dvo.resource_record_name} 3600 IN ${dvo.resource_record_type} ${dvo.resource_record_value}\n  %{ endfor ~}\n  EOT\n}\n</code></pre> <pre><code>$ TF_VAR_server_domain=spacelift.example.com tofu apply\n\n# [...]\n\nOutputs:\n\ncertificate-verification-records = &lt;&lt;EOT\n\u2139\ufe0f  Please create the following entries in your DNS zone\n\n_68bc8119c6de5687c7a671e64af8013d.spacelift.example.com. 3600 IN CNAME _abcdef1234567896e93aa2f43cb67bce.xlxgrxvvxj.acm-validations.aws.\n\nEOT\n</code></pre> <p></p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#deploy-infrastructure","title":"Deploy infrastructure","text":"<p>We provide Terraform modules to deploy Spacelift's infrastructure requirements as well as a module to deploy the Spacelift services to ECS.</p> <p>Some parts of these modules can be customized to avoid deploying parts of the infra in case you want to handle that yourself. For example, you may want to disable the database if you already have a Postgres instance and want to reuse it, or you may want to provide your own IAM roles or enable CloudWatch logging.</p> <p>Before you start, set a few environment variables that will be used by the Spacelift modules:</p> <pre><code># Set this to the AWS region you wish to deploy Spacelift to, e.g. \"eu-west-1\".\nexport TF_VAR_aws_region=&lt;aws-region&gt;\n\n# Set this to the domain name you want to access Spacelift from, e.g. \"spacelift.example.com\".\nexport TF_VAR_server_domain=&lt;server-domain&gt;\n\n# Set this to the domain name you want to use for MQTT communication with the workers, e.g. \"mqtt.spacelift.example.com\".\nexport TF_VAR_mqtt_domain=&lt;mqtt-domain&gt;\n\n# Set this to the username for the admin account to use during setup.\nexport TF_VAR_admin_username=&lt;admin-username&gt;\n\n# Set this to the password for that account.\nexport TF_VAR_admin_password=&lt;admin-password&gt;\n\n# Extract this from your archive: self-hosted-v3.0.0.tar.gz\nexport TF_VAR_spacelift_version=v3.0.0\n\n# Set this to the token given to you by your Spacelift representative.\nexport TF_VAR_license_token=&lt;license-token&gt;\n\n# If you want to pass your own ACM certificate to use, uncomment the following variable and set it\n# to the ARN of the certificate you want to use. Please note, the ACM certificate must be successfully\n# issued, otherwise deploying the services will fail.\n#\n# \u2139\ufe0f If you have configured your certificate using OpenTofu or Terraform in the previous step, you can just ignore this\n# variable since you might want to reference the aws_acm_certificate_validation resource directly.\nexport TF_VAR_server_certificate_arn=\"\"\n\n# If you want to automatically send usage data to Spacelift, uncomment the following variable.\n#export TF_VAR_enable_automatic_usage_data_reporting=\"true\"\n</code></pre> <p>Note</p> <p>The admin login/password combination is only used for the very first login to the Spacelift instance. It can be removed after the initial setup. More information can be found in the initial setup section.</p> <p>Below is an example of how to use these modules:</p> <pre><code>variable \"aws_region\" {\n  type        = string\n  description = \"The AWS region you want to install Spacelift into.\"\n  default     = \"eu-north-1\"\n}\n\nvariable \"server_domain\" {\n  type        = string\n  description = \"The domain name you want to use for your Spacelift instance (e.g. spacelift.example.com).\"\n}\n\nvariable \"mqtt_domain\" {\n  type        = string\n  description = \"The domain name you want to use for your Spacelift instance (e.g. spacelift.example.com).\"\n}\n\nvariable \"admin_username\" {\n  type        = string\n  description = \"The admin username for initial setup purposes.\"\n}\n\nvariable \"admin_password\" {\n  type        = string\n  description = \"The admin password for initial setup purposes.\"\n}\n\nvariable \"spacelift_version\" {\n  type        = string\n  description = \"The version of Spacelift being deployed. This is used to decide what ECR image tag to use.\"\n}\n\nvariable \"license_token\" {\n  type        = string\n  description = \"The license token for selfhosted, issued by Spacelift.\"\n}\n\nvariable \"server_certificate_arn\" {\n  type        = string\n  description = \"The ARN of the certificate to use for the Spacelift server.\"\n}\n\nvariable \"deploy_services\" {\n  type        = bool\n  description = \"Whether to deploy the Spacelift ECS services or not.\"\n  default     = false\n}\n\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 6.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = {\n      \"app\" = \"spacelift-selfhosted\"\n    }\n  }\n}\n\n# Deploy the basic infrastructure needed for Spacelift to function.\nmodule \"spacelift-infra\" {\n  source = \"github.com/spacelift-io/terraform-aws-spacelift-selfhosted?ref=v1.5.0\"\n\n  region = var.aws_region\n\n  rds_instance_configuration = {\n    \"primary\" : {\n      instance_identifier : \"primary\"\n      instance_class : \"db.serverless\"\n    }\n  }\n\n  rds_serverlessv2_scaling_configuration = {\n    min_capacity = 0.5\n    max_capacity = 5.0\n  }\n\n  website_endpoint = \"https://${var.server_domain}\"\n}\n\n# Deploy the ECS services that run Spacelift.\nmodule \"spacelift-services\" {\n  count = var.deploy_services ? 1 : 0\n\n  source = \"github.com/spacelift-io/terraform-aws-ecs-spacelift-selfhosted?ref=v1.3.0\"\n\n  region        = var.aws_region\n  unique_suffix = module.spacelift-infra.unique_suffix\n  server_domain = var.server_domain\n\n  # NOTE: there's nothing special about port number 1984. Your workers just need to be able to access this port\n  # on the Spacelift server.\n  mqtt_broker_endpoint                 = \"tls://${var.mqtt_domain}:1984\"\n  vpc_id                               = module.spacelift-infra.vpc_id\n  server_lb_subnets                    = module.spacelift-infra.public_subnet_ids\n\n  # It's also possible to reference the certificate resource ARN directly from the previous step.\n  # That could be useful if you want to keep a single terraform codebase for both certificate provisioning and the\n  # spacelift installation.\n  # You can uncomment the line below and get rid of the var.server_certificate_arn variable if you want to do so.\n  #server_lb_certificate_arn            = aws_acm_certificate_validation.server-certificate.certificate_arn\n  server_lb_certificate_arn            = var.server_certificate_arn\n\n  server_security_group_id             = module.spacelift-infra.server_security_group_id\n  mqtt_lb_subnets                      = module.spacelift-infra.public_subnet_ids\n  ecs_subnets                          = module.spacelift-infra.private_subnet_ids\n  admin_username                       = var.admin_username\n  admin_password                       = var.admin_password\n  backend_image                        = module.spacelift-infra.ecr_backend_repository_url\n  backend_image_tag                    = var.spacelift_version\n  launcher_image                       = module.spacelift-infra.ecr_launcher_repository_url\n  launcher_image_tag                   = var.spacelift_version\n  license_token                        = var.license_token\n  database_url                         = module.spacelift-infra.database_url\n  database_read_only_url               = module.spacelift-infra.database_read_only_url\n  deliveries_bucket_name               = module.spacelift-infra.deliveries_bucket_name\n  large_queue_messages_bucket_name     = module.spacelift-infra.large_queue_messages_bucket_name\n  metadata_bucket_name                 = module.spacelift-infra.metadata_bucket_name\n  modules_bucket_name                  = module.spacelift-infra.modules_bucket_name\n  policy_inputs_bucket_name            = module.spacelift-infra.policy_inputs_bucket_name\n  run_logs_bucket_name                 = module.spacelift-infra.run_logs_bucket_name\n  states_bucket_name                   = module.spacelift-infra.states_bucket_name\n  uploads_bucket_name                  = module.spacelift-infra.uploads_bucket_name\n  uploads_bucket_url                   = module.spacelift-infra.uploads_bucket_url\n  user_uploaded_workspaces_bucket_name = module.spacelift-infra.user_uploaded_workspaces_bucket_name\n  workspace_bucket_name                = module.spacelift-infra.workspace_bucket_name\n  kms_encryption_key_arn               = module.spacelift-infra.kms_encryption_key_arn\n  kms_signing_key_arn                  = module.spacelift-infra.kms_signing_key_arn\n  kms_key_arn                          = module.spacelift-infra.kms_key_arn\n  drain_security_group_id              = module.spacelift-infra.drain_security_group_id\n  scheduler_security_group_id          = module.spacelift-infra.scheduler_security_group_id\n\n  server_log_configuration = {\n    logDriver : \"awslogs\",\n    options : {\n      \"awslogs-region\" : var.aws_region,\n      \"awslogs-group\" : \"/ecs/spacelift-server\",\n      \"awslogs-create-group\" : \"true\",\n      \"awslogs-stream-prefix\" : \"server\"\n    }\n  }\n\n  drain_log_configuration = {\n    logDriver : \"awslogs\",\n    options : {\n      \"awslogs-region\" : var.aws_region,\n      \"awslogs-group\" : \"/ecs/spacelift-drain\",\n      \"awslogs-create-group\" : \"true\",\n      \"awslogs-stream-prefix\" : \"drain\"\n    }\n  }\n\n  scheduler_log_configuration = {\n    logDriver : \"awslogs\",\n    options : {\n      \"awslogs-region\" : var.aws_region,\n      \"awslogs-group\" : \"/ecs/spacelift-scheduler\",\n      \"awslogs-create-group\" : \"true\",\n      \"awslogs-stream-prefix\" : \"scheduler\"\n    }\n  }\n}\n\noutput \"server_lb_dns_name\" {\n  value = var.deploy_services ? module.spacelift-services[0].server_lb_dns_name : \"\"\n}\n\noutput \"mqtt_lb_dns_name\" {\n  value = var.deploy_services ? module.spacelift-services[0].mqtt_lb_dns_name : \"\"\n}\n\noutput \"shell\" {\n  value = module.spacelift-infra.shell\n}\n\noutput \"tfvars\" {\n  value     = module.spacelift-infra.tfvars\n  sensitive = true\n}\n</code></pre> <p>Feel free to take a look at the documentation for the terraform-aws-ecs-spacelift-selfhosted module before applying your infrastructure in case there are any settings that you wish to adjust. Once you are ready, apply your changes:</p> <pre><code>tofu apply\n</code></pre> <p>Once applied, you should grab all variables that need to be exported in the shell that will be used in next steps. We expose a <code>shell</code> output in terraform that you can source directly for convenience.</p> <pre><code># Source in your shell all the required env vars to continue the installation process\n$(tofu output -raw shell)\n\n# Output the required tfvars that will be used in further applies. Note that the \".auto.tfvars\"\n# filename is being used to allow the variables to be automatically loaded by OpenTofu.\ntofu output -raw tfvars &gt; spacelift.auto.tfvars\n</code></pre> <p>Info</p> <p>During this guide you'll export shell variables that will be useful in future steps. So please keep the same shell open for the entire guide.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#push-images-to-elastic-container-registry","title":"Push images to Elastic Container Registry","text":"<p>Assuming you have sourced the <code>shell</code> output as described in the previous section, you can run the following commands to upload the container images to your container registries and the launcher binary to the binaries S3 bucket:</p> <pre><code># Login to the private ECR\naws ecr get-login-password --region \"${AWS_REGION}\" | docker login --username AWS --password-stdin \"${PRIVATE_ECR_LOGIN_URL}\"\n\ntar -xzf self-hosted-${TF_VAR_spacelift_version}.tar.gz -C .\n\ndocker image load --input=\"self-hosted-${TF_VAR_spacelift_version}/container-images/spacelift-launcher.tar\"\ndocker tag \"spacelift-launcher:${TF_VAR_spacelift_version}\" \"${LAUNCHER_IMAGE}:${TF_VAR_spacelift_version}\"\ndocker push \"${LAUNCHER_IMAGE}:${TF_VAR_spacelift_version}\"\n\ndocker image load --input=\"self-hosted-${TF_VAR_spacelift_version}/container-images/spacelift-backend.tar\"\ndocker tag \"spacelift-backend:${TF_VAR_spacelift_version}\" \"${BACKEND_IMAGE}:${TF_VAR_spacelift_version}\"\ndocker push \"${BACKEND_IMAGE}:${TF_VAR_spacelift_version}\"\n\naws s3 cp --no-guess-mime-type \"./self-hosted-${TF_VAR_spacelift_version}/bin/spacelift-launcher\" \"s3://${BINARIES_BUCKET_NAME}/spacelift-launcher\"\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#deploy-spacelift","title":"Deploy Spacelift","text":"<p>In this section, we'll deploy the Spacelift services to your ECS cluster, and then deploy an initial worker pool.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#deploy-application","title":"Deploy application","text":"<p>To deploy the services, set the following environment variable to enable the services module to be deployed:</p> <pre><code>export TF_VAR_deploy_services=\"true\"\n</code></pre> <p>Now go ahead and run <code>tofu apply</code> again to deploy the ECS cluster and services.</p> <p>Once this module has been applied successfully, you should be able to setup DNS entries for the server and MQTT broker endpoints using the <code>server_lb_dns_name</code> and <code>mqtt_lb_dns_name</code> outputs.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#configure-your-dns-zone","title":"Configure your DNS zone","text":"<p>You need to set two CNAMEs in your DNS zone. One for the main application and one for the MQTT endpoint for the workers.</p> OpenTofu / TerraformManually <p>You can add the following entries to your OpenTofu code.</p> <pre><code>data \"aws_route53_zone\" \"spacelift-zone\" {\n  # Update this entry to match your desired DNS zone\n  name = \"example.com.\"\n}\n\n# This one is the main address of your Spacelift installation.\n# It should be a CNAME to the tofu output server_lb_dns_name.\nresource \"aws_route53_record\" \"server\" {\n  zone_id = data.aws_route53_zone.spacelift-zone.zone_id\n  name    = var.server_domain\n  type    = \"CNAME\"\n  ttl     = 300\n  records = [module.spacelift-services[0].server_lb_dns_name]\n}\n\n# This one is the address of your MQTT endpoint.\n# It should be a CNAME to the tofu output mqtt_lb_dns_name.\nresource \"aws_route53_record\" \"mqtt\" {\n  zone_id = data.aws_route53_zone.spacelift-zone.zone_id\n  name    = var.mqtt_domain\n  type    = \"CNAME\"\n  ttl     = 300\n  records = [module.spacelift-services[0].mqtt_lb_dns_name]\n}\n</code></pre> <pre><code># This one is the main address of your Spacelift installation.\n# It should be a CNAME to the tofu output server_lb_dns_name.\n${TF_VAR_server_domain}             300 IN  CNAME     $(tofu output -raw server_lb_dns_name)\n\n# This one is the address of your MQTT endpoint.\n# It should be a CNAME to the tofu output mqtt_lb_dns_name.\n${TF_VAR_mqtt_domain}               300 IN  CNAME     $(tofu output -raw mqtt_lb_dns_name)\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#vcs-gateway-service","title":"VCS Gateway Service","text":"<p>Ideally, your VCS provider should be accessible from both the Spacelift backend and its workers. If direct access is not possible, you can use VCS Agent Pools to proxy the connections from the Spacelift backend to your VCS provider.</p> <p>The VCS Agent Pool architecture introduces a separate ECS service, deployed alongside the Spacelift backend, and exposed via a dedicated Application Load Balancer. This load balancer listens on port 443 and requires a valid TLS certificate to be attached to it.</p> <p>To support this setup, extend your module configuration with the following:</p> <pre><code>module \"spacelift-infra\" {\n  create_vcs_gateway = true\n\n  # Other settings are omitted for brevity\n}\n\nmodule \"spacelift-services\" {\n  vcs_gateway_domain = \"vcs-gateway.mycorp.io\" # The DNS record for the VCS Gateway service, without protocol.\n  vcs_gateway_security_group_id = module.spacelift-infra.vcs_gateway_security_group_id\n  vcs_gateway_certificate_arn = \"&lt;VCS Gateway certificate ARN&gt;\" # Note that this certificate MUST be successfully issued. It cannot be attached to the load balancer in a pending state.\n  vcs_gateway_lb_subnets = module.spacelift-infra.public_subnet_ids # The subnets for the load balancer. Make these public if the LB is internet-facing (default). The LB scheme can be modified with the `vcs_gateway_internal` variable.\n\n  # Other settings are omitted for brevity\n}\n</code></pre> <p>Don't forget to set up the DNS record for the VCS Gateway service, pointing to the load balancer's DNS name:</p> <pre><code>resource \"aws_route53_record\" \"vcs_gateway\" {\n  zone_id = data.aws_route53_zone.spacelift-zone.zone_id\n  name    = \"vcs-gateway.mycorp.io\"\n  type    = \"CNAME\"\n  ttl     = 300\n  records = [module.spacelift-services.vcs_gateway_lb_dns_name]\n}\n</code></pre> <p>With the backend now configured, proceed to the VCS Agent Pools guide to complete the setup.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#next-steps","title":"Next steps","text":"<p>Now that your Spacelift installation is up and running, take a look at the initial installation section for the next steps to take.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#create-a-worker-pool","title":"Create a worker pool","text":"<p>This section will show you how to deploy an EC2 based worker using our terraform-aws-spacelift-workerpool-on-ec2 module.</p> <p>The first step is to follow the instructions in our worker pool documentation to generate credentials for your worker pool, and to create a new pool in Spacelift.</p> <p>Once you have a private key and token for your worker pool, set the following Terraform variables:</p> <pre><code># The ID of your worker pool, for example 01JPA4M2M7MCYF8JZBS4447JPA. You can get this from the\n# worker pool page in Spacelift.\nexport TF_VAR_worker_pool_id=\"&lt;worker-pool-id&gt;\"\n\n# The token for your worker pool, downloaded from Spacelift when creating the pool.\nexport TF_VAR_worker_pool_config=\"&lt;worker-pool-token&gt;\"\n\n# The base64-encoded private key for your worker pool. Please refer to the worker\n# pool documentation for commands to use to base64-encode the key.\nexport TF_VAR_worker_pool_private_key=\"&lt;worker-pool-private-key&gt;\"\n</code></pre> <p>Next, you can use the following code to deploy your worker pool. Note that this deploys the workers into the same VPC as Spacelift, but this is not required for the workers to function. Please refer to the worker network requirements for more details if you wish to adjust this.</p> <pre><code>variable \"vpc_id\" {\n  type        = string\n  description = \"The VPC Spacelift is installed into.\"\n}\n\nvariable \"availability_zones\" {\n  type        = list(string)\n  description = \"The availability zones to deploy workers to.\"\n}\n\nvariable \"private_subnet_ids\" {\n  type        = list(string)\n  description = \"The subnets to use for the workers.\"\n}\n\nvariable \"aws_region\" {\n  type        = string\n  description = \"AWS region to deploy resources.\"\n}\n\nvariable \"worker_pool_id\" {\n  type        = string\n  description = \"The ID of the worker pool.\"\n}\n\nvariable \"worker_pool_config\" {\n  type        = string\n  description = \"The worker pool configuration to use.\"\n}\n\nvariable \"worker_pool_private_key\" {\n  type        = string\n  description = \"The worker pool private key.\"\n}\n\nvariable \"binaries_bucket_name\" {\n  type        = string\n  description = \"The URI to the launcher binary in S3.\"\n}\n\nlocals {\n  launcher_s3_uri = \"s3://${var.binaries_bucket_name}/spacelift-launcher\"\n}\n\ndata \"aws_security_group\" \"default\" {\n  name   = \"default\"\n  vpc_id = var.vpc_id\n}\n\nmodule \"default-pool\" {\n  source = \"github.com/spacelift-io/terraform-aws-spacelift-workerpool-on-ec2?ref=v5.2.0\"\n\n  secure_env_vars = {\n    SPACELIFT_TOKEN            = var.worker_pool_config\n    SPACELIFT_POOL_PRIVATE_KEY = var.worker_pool_private_key\n  }\n  configuration = &lt;&lt;-EOT\n    export SPACELIFT_SENSITIVE_OUTPUT_UPLOAD_ENABLED=true\n  EOT\n\n  min_size           = 2\n  max_size           = 2\n  worker_pool_id     = var.worker_pool_id\n  security_groups    = [data.aws_security_group.default.id]\n  vpc_subnets        = var.private_subnet_ids\n\n  selfhosted_configuration = {\n    s3_uri = local.launcher_s3_uri\n  }\n}\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-ecs.html#deletion-uninstall","title":"Deletion / uninstall","text":"<p>Before running <code>tofu destroy</code> on the infrastructure, you may want to set the following properties for the terraform-aws-spacelift-selfhosted module to allow the RDS, ECR and S3 resources to be cleaned up properly:</p> <pre><code>module \"spacelift\" {\n  source = \"github.com/spacelift-io/terraform-aws-spacelift-selfhosted?ref=v1.5.0\"\n\n  # Other settings...\n\n  # Disable delete protection for RDS and S3 to allow resources to be cleaned up\n  rds_delete_protection_enabled = false\n  s3_retain_on_destroy          = false\n  ecr_force_delete              = true\n}\n</code></pre> <p>Remember to apply those changes before running <code>tofu destroy</code>.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html","title":"Deploying to EKS","text":""},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#deploying-to-eks","title":"Deploying to EKS","text":"<p>This guide provides a way to quickly get Spacelift up and running on an Elastic Kubernetes Service (EKS) cluster. In this guide we show a relatively simple networking setup where Spacelift is accessible via a public load balancer, but you can adjust this to meet your requirements as long as you meet the basic networking requirements for Spacelift.</p> <p>To deploy Spacelift on EKS you need to take the following steps:</p> <ol> <li>Deploy your basic infrastructure components.</li> <li>Push the Spacelift images to your Elastic Container Registry.</li> <li>Deploy the Spacelift backend services using our Helm chart.</li> </ol>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#overview","title":"Overview","text":"<p>The illustration below shows what the infrastructure looks like when running Spacelift in EKS.</p> <p></p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#networking","title":"Networking","text":"<p>Info</p> <p>More details regarding networking requirements for Spacelift can be found on this page.</p> <p>This section will solely focus on how the EKS infrastructure will be configured to meet Spacelift's requirements.</p> <p>In this guide we'll create a new VPC with public and private subnets. The public subnets will contain the following items to allow communication between Spacelift and the external internet:</p> <ul> <li>An Application Load Balancer to allow inbound HTTPS traffic to reach the Spacelift server instances. This load balancer will be created automatically via a Kubernetes Ingress.</li> <li>An Internet Gateway to allow inbound access to the load balancer.</li> <li>A NAT Gateway to allow egress traffic from the Spacelift services.</li> </ul> <p>The private subnets contain the Spacelift RDS Postgres database, along with the Spacelift Kubernetes pods and are not directly accessible via the internet.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#object-storage","title":"Object Storage","text":"<p>The Spacelift instance needs an object storage backend to store Terraform state files, run logs, and other things. Several S3 buckets will be created in this guide. This is a hard requirement for running Spacelift.</p> <p>Spacelift uses the AWS SDK default credential provider chain for S3 authentication, supporting environment variables, shared credential files and IAM roles for EKS. More details about object storage requirements and authentication can be found here.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#database","title":"Database","text":"<p>Spacelift requires a PostgreSQL database to operate. In this guide we'll create a new Aurora Serverless RDS instance. You can also reuse an existing instance and create a new database in it. In that case you'll have to adjust the database URL and other settings across the guide. It's also up to you to configure appropriate networking to expose this database to Spacelift's VPC.</p> <p>You can switch the <code>create_database</code> option to false in the terraform module to disable creating an RDS instance.</p> <p>More details about database requirements for Spacelift can be found here.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#eks","title":"EKS","text":"<p>In this guide, we'll create a new EKS cluster to run Spacelift. The EKS cluster will use a very simple configuration including an internet-accessible public endpoint.</p> <p>The following services will be deployed as Kubernetes pods:</p> <ul> <li>The scheduler.</li> <li>The drain.</li> <li>The server.</li> </ul> <p>The scheduler is the component that handles recurring tasks. It creates new entries in a message queue when a new task needs to be performed.</p> <p>The drain is an async background processing component that picks up items from the queue and processes events.</p> <p>The server hosts the Spacelift GraphQL API, REST API and serves the embedded frontend assets. It also contains the MQTT server to handle interactions with workers. The server is exposed to the outside world using an Application Load Balancer for HTTP traffic, and a Network Load Balancer for MQTT traffic.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#workers","title":"Workers","text":"<p>In this guide Spacelift workers will also be deployed in EKS. That means that your Spacelift runs will be executed in the same environment as the app itself (we recommend using another K8s namespace).</p> <p>If you want to run workers outside the EKS cluster created for Spacelift, you can set the <code>mqtt_broker_domain</code> option of the Terraform module below. In order for this to work, you will also need to perform some additional tasks like setting up a DNS record for the MQTT broker endpoint, and making the launcher image available to your external workers if you wish to run them in Kubernetes. If you're unsure what this entails, we recommend that you stick with the default option.</p> <p>We highly recommend running your Spacelift workers within the same cluster, in a dedicated namespace. This simplifies the infrastructure deployment and makes it more secure since your runs are executed in the same environment.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#requirements","title":"Requirements","text":"<p>Before proceeding with the next steps, the following tools must be installed on your computer.</p> <ul> <li>AWS CLI v2.</li> <li>Docker.</li> <li>Helm.</li> <li>OpenTofu or Terraform.</li> </ul> <p>Info</p> <p>In the following sections of the guide, OpenTofu will be used to deploy the infrastructure needed for Spacelift. If you are using Terraform, simply swap <code>tofu</code> for <code>terraform</code>.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#server-certificate","title":"Server certificate","text":"<p>Spacelift should run under an HTTPS endpoint, so you need to provide a valid certificate to the Ingress resource deployed by Spacelift. In this guide we will assume that you already have an ACM certificate for the domain that you wish to host Spacelift on.</p> <p>Warning</p> <p>Please note, your certificate must be in the Issued status before you will be able to access Spacelift.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#deploy-infrastructure","title":"Deploy infrastructure","text":"<p>We provide a Terraform module to deploy Spacelift's infrastructure requirements.</p> <p>Some parts of the module can be customized to avoid deploying parts of the infra in case you want to handle that yourself. For example, you may want to disable the database if you already have a Postgres instance and want to reuse it.</p> <p>Note</p> <p>If you want to reuse an existing cluster, you can read this section of the EKS module. You'll need to deploy all the infrastructure dependencies (object storage and database) and make sure your cluster is able to reach them.</p> <p>Before you start, set a few environment variables that will be used by the Spacelift modules:</p> <pre><code># Extract this from your archive: self-hosted-v3.0.0.tar.gz\nexport TF_VAR_spacelift_version=\"v3.0.0\"\n\n# The AWS region you want to deploy Spacelift to.\nexport TF_VAR_aws_region=\"eu-west-1\"\n\n# Configure the Spacelift license\nexport TF_VAR_license_token=\"&lt;license-received-from-Spacelift&gt;\"\n\n# Set this to the domain name you want to access Spacelift from.\nexport TF_VAR_server_domain=\"spacelift.example.com\"\n\n# Uncomment and set the following domain if you want to run Spacelift workers outside of\n# the EKS cluster Spacelift is running in.\n#export TF_VAR_mqtt_broker_domain=\"mqtt.spacelift.example.com\"\n\n# Configure a default temporary admin account that could be used to setup the instance.\nexport TF_VAR_admin_username=\"admin\"\nexport TF_VAR_admin_password=\"&lt;password-here&gt;\"\n\n# Set the following to the ARN of the AWS ACM certificate you want to use.\nexport TF_VAR_server_acm_arn=\"&lt;ACM certificate ARN&gt;\"\n\n# Uncomment the following line to enable automatically sharing usage data via our metrics endpoint.\n# If you don't enable this, you can still export the usage data via the Web UI.\n# export TF_VAR_spacelift_public_api=\"https://app.spacelift.io\"\n</code></pre> <p>Note</p> <p>The admin login/password combination is only used for the very first login to the Spacelift instance. It can be removed after the initial setup. More information can be found in the initial setup section.</p> <p>Below is an example of how to use this module:</p> <pre><code>variable \"spacelift_version\" {\n  type = string\n}\n\nvariable \"aws_region\" {\n  type = string\n}\n\nvariable \"license_token\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"k8s_namespace\" {\n  type    = string\n  default = \"spacelift\"\n}\n\nvariable \"server_domain\" {\n  type = string\n}\n\nvariable \"mqtt_broker_domain\" {\n  type    = string\n  default = null\n}\n\nvariable \"admin_username\" {\n  type      = string\n}\n\nvariable \"admin_password\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"server_acm_arn\" {\n  type        = string\n}\n\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = {\n      \"app\" = \"spacelift-selfhosted\"\n    }\n  }\n}\n\nmodule \"spacelift\" {\n  source = \"github.com/spacelift-io/terraform-aws-eks-spacelift-selfhosted?ref=v2.1.0\"\n\n  spacelift_version  = var.spacelift_version\n  aws_region         = var.aws_region\n  license_token      = var.license_token\n  k8s_namespace      = var.k8s_namespace\n  server_domain      = var.server_domain\n  mqtt_broker_domain = var.mqtt_broker_domain\n  admin_username     = var.admin_username\n  admin_password     = var.admin_password\n  server_acm_arn     = var.server_acm_arn\n}\n\noutput \"shell\" {\n  sensitive = true\n  value     = module.spacelift.shell\n}\n\noutput \"kubernetes_ingress_class\" {\n  value = module.spacelift.kubernetes_ingress_class\n}\n\noutput \"kubernetes_secrets\" {\n  sensitive = true\n  value     = module.spacelift.kubernetes_secrets\n}\n\noutput \"helm_values\" {\n  value     = module.spacelift.helm_values\n}\n</code></pre> <p>Feel free to take a look at the documentation for the terraform-aws-eks-spacelift-selfhosted module before applying your infrastructure in case there are any settings that you wish to adjust. Once you are ready, apply your changes:</p> <pre><code>tofu apply\n</code></pre> <p>Once applied, you should grab all variables that need to be exported in the shell that will be used in next steps. We expose a <code>shell</code> output in terraform that you can source directly for convenience.</p> <pre><code># Source in your shell all the required env vars to continue the installation process\n$(tofu output -raw shell)\n</code></pre> <p>Info</p> <p>During this guide you'll export shell variables that will be useful in future steps. So please keep the same shell open for the entire guide.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#push-images-to-elastic-container-registry","title":"Push images to Elastic Container Registry","text":"<p>Assuming you have sourced the <code>shell</code> output as described in the previous section, you can run the following commands to upload the container images to your container registries and the launcher binary to the binaries S3 bucket:</p> <pre><code># Login to the private ECR\naws ecr get-login-password --region \"${AWS_REGION}\" | docker login --username AWS --password-stdin \"${PRIVATE_ECR_LOGIN_URL}\"\n\ntar -xzf self-hosted-${SPACELIFT_VERSION}.tar.gz -C .\n\ndocker image load --input=\"self-hosted-${TF_VAR_spacelift_version}/container-images/spacelift-launcher.tar\"\ndocker tag \"spacelift-launcher:${SPACELIFT_VERSION}\" \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\ndocker push \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\n\ndocker image load --input=\"self-hosted-${TF_VAR_spacelift_version}/container-images/spacelift-backend.tar\"\ndocker tag \"spacelift-backend:${SPACELIFT_VERSION}\" \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\ndocker push \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\n\n# Uploading the launcher binary is only needed if you plan to run workers outside of Kubernetes, so\n# you can skip the following command if you don't intend on doing that.\naws s3 cp --no-guess-mime-type \"./self-hosted-${TF_VAR_spacelift_version}/bin/spacelift-launcher\" \"s3://${BINARIES_BUCKET_NAME}/spacelift-launcher\"\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#deploy-spacelift","title":"Deploy Spacelift","text":"<p>First, we need to configure Kubernetes credentials to interact with the EKS cluster.</p> <pre><code># We set a kubeconfig so we do not interfere with any existing config.\nexport KUBECONFIG=${HOME}/.kube/config_spacelift\naws eks update-kubeconfig --name ${EKS_CLUSTER_NAME} --region ${AWS_REGION}\n</code></pre> <p>Warning</p> <p>Make sure the above <code>KUBECONFIG</code> environment variable is present when running the helm commands later in this guide.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#create-kubernetes-namespace","title":"Create Kubernetes namespace","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: \"$K8S_NAMESPACE\"\n  labels:\n    eks.amazonaws.com/pod-readiness-gate-inject: \"enabled\"\nEOF\n</code></pre> <p>Info</p> <p>In the command above, we're labelling the namespace with <code>eks.amazonaws.com/pod-readiness-gate-inject</code>. The reason for this is to enable pod readiness gates for the AWS load balancer controller. In addition the label key differs from the <code>elbv2.k8s.aws/pod-readiness-gate-inject</code> key mentioned in the load balancer controller docs because this guide is using EKS Auto.</p> <p>Pod readiness gates are optional, but can help to prevent situations where requests are routed to pods that aren't ready to handle them when using the AWS load balancer controller.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#create-ingressclass","title":"Create IngressClass","text":"<p>To allow access to the Spacelift HTTP server via an AWS Application Load Balancer, we need to create an IngressClassParams and IngressClass resource to specify the subnets and ACM certificate to use. The Terraform module provides a <code>kubernetes_ingress_class</code> output for convenience that you can pass to <code>kubectl apply</code> to create the resources:</p> <pre><code>tofu output -raw kubernetes_ingress_class | kubectl apply -f -\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#create-secrets","title":"Create secrets","text":"<p>The Spacelift services need various environment variables to be configured to operate correctly. In this guide we will create three Spacelift secrets to pass these variables to the Spacelift backend services:</p> <ul> <li><code>spacelift-shared</code> - contains variables used by all services.</li> <li><code>spacelift-server</code> - contains variables specific to the Spacelift server.</li> <li><code>spacelift-drain</code> - contains variables specific to the Spacelift drain.</li> </ul> <p>For convenience, the <code>terraform-aws-eks-spacelift-selfhosted</code> Terraform module provides a <code>kubernetes_secrets</code> output that you can pass to <code>kubectl apply</code> to create the secrets:</p> <pre><code>tofu output -raw kubernetes_secrets | kubectl apply -f -\n</code></pre> <p>To find out more about all of the configuration options that are available, please see the reference section of this documentation.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#deploy-application","title":"Deploy application","text":"<p>You need to provide a number of configuration options to Helm when deploying Spacelift to configure it correctly for your environment. You can generate a Helm values.yaml file to use via the <code>helm_values</code> output variable of the <code>terraform-aws-eks-spacelift-selfhosted</code> Terraform module:</p> <pre><code>tofu output -raw helm_values &gt; spacelift-values.yaml\n</code></pre> <p>Feel free to take a look at this file to understand what is being configured. Once you're happy, run the following command to deploy Spacelift:</p> <pre><code>helm upgrade \\\n  --repo https://downloads.spacelift.io/helm \\\n  spacelift \\\n  spacelift-self-hosted \\\n  --install --wait --timeout 20m \\\n  --namespace \"$K8S_NAMESPACE\" \\\n  --values \"spacelift-values.yaml\"\n</code></pre> <p>Tip</p> <p>You can follow the deployment progress with: <code>kubectl logs -n ${K8S_NAMESPACE} deployments/spacelift-server</code></p> <p>Once the chart has deployed correctly, you can get the information you need to setup DNS entries for Spacelift. To get the domain name of the Spacelift application load balancer you can use the <code>ADDRESS</code> field from the <code>kubectl get ingresses</code> command, like in the following example:</p> <pre><code>$ kubectl get ingresses --namespace \"$K8S_NAMESPACE\"\nNAME        CLASS   HOSTS                     ADDRESS                                                                    PORTS     AGE\nserver-v4   alb     spacelift.example.com     k8s-spacelif-serverv4-1234567890-1234567890.eu-west-1.elb.amazonaws.com    80, 443   5m32s\n</code></pre> <p>If you are using external workers, you can get the domain name of your MQTT broker from the <code>EXTERNAL-IP</code> field from the <code>kubectl get services</code> command, like in the following example:</p> <pre><code>$ kubectl get services --namespace \"$K8S_NAMESPACE\" \"spacelift-mqtt\"\nNAME             TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)          AGE\nspacelift-mqtt   LoadBalancer   172.20.252.37   k8s-spacelif-spacelif-1234567890-1234567890.elb.eu-north-1.amazonaws.com  1984:30498/TCP   5m49s\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#configure-your-dns-zone","title":"Configure your DNS zone","text":"<p>You should now go ahead and create appropriate CNAME entries to allow access to your Spacelift and (if using external workers) MQTT addresses.</p> <pre><code># This one is the main address of your Spacelift installation.\n# It should be a CNAME to the server-v4 ingress (see above).\n${TF_VAR_server_domain}                    300 IN  CNAME     &lt;lb-name&gt;.&lt;region&gt;.elb.amazonaws.com\n\n# This one is the address of your MQTT endpoint.\n# It should be a CNAME to the external IP of the spacelift-mqtt service (see above).\n${TF_VAR_mqtt_broker_domain}               300 IN  CNAME     &lt;lb-name&gt;.elb.&lt;region&gt;.amazonaws.com\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#next-steps","title":"Next steps","text":"<p>Now that your Spacelift installation is up and running, take a look at the initial installation section for the next steps to take.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#create-a-worker-pool","title":"Create a worker pool","text":"<p>We recommend that you deploy workers in a dedicated namespace.</p> <pre><code># Choose a namespace to deploy the workers to\nexport K8S_WORKER_POOL_NAMESPACE=\"spacelift-workers\"\nkubectl create namespace $K8S_WORKER_POOL_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>Warning</p> <p>When creating your <code>WorkerPool</code>, make sure to configure resources. This is highly recommended because otherwise very high resources requests can be set automatically by your admission controller.</p> <p>Also make sure to deploy the WorkerPool and its secrets into the correct namespace we just created by adding <code>-n ${K8S_WORKER_POOL_NAMESPACE}</code> to the commands in the guide below.</p> <p>\u27a1\ufe0f You need to follow this guide for configuring Kubernetes Workers.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-eks.html#deletion-uninstall","title":"Deletion / uninstall","text":"<p>Before running <code>tofu destroy</code> on the infrastructure, it is recommended to do a proper clean-up in the K8s cluster. That's because the Spacelift helm chart creates some AWS resources (such as load balancers) that are not managed by Terraform. If you do not remove them from K8s, <code>tofu destroy</code> will complain because some resources like networks cannot be removed if not empty.</p> <pre><code>helm uninstall -n $K8S_NAMESPACE spacelift\nkubectl delete namespace $K8S_WORKER_POOL_NAMESPACE\nkubectl delete namespace $K8S_NAMESPACE\n</code></pre> <p>Note</p> <p>Namespace deletions in Kubernetes can take a while or even get stuck. If that happens, you need to remove the finalizers from the stuck resources.</p> <p>Before running <code>tofu destroy</code> on the infrastructure, you may want to set the following properties for the terraform-aws-spacelift-selfhosted module to allow the RDS, ECR and S3 resources to be cleaned up properly:</p> <pre><code>module \"spacelift\" {\n  source = \"github.com/spacelift-io/terraform-aws-eks-spacelift-selfhosted?ref=v2.1.0\"\n\n  # Other settings..\n\n  rds_delete_protection_enabled = false\n  s3_retain_on_destroy          = false\n  ecr_force_delete              = true\n}\n</code></pre> <p>Remember to apply those changes before running <code>tofu destroy</code>.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html","title":"Deploying to GKE","text":""},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#deploying-to-gke","title":"Deploying to GKE","text":"<p>This guide provides a way to quickly get Spacelift up and running on a Google Kubernetes Engine (GKE) cluster. In this guide we show a relatively simple networking setup where Spacelift is accessible via a public load balancer, but you can adjust this to meet your requirements as long as you meet the basic networking requirements for Spacelift.</p> <p>To deploy Spacelift on GKE you need to take the following steps:</p> <ol> <li>Deploy your cluster and other infrastructure components.</li> <li>Push the Spacelift images to your artifact registry.</li> <li>Deploy the Spacelift backend services using our Helm chart.</li> </ol>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#overview","title":"Overview","text":"<p>The illustration below shows what the infrastructure looks like when running Spacelift in GKE.</p> <p></p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#networking","title":"Networking","text":"<p>Info</p> <p>More details regarding networking requirements for Spacelift can be found on this page.</p> <p>This section will solely focus on how the GCP infrastructure will be configured to meet Spacelift's requirements.</p> <p>In this guide we'll create a new VPC network and subnetwork to allocate IPs for nodes, pods and services running in the cluster. We'll create a GKE VPC native cluster.</p> <p>The Spacelift instance deployed in this guide is dual stack IPv4/IPv6. That's why for example we'll allocate two static addresses, and create two <code>Ingress</code> resources.</p> <p>The IPv4 outgoing traffic is handled with SNAT. For that we'll deploy a Cloud NAT component in GCP. For IPv6 traffic, each pod will have its own publicly routable address, so no further action is needed here.</p> <p>The database will be allocated a private IP in the VPC, and we'll connect to this from pods running in the cluster using cloud-sql-proxy.</p> <p>Incoming HTTPS traffic will be handled by Load balancers. Those load balancers are automatically created when deploying Ingress resources in the cluster. They will bind reserved static v4 and v6 IP addresses that you can add to your DNS zone.</p> <p>Warning</p> <p>Although load balancers may appear ready to handle traffic immediately after setup, there is often a brief delay before traffic is routed correctly. If the setup is complete but you are experiencing connection reset issues, please wait at least five minutes before starting your investigation.</p> <p>It's also possible to deploy Spacelift in an existing VPC, for that you need to set <code>enable_network</code> option to <code>false</code>, and provide your own network.</p> Terraform code example <pre><code>resource \"google_compute_network\" \"default\" {\n  name                     = \"test\"\n  auto_create_subnetworks  = false\n  enable_ula_internal_ipv6 = true\n}\n\nresource \"google_compute_subnetwork\" \"default\" {\n  name             = \"test\"\n  network          = google_compute_network.default.id\n  region           = var.region\n  stack_type       = \"IPV4_IPV6\"\n  ipv6_access_type = \"EXTERNAL\"\n  ip_cidr_range    = \"10.0.0.0/16\"\n\n  # Those ranges are required for GKE\n  secondary_ip_range {\n    range_name = \"services\"\n    ip_cidr_range = \"192.168.16.0/22\"\n  }\n  secondary_ip_range {\n    range_name = \"pods\"\n    ip_cidr_range = \"192.168.0.0/20\"\n  }\n}\n\nmodule \"spacelift\" {\n  source = \"github.com/spacelift-io/terraform-google-spacelift-selfhosted?ref=v1.0.0\"\n\n  region         = var.region\n  project        = var.project\n  website_domain = var.app_domain\n  database_tier  = \"db-f1-micro\"\n\n  enable_network = false\n  network        = google_compute_network.default\n  subnetwork     = google_compute_subnetwork.default\n}\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#object-storage","title":"Object Storage","text":"<p>The Spacelift instance needs an object storage backend to store Terraform state files, run logs, and other things. Several Google Storage buckets will be created in this guide. This is a hard requirement for running Spacelift.</p> <p>Spacelift uses Application Default Credentials (ADC) for Google Cloud Storage authentication, supporting multiple credential sources including attached service accounts and Workload Identity Federation for GKE environments. More details about object storage requirements and authentication can be found here.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#database","title":"Database","text":"<p>Spacelift requires a PostgreSQL database to operate. In this guide we'll create a new dedicated Cloud SQL instance. You can also reuse an existing instance and create a new database in it. In that case you'll have to adjust the database URL and other settings across the guide. It's also up to you to configure appropriate networking to expose this database to Spacelift's VPC.</p> <p>You can switch the <code>enable_database</code> option to false in the terraform module to ask not create a Cloud SQL instance.</p> <p>More details about database requirements for Spacelift can be found here.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#gke","title":"GKE","text":"<p>In this guide, we'll deploy a new autopilot GKE cluster to deploy Spacelift. The Spacelift application can be deployed using a Helm chart. The chart will deploy 3 main components:</p> <ul> <li>The scheduler.</li> <li>The drain.</li> <li>The server.</li> </ul> <p>The scheduler is the component that handles recurring tasks. It creates new entries in a message queue when a new task needs to be performed.</p> <p>The drain is an async background processing component that picks up items from the queue and processes events.</p> <p>The server hosts the Spacelift GraphQL API, REST API and serves the embedded frontend assets. It also contains the MQTT server to handle interactions with workers. The server is exposed to the outside world using a <code>Ingress</code> resources. There is also a MQTT <code>Service</code> to exposes the broker to workers.</p> <p>This MQTT Service is a <code>ClusterIP</code> by default, but can be switched to a <code>LoadBalancer</code> if you need to expose the MQTT service to workers from the outside of the VPC.</p> <p>If you have a cluster running already, it's also possible to deploy Spacelift in it. For that, you can set <code>enable_gke</code> options to <code>false</code> and provide a reference to your own resources.</p> <p>You may also want to disable creation of a new VPC and set <code>enable_network</code> to <code>false</code>. See networking section above for more details.</p> <p>In that situation, you need to provide a <code>node_service_account</code> input that should reference the service account used by your cluster nodes. This is used to grant your nodes permission to pull images from the artifact registry repository that will contain Spacelift docker images.</p> Terraform code example <pre><code>resource \"google_compute_network\" \"default\" {\n  name                     = \"test\"\n  auto_create_subnetworks  = false\n  enable_ula_internal_ipv6 = true\n}\n\nresource \"google_service_account\" \"gke-nodes\" {\n  account_id = \"gke-nodes\"\n}\n\nmodule \"spacelift\" {\n  source = \"github.com/spacelift-io/terraform-google-spacelift-selfhosted?ref=v1.0.0\"\n\n  region         = var.region\n  project        = var.project\n  website_domain = var.app_domain\n  database_tier  = \"db-f1-micro\"\n\n  enable_gke           = false\n  enable_network       = false\n  node_service_account = google_service_account.gke-nodes\n  network              = google_compute_network.default\n}\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#workers","title":"Workers","text":"<p>In this guide Spacelift workers will also be deployed in GKE. That means that your Spacelift runs will be executed in the same environment as the app itself (we recommend using another K8s namespace).</p> <p>If you want to run workers from the outside of the VPC, you can switch the <code>enable_external_workers</code> option of the Terraform module below. Then you'll have to do some extra step and configuration while following the guide. If you're unsure what this entails, it is recommended to stick with the default option.</p> <p>We highly recommend running your Spacelift workers within the same cluster, in a dedicated namespace. This simplifies the infrastructure deployment and makes it more secure since your runs are executed in the same environment.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#requirements","title":"Requirements","text":"<p>Before proceeding with the next steps, the following tools must be installed on your computer.</p> <ul> <li>Google Cloud CLI.</li> <li>Docker.</li> <li>Helm.</li> <li>OpenTofu or Terraform.</li> </ul> <p>Info</p> <p>In the following sections of the guide, OpenTofu will be used to deploy the infrastructure needed for Spacelift. If you are using Terraform, simply swap <code>tofu</code> for <code>terraform</code>.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#generate-encryption-key","title":"Generate encryption key","text":"<p>Spacelift requires an RSA key to encrypt sensitive information stored in the Postgres database. Please follow the instructions in the RSA Encryption section of our reference documentation to generate a new key.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#deploy-infrastructure","title":"Deploy infrastructure","text":"<p>We provide a Terraform module to help you deploy Spacelift's infrastructure requirements. Some parts of this module can be customized to avoid deploying part of the infra in case you want to handle that yourself. For example, you may want to disable the database if you already have a Cloud SQL instance and want to reuse it.</p> <p>Before you start, set a few environment variables that will be used by the Spacelift modules:</p> <pre><code># Extract this from your archive: self-hosted-v3.0.0.tar.gz\nexport TF_VAR_spacelift_version=\"v3.0.0\"\n\n# Configure a default temporary admin account that could be used to setup the instance.\nexport TF_VAR_admin_username=\"admin\"\nexport TF_VAR_admin_password=\"&lt;password-here&gt;\"\n\n# Configure the Spacelift license\nexport TF_VAR_license_token=\"&lt;license-received-from-Spacelift&gt;\"\n\n# Set this to the base64-encoded RSA private key that you generated earlier in the \"Generate encryption key\" section of this guide.\nexport TF_VAR_encryption_rsa_private_key=\"&lt;base64-encoded-private-key&gt;\"\n\n# Set this to the GCP region you want to deploy your infrastructure to, e.g. europe-west6.\nexport TF_VAR_region=\"&lt;gcp-region&gt;\"\n\n# Set this to the name of your GCP project.\nexport TF_VAR_project=\"&lt;project&gt;\"\n\n# Uncomment the following line to enable automatically sharing usage data via our metrics endpoint.\n# If you don't enable this, you can still export the usage data via the Web UI.\n# export TF_VAR_spacelift_public_api=\"https://app.spacelift.io\"\n</code></pre> <p>Note</p> <p>The admin login/password combination is only used for the very first login to the Spacelift instance. It can be removed after the initial setup. More information can be found in the initial setup section.</p> <p>Below is a small example of how to use this module:</p> <pre><code>variable \"spacelift_version\" {\n  type = string\n}\n\nvariable \"license_token\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"encryption_rsa_private_key\" {\n  type      = string\n  sensitive = true\n}\n\n# Do not prefix with https://\n# e.g.: spacelift.mycompany.com\nvariable \"website_domain\" {\n  type = string\n}\n\nvariable \"admin_username\" {\n  type      = string\n}\n\nvariable \"admin_password\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"region\" {\n  type = string\n}\n\nvariable \"project\" {\n  type = string\n}\n\nprovider \"google\" {\n  region         = var.region\n  project        = var.project\n  default_labels = { \"app\" = \"spacelift\" }\n}\n\nmodule \"spacelift\" {\n  source = \"github.com/spacelift-io/terraform-google-spacelift-selfhosted?ref=v1.0.0\"\n\n  region                     = var.region\n  project                    = var.project\n  website_domain             = var.website_domain\n  database_tier              = \"db-f1-micro\"\n  enable_external_workers    = false # Switch to true to enable running workers outside of the VPC.\n  spacelift_version          = var.spacelift_version\n  license_token              = var.license_token\n  encryption_rsa_private_key = var.encryption_rsa_private_key\n  k8s_namespace              = \"spacelift\"\n  admin_username             = var.admin_username\n  admin_password             = var.admin_password\n}\n\noutput \"shell\" {\n  value     = module.spacelift.shell\n  sensitive = true\n}\n\noutput \"kubernetes_secrets\" {\n  sensitive = true\n  value     = module.spacelift.kubernetes_secrets\n}\n\noutput \"helm_values\" {\n  value     = module.spacelift.helm_values\n}\n</code></pre> <p>Feel free to take a look at the documentation for the terraform-google-spacelift-selfhosted module before applying your infrastructure in case there are any settings that you wish to adjust. Once you are ready, apply your changes:</p> <pre><code>tofu apply\n</code></pre> <p>Info</p> <p>If you encounter the following error: <code>googleapi: Error 400: Service account service-xxxxxxxxxxxx@service-networking.iam.gserviceaccount.com does not exist</code>, it means that the Service Networking service account was not created for your project. To manually provision this service account, run the following command: </p><pre><code># Replace YOUR_PROJECT_ID with your actual GCP project ID. This command will create the required managed service account so it can be used in IAM policies and network configurations.\ngcloud beta services identity create \\\n  --service=servicenetworking.googleapis.com \\\n  --project=YOUR_PROJECT_ID\n</code></pre><p></p> <p>Once applied, you should grab all variables that need to be exported in the shell that will be used in next steps. We expose a <code>shell</code> output in terraform that you can source directly for convenience.</p> <pre><code># Source in your shell all the required env vars to continue the installation process\n$(tofu output -raw shell)\n</code></pre> <p>Info</p> <p>During this guide you'll export shell variables that will be useful in future steps. So please keep the same shell open for the entire guide.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#configure-your-dns-zone","title":"Configure your DNS zone","text":"<p>Configure the following records in your DNS zone. The <code>${PUBLIC_IP_ADDRESS}</code> and <code>${PUBLIC_IPV6_ADDRESS}</code> environment variables should be available in your shell from the previous step.</p> <pre><code>spacelift.example.com        3600 IN  A     ${PUBLIC_IP_ADDRESS}\nspacelift.example.com        3600 IN  AAAA  ${PUBLIC_IPV6_ADDRESS}\n\n; Optional - only if you enabled external workers\nmqtt.spacelift.example.com   3600 IN  A     ${MQTT_IP_ADDRESS}\nmqtt.spacelift.example.com   3600 IN  AAAA  ${MQTT_IPV6_ADDRESS}\n</code></pre> <p>Info</p> <p>It is useful to configure these entries ASAP since they will be used by the Let's Encrypt handshake later. So it's better to do this right now, and continue the setup while records are being propagated.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#configure-database","title":"Configure database","text":"<p>We need to grant privileges to the SQL user to be able to create roles. The Spacelift application runs DB migrations on startup, and it needs to be able to create certain objects in the database.</p> <p>As of now, there are two ways to do that. Either in the UI or locally using the <code>cloud-sql-proxy</code>.</p> Cloud consoleLocally with cloud-sql-proxy <p>You need to click on the database instance in the GCP console, then go to the <code>Cloud SQL Studio</code> tab on the left. Choose <code>postgres</code> database, use <code>postgres</code> as username and the root password from the Terraform module's output (<code>$DB_ROOT_PASSWORD</code>).</p> <p></p> <p>Replace the variables with the ones from Terraform output, and execute following queries:</p> <pre><code>ALTER USER \"$DATABASE_USER\" CREATEROLE;\nGRANT ALL PRIVILEGES ON DATABASE \"$DATABASE_NAME\" TO \"$DATABASE_USER\";\n</code></pre> <p>You can find more info about how to install cloud-sql-proxy in the official documentation.</p> <pre><code># Runs the proxy in the background\n./cloud-sql-proxy ${DATABASE_CONNECTION_NAME}&amp;\n\n# Grant sql user permissions to the database\nPGPASSWORD=${DB_ROOT_PASSWORD} \\\n    psql -h 127.0.0.1 -U postgres -c \"ALTER USER \\\"${DATABASE_USER}\\\" CREATEROLE; GRANT ALL PRIVILEGES ON DATABASE \\\"${DATABASE_NAME}\\\" TO \\\"${DATABASE_USER}\\\";\"\n\n# Stop the cloud sql proxy\nkill %1\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#push-images-to-artifact-registry","title":"Push images to Artifact Registry","text":"<p>From the previous terraform apply step, you need to grab the URL of the registry from the output and push our docker images to it.</p> <pre><code># Login to Google Artifact Registry\ngcloud auth login\ngcloud auth configure-docker ${ARTIFACT_REGISTRY_DOMAIN}\n</code></pre> <pre><code>tar -xzf self-hosted-${SPACELIFT_VERSION}.tar.gz -C .\n\ndocker image load --input=\"self-hosted-${TF_VAR_spacelift_version}/container-images/spacelift-launcher.tar\"\ndocker tag \"spacelift-launcher:${SPACELIFT_VERSION}\" \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\ndocker push \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\n\ndocker image load --input=\"self-hosted-${TF_VAR_spacelift_version}/container-images/spacelift-backend.tar\"\ndocker tag \"spacelift-backend:${SPACELIFT_VERSION}\" \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\ndocker push \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#deploy-spacelift","title":"Deploy Spacelift","text":"<p>First, we need to configure Kubernetes credentials to interact with the GKE cluster.</p> <pre><code># We set a kubeconfig so we do not mess up with any existing config.\nexport KUBECONFIG=${HOME}/.kube/config_spacelift\ngcloud container clusters get-credentials ${GKE_CLUSTER_NAME} --location ${GCP_LOCATION} --project ${GCP_PROJECT}\n</code></pre> <p>Warning</p> <p>Make sure the above <code>KUBECONFIG</code> environment variable is present when running following helm commands.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#cert-manager","title":"Cert manager","text":"<p>Spacelift should run under valid HTTPS endpoints, so you need to provide valid certificates to Ingress resources deployed by Spacelift. One simple way to achieve that is to use cert-manager to generate Let's Encrypt certificates.</p> <p>If you already have cert-manager running in your cluster and know how to configure Certificates on Ingress resources, you can skip this step.</p> <pre><code>helm repo add jetstack https://charts.jetstack.io --force-update\n\nhelm upgrade \\\n    --install \\\n    cert-manager jetstack/cert-manager \\\n    --namespace cert-manager \\\n    --create-namespace \\\n    --version v1.16.2 \\\n    --set crds.enabled=true \\\n    --set global.leaderElection.namespace=cert-manager \\\n    --set resources.requests.cpu=100m \\\n    --set resources.requests.memory=256Mi \\\n    --set webhook.resources.requests.cpu=100m \\\n    --set webhook.resources.requests.memory=256Mi \\\n    --set cainjector.resources.requests.cpu=100m \\\n    --set cainjector.resources.requests.memory=256Mi\n</code></pre> <p>Info</p> <p>Note that this command can take a few minutes to finish as GKE scales up itself to be able to run the pods.</p> <p>Next, we will configure an issuer to tell cert-manager how to generate certificates. In this guide we'll use ACME with Let's Encrypt and HTTP01 challenges.</p> <p>Note</p> <p>It is highly recommended to test against the Let's Encrypt staging environment before using the production environment. This will allow you to get things right before issuing trusted certificates and reduce the chance of hitting rate limits. Note that the staging root CA is untrusted by browsers, and Spacelift workers won't be able to connect to the server endpoint either.</p> ProductionStaging (optional) <pre><code>export ACME_EMAIL=\"your email\"\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: $ACME_EMAIL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: prod-issuer-account-key\n    solvers:\n    - http01:\n        ingress:\n          ingressClassName: nginx\nEOF\n</code></pre> <pre><code>export ACME_EMAIL=\"your email\"\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: $ACME_EMAIL\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: staging-issuer-account-key\n    solvers:\n    - http01:\n        ingress:\n          ingressClassName: nginx\nEOF\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#install-spacelift","title":"Install Spacelift","text":""},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#create-kubernetes-namespace","title":"Create Kubernetes namespace","text":"<pre><code>kubectl create namespace $K8S_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#create-secrets","title":"Create secrets","text":"<p>The Spacelift services need various environment variables to be configured in order to function correctly. In this guide we will create three Spacelift secrets to pass these variables to the Spacelift backend services:</p> <ul> <li><code>spacelift-shared</code> - contains variables used by all services.</li> <li><code>spacelift-server</code> - contains variables specific to the Spacelift server.</li> <li><code>spacelift-drain</code> - contains variables specific to the Spacelift drain.</li> </ul> <p>For convenience, the <code>terraform-google-spacelift-selfhosted</code> Terraform module provides a kubernetes_secrets output that you can pass to kubectl apply to create the secrets:</p> <pre><code>tofu output -raw kubernetes_secrets &gt; secrets.yaml\n# If you are deploying your own database outside of the provided OpenTofu module, you also have to provide following vars\n# for the spacelift-shared secret.\n#\n# - DATABASE_URL\n# - DATABASE_READ_ONLY_URL (optional, only if you want to have a read replica)\nkubectl apply -f secrets.yaml\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#deploy-application","title":"Deploy application","text":"<p>You need to provide a number of configuration options to Helm when deploying Spacelift to configure it correctly for your environment. You can generate a Helm values.yaml file to use via the <code>helm_values</code> output variable of the <code>terraform-google-spacelift-selfhosted</code> Terraform module:</p> <pre><code>tofu output -raw helm_values &gt; spacelift-values.yaml\n</code></pre> <p>Feel free to take a look at this file to understand what is being configured. Once you're happy, run the following command to deploy Spacelift:</p> <pre><code>helm upgrade \\\n  --repo https://downloads.spacelift.io/helm \\\n  spacelift \\\n  spacelift-self-hosted \\\n  --install --wait --timeout 20m \\\n  --namespace \"$K8S_NAMESPACE\" \\\n  --values \"spacelift-values.yaml\"\n</code></pre> <p>Tip</p> <p>You can follow the deployment progress with: <code>kubectl logs -n ${K8S_NAMESPACE} deployments/spacelift-server</code></p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#next-steps","title":"Next steps","text":"<p>Now that your Spacelift installation is up and running, take a look at the initial installation section for the next steps to take.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#create-a-worker-pool","title":"Create a worker pool","text":"<p>We recommend that you deploy workers in a dedicated namespace.</p> <pre><code># Choose a namespace to deploy the workers to\nexport K8S_WORKER_POOL_NAMESPACE=\"spacelift-workers\"\nkubectl create namespace $K8S_WORKER_POOL_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>Warning</p> <p>When creating your <code>WorkerPool</code>, make sure to configure resources. This is highly recommended because otherwise very high resources requests can be set automatically by your admission controller.</p> <p>Also make sure to deploy the WorkerPool and its secrets into the correct namespace we just created by adding <code>-n ${K8S_WORKER_POOL_NAMESPACE}</code> to the commands in the guide below.</p> <p>\u27a1\ufe0f You need to follow this guide for configuring Kubernetes Workers.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-gke.html#deletion-uninstall","title":"Deletion / uninstall","text":"<p>Before running <code>tofu destroy</code> on the infrastructure, we recommend that you remove the Spacelift resources from your Kubernetes cluster. This is because the Spacelift helm chart creates some GCP resources (such as a network endpoint group) that are not managed by Terraform. If you do not remove them from Kubernetes, <code>tofu destroy</code> will complain because some resources like networks cannot be removed if not empty.</p> <p>Note</p> <p>The <code>database_deletion_protection</code> variable in the Terraform module controls whether the database can be automatically deleted. If set to <code>true</code> \u2014 or if omitted, as it defaults to <code>true</code> \u2014 the database will be protected from deletion. This means running <code>tofu destroy</code> will not delete the database, and you will need to remove it manually.</p> <pre><code>helm uninstall -n $K8S_NAMESPACE spacelift\nhelm uninstall -n cert-manager cert-manager\nkubectl delete namespace $K8S_WORKER_POOL_NAMESPACE\nkubectl delete namespace $K8S_NAMESPACE\nkubectl delete namespace cert-manager\n\ntofu destroy\n</code></pre> <p>Note</p> <p>Namespace deletions in Kubernetes can take a while or even get stuck. If that happens, you need to remove the finalizers from the stuck resources.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html","title":"Deploying to an on-prem Kubernetes cluster","text":""},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#deploying-to-an-on-prem-kubernetes-cluster","title":"Deploying to an on-prem Kubernetes cluster","text":"<p>This guide provides a way to quickly get Spacelift up and running on your Kubernetes cluster.</p> <p>Warning</p> <pre><code>Creating a database in Kubernetes, while possible, is not recommended. If you choose to do so, set up a volume for continuity of operations in the event of a power outage.\n</code></pre> <p>To deploy Spacelift on-premises, you need to take the following steps:</p> <ol> <li>Push the Spacelift images to your container registry.</li> <li>Create buckets and lifetime policies in MinIO</li> <li>Deploy the Spacelift backend services using our Helm chart.</li> </ol>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#overview","title":"Overview","text":"<p>The illustration below shows what the infrastructure looks like when running Spacelift in the Kubernetes cluster.</p> <p></p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#networking","title":"Networking","text":"<p>More details regarding networking requirements for Spacelift can be found on this page.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#object-storage","title":"Object Storage","text":"<p>The Spacelift instance needs an object storage backend to store Terraform state files, run logs, and other artifacts. For on-premises deployment, we expect that MinIO will be deployed in your cluster. Please check the official docs for deployment options.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#exposing-minio","title":"Exposing MinIO","text":"<p>MinIO should also be exposed outside the cluster to make presigned URLs accessible to clients, such as users uploading state files, workers downloading workspaces as well as <code>spacectl</code>'s local preview feature.</p> <p>You must configure CORS policies on the MinIO side to allow your Spacelift installation to perform cross-origin requests. Indeed, files uploaded from the browser are directly sent to the MinIO endpoint without going through Spacelift.</p> <p>Warning</p> <p>Spacelift adds metadata on uploaded objects using underscore in their names. Some reverse proxies filter out those headers, and that might cause uploads to fail.</p> <p>If you are deploying ingress-nginx using Helm to expose your MinIO instance, you need to configure the following values:</p> <pre><code># values.yml\ncontroller:\n  config:\n    enable-underscores-in-headers: \"true\"\n</code></pre> <p>You must provide the required buckets as this is a hard requirement for running Spacelift.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#database","title":"Database","text":"<p>Spacelift requires a PostgreSQL database to operate.</p> <p>More details about database requirements for Spacelift can be found here.</p> <p>Warning</p> <pre><code>Creating a database in Kubernetes, while possible, is not recommended. If you choose to do so, set up a volume for continuity of operations in the event of a power outage.\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#kubernetes","title":"Kubernetes","text":"<p>The Spacelift application can be deployed using a Helm chart. The chart will deploy the 3 main components:</p> <ul> <li>The scheduler.</li> <li>The drain.</li> <li>The server.</li> </ul> <p>The scheduler is the component that handles recurring tasks. It creates new entries in a message queue when a new task needs to be performed.</p> <p>The drain is an async background processing component that picks up items from message queues and processes events.</p> <p>The server hosts the Spacelift GraphQL API, REST API and serves the embedded frontend assets. It also contains the MQTT server to handle interactions with workers. The server is exposed to the outside world using an <code>Ingress</code> resource. There is also an MQTT <code>Service</code> that exposes the broker to workers.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#workers","title":"Workers","text":"<p>In this guide, Spacelift workers will also be deployed in your Kubernetes cluster. That means your Spacelift runs will be executed in the same environment as the app itself (we recommend using a separate K8s namespace).</p> <p>We highly recommend running your Spacelift workers within the same cluster, in a dedicated namespace. This simplifies the infrastructure deployment and makes it more secure since your runs are executed in the same environment since you don't need to expose the MQTT server with a load balancer.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#requirements","title":"Requirements","text":"<p>Before proceeding with the next steps, the following tools must be installed on your computer:</p> <ul> <li>Docker</li> <li>Helm</li> <li>OpenTofu or Terraform</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#generate-encryption-key","title":"Generate encryption key","text":"<p>Spacelift requires an RSA key to encrypt sensitive information stored in the Postgres database. Please follow the instructions in the RSA Encryption section of our reference documentation to generate a new key.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#init","title":"Init","text":"<p>Before you start, set a few environment variables that will be used in this guide:</p> <pre><code># Extract this from your archive: self-hosted-v3.0.0.tar.gz\nexport SPACELIFT_VERSION=\"v3.0.0\"\n\nexport SERVER_DOMAIN=\"your domain of self-hosted Spacelift\"\n\n# The Kubernetes namespace to deploy Spacelift to\nexport K8S_NAMESPACE=\"spacelift\"\n\n# Configure a default temporary admin account that could be used to setup the instance.\nexport ADMIN_USERNAME=\"admin\"\nexport ADMIN_PASSWORD=\"&lt;password-here&gt;\"\n\n# Configure the Spacelift license\nexport LICENSE_TOKEN=\"&lt;license-received-from-Spacelift&gt;\"\n\n# Set this to the base64-encoded RSA private key that you generated earlier in the \"Generate encryption key\" section of this guide.\nexport ENCRYPTION_RSA_PRIVATE_KEY=\"&lt;base64-encoded-private-key&gt;\"\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#push-images-to-container-registry","title":"Push images to Container Registry","text":"<p>You need to provide container registries for the backend image and the launcher image. This script assumes that you are logged in into your registry.</p> DockerPodman <pre><code>export LAUNCHER_IMAGE=\"your docker registry address for the launcher image\"\nexport BACKEND_IMAGE=\"your docker registry address for the backend image\"\n\ntar -xzf self-hosted-${SPACELIFT_VERSION}.tar.gz -C .\n\ndocker image load --input=\"self-hosted-${SPACELIFT_VERSION}/container-images/spacelift-launcher.tar\"\ndocker tag \"spacelift-launcher:${SPACELIFT_VERSION}\" \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\ndocker push \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\n\ndocker image load --input=\"self-hosted-${SPACELIFT_VERSION}/container-images/spacelift-backend.tar\"\ndocker tag \"spacelift-backend:${SPACELIFT_VERSION}\" \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\ndocker push \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\n</code></pre> <pre><code>export LAUNCHER_IMAGE=\"your podman registry address for the launcher image\"\nexport BACKEND_IMAGE=\"your podman registry address for the backend image\"\n\ntar -xzf self-hosted-${SPACELIFT_VERSION}.tar.gz -C .\n\npodman load -i {self-hosted-${SPACELIFT_VERSION}/container-images/spacelift-launcher.tar}\npodman tag \"spacelift-launcher:${SPACELIFT_VERSION}\" \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\npodman push \"${LAUNCHER_IMAGE}:${SPACELIFT_VERSION}\"\n\npodman load -i {self-hosted-${SPACELIFT_VERSION}/container-images/spacelift-backend.tar}\npodman tag \"spacelift-backend:${SPACELIFT_VERSION}\" \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\npodman push \"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\"\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#setup-required-buckets-in-minio","title":"Setup required buckets in MinIO","text":"<p>You need to create all the required buckets in MinIO. This can be done using a few different options:</p> <ul> <li>Using the MinIO mc client</li> <li>Using OpenTofu or Terraform</li> <li>Manually in the MinIO console</li> </ul> <p>Here is a minimum example using Terraform:</p> Click to expand Terraform code for MinIO buckets <pre><code>terraform {\n  required_providers {\n    minio = {\n      source  = \"aminueza/minio\"\n      version = \"3.5.0\"\n    }\n  }\n}\n\nprovider \"minio\" {\n  # minio_ssl = true # enable if using ssl\n}\n\nvariable \"retain_on_destroy\" {\n  default = false\n}\n\nresource \"minio_s3_bucket\" \"binaries\" {\n  bucket        = \"spacelift-binaries\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"deliveries\" {\n  bucket        = \"spacelift-deliveries\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"large_queue\" {\n  bucket        = \"spacelift-large-queue-messages\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"metadata\" {\n  bucket        = \"spacelift-metadata\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"modules\" {\n  bucket        = \"spacelift-modules\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"policy\" {\n  bucket        = \"spacelift-policy-inputs\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"run_logs\" {\n  bucket        = \"spacelift-run-logs\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"states\" {\n  bucket        = \"spacelift-states\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"uploads\" {\n  bucket        = \"spacelift-uploads\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"user_uploads\" {\n  bucket        = \"spacelift-user-uploaded-workspaces\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket\" \"workspace\" {\n  bucket        = \"spacelift-workspaces\"\n  force_destroy = !var.retain_on_destroy\n}\n\nresource \"minio_s3_bucket_versioning\" \"binaries\" {\n  bucket = minio_s3_bucket.binaries.bucket\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"minio_s3_bucket_versioning\" \"modules\" {\n  bucket = minio_s3_bucket.modules.bucket\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"minio_s3_bucket_versioning\" \"policy\" {\n  bucket = minio_s3_bucket.policy.bucket\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"minio_s3_bucket_versioning\" \"run_logs\" {\n  bucket = minio_s3_bucket.run_logs.bucket\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"minio_s3_bucket_versioning\" \"states\" {\n  bucket = minio_s3_bucket.states.bucket\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"minio_s3_bucket_versioning\" \"uploads\" {\n  bucket = minio_s3_bucket.uploads.bucket\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n\n}\n\nresource \"minio_s3_bucket_versioning\" \"user_uploads\" {\n  bucket = minio_s3_bucket.user_uploads.bucket\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"minio_s3_bucket_versioning\" \"workspace\" {\n  bucket = minio_s3_bucket.workspace.bucket\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"minio_ilm_policy\" \"deliveries\" {\n  bucket = minio_s3_bucket.deliveries.bucket\n  rule {\n    id         = \"expire-after-1-day\"\n    status     = \"Enabled\"\n    expiration = \"1d\"\n  }\n}\n\nresource \"minio_ilm_policy\" \"large_queue\" {\n  bucket = minio_s3_bucket.large_queue.bucket\n  rule {\n    id         = \"expire-after-2-days\"\n    status     = \"Enabled\"\n    expiration = \"2d\"\n  }\n}\n\nresource \"minio_ilm_policy\" \"metadata\" {\n  bucket = minio_s3_bucket.metadata.bucket\n  rule {\n    id         = \"expire-after-2-days\"\n    status     = \"Enabled\"\n    expiration = \"2d\"\n  }\n}\n\nresource \"minio_ilm_policy\" \"policy\" {\n  bucket = minio_s3_bucket.policy.bucket\n  rule {\n    id         = \"expire-after-120-days\"\n    status     = \"Enabled\"\n    expiration = \"120d\"\n  }\n}\n\nresource \"minio_ilm_policy\" \"run_logs\" {\n  bucket = minio_s3_bucket.run_logs.bucket\n  rule {\n    id         = \"expire-after-60-days\"\n    status     = \"Enabled\"\n    expiration = \"60d\"\n  }\n}\n\nresource \"minio_ilm_policy\" \"uploads\" {\n  bucket = minio_s3_bucket.uploads.bucket\n  rule {\n    id         = \"expire-after-1-day\"\n    status     = \"Enabled\"\n    expiration = \"1d\"\n  }\n}\n\nresource \"minio_ilm_policy\" \"user_uploads\" {\n  bucket = minio_s3_bucket.user_uploads.bucket\n  rule {\n    id         = \"expire-after-1-day\"\n    status     = \"Enabled\"\n    expiration = \"1d\"\n  }\n}\n\nresource \"minio_ilm_policy\" \"workspace\" {\n  bucket = minio_s3_bucket.workspace.bucket\n  rule {\n    id         = \"expire-after-90-days\"\n    status     = \"Enabled\"\n    expiration = \"90d\"\n  }\n}\n</code></pre> <p>Export the buckets env vars that will be used in the service configurations.</p> <pre><code>export OBJECT_STORAGE_BUCKET_DELIVERIES=\"spacelift-deliveries\"\nexport OBJECT_STORAGE_BUCKET_LARGE_QUEUE_MESSAGES=\"spacelift-large-queue-messages\"\nexport OBJECT_STORAGE_BUCKET_MODULES=\"spacelift-modules\"\nexport OBJECT_STORAGE_BUCKET_POLICY_INPUTS=\"spacelift-policy-inputs\"\nexport OBJECT_STORAGE_BUCKET_RUN_LOGS=\"spacelift-run-logs\"\nexport OBJECT_STORAGE_BUCKET_STATES=\"spacelift-states\"\nexport OBJECT_STORAGE_BUCKET_USER_UPLOADED_WORKSPACES=\"spacelift-user-uploaded-workspaces\"\nexport OBJECT_STORAGE_BUCKET_WORKSPACE=\"spacelift-workspaces\"\nexport OBJECT_STORAGE_BUCKET_UPLOADS=\"spacelift-uploads\"\nexport OBJECT_STORAGE_BUCKET_METADATA=\"spacelift-metadata\"\n\nexport OBJECT_STORAGE_BUCKET_UPLOADS_URL=\"https://address-of-minio-instance\"\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#deploy-spacelift","title":"Deploy Spacelift","text":""},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#ingress-controller","title":"Ingress controller","text":"<p>This guide uses an Ingress resource to expose the Spacelift <code>server</code> to users. In order for this to work, you will need to already have an Ingress Controller available in your Kubernetes cluster. The choice and installation of ingress controller is outside the scope of this guide.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#cert-manager","title":"Cert manager","text":"<p>Spacelift should run under valid HTTPS endpoints, so you need to provide valid certificates to the Ingress resources deployed by Spacelift. One simple way to achieve that is to use cert-manager to generate Let's Encrypt certificates.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#install-spacelift","title":"Install Spacelift","text":""},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#create-kubernetes-namespace","title":"Create Kubernetes namespace","text":"<pre><code>kubectl create namespace $K8S_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#create-minio-credentials","title":"Create MinIO credentials","text":"<p>Before proceeding, we need to create a set of credentials in MinIO to allow the Spacelift app to interact with it.</p> Using the consoleUsing the MinIO controller <p>Login to the MinIO console and configure a new access key for spacelift</p> <p>Info</p> <p>The MinIO console is usually reachable on port <code>9443</code>, and the default credentials are <code>minio / minio123</code>.</p> <p></p> <p>If you are using the MinIO controller to manage your tenant, you might want to configure the <code>users</code> key of the tenant spec.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#create-secrets","title":"Create secrets","text":"<p>The Spacelift services need various environment variables to be configured in order to function correctly. In this guide we will create three Spacelift secrets to pass these variables to the Spacelift backend services:</p> <ul> <li><code>spacelift-shared</code> - contains variables used by all services.</li> <li><code>spacelift-server</code> - contains variables specific to the Spacelift server.</li> <li><code>spacelift-drain</code> - contains variables specific to the Spacelift drain.</li> </ul> <pre><code># Reference here the MinIO service of your cluster\nexport OBJECT_STORAGE_MINIO_ENDPOINT=\"myminio-hl.tenant-ns.svc.cluster.local:9000\"\n# Set this to false if you're not using a secure connection for MinIO\nexport OBJECT_STORAGE_MINIO_USE_SSL=\"true\"\n# Set to \"true\" if you are using a self signed certificate for MinIO\nexport OBJECT_STORAGE_MINIO_ALLOW_INSECURE=\"false\"\n# Configure credentials that you should have created in the previous step\nexport OBJECT_STORAGE_MINIO_ACCESS_KEY_ID=\"CHANGEME\"\nexport OBJECT_STORAGE_MINIO_SECRET_ACCESS_KEY=\"CHANGEME\"\n\n# Replace this with your database credentials\nexport DATABASE_URL=\"postgres://&lt;username&gt;:&lt;password&gt;@&lt;db-url&gt;/&lt;db-name&gt;?statement_cache_capacity=0\"\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: spacelift-shared\n  namespace: ${K8S_NAMESPACE}\ntype: Opaque\nstringData:\n  SERVER_DOMAIN: ${SERVER_DOMAIN}\n  MQTT_BROKER_TYPE: builtin\n  MQTT_BROKER_ENDPOINT: tls://spacelift-mqtt.${K8S_NAMESPACE}.svc.cluster.local:1984\n  ENCRYPTION_TYPE: rsa\n  ENCRYPTION_RSA_PRIVATE_KEY: ${ENCRYPTION_RSA_PRIVATE_KEY}\n  MESSAGE_QUEUE_TYPE: postgres\n  OBJECT_STORAGE_TYPE: minio\n  OBJECT_STORAGE_MINIO_ENDPOINT: ${OBJECT_STORAGE_MINIO_ENDPOINT}\n  OBJECT_STORAGE_MINIO_USE_SSL: \"${OBJECT_STORAGE_MINIO_USE_SSL}\"\n  OBJECT_STORAGE_MINIO_ALLOW_INSECURE: \"${OBJECT_STORAGE_MINIO_ALLOW_INSECURE}\"\n  OBJECT_STORAGE_MINIO_ACCESS_KEY_ID: ${OBJECT_STORAGE_MINIO_ACCESS_KEY_ID}\n  OBJECT_STORAGE_MINIO_SECRET_ACCESS_KEY: ${OBJECT_STORAGE_MINIO_SECRET_ACCESS_KEY}\n  OBJECT_STORAGE_BUCKET_DELIVERIES: ${OBJECT_STORAGE_BUCKET_DELIVERIES}\n  OBJECT_STORAGE_BUCKET_LARGE_QUEUE_MESSAGES: ${OBJECT_STORAGE_BUCKET_LARGE_QUEUE_MESSAGES}\n  OBJECT_STORAGE_BUCKET_MODULES: ${OBJECT_STORAGE_BUCKET_MODULES}\n  OBJECT_STORAGE_BUCKET_POLICY_INPUTS: ${OBJECT_STORAGE_BUCKET_POLICY_INPUTS}\n  OBJECT_STORAGE_BUCKET_RUN_LOGS: ${OBJECT_STORAGE_BUCKET_RUN_LOGS}\n  OBJECT_STORAGE_BUCKET_STATES: ${OBJECT_STORAGE_BUCKET_STATES}\n  OBJECT_STORAGE_BUCKET_USER_UPLOADED_WORKSPACES: ${OBJECT_STORAGE_BUCKET_USER_UPLOADED_WORKSPACES}\n  OBJECT_STORAGE_BUCKET_WORKSPACE: ${OBJECT_STORAGE_BUCKET_WORKSPACE}\n  OBJECT_STORAGE_BUCKET_USAGE_ANALYTICS: \"\"\n  OBJECT_STORAGE_BUCKET_UPLOADS_URL: ${OBJECT_STORAGE_BUCKET_UPLOADS_URL}\n  DATABASE_URL: ${DATABASE_URL}\n  LICENSE_TYPE: jwt\n  LICENSE_TOKEN: ${LICENSE_TOKEN}\nEOF\n\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: spacelift-server\n  namespace: ${K8S_NAMESPACE}\ntype: Opaque\nstringData:\n  ADMIN_USERNAME: ${ADMIN_USERNAME}\n  ADMIN_PASSWORD: ${ADMIN_PASSWORD}\n  OBJECT_STORAGE_BUCKET_UPLOADS: ${OBJECT_STORAGE_BUCKET_UPLOADS}\n  WEBHOOKS_ENDPOINT: https://${SERVER_DOMAIN}/webhooks\nEOF\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: spacelift-drain\n  namespace: ${K8S_NAMESPACE}\ntype: Opaque\nstringData:\n  LAUNCHER_IMAGE: ${LAUNCHER_IMAGE}\n  LAUNCHER_IMAGE_TAG: ${SPACELIFT_VERSION}\n  OBJECT_STORAGE_BUCKET_METADATA: ${OBJECT_STORAGE_BUCKET_METADATA}\nEOF\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#deploy-application","title":"Deploy application","text":"<p>You need to provide a number of configuration options to Helm when deploying Spacelift to configure it correctly for your environment.</p> <p>Take a look at the helm values.</p> <p>Here is the minimal command to deploy Spacelift:</p> <pre><code>helm upgrade \\\n    --repo https://downloads.spacelift.io/helm \\\n    --wait --timeout 20m \\\n    --install \\\n    -n $K8S_NAMESPACE \\\n    spacelift \\\n    spacelift-self-hosted \\\n    --set shared.serverHostname=\"${SERVER_DOMAIN}\" \\\n    --set shared.image=\"${BACKEND_IMAGE}:${SPACELIFT_VERSION}\" \\\n    --set shared.secretRef=\"spacelift-shared\" \\\n    --set server.secretRef=\"spacelift-server\" \\\n    --set drain.secretRef=\"spacelift-drain\"\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#next-steps","title":"Next steps","text":"<p>Now that your Spacelift installation is up and running, take a look at the initial installation section for the next steps to take.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#create-a-worker-pool","title":"Create a worker pool","text":"<p>We recommend that you deploy workers in a dedicated namespace.</p> <pre><code># Choose a namespace to deploy the workers to\nexport K8S_WORKER_POOL_NAMESPACE=\"spacelift-workers\"\nkubectl create namespace $K8S_WORKER_POOL_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>Warning</p> <p>When creating your <code>WorkerPool</code>, make sure to configure resources. This is highly recommended because otherwise very high resources requests can be set automatically by your admission controller.</p> <p>Also make sure to deploy the WorkerPool and its secrets into the correct namespace we just created by adding <code>-n ${K8S_WORKER_POOL_NAMESPACE}</code> to the commands in the guide below.</p> <p>\u27a1\ufe0f You need to follow this guide for configuring Kubernetes Workers.</p>"},{"location":"installing-spacelift/reference-architecture/guides/deploying-to-onprem.html#deletion-uninstall","title":"Deletion / uninstall","text":"<pre><code>helm uninstall -n $K8S_NAMESPACE spacelift\nkubectl delete namespace $K8S_WORKER_POOL_NAMESPACE\nkubectl delete namespace $K8S_NAMESPACE\nkubectl delete namespace cert-manager\n</code></pre> <p>Note</p> <p>Namespace deletions in Kubernetes can take a while or even get stuck. If that happens, you need to remove the finalizers from the stuck resources.</p>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html","title":"Disaster recovery","text":""},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#disaster-recovery","title":"Disaster recovery","text":"<p>This page outlines some considerations to take when creating a disaster recovery plan for your Spacelift installation. Because disaster recovery plans are very specific to your organization and the way that your are deploying Spacelift, this page does not provide step-by-step instructions for enabling disaster recovery, but instead aims to explain the parts of your Spacelift installation that you need to think about when planning for a disaster.</p>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#stateless-components","title":"Stateless components","text":"<p>Spacelift consists of the following stateless components:</p> <ul> <li>The server.</li> <li>The drain.</li> <li>The scheduler.</li> </ul> <p>These components do not store any data internally, and rely on the stateful components for storage. In the event of a disaster, you can simply deploy new copies of these components and point them at replacement dependencies.</p> <p>You can also have backup copies of these components already deployed and ready. However please note that these components will only successfully start up if they are able to connect to a valid Postgres database.</p>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#stateful-components","title":"Stateful components","text":"<p>The two main stateful components in a Spacelift installation are:</p> <ul> <li>The Postgres database.</li> <li>The object storage buckets.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#database","title":"Database","text":"<p>The Postgres database contains the main configuration information for your Spacelift installation. You should make sure it is backed up according to your standard organization requirements.</p> <p>In the event of a disaster, you will want to make sure that this database is available to your replacement Spacelift services. Depending on your deployment scenario, this can involve options like the following:</p> <ul> <li>Restoring the database from a recent backup.</li> <li>Failing over to a secondary region.</li> <li>Using a globally available database that is available from a backup region.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#object-storage","title":"Object storage","text":"<p>We use different object storage buckets for different purposes. Some of these buckets only contain short-lived data used for temporary processing within Spacelift, whereas other buckets contain more critical data that you will want to make sure is available in the event of a disaster. The following buckets contain critical data that you will not want to lose in the event of a disaster:</p> <ul> <li>Modules.</li> <li>Policy inputs.</li> <li>Run logs.</li> <li>States.</li> </ul> <p>The loss of data in your other object storage buckets may lead to temporary issues like run failures, but will not cause a problem long term.</p> <p>In the event of a disaster you will want to make sure that your object storage data is available to your replacement Spacelift services. This may involve using buckets that are automatically replicated across multiple regions, or by setting up replication to copy your data to one or more backup regions.</p> <p>Info</p> <p>If you are architecting a multi-region failover solution, you may want to consider setting up replication rules to copy objects back to your primary region in the case of a failover to your backup region. This means that if you want to later fail back to your primary region (particularly when running pre-planned DR tests) any data created in the backup region will be copied back.</p>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#encryption","title":"Encryption","text":"<p>In the event of a disaster, it is crucial that you have access to the key used to encrypt data in the Spacelift database.</p>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#kms","title":"KMS","text":"<p>When using KMS keys for encryption, it is important to carefully choose between using a single or multi-region key because this property cannot be changed after key creation. If you want to setup a backup region for disaster recovery purposes, make sure to use a global key, and replicate that key to your backup region.</p>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#rsa","title":"RSA","text":"<p>When using RSA keys for encryption you should ensure that the RSA key you are using is available in the case of a disaster.</p>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#message-queues","title":"Message queues","text":"<p>Although the message queues do technically store data, this information is generally very short-lived and it is not an issue if some data in your queues is lost during a disaster. In addition, when using the Postgres message queue implementation all of your queue data is stored in your Postgres database meaning that you only need to take the database into consideration.</p>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#workers","title":"Workers","text":""},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#built-in-mqtt-broker","title":"Built-in MQTT broker","text":"<p>When using the built-in MQTT broker you do not need to do anything other than ensure that your Spacelift Server and MQTT broker DNS entries point at the correct services after disaster recovery. In this situation the workers should reconnect automatically, and failing that you can manually restart them.</p>"},{"location":"installing-spacelift/reference-architecture/guides/disaster-recovery.html#iot-core","title":"IoT Core","text":"<p>When using IoT Core as your MQTT broker there are a few other considerations to make:</p> <ul> <li>You must setup a custom IoT broker domain configuration if you want to be able to failover to a backup region. This allows you to update your DNS to point at a backup region in the case of a disaster without needing to re-register your workers.</li> <li>You should configure the <code>MQTT_BROKER_ENDPOINT</code> environment variable to point at your custom domain name.</li> <li>You should configure the <code>AWS_SECONDARY_REGION</code> environment variable to point at your backup region. This ensures that your worker certificates are automatically replicated to your backup region.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides/first-setup.html","title":"First setup","text":""},{"location":"installing-spacelift/reference-architecture/guides/first-setup.html#first-setup","title":"First setup","text":"<p>After deploying Spacelift to your environment there are a few steps required to get you up and running:</p> <ol> <li>Login to your instance using admin credentials.</li> <li>Create an account.</li> <li>Start configuring your account.</li> </ol>"},{"location":"installing-spacelift/reference-architecture/guides/first-setup.html#logging-in-using-admin-credentials","title":"Logging in using admin credentials","text":"<p>In order to create an account you need to be able to login with admin credentials. You can find information about configuring them in the configuration reference.</p> <p>Once admin login is enabled, go to your Spacelift installation in your browser. The URL to use is determined by the <code>SERVER_DOMAIN</code> environment variable.</p> <p>When the page loads you will be redirected to the admin login screen:</p> <p></p> <p>Enter your username and password to login.</p> <p>Tip</p> <p>You can use the admin login page to login to your Spacelift instance even after completing setup. To do this just go to <code>https://${SERVER_DOMAIN}/admin-login</code>.</p>"},{"location":"installing-spacelift/reference-architecture/guides/first-setup.html#creating-an-account","title":"Creating an account","text":"<p>After logging in, the next step is to choose a name for your account:</p> <p></p> <p>The account name is used for a number of purposes including but not limited to the following:</p> <ul> <li>For informational purposes in the Spacelift frontend.</li> <li>When generating the default AWS external ID and session names when performing role assumption as part of the AWS Cloud integration.</li> <li>As part of the URL when accessing Terraform modules and providers via the Spacelift private registry.</li> </ul> <p>The account name must meet the following requirements:</p> <ul> <li>It must start with an upper or lowercase letter or number.</li> <li>It must only contain upper or lowercase letters, numbers, <code>-</code> or <code>_</code>.</li> <li>It must be between 2 and 38 characters long.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides/first-setup.html#start-configuring-your-account","title":"Start configuring your account","text":"<p>Congratulations! At this point your Spacelift account has been created, and you should have been redirected to the launchpad screen:</p> <p></p> <p>The \"Get started checklist\" section in the launchpad will guide you through the most important next steps to take. Certain steps like configuring SSO, setting up Slack and inviting teammates are optional, but at a minimum you will want to create your first worker pool and setup a VCS integration.</p>"},{"location":"installing-spacelift/reference-architecture/guides/first-setup.html#disabling-admin-login","title":"Disabling admin login","text":"<p>Once you have configured SSO, you may want to disable the admin login by removing the relevant environment variables. If you need to re-enable this in future you can simply re-add them.</p>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html","title":"Observability","text":""},{"location":"installing-spacelift/reference-architecture/guides/observability.html#observability","title":"Observability","text":"<p>This guide provides recommendations for monitoring your Self-Hosted Spacelift installation to ensure it's running correctly. Proper monitoring helps identify potential issues before they impact your operations and ensures the reliability of your Spacelift infrastructure.</p>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#metrics-to-monitor","title":"Metrics to Monitor","text":""},{"location":"installing-spacelift/reference-architecture/guides/observability.html#core-services","title":"Core Services","text":"<p>The following table shows the core metrics that you should monitor for each of your Spacelift services:</p> Service Metric Description Server CPU usage Processor utilization Memory usage RAM consumption Load balancer Response time Time to process API requests Error rate Percentage of 5xx responses Scheduler CPU usage Processor utilization Memory usage RAM consumption Drain CPU usage Processor utilization Memory usage RAM consumption Database CPU usage Processor utilization Memory usage RAM consumption Connection count Active DB connections"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#message-queues","title":"Message queues","text":"<p>The Drain service uses a number of different message queues to perform asynchronous processing of certain operations. The main metric you should monitor for the message queues is the queue length. When the drain is operating correctly, messages should be processed very quickly and you should not expect to see large backlogs (hundreds of messages) for long periods of time.</p> <p>One caveat to this is the webhooks queue. Because webhooks processing involves making lots of requests to your source control system, they can sometimes take several minutes to process. It is not unusual to see small backlogs on the webhooks queue, or messages that take several minutes to process. This is ok as long as the messages are eventually being processed and the queue length is not constantly increasing.</p> <p>If backlogs start to build up and require immediate attention, you have two mitigation options:</p> <ul> <li>Adjust the message processing timeout - The <code>MESSAGE_PROCESSING_TIMEOUT_SECONDS</code> environment variable controls how long messages can process before timing out (default: 900 seconds). Reducing this value (e.g., to 300 seconds) causes long-running messages to return to the queue faster, allowing other messages to be processed. This variable is exposed via the <code>.services.drain.message_processing_timeout_seconds</code> configuration option for CloudFormation deployments, and can be directly passed as an environment variable for Terraform deployments.</li> <li>Scale the Drain service - For CloudFormation deployments, increase the <code>.services.drain.desired_count</code> parameter to run more Drain instances in parallel. For Terraform deployments, you could configure autoscaling based on queue length metrics - this is an easy task for SQS queues as they automatically provide metrics through CloudWatch, but require some additional setup for Postgres-based queues where you need to rely on telemetry (see below).</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#sqs-metrics","title":"SQS metrics","text":"<p>The message queue length is easy to monitor for SQS-based message queues - you can use the <code>ApproximateNumberOfMessagesVisible</code> metric provided by SQS.</p>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#postgres-metrics","title":"Postgres metrics","text":"<p>For the <code>postgres</code>-based message queue however, you will need telemetry enabled. When telemetry is enabled, we expose the following metrics:</p> <ul> <li><code>postgres_queue.messages.sent</code> (counter) - incremented when a message is sent to the queue.</li> <li><code>postgres_queue.messages.received</code> (counter) - incremented when a message is received from the queue.</li> <li><code>postgres_queue.messages.changed_visibility</code> (counter) - incremented when a message visibility is changed.</li> <li><code>postgres_queue.messages.deleted</code> (counter) - incremented when a message is deleted from the queue.</li> <li><code>postgres_queue.messages.total</code> (gauge) - total number of messages in the queue.</li> <li><code>postgres_queue.messages.visible</code> (gauge) - number of visible messages in the queue.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#worker-pool-controller-kubernetes","title":"Worker Pool Controller (Kubernetes)","text":"<p>For Kubernetes worker pool deployments, you can monitor the worker pool controller using Prometheus metrics. These metrics are available in the <code>spacelift_workerpool_controller</code> namespace. See the Controller metrics section for more details.</p> Metric Description <code>spacelift_workerpool_controller_run_startup_duration_seconds</code> (histogram) Time between when a job assignment is received and the worker container is started <code>spacelift_workerpool_controller_worker_creation_errors_total</code> (counter) Total number of worker creation errors <code>spacelift_workerpool_controller_worker_idle_total</code>  (gauge) Number of idle workers <code>spacelift_workerpool_controller_worker_total</code> (gauge) Total number of workers"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#telemetry","title":"Telemetry","text":"<p>Telemetry and tracing can help diagnose complex issues but is not required for basic monitoring. If you decide to implement tracing:</p> <ul> <li>Configure an appropriate backend (Datadog, AWS X-Ray, or OpenTelemetry).</li> <li>Focus on high-value traces (API requests, run execution, etc.).</li> <li>Use sampling in production to reduce overhead.</li> </ul> <p>Refer to the Telemetry reference for configuration options.</p>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#logging","title":"Logging","text":"<p>Setting up proper log collection is strongly recommended - it\u2019s a key part of running a healthy self-hosted installation. Without it, identifying and fixing issues becomes much harder and more time-consuming.</p> <p>The sections below outline what logs are available and how to collect them across the different components of your Spacelift setup.</p>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#core-services_1","title":"Core services","text":"<p>All 3 core services (server, scheduler, and drain) log to <code>stdout</code> and <code>stderr</code>. We at Spacelift primarily use traces for debugging, so you won't find many \"info\" level logs. On the other hand, errors and terminal failures will be present.</p>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#docker-based-worker-pools","title":"Docker-based worker pools","text":"<p>Our Docker-based worker pools log to <code>/var/log/spacelift/error|info.log</code> files.</p> <p>Note that in case of a startup failure, the worker will terminate immediately so you won't have a chance to see the logs. We provide an option to not terminate on failure for the below two types of deployments:</p> <ul> <li>Cloudformation - the worker pool deployment stack has a PowerOffOnError variable. If set to <code>false</code>, the worker pool will not terminate on startup failure.</li> <li>terraform-aws-spacelift-workerpool-on-ec2 Terraform module - this module has a <code>selfhosted_configuration</code> variable that must be provided for self-hosted installations. The variable has an embedded <code>power_off_on_error</code> field.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#kubernetes-based-worker-pools","title":"Kubernetes-based worker pools","text":"<p>The Kubernetes-based worker pools log to <code>stdout</code> and <code>stderr</code>. The documentation has a dedicated section on troubleshooting that provides more details on how to retrieve logs. You can use any Kubernetes log collection tool (e.g., Fluentd, Fluent Bit, Loki) to collect and aggregate these logs.</p>"},{"location":"installing-spacelift/reference-architecture/guides/observability.html#further-reading","title":"Further Reading","text":"<ul> <li>Telemetry Configuration.</li> <li>Prometheus Integration.</li> <li>Datadog Integration.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/reference.html","title":"Configuration reference","text":""},{"location":"installing-spacelift/reference-architecture/reference.html#configuration-reference","title":"Configuration reference","text":"<p>The following sections contain detailed information about configuring Spacelift:</p> <ul> <li>General configuration.</li> <li>Encryption.</li> <li>Message queues.</li> <li>MQTT broker.</li> <li>Networking.</li> <li>Object storage.</li> <li>Slack.</li> <li>Telemetry.</li> <li>Usage reporting.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/reference/encryption.html","title":"Encryption","text":""},{"location":"installing-spacelift/reference-architecture/reference/encryption.html#encryption","title":"Encryption","text":"<p>Spacelift requires an encryption key to store sensitive information in the Postgres database, as well as to sign tokens used for authenticating requests. Currently there are two options that can be used:</p> <ul> <li>KMS keys when deploying to AWS.</li> <li>An RSA key when deploying to other environments.</li> </ul> <p>Warning</p> <p>Please be very careful with your encryption keys regardless of the option you use. If you choose to use KMS you cannot currently switch to an RSA key later, and vice-versa. If you lose access to your encryption key, you will lose access to any credentials and secrets encrypted using that key within Spacelift. Recovering from this will either require deleting and re-creating the affected items (stacks, contexts, VCS integrations, etc), or completely dropping your Spacelift database and re-creating it.</p>"},{"location":"installing-spacelift/reference-architecture/reference/encryption.html#configuration","title":"Configuration","text":"<p>The following environment variables can be used to configure encryption:</p> Environment variable Required Description <code>ENCRYPTION_TYPE</code> No Can be set to either <code>kms</code> or <code>rsa</code>. Defaults to <code>kms</code>. <code>ENCRYPTION_KMS_ENCRYPTION_KEY_ID</code> For <code>kms</code> The ID of the KMS key used for encryption. <code>ENCRYPTION_KMS_SIGNING_KEY_ID</code> For <code>kms</code> The ID of the KMS key used for signing JWTs. <code>ENCRYPTION_RSA_PRIVATE_KEY</code> For <code>rsa</code> An RSA private key in PEM format, encoded using base-64."},{"location":"installing-spacelift/reference-architecture/reference/encryption.html#kms","title":"KMS","text":"<p>When using KMS, two keys are required:</p> <ul> <li>A key used for signing JWTs with a key usage of <code>SIGN_VERIFY</code> and a key spec of <code>RSA_4096</code>.</li> <li>A key used for encryption with a key usage of <code>ENCRYPT_DECRYPT</code> and a key spec of <code>SYMMETRIC_DEFAULT</code>.</li> </ul> <p>Tip</p> <p>It is important to carefully choose between using a single-region or multi-region KMS key for the encryption key. KMS does not support changing a key from single to multi-region after key creation. Choosing a single-region key can prevent you from being able to switch Spacelift to another AWS region, or to configure a failover region.</p>"},{"location":"installing-spacelift/reference-architecture/reference/encryption.html#rsa","title":"RSA","text":"<p>When using RSA, you need to generate an RSA private key that is not password protected. For example you could use the following openssl command:</p> <pre><code>openssl req -new -newkey rsa:4096 -nodes -keyout spacelift.key -subj \"/CN=Spacelift RSA key\"\n</code></pre> <p>The common name specified in the command above is purely informative and can be changed.</p> <p>This RSA key is used to encrypt a symmetric AES-256 key that is generated during the initial setup. The encrypted AES key is then stored in the Postgres database.</p> <p>This key is then used to perform cryptographic operations, such as encrypting and decrypting sensitive data entries in the database.</p> <p>Info</p> <p>You need to encode the private key using base-64 before passing it to the <code>ENCRYPTION_RSA_PRIVATE_KEY</code> environment variable. The simplest approach is to just run <code>cat spacelift.key | base64 -w 0</code> in your command line. For Mac users, the command is <code>cat spacelift.key | base64 -b 0</code>.</p>"},{"location":"installing-spacelift/reference-architecture/reference/general-configuration.html","title":"General configuration","text":""},{"location":"installing-spacelift/reference-architecture/reference/general-configuration.html#general-configuration","title":"General configuration","text":"<p>This page lists the basic configuration options available for running the Spacelift backend services. It contains a command reference, and then separate sections for various configuration options grouped by functionality.</p>"},{"location":"installing-spacelift/reference-architecture/reference/general-configuration.html#command-reference","title":"Command reference","text":"<p>The Spacelift backend services are distributed as a container image. This container image contains a <code>spacelift</code> binary that is used to run each of the Spacelift services. The following table lists the services along with the command used to run them:</p> Service Command Description Server <code>spacelift backend server</code> Runs the HTTP server and (optionally) built-in MQTT broker. Drain <code>spacelift backend drain-local</code> Runs the asynchronous event processing service. Scheduler <code>spacelift scheduler</code> Runs the service responsible for triggering scheduled background tasks (cron jobs)."},{"location":"installing-spacelift/reference-architecture/reference/general-configuration.html#basic-settings","title":"Basic settings","text":"<p>The following basic settings need to be configured:</p> Environment variable Required Description <code>SERVER_DOMAIN</code> Yes This should be set to the domain that users should use to access Spacelift. For example <code>spacelift.myorg.com</code>. <code>DATABASE_URL</code> Yes This should be set to the URL of your Postgres database. For example <code>postgres://postgres@localhost:5432/postgres</code>."},{"location":"installing-spacelift/reference-architecture/reference/general-configuration.html#admin-login","title":"Admin Login","text":"<p>You can enable an admin login that can be accessed via username and password using the <code>/admin-login</code> URL. This can be useful during first time installation, or as a break-glass procedure if you become unable to login via SSO.</p> <p>To enable this, add the following environment variables to your Spacelift server container:</p> Environment variable Description <code>ADMIN_USERNAME</code> The username for the admin account. <code>ADMIN_PASSWORD</code> The password for the admin account. <p>To disable admin logins, either remove the two environment variables or set them to empty strings.</p>"},{"location":"installing-spacelift/reference-architecture/reference/general-configuration.html#licensing","title":"Licensing","text":"<p>We support two ways of providing a license to your Spacelift instance: via AWS License Manager, or via a license token. The following table lists the license-related configuration options:</p> Environment variable Required Description <code>LICENSE_TYPE</code> No Can be set to either <code>aws</code> or <code>jwt</code>. Defaults to <code>aws</code>. <code>LICENSE_TOKEN</code> For <code>jwt</code> The JWT containing your license details."},{"location":"installing-spacelift/reference-architecture/reference/general-configuration.html#cloud-provider-authentication","title":"Cloud provider authentication","text":"<p>Spacelift requires credentials to your cloud provider to enable certain pieces of functionality, for example access to object storage buckets and role assumption in AWS. The following sections explain how authentication works for the different supported cloud providers.</p>"},{"location":"installing-spacelift/reference-architecture/reference/general-configuration.html#aws","title":"AWS","text":"<p>We use the AWS Go SDK for accessing AWS services. The AWS documentation about configuring the SDK outlines the various options available for authentication. Typically the best approach when running Spacelift in AWS is to create a role for the Spacelift backend services, and then grant the relevant permissions to that role to access the various AWS services required by Spacelift.</p> <p>When using AWS, please make sure that the following environment variables are set:</p> Environment variable Required Description <code>AWS_ACCOUNT_ID</code> Yes Set to the ID of the account you are deploying Spacelift into. <code>AWS_PARTITION</code> No The AWS partition you are deploying Spacelift into. Defaults to <code>aws</code>. <code>AWS_REGION</code> Yes The region you are deploying Spacelift into. <code>AWS_DEFAULT_REGION</code> Yes Set to the same value as <code>AWS_REGION</code>. <code>AWS_SECONDARY_REGION</code> No The failover region to use when configuring disaster recovery."},{"location":"installing-spacelift/reference-architecture/reference/general-configuration.html#google-cloud","title":"Google Cloud","text":"<p>We use the Google Cloud Go SDK for accessing Google Cloud services. You can use Application Default Credentials to configure access to your account.</p> <p>When using Google Cloud, please make sure that the following environment variable is set:</p> Environment variable Description <code>GCP_PROJECT</code> The GCP project containing your GCP resource (e.g. object storage buckets)."},{"location":"installing-spacelift/reference-architecture/reference/message-queues.html","title":"Message queues","text":""},{"location":"installing-spacelift/reference-architecture/reference/message-queues.html#message-queues","title":"Message queues","text":"<p>Spacelift uses a number of message queues to support asynchronous processing. We support two options for message queues: AWS SQS, or a built-in message broker that uses your Postgres database.</p> <p>For new installations we suggest using the Postgres message broker. The main exception to this is if you want to use AWS IoT Core rather than our built-in MQTT broker. In that case you must use SQS as IoT Core can only deliver messages to SQS queues by design.</p>"},{"location":"installing-spacelift/reference-architecture/reference/message-queues.html#configuration","title":"Configuration","text":"<p>The following environment variables can be used to configure message queues:</p> Environment variable Required Description <code>MESSAGE_QUEUE_TYPE</code> No Can be set to either <code>sqs</code> or <code>postgres</code>. Defaults to <code>sqs</code>. <code>MESSAGE_QUEUE_SQS_ASYNC_FIFO_URL</code> For <code>sqs</code> The URL of the SQS queue used for processing async jobs in FIFO order. Required when <code>MESSAGE_QUEUE_TYPE</code> is set to <code>sqs</code>. <code>MESSAGE_QUEUE_SQS_ASYNC_URL</code> For <code>sqs</code> The URL of the SQS queue used for processing async jobs. Required when <code>MESSAGE_QUEUE_TYPE</code> is set to <code>sqs</code>. <code>MESSAGE_QUEUE_SQS_CRONJOBS_URL</code> For <code>sqs</code> The URL of the SQS queue used for processing cronjobs. Required when <code>MESSAGE_QUEUE_TYPE</code> is set to <code>sqs</code>. <code>MESSAGE_QUEUE_SQS_DLQ_FIFO_URL</code> For <code>sqs</code> The URL of the SQS queue used as the deadletter queue for async FIFO jobs. Required when <code>MESSAGE_QUEUE_TYPE</code> is set to <code>sqs</code>. <code>MESSAGE_QUEUE_SQS_DLQ_URL</code> For <code>sqs</code> The URL of the SQS queue used as the deadletter queue for async jobs. Required when <code>MESSAGE_QUEUE_TYPE</code> is set to <code>sqs</code>. <code>MESSAGE_QUEUE_SQS_EVENTS_INBOX_URL</code> For <code>sqs</code> The URL of the SQS queue used for processing events. Required when <code>MESSAGE_QUEUE_TYPE</code> is set to <code>sqs</code>. <code>MESSAGE_QUEUE_SQS_IOT_URL</code> For <code>sqs</code> The URL of the SQS queue used for processing IoT messages from workers. Required when <code>MESSAGE_QUEUE_TYPE</code> is set to <code>sqs</code>. <code>MESSAGE_QUEUE_SQS_WEBHOOKS_URL</code> For <code>sqs</code> The URL of the SQS queue used for processing inbound VCS webhooks. Required when <code>MESSAGE_QUEUE_TYPE</code> is set to <code>sqs</code>."},{"location":"installing-spacelift/reference-architecture/reference/message-queues.html#sqs","title":"SQS","text":"<p>When using SQS the following queues need to be created, along with the suggested configuration options:</p> Name Visibility timeout Message retention Description Async 300s Default Used for processing async tasks. Async (FIFO) 300s Default Used for processing async tasks that must be processed in a certain order (e.g. run state changes). Cronjobs 300s 3600s Used to trigger scheduled task processing. Deadletter queue 300s Default Used to store messages that have failed processing too many times. Deadletter queue (FIFO) 300s Default Used to store messages from FIFO queues that have failed processing too many times. Events inbox 300s Default Used for processing async tasks. IoT 45 Default Used for processing messages sent by Spacelift workers to the IoT Core MQTT broker. Webhooks 600s Default Used for processing messages received from VCS system webhooks. <p>In addition, you should use the following configuration options for all the queues:</p> <ul> <li>A max receive count of <code>3</code>.</li> <li>The \"Async (FIFO)\" queue should have its redrive policy configured to send messages to the \"Deadletter queue (FIFO)\".</li> <li>All other queues should use the \"Deadletter queue\".</li> </ul>"},{"location":"installing-spacelift/reference-architecture/reference/message-queues.html#iot-core-topic-rule","title":"IoT Core topic rule","text":"<p>In order to allow messages sent from Spacelift workers to be processed by the backend services, an IoT Core topic rule needs to be created to publish messages sent to certain topics onto your IoT SQS queue.</p> <p>The rule should look something like this:</p> <pre><code>resource \"aws_iot_topic_rule\" \"iot-to-sqs\" {\n  # var.iot_namespace is configurable, but needs to match whatever you use for `MQTT_BROKER_IOTCORE_NAMESPACE` (defaults to `spacelift`).\n  name        = var.iot_namespace\n  description = \"Send all messages published in the ${var.iot_namespace} namespace to the ${aws_sqs_queue.iot.name} queue\"\n  enabled     = true\n  sql         = \"SELECT *, Timestamp() as timestamp, topic(3) as worker_pool_ulid, topic(4) as worker_ulid FROM '${var.iot_namespace}/writeonly/#'\"\n  sql_version = \"2016-03-23\"\n\n  sqs {\n    # queue_url should reference your IoT queue\n    queue_url  = aws_sqs_queue.iot.id\n\n    # role_arn should point to a role that is able to perform `sqs:SendMessage` on the IoT SQS queue.\n    role_arn   = aws_iam_role.iot.arn\n    use_base64 = true\n  }\n}\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/reference/message-queues.html#postgres","title":"Postgres","text":"<p>The Postgres message broker doesn't require any specific additional configuration other than setting the <code>MESSAGE_QUEUE_TYPE</code> environment variable to <code>postgres</code>.</p>"},{"location":"installing-spacelift/reference-architecture/reference/message-queues.html#monitoring-scaling","title":"Monitoring &amp; scaling","text":"<p>For detailed information on monitoring message queue performance and metrics, see the observability guide.</p>"},{"location":"installing-spacelift/reference-architecture/reference/mqtt-broker.html","title":"MQTT Broker","text":""},{"location":"installing-spacelift/reference-architecture/reference/mqtt-broker.html#mqtt-broker","title":"MQTT Broker","text":"<p>Spacelift requires an MQTT broker for worker communication. There are currently two supported broker types:</p> <ul> <li>AWS IoT Core. Recommended for customers with existing Self-Hosted installation that were setup before the built-in broker became available.</li> <li>A built-in MQTT broker bundled with the Spacelift server service. Recommended for new installations and any installations outside of AWS.</li> </ul>"},{"location":"installing-spacelift/reference-architecture/reference/mqtt-broker.html#configuration","title":"Configuration","text":"<p>The following environment variables can be used to configure your MQTT broker:</p> Environment variable Required Description <code>MQTT_BROKER_TYPE</code> No Can be set to either <code>iotcore</code> (AWS IoT Core) or <code>builtin</code>. Defaults to <code>iotcore</code>. <code>MQTT_BROKER_ENDPOINT</code> Yes The endpoint workers use to connect to the MQTT broker. <code>MQTT_BROKER_BUILTIN_LOG_LEVEL</code> No The log level used by the built-in MQTT broker. Defaults to <code>error</code>. <code>MQTT_BROKER_IOTCORE_NAMESPACE</code> For <code>iotcore</code> The top-level namespace to use for MQTT topics when using IoT Core. Defaults to <code>spacelift</code>."},{"location":"installing-spacelift/reference-architecture/reference/mqtt-broker.html#iot-core","title":"IoT Core","text":""},{"location":"installing-spacelift/reference-architecture/reference/mqtt-broker.html#message-queue-type","title":"Message queue type","text":"<p>When using IoT Core, you must use SQS instead of the built-in Postgres message queue. This is required because our IoT Core implementation relies on an IoT topic rule to automatically publish messages from workers onto the IoT message queue.</p>"},{"location":"installing-spacelift/reference-architecture/reference/mqtt-broker.html#broker-endpoint","title":"Broker endpoint","text":"<p>When using IoT Core the endpoint should be in the format <code>&lt;id&gt;-ats.iot.&lt;region&gt;.amazonaws.com</code>. You can find this address by running the following AWS CLI command (replacing <code>&lt;region&gt;</code> with the AWS region of your install):</p> <pre><code>aws iot describe-endpoint --endpoint-type iot:Data-ATS --region \"&lt;region&gt;\" --no-cli-pager --output json  | jq -r '.endpointAddress'\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/reference/mqtt-broker.html#topic-namespace","title":"Topic Namespace","text":"<p>Spacelift uses the following two IoT topic formats:</p> <ul> <li><code>&lt;namespace&gt;/readonly/&lt;workerpool-ulid&gt;/&lt;worker-ulid&gt;</code> - used for Spacelift to send messages to workers.</li> <li><code>&lt;namespace&gt;/writeonly/&lt;workerpool-ulid&gt;/&lt;worker-ulid&gt;</code> - used for workers to send messages to Spacelift.</li> </ul> <p>The namespace can be anything you want, and defaults to <code>spacelift</code>. The only requirement is that it needs to match the topic rule setup when configuring your IoT SQS queue.</p>"},{"location":"installing-spacelift/reference-architecture/reference/mqtt-broker.html#built-in-broker","title":"Built-in broker","text":""},{"location":"installing-spacelift/reference-architecture/reference/mqtt-broker.html#broker-endpoint_1","title":"Broker endpoint","text":"<p>When using the built-in IoT broker the endpoint should be in the format <code>&lt;hostname&gt;:&lt;port&gt;</code>. You can choose any hostname and port number for the broker other than <code>1983</code> (which is reserved for the HTTP server). The only requirement is that it is accessible from your workers.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html","title":"Networking","text":""},{"location":"installing-spacelift/reference-architecture/reference/networking.html#networking","title":"Networking","text":"<p>Spacelift is made up of a number of containerised components, along with certain external dependencies like a Postgres database and object storage. The following sections explain the different components that make up Spacelift, explains the role they perform and the specific networking requirements that they have.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#container-registries","title":"Container registries","text":"<p>Spacelift relies on two container images to function: the backend image, and the launcher image. The environment you deploy Spacelift into (for example a Kubernetes or ECS cluster) needs to be able to pull the backend image. If you deploy your workers using our Kubernetes operator the Kubernetes cluster will need to be able to pull the launcher image as well.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#server","title":"Server","text":"<p>The server runs an HTTP server as well as an MQTT broker. The server is responsible for serving the frontend, providing HTTP APIs, as well as enabling communication with Spacelift workers over MQTT. The server uses MQTT for certain aspects of worker communication to allow it to \"broadcast\" messages to workers, even if they are not directly accessible by the Spacelift backend components.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#ingress","title":"Ingress","text":"Name Port Protocol Optional/Required Description HTTP 1983 TCP Required Used for serving HTTP requests like the frontend, GraphQL API and inbound webhooks. MQTT 1984 TCP Optional Used for serving the MQTT broker server. Required when using the built-in MQTT broker."},{"location":"installing-spacelift/reference-architecture/reference/networking.html#egress","title":"Egress","text":"Name Port Protocol Optional/Required Description Postgres User-defined TCP Required Outbound access to the Postgres database. By default this is <code>5432</code>. Object Storage 443 TCP Required Outbound access to your object storage buckets. VCS 443 TCP Required Outbound access to customer source control system. Message Queue 443 TCP Optional Only required when using SQS. MQTT Broker 443 TCP Optional Only required when using IoT Core. KMS 443 TCP Optional Only required when using KMS."},{"location":"installing-spacelift/reference-architecture/reference/networking.html#drain","title":"Drain","text":"<p>The drain handles asynchronous job processing. This component is responsible for processing inbound webhooks from your VCS system, along with other tasks like run scheduling.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#ingress_1","title":"Ingress","text":"<p>No inbound access to the drain is required.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#egress_1","title":"Egress","text":"Name Port Protocol Optional/Required Description Postgres User-defined TCP Required Outbound access to the Postgres database. By default this is <code>5432</code>. Object Storage 443 TCP Required Outbound access to your object storage buckets. VCS 443 TCP Required Outbound access to your source control system. Message Queue 443 TCP Optional Only required when using SQS MQTT Broker 443 TCP Optional Only required when using IoT Core. KMS 443 TCP Optional Only required when using KMS."},{"location":"installing-spacelift/reference-architecture/reference/networking.html#scheduler","title":"Scheduler","text":"<p>The scheduler handles triggering routine cron jobs required for Spacelift to function. Processing of these jobs once they are triggered is handled by the drain.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#ingress_2","title":"Ingress","text":"<p>No inbound access to the scheduler is required.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#egress_2","title":"Egress","text":"Name Port Protocol Optional/Required Description Postgres User-defined TCP Required Outbound access to the Postgres database. By default this is <code>5432</code>. Message Queue 443 TCP Optional Only required when using SQS"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#workers","title":"Workers","text":"<p>Workers are responsible for executing runs and tasks within Spacelift. This is where the execution of your infrastructure as code tools is performed. Workers do not need to be deployed to the same network as the Spacelift backend, allowing you to manage infrastructure in other cloud environments than the Spacelift backend is deployed to or even on-prem.</p> <p>For more information on workers please see our worker pool documentation.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#ingress_3","title":"Ingress","text":"<p>No inbound access to your workers is required.</p>"},{"location":"installing-spacelift/reference-architecture/reference/networking.html#egress_3","title":"Egress","text":"Name Port Protocol Optional/Required Description Spacelift Server 443 TCP Required Access to the Spacelift Server for making synchronous requests (for example notifying state changes and retrieving run log URLs). MQTT Broker 443 or user-defined TCP Required Used for receiving broadcast messages from the server (for example job scheduling messages). The exact hostname and port depends on the type of MQTT broker in use. Object Storage 443 TCP Required Used to access run information and upload run logs using pre-signed URLs generated by the Spacelift backend services. VCS 443 TCP Required Access to your source control system to download source code. Container registries 443 TCP Required Access to the Spacelift launcher image (for Kubernetes workers), along with the registry containing any custom runner images. Infrastructure User-defined TCP Required Access to the APIs for any infrastructure components you are using Spacelift to manage."},{"location":"installing-spacelift/reference-architecture/reference/object-storage.html","title":"Object storage","text":""},{"location":"installing-spacelift/reference-architecture/reference/object-storage.html#object-storage","title":"Object storage","text":"<p>Spacelift requires access to object storage buckets to store certain pieces of data. Currently AWS S3, Google Cloud Storage, Azure Blob Storage and MinIO are supported.</p> <p>The following table explains each of the buckets that are required. For each bucket, we indicate whether they should have versioning enabled, as well as the retention rules that should be configured. Where the retention is listed as \"user configurable\", you can adjust the retention to suit your needs. A suggested value is included to give you a reasonable starting point.</p> Name Versioning Retention Description Deliveries No User configurable (1 day) Used to store the request and response bodies of outbound webhooks. Large queue messages No 2 days Stores inbound webhook payloads that are too large to be posted to a message queue. Metadata No 2 days Stores the metadata required by workers to process runs. Modules Suggested Infinite Stores Terraform modules and providers. Policy inputs No User configurable (7 days) Stores sampled policy evaluations. Run logs Suggested User configurable (60 days) Stores the content of run logs. States Required Infinite Stores OpenTofu/Terraform state files when using the built-in Spacelift state server. Uploads Suggested 1 day Used to temporarily upload files from the frontend. User uploaded workspaces Suggested 1 day Used to store workspaces uploaded from user machines as part of the local preview functionality. Workspace Suggested 7 days Used to temporarily store the workspaces of runs that go into an unconfirmed state."},{"location":"installing-spacelift/reference-architecture/reference/object-storage.html#configuration","title":"Configuration","text":"<p>The following environment variables can be used to configure object storage:</p> Environment variable Required Description <code>OBJECT_STORAGE_TYPE</code> No Can be set to <code>aws</code> (AWS S3), <code>gcp</code> (Google Cloud Storage), <code>azure</code> (Azure Blob Storage), <code>minio</code> (MinIO). Defaults to <code>aws</code>. <code>OBJECT_STORAGE_AZURE_ACCOUNT_URL</code> Yes if <code>azure</code> The URL of the Azure storage account e.g. https://{account}.blob.core.windows.net <code>OBJECT_STORAGE_MINIO_ENDPOINT</code> Yes if <code>minio</code> The MinIO server endpoint URL (e.g., minio.example.com:9000). <code>OBJECT_STORAGE_MINIO_ACCESS_KEY_ID</code> Yes if <code>minio</code> The access key ID for MinIO authentication. <code>OBJECT_STORAGE_MINIO_SECRET_ACCESS_KEY</code> Yes if <code>minio</code> The secret access key for MinIO authentication. <code>OBJECT_STORAGE_MINIO_USE_SSL</code> No Whether to use SSL/TLS for MinIO connections. Defaults to <code>false</code>. <code>OBJECT_STORAGE_MINIO_ALLOW_INSECURE</code> No Whether to allow insecure SSL connections to MinIO. Defaults to <code>false</code>. <code>OBJECT_STORAGE_BUCKET_DELIVERIES</code> Yes Bucket where webhook delivery traces are stored. <code>OBJECT_STORAGE_BUCKET_LARGE_QUEUE_MESSAGES</code> Yes Bucket where message payloads too large for storing on message queues are stored. <code>OBJECT_STORAGE_BUCKET_METADATA</code> Yes Bucket used to store metadata needed for workers to execute runs. <code>OBJECT_STORAGE_BUCKET_MODULES</code> Yes Bucket where OpenTofu/Terraform module source is stored. <code>OBJECT_STORAGE_BUCKET_POLICY_INPUTS</code> Yes Bucket where policy evaluation samples are stored. <code>OBJECT_STORAGE_BUCKET_RUN_LOGS</code> Yes Bucket where run logs are stored. <code>OBJECT_STORAGE_BUCKET_STATES</code> Yes Bucket used to store stack state files. <code>OBJECT_STORAGE_BUCKET_UPLOADS</code> Yes Bucket used to temporarily store files uploaded from the frontend. <code>OBJECT_STORAGE_BUCKET_UPLOADS_URL</code> Yes The URL of the uploads bucket. This is used to generate a Content Security Policy (CSP) to allow the frontend to upload to this bucket. <code>OBJECT_STORAGE_BUCKET_USER_UPLOADED_WORKSPACES</code> Yes Bucket where workspaces uploaded as part of local preview functionality are stored temporarily. <code>OBJECT_STORAGE_BUCKET_WORKSPACE</code> Yes Bucket where run workspaces are stored."},{"location":"installing-spacelift/reference-architecture/reference/object-storage.html#access-requirements","title":"Access requirements","text":"<p>None of our buckets need public access - they just need access from the Spacelift backend services. For certain situations where access to buckets is required from outside the backend services, for example when uploading state files from the frontend, or when workers upload run logs, we rely on pre-signed URLs that are only valid for a certain period of time.</p>"},{"location":"installing-spacelift/reference-architecture/reference/object-storage.html#authentication","title":"Authentication","text":"AWS S3Azure Blob StorageGoogle Cloud StorageMinIO <p>When using AWS S3, Spacelift uses the default AWS SDK credential provider chain, which automatically finds credentials in the following order: environment variables (<code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>), shared credential files and IAM roles for compute resources like EC2, ECS and EKS.</p> <p>When using Azure Blob Storage, Spacelift uses the DefaultAzureCredential authentication chain. This automatically supports multiple authentication methods including environment variables, Workload Identity (recommended for AKS), and Managed Identity.</p> <p>When using Google Cloud Storage, Spacelift uses Application Default Credentials (ADC), which automatically finds credentials from multiple sources including: <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable, attached service accounts for compute resources, and Workload Identity Federation for containerized environments like GKE.</p> <p>When using MinIO, Spacelift requires explicit configuration through environment variables: <code>OBJECT_STORAGE_MINIO_ENDPOINT</code> for the server endpoint, <code>OBJECT_STORAGE_MINIO_ACCESS_KEY_ID</code> for the access key, and <code>OBJECT_STORAGE_MINIO_SECRET_ACCESS_KEY</code> for the secret key.</p>"},{"location":"installing-spacelift/reference-architecture/reference/object-storage.html#uploads-bucket-cors-configuration","title":"Uploads bucket CORS configuration","text":"<p>The uploads bucket is used to allow users to upload certain items like Terraform state files from the frontend. To allow this to work, the bucket needs to be configured with the frontend URL as an allowed origin. The following examples show how to configure this using Terraform assuming you were hosting Spacelift at <code>spacelift.myorg.com</code>:</p> AWS S3Google Cloud StorageAzure Blob Storage <pre><code>resource \"aws_s3_bucket_cors_configuration\" \"spacelift-uploads\" {\n    bucket = aws_s3_bucket.spacelift-uploads.id\n\n    cors_rule {\n        allowed_headers = [\"*\"]\n        allowed_methods = [\"PUT\", \"POST\"]\n        allowed_origins = [\"https://spacelift.myorg.com\"]\n    }\n}\n</code></pre> <pre><code>resource \"google_storage_bucket\" \"spacelift-uploads\" {\n    name = # bucket name\n    location = # location\n\n    public_access_prevention = \"enforced\"\n\n    cors {\n        origin = [\"https://spacelift.myorg.com\"]\n        method = [\"PUT\", \"POST\"]\n        response_header = [\"*\"]\n        max_age_seconds = 3600\n    }\n\n    ... # more configuration options\n}\n</code></pre> <pre><code>resource \"azurerm_storage_account\" \"spacelift_storage_account\" {\n  # Some of the configuration options are omitted for brevity.\n  blob_properties {\n    versioning_enabled = true\n    cors_rule {\n      allowed_headers    = [\"*\"]\n      allowed_methods    = [\"PUT\", \"POST\"]\n      allowed_origins    = [\"https://spacelift.myorg.com\"]\n      exposed_headers    = [\"*\"]\n      max_age_in_seconds = 3600\n    }\n  }\n}\n</code></pre> <p>Note that the platform-specific Terraform modules (terraform-azure-spacelift-selfhosted, terraform-google-spacelift-selfhosted, terraform-aws-spacelift-selfhosted) take care of the CORS configuration.</p>"},{"location":"installing-spacelift/reference-architecture/reference/slack.html","title":"Slack","text":""},{"location":"installing-spacelift/reference-architecture/reference/slack.html#slack","title":"Slack","text":"<p>The Spacelift Slack integration relies on a Slack app being created to allow you to link Spacelift to your Slack workspace. There are two ways you can do this:</p> <ol> <li>Dynamically via your organization settings.</li> <li>Statically via the environment.</li> </ol> <p>To find out how to dynamically configure your Slack app via the frontend, take a look at our Slack integration documentation. This page explains how to configure Slack statically via the environment.</p>"},{"location":"installing-spacelift/reference-architecture/reference/slack.html#networking","title":"Networking","text":"<p>Please note that in order for Slack to send messages to Spacelift (for example Slash Commands), Slack needs to be able to access your Spacelift server. This means that if you want the full functionality of the integration to work your Spacelift server needs to be accessible via the public internet.</p>"},{"location":"installing-spacelift/reference-architecture/reference/slack.html#create-slack-app","title":"Create Slack app","text":"<p>Before you can configure Slack, you need to create a Slack app in your workspace. This app provides credentials necessary for Spacelift and Slack to communicate, and also tells Slack about certain endpoints in Spacelift that it needs to use.</p> <p>Create a new Slack app in your workspace by navigating to https://api.slack.com/apps and following these instructions:</p> <ul> <li>Click \"Create New App\"</li> <li>Choose \"From an app manifest\"</li> <li>Select your workspace</li> <li>Paste following manifest, replacing <code>&lt;your-domain&gt;</code> with the domain you want to host the self-hosted Spacelift instance on.</li> </ul> <pre><code>{\n  \"display_information\": {\n    \"name\": \"Spacelift\",\n    \"description\": \"Taking your infra-as-code to the next level\",\n    \"background_color\": \"#131417\",\n    \"long_description\": \"Spacelift is a sophisticated and compliant infrastructure delivery platform for Terraform (including Terragrunt), Pulumi, CloudFormation, Ansible, and Kubernetes.\\r\\n\\r\\n\u2022 No lock-in. Under the hood, Spacelift uses your choice of Infrastructure as Code providers: open-source projects with vibrant ecosystems and a multitude of existing providers, modules, and tutorials.\\r\\n\\r\\n\u2022 Works with your Git flow. Spacelift integrates with GitHub (and other VCSes) to provide feedback on commits and Pull Requests, allowing you and your team to preview the changes before they are applied.\\r\\n\\r\\n\u2022 Drift detection. Spacelift natively detects drift, and can optionally revert it, to provide visibility and awareness to those \\\"changes\\\" that will inevitably happen.\\r\\n\\r\\n\u2022 Policy as a Code. With Open Policy Agent (OPA) Rego, you can programmatically define policies, approval flows, and various decision points within your Infrastructure as Code flow.\\r\\n\\r\\n\u2022 Customize your runtime. Spacelift uses Docker to run its workflows, which allows you to fully control your execution environment.\\r\\n\\r\\n\u2022 Share config using contexts. Spacelift contexts are collections of configuration files and environment variables that can be attached to multiple stacks.\\r\\n\\r\\n\u2022 Look ma, no credentials. Spacelift integrates with identity management systems from major cloud providers; AWS, Azure, and Google Cloud, allowing you to set up limited temporary access to your resources without the need to supply powerful static credentials.\\r\\n\\r\\n\u2022 Manage programmatically. With the Terraform provider, you can manage Spacelift resources as code.\\r\\n\\r\\n\u2022 Protect your state. Spacelift supports a sophisticated state backend and can optionally manage the state on your behalf.\"\n  },\n  \"features\": {\n    \"bot_user\": {\n      \"display_name\": \"Spacelift\",\n      \"always_online\": true\n    },\n    \"slash_commands\": [\n      {\n        \"command\": \"/spacelift\",\n        \"url\": \"https://&lt;your-domain&gt;/webhooks/slack\",\n        \"description\": \"Get notified about Spacelift events\",\n        \"usage_hint\": \"subscribe, unsubscribe or help\",\n        \"should_escape\": false\n      }\n    ]\n  },\n  \"oauth_config\": {\n    \"redirect_urls\": [\"https://&lt;your-domain&gt;/slack_oauth\"],\n    \"scopes\": {\n      \"bot\": [\n        \"channels:read\",\n        \"chat:write\",\n        \"chat:write.public\",\n        \"commands\",\n        \"links:write\",\n        \"team:read\",\n        \"users:read\"\n      ]\n    }\n  },\n  \"settings\": {\n    \"event_subscriptions\": {\n      \"request_url\": \"https://&lt;your-domain&gt;/webhooks/slack\",\n      \"bot_events\": [\"app_uninstalled\"]\n    },\n    \"interactivity\": {\n      \"is_enabled\": true,\n      \"request_url\": \"https://&lt;your-domain&gt;/webhooks/slack\"\n    },\n    \"org_deploy_enabled\": false,\n    \"socket_mode_enabled\": false,\n    \"token_rotation_enabled\": false\n  }\n}\n</code></pre>"},{"location":"installing-spacelift/reference-architecture/reference/slack.html#configuration","title":"Configuration","text":"<p>The following table contains the environment variables required for the Slack integration to work:</p> Environment variable Description <code>SLACK_APP_CLIENT_ID</code> Corresponds to the Client ID of your Slack app. <code>SLACK_APP_CLIENT_SECRET</code> Corresponds to the Client Secret of your Slack app. <code>SLACK_SECRET</code> Corresponds to the Signing Secret of your Slack app."},{"location":"installing-spacelift/reference-architecture/reference/telemetry.html","title":"Telemetry","text":""},{"location":"installing-spacelift/reference-architecture/reference/telemetry.html#telemetry","title":"Telemetry","text":"<p>The Spacelift backend services emit telemetry data that can be collected and visualised using a variety of tools. This document describes how to configure Spacelift to emit those data.</p>"},{"location":"installing-spacelift/reference-architecture/reference/telemetry.html#supported-telemetry-backends","title":"Supported telemetry backends","text":"<p>The telemetry backend can be set by configuring the <code>OBSERVABILITY_VENDOR</code> environment variable. The following backends are supported:</p> Backend Environment variable value Notes Datadog <code>Datadog</code> Uses the Datadog agent for both tracing and metrics. AWS X-Ray <code>AWS</code> Uses the AWS X-Ray daemon for tracing and CloudWatch for metrics. OpenTelemetry <code>OpenTelemetry</code> Uses the OpenTelemetry collector for both tracing and metrics. Disabled <code>Disabled</code> Disables telemetry collection entirely. Same effect as not having the variable. <p>Note</p> <p>All of our services (server, drain, scheduler and workers) support the <code>OBSERVABILITY_VENDOR</code> configuration.</p> <p>Note</p> <p>When using X-Ray, make sure <code>cloudwatch:PutMetricsData</code> IAM permission is granted to the service's role.</p>"},{"location":"installing-spacelift/reference-architecture/reference/telemetry.html#configuration-options","title":"Configuration options","text":"<p>All backends share a common requirement: the only mandatory configuration is the respective agent's address, provided as an environment variable. Additional settings are optional.</p> <p>Note</p> <p>Spacelift automatically sets the service name based on the active service, so we recommend not to define <code>DD_SERVICE</code>, <code>AWS_XRAY_TRACING_NAME</code>, or <code>OTEL_SERVICE_NAME</code>.</p>"},{"location":"installing-spacelift/reference-architecture/reference/telemetry.html#datadog","title":"Datadog","text":"<p>The Datadog SDK's only required environment variable is the <code>DD_AGENT_HOST</code> environment variable. There are a few other optional environment variables that can be set, such as <code>DD_ENV</code>, <code>DD_SERVICE</code>, <code>DD_TAGS</code>, etc. For a full list of available environment variables, see the Datadog Go SDK documentation.</p>"},{"location":"installing-spacelift/reference-architecture/reference/telemetry.html#aws-x-ray","title":"AWS X-Ray","text":"<p>For X-Ray, you need to provide the <code>AWS_XRAY_DAEMON_ADDRESS</code> variable. Optional variables include <code>AWS_XRAY_CONTEXT_MISSING</code>, <code>AWS_XRAY_TRACING_NAME</code>, etc. See the AWS X-Ray Go SDK documentation for more details.</p>"},{"location":"installing-spacelift/reference-architecture/reference/telemetry.html#opentelemetry","title":"OpenTelemetry","text":"<p>OpenTelemetry requires <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> to point to the OpenTelemetry collector. Important: the Spacelift application is configured to use the GRPC protocol for communication with the agent so make sure it is enabled on the collector's side. Eg.:</p> <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n</code></pre> <p>In this example, the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> should be set to something like <code>http://&lt;collector-address&gt;:4317</code>. Please note that even though the endpoint is GRPC, the <code>http://</code> prefix is still needed.</p> <p>The rest of the configuration options can be found in the OpenTelemetry SDK documentation.</p>"},{"location":"installing-spacelift/reference-architecture/reference/usage-reporting.html","title":"Usage reporting","text":""},{"location":"installing-spacelift/reference-architecture/reference/usage-reporting.html#usage-reporting","title":"Usage reporting","text":"<p>Spacelift supports three methods for reporting usage: automatic, manual, and via script. Please choose the method that best suits your environment and operational requirements to ensure timely and accurate reporting.</p>"},{"location":"installing-spacelift/reference-architecture/reference/usage-reporting.html#automatic","title":"Automatic","text":"<p>Best for: Standard deployments with internet access.</p> <p>The easiest way to enable usage reporting is to set the <code>SPACELIFT_PUBLIC_API</code> environment variable to point to our public API (<code>https://app.spacelift.io</code>). Ensure that the Drain service can access this endpoint.</p> Environment variable Required Description <code>SPACELIFT_PUBLIC_API</code> No Should be pointed at Spacelift's public API (<code>https://app.spacelift.io</code>). Defaults to empty string."},{"location":"installing-spacelift/reference-architecture/reference/usage-reporting.html#manual","title":"Manual","text":"<p>Best for: Air-gapped environments or networks with restricted internet access.</p> <p>If your instance cannot reach the public API (e.g., due to network restrictions), you can export usage data manually:</p> <p>Navigate to Organization Settings in the Spacelift UI, select Usage Export from the left menu, choose the desired date range and click Export to download the report. Finally, share the file with your Spacelift contact.</p> <p></p>"},{"location":"installing-spacelift/reference-architecture/reference/usage-reporting.html#python-script","title":"Python script","text":"<p>Best for: Automated workflows or programmatic access to usage data.</p> <p>You can also use the self-hosted-usage-data-exporter script to fetch and optionally upload usage data.</p> <p>By default, it generates a local report. Use the <code>--send-to-spacelift</code> flag to upload it automatically to Spacelift\u2019s backend, eliminating the need to share the file manually.</p>"},{"location":"integrations/api-development-with-mcp.html","title":"Spacelift API integration via spacectl MCP server","text":""},{"location":"integrations/api-development-with-mcp.html#spacelift-api-integration-via-spacectl-mcp-server","title":"Spacelift API integration via <code>spacectl</code> MCP server","text":"<p>You don't need to learn the Spacelift GraphQL API. We've built GraphQL introspection tooling into <code>spacectl</code>'s MCP server that lets your coding assistant discover and use the API automatically.</p>"},{"location":"integrations/api-development-with-mcp.html#how-it-works","title":"How it works","text":"<p>The <code>spacectl</code> MCP server includes:</p> <ul> <li>Complete GraphQL schema introspection.</li> <li>Authentication guide with working examples.</li> <li>Field and operation search capabilities.</li> <li>Live API exploration.</li> </ul> <p>Your coding assistant uses these tools to discover the API structure, understand authentication, and generate working code in any language.</p>"},{"location":"integrations/api-development-with-mcp.html#setup","title":"Setup","text":""},{"location":"integrations/api-development-with-mcp.html#install-spacectl","title":"Install <code>spacectl</code>","text":"<p>First, install <code>spacectl</code> following the installation instructions in the repository.</p> <p>After installation, log in to your Spacelift account:</p> <pre><code>spacectl profile login\n</code></pre> <p>See the <code>spacectl</code> documentation for more details on authentication and usage.</p>"},{"location":"integrations/api-development-with-mcp.html#configure-your-coding-assistant","title":"Configure your coding assistant","text":"<p>Configure <code>spacectl</code> as an MCP server in your coding assistant.</p> <p>Claude Code example:</p> <p>Add this to your Claude Code MCP configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"spacectl\": {\n      \"command\": \"spacectl\",\n      \"args\": [\"mcp\", \"server\"]\n    }\n  }\n}\n</code></pre> <p>The server provides tools for API discovery and includes authentication handling.</p>"},{"location":"integrations/api-development-with-mcp.html#authentication","title":"Authentication","text":"<p>The MCP server includes a complete authentication guide covering:</p> <ul> <li>API key setup and token exchange</li> <li>GitHub token authentication (for GitHub SSO accounts)</li> <li>OIDC-based authentication</li> <li><code>spacectl</code> CLI token export</li> </ul> <p>Your coding assistant will automatically retrieve this information when building applications.</p>"},{"location":"integrations/api-development-with-mcp.html#what-your-coding-assistant-can-discover","title":"What your coding assistant can discover","text":""},{"location":"integrations/api-development-with-mcp.html#api-structure","title":"API structure","text":"<ul> <li>All available queries, mutations, and subscriptions</li> <li>Field definitions and types</li> <li>Required vs optional parameters</li> <li>Return types and nested structures</li> </ul>"},{"location":"integrations/api-development-with-mcp.html#operations","title":"Operations","text":"<ul> <li>Stack management (create, update, delete, trigger runs)</li> <li>Module registry access</li> <li>Resource monitoring</li> <li>Run execution and monitoring</li> <li>Policy management</li> </ul>"},{"location":"integrations/api-development-with-mcp.html#authentication-flows","title":"Authentication flows","text":"<ul> <li>Token exchange patterns</li> <li>Error handling</li> <li>Token refresh logic</li> <li>Permission requirements</li> </ul>"},{"location":"integrations/api-development-with-mcp.html#development-workflow","title":"Development workflow","text":""},{"location":"integrations/api-development-with-mcp.html#step-1-tell-your-assistant-what-you-want-to-build","title":"Step 1: Tell your assistant what you want to build","text":"<ul> <li>\"Build a React dashboard showing stack status\"</li> <li>\"Create a Python script for automated deployments\"</li> <li>\"Make a CLI tool for managing stacks\"</li> </ul>"},{"location":"integrations/api-development-with-mcp.html#step-2-assistant-explores-the-api","title":"Step 2: Assistant explores the API","text":"<ul> <li>Introspects GraphQL schema</li> <li>Finds relevant operations</li> <li>Understands data structures</li> </ul>"},{"location":"integrations/api-development-with-mcp.html#step-3-assistant-generates-working-code","title":"Step 3: Assistant generates working code","text":"<ul> <li>Handles authentication setup</li> <li>Creates properly typed API clients</li> <li>Implements error handling</li> <li>Follows best practices</li> </ul>"},{"location":"integrations/api-development-with-mcp.html#example-applications","title":"Example applications","text":"<p>Your assistant can build:</p> <ul> <li>Dashboards: Stack monitoring, deployment history, resource visualization</li> <li>Automation: CI/CD integrations, scheduled deployments, compliance checking</li> <li>Mobile apps: Deployment approvals, status monitoring, notifications</li> <li>CLI tools: Developer productivity, batch operations, administrative tasks</li> <li>Integrations: Slack bots, ticketing systems, monitoring tools</li> </ul>"},{"location":"integrations/api-development-with-mcp.html#language-support","title":"Language support","text":"<p>The introspection works with any language or framework:</p> <ul> <li>Web: React, Vue, Angular, Next.js, plain Javascript</li> <li>Backend: Node.js, Python, Go, Java, C#, Ruby</li> <li>Mobile: React Native, Flutter, native development</li> <li>Desktop: Electron, native applications</li> <li>Infrastructure: Terraform providers, Kubernetes operators</li> </ul>"},{"location":"integrations/api-development-with-mcp.html#getting-started","title":"Getting started","text":"<ol> <li>Install <code>spacectl</code>.</li> <li>Configure it as an MCP server in your coding assistant.</li> <li>Start building. The assistant will handle API discovery, authentication setup, and code generation automatically.</li> </ol> <p>No API documentation reading required.</p>"},{"location":"integrations/api.html","title":"GraphQL API","text":""},{"location":"integrations/api.html#graphql-api","title":"GraphQL API","text":""},{"location":"integrations/api.html#graphql","title":"GraphQL","text":"<p>GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL:</p> <ul> <li>Provides a complete and understandable description of the data in your API.</li> <li>Gives clients the power to ask for exactly what they need and nothing more.</li> <li>Makes it easier to evolve APIs over time.</li> <li>Enables powerful developer tools.</li> </ul> <p>Spacelift provides a GraphQL API for you to control your Spacelift account programmatically and/or through an API Client if you choose to do so. A smaller subset of this API is also used by the Spacelift Terraform provider, as well as the Spacelift CLI (spacectl). The API can be accessed at the <code>/graphql</code> endpoint of your account using <code>POST</code> HTTPS method.</p> <p>Quick start with AI coding assistants</p> <p>The fastest way to build applications against our API is using a coding assistant with our MCP server. You don't need to learn the GraphQL API because the assistant discovers it automatically. See API development with MCP for setup instructions.</p>"},{"location":"integrations/api.html#example-request-and-response","title":"Example request and response","text":"<pre><code>$ curl --request POST \\\n  --url https://&lt;account-name&gt;.app.spacelift.io/graphql \\\n  --header 'Authorization: Bearer &lt;token&gt;' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\"query\":\"{ stacks { id name, administrative, createdAt, description }}\"}'\n</code></pre> <p>The request body looks like this when formatted a bit nicer:</p> <pre><code>{\n  stacks\n  {\n    id\n    name,\n    administrative,\n    createdAt,\n    description\n  }\n}\n</code></pre> <p>And the response looks like this:</p> <pre><code>{\n  \"data\": {\n    \"stacks\": [\n      {\n        \"id\": \"my-stack-1\",\n        \"name\": \"My Stack 1\",\n        \"administrative\": false,\n        \"createdAt\": 1672916942,\n        \"description\": \"The is my first stack\"\n      },\n      {\n        \"id\": \"my-stack-2\",\n        \"name\": \"My Stack 2\",\n        \"administrative\": false,\n        \"createdAt\": 1674218834,\n        \"description\": \"The is my second stack\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"integrations/api.html#what-tool-should-i-use-to-authenticate","title":"What tool should I use to authenticate?","text":"<p>Our recommendation is to use the Spacelift API Key to authenticate with the GraphQL API.</p> <p>Our tool of choice is Insomnia, a free, open-source tool that allows you to easily create and manage API requests. You can also use Postman, but the walkthrough in this guide will be based on Insomnia.</p>"},{"location":"integrations/api.html#view-the-graphql-schema","title":"View the GraphQL schema","text":"<p>Our GraphQL schema is self-documenting. The best way to view the latest documentation is using a dedicated GraphQL client like Insomnia or GraphiQL. You can also view the documentation using a static documentation website generator like GraphDoc.</p> <p>Make sure to provide a valid JWT bearer token as described in Authenticating with the GraphQL API.</p> <p>Note</p> <p>The latest version of Postman does not currently support viewing GraphQL Schemas from a URL, but does support autocompletion.</p>"},{"location":"integrations/api.html#insomnia","title":"Insomnia","text":"<ol> <li>Enter the GraphQL Endpoint for your Spacelift account.</li> <li>Click Schema, then Show Documentation.   </li> </ol>"},{"location":"integrations/api.html#graphiql","title":"GraphiQL","text":"<ol> <li>Enter the GraphQL Endpoint for your Spacelift Account, then click Docs.     </li> <li>Use the Documentation Explorer within GraphiQL.     </li> </ol>"},{"location":"integrations/api.html#example-usage","title":"Example usage","text":"<p>In this example, we'll generate your Spacelift token with spacectl and use it to communicate with Spacelift.</p> <p>Prerequisites:</p> <ul> <li>Insomnia downloaded and installed.</li> <li>Spacelift account with admin access (for ability to create API Keys).</li> </ul>"},{"location":"integrations/api.html#authenticating-with-the-graphql-api","title":"Authenticating with the GraphQL API","text":"<p>If your Spacelift account were called <code>example</code>, you could access your GraphQL by sending POST requests to: <code>https://example.app.spacelift.io/graphql</code>.</p> <p>All requests need to be authenticated using a JWT bearer token. There are currently three ways of obtaining this token:</p> <ol> <li>Spacelift API Key: For long-term use (recommended).</li> <li>SpaceCTL CLI: For temporary use.</li> <li>Personal GitHub Token</li> </ol>"},{"location":"integrations/api.html#spacelift-api-key","title":"Spacelift API key","text":"<p>You can generate the JWT token with a Spacelift API key, ideal for long-term use. Spacelift supports creating and managing machine users with programmatic access to the Spacelift GraphQL API. These \"machine users\" are called API Keys and can be created by Spacelift admins through the Settings panel.</p> <p>There are two types of API keys: more traditional, secret-based keys, and keys based on OIDC identity federation.</p> <p>API key billing</p> <p>API keys are virtual users and are billed like regular users, too. Thus, each API key used (exchanged for a token) during any given billing cycle counts against the total number of users.</p>"},{"location":"integrations/api.html#secret-based-api-keys","title":"Secret-based API keys","text":"<p>Secret-based keys exchange an API key ID and secret for a JWT token, identical to how IAM user keys work. They're more flexible, but less secure because they involve static credentials. Here is how to create a secret-based API key in the Spacelift UI:</p> <ol> <li>In the lower right hand corner menu, click your account name and select Organization settings.     </li> <li>In the Access section, click API keys, then Create API key.     </li> <li>Fill in the details for your API key:     <ul> <li>Name: An arbitrary key name. We recommend you choose something memorable, ideally reflecting the purpose of the key.</li> <li>Type: Select Secret.</li> <li>Space: Select the spaces the key should have access to, along with access level (reader vs writer). If you are using login policies, you will need to define the API key in the policy for non-admin keys.</li> <li>Groups: Enter the group(s) the key should belong to. Groups give the API key a virtual group membership for scenarios where you'd prefer to control access to resources on group/team level rather than individual level.</li> </ul> </li> <li> <p>Click Create. The API key will be generated in a file and automatically downloaded to your device.</p> <p></p> <ul> <li>The file contains the API token in two forms: one to be used with our API, and the other as a <code>.terraformrc</code> snippet to access your private modules outside of Spacelift.</li> </ul> </li> </ol> <p>Note</p> <p>Giving \"admin\" permissions on the \"root\" space makes the key administrative.</p> <p>The config file looks something like this:</p> <pre><code># Spacelift API Key Configuration\n\n[credentials]\napi_key_id     = ID_VALUEW2EWGQ9F7AVF41CG1\napi_key_secret = SECRET_VALUE40ffc46887297384892384789239\n\n# Usage Options:\n#\n# - Programmatic Access:\n#   Use the api_key_secret above in your API calls\n#\n# - UI Login:\n#   Visit /apikeytoken and enter the credentials above\n\n# Terraform Module Access:\n# Add this snippet to your .terraformrc file to access \n# Spacelift-hosted Terraform modules outside of Spacelift:\n\ncredentials \"spacelift.io\" {\n  token = \"TOKEN_VALUEQwZmZjNDY4ODdiMjI2ZWE4NDhjMWQwNWZiMWE5MGU4NWMwZTFlY2Q4NDAxMGI2ZjA2NzkwMmI1YmVlMWNmMGE\"\n}\n</code></pre> <p>Warning</p> <p>Make sure you save this data somewhere on your end. Spacelift doesn't store the token, and it cannot be retrieved or recreated afterwards.</p> <p>For programmatic access, exchange the key ID and secret pair for an API token using a GraphQL mutation:</p> <pre><code>mutation GetSpaceliftToken($id: ID!, $secret: String!) {\n  apiKeyUser(id: $id, secret: $secret) {\n    jwt\n  }\n}\n</code></pre> <p>Once you obtain the token, you can use it to authenticate your requests to the Spacelift API.</p>"},{"location":"integrations/api.html#oidc-based-api-keys","title":"OIDC-based API keys","text":"<p>OIDC-based API keys are a more secure alternative to secret-based API keys. They're based on the OpenID Connect protocol and are more secure because they don't involve static credentials. They're also more flexible because they can be used to authenticate with Spacelift using any OIDC identity provider.</p> <ol> <li>In the lower right hand corner menu, click your account name and select Organization settings.     </li> <li>In the Access section, click API keys, then Create API key.     </li> <li>Fill in the details for your API key:     <ul> <li>Name: An arbitrary key name. We recommend you choose something memorable, ideally reflecting the purpose of the key.</li> <li>Type: Select OIDC.</li> <li>Issuer: The URL your OIDC provider reports as the token issuer in the <code>iss</code> claim of your JWT token. For GitHub Actions, this is <code>https://token.actions.githubusercontent.com</code>.</li> <li>Client ID (audience): The client ID of the OIDC application you created in the identity provider, in the <code>aud</code> claim of your JWT token. Some identity providers allow this to be customized.</li> <li>Subject Expression: A regular expression that needs to match the <code>sub</code> claim of your JWT token. Use this to restrict API key access to a specific source.</li> <li>Space: Select the spaces the key should have access to, along with access level (reader vs writer). If you are using login policies, you will need to define the API key in the policy for non-admin keys.</li> <li>Groups: Enter the group(s) the key should belong to. Groups give the API key a virtual group membership for scenarios where you'd prefer to control access to resources on group/team level rather than individual level.</li> </ul> </li> <li>Click Create. The API key will be generated in a file and automatically downloaded to your device.</li> </ol> <p>Warning</p> <p>Make sure you save the data in your API key file somewhere on your end. Spacelift doesn't store the token, and it cannot be retrieved or recreated afterwards.</p> <p>Here is a sample workflow using the key we just created and spacectl in GitHub Actions, without the need for any static credentials:</p> <pre><code>name: List Spacelift stacks\n\non: [push]\n\njobs:\n  test:\n    name: List Spacelift stacks\n    runs-on: ubuntu-latest\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    steps:\n      - name: Generate token\n        run: |\n          OIDC_TOKEN=$(curl -H \"Authorization: bearer $ACTIONS_ID_TOKEN_REQUEST_TOKEN\" \"$ACTIONS_ID_TOKEN_REQUEST_URL&amp;audience=myorg.app.spacelift.io\" | jq --raw-output '.value')\n          echo \"OIDC_TOKEN=$OIDC_TOKEN\" &gt;&gt; $GITHUB_ENV\n\n      - name: Install spacectl\n        uses: spacelift-io/setup-spacectl@main\n\n      - name: List stacks\n        env:\n          # You will want to replace this endpoint (and the audience) with your\n          # own Spacelift account's endpoint.\n          SPACELIFT_API_KEY_ENDPOINT: https://myorg.app.spacelift.io\n          SPACELIFT_API_KEY_ID: ${{ env.SPACELIFT_KEY_ID }}\n          SPACELIFT_API_KEY_SECRET: ${{ env.OIDC_TOKEN }}\n        run: |\n          spacectl whoami\n          spacectl stack list\n</code></pre> <p>For programmatic access, exchange the key ID and secret pair for an API token using a GraphQL mutation:</p> <pre><code>mutation GetSpaceliftToken($id: ID!, $secret: String!) {\n  apiKeyUser(id: $id, secret: $OIDC_TOKEN) {\n    jwt\n  }\n}\n</code></pre> <p>Once you obtain the token, you can use it to authenticate your requests to the Spacelift API.</p> <p>Note</p> <p>OIDC-based API keys do not provide special access to OpenTofu/Terraform modules. They are only used to authenticate with the Spacelift API.</p>"},{"location":"integrations/api.html#spacectl-cli","title":"SpaceCTL CLI","text":"<p>You can generate the JWT token using the Spacelift spacectl CLI. We consider this the easiest method, as the heavy lifting to obtain the token is done for you.</p> <ol> <li>Follow the instructions on the <code>spacectl</code> GitHub repository to install the CLI on your machine.</li> <li>Authenticate to your Spacelift account using <code>spacectl profile login</code>.</li> <li>Once authenticated, run <code>spacectl profile export-token</code> to receive the bearer token needed for future GraphQL queries/mutations.</li> </ol>"},{"location":"integrations/api.html#personal-github-token","title":"Personal GitHub token","text":"<p>Info</p> <p>This option is only available to accounts using GitHub as their identity provider. If you have enabled any other Single Sign-On methods on your account, this method will not work and you will need to use the Spacelift API Key method instead.</p> <ol> <li>Using a GitHub Account that has access to your Spacelift account, create a GitHub Personal Access Token.</li> <li>Copy the value of the token to a secure location.</li> <li>Using your favorite API Client (e.g. Insomnia or GraphiQL), make a GraphQL POST request to your account's GraphQL endpoint (example below).</li> </ol>"},{"location":"integrations/api.html#request-details","title":"Request details","text":"<p>POST to <code>https://example.app.spacelift.io/graphql</code>. Replace \"example\" with your Spacelift account name.</p>"},{"location":"integrations/api.html#query","title":"Query","text":"<p>Pass in token as a query variable for this example. When making a GraphQL query with your favorite API Client, you should see a section called GraphQL variables where you can pass in an input.</p> <pre><code>mutation GetSpaceliftToken($token: String!) {\n  oauthUser(token: $token) {\n    jwt\n  }\n}\n</code></pre>"},{"location":"integrations/api.html#graphql-variables-input","title":"GraphQL variables input","text":"<pre><code>{\n    \"token\": \"PASTE-TOKEN-VALUE-HERE\"\n}\n</code></pre> <p>This query should return your JWT bearer token, which you can use to authenticate other queries by using it as the bearer token in your requests. If you want to automatically access the API reliably, we suggest the Spacelift API Key approach, as Spacelift tokens expire after 1 hour.</p>"},{"location":"integrations/api.html#insomnia-setup","title":"Insomnia setup","text":"<p>You can create request libraries in Insomnia to make it easier to work with the Spacelift API. You can also automate the JWT token generation process using the Environment Variables feature.</p>"},{"location":"integrations/api.html#copy-the-schema","title":"Copy the schema","text":"<p>Copy the following JSON to your clipboard:</p> Click here to expand <pre><code>{\n    \"_type\": \"export\",\n    \"__export_format\": 4,\n    \"__export_date\": \"2023-01-23T19:49:05.605Z\",\n    \"__export_source\": \"insomnia.desktop.app:v2022.7.0\",\n    \"resources\": [\n  {\n          \"_id\": \"req_d7fb83c13cc945da9e21cd9b94722d3d\",\n          \"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n          \"modified\": 1674497188638,\n          \"created\": 1656577781496,\n          \"url\": \"{{ _.BASE_URL }}/graphql\",\n          \"name\": \"Authentication - Get JWT\",\n          \"description\": \"\",\n          \"method\": \"POST\",\n          \"body\": {\n              \"mimeType\": \"application/graphql\",\n              \"text\": \"{\\\"query\\\":\\\"mutation GetSpaceliftToken($keyId: ID!, $keySecret: String!) {\\\\n  apiKeyUser(id: $keyId, secret: $keySecret) {\\\\n    id\\\\n\\\\t\\\\tjwt\\\\n  }\\\\n}\\\",\\\"variables\\\":{\\\"keyId\\\":\\\"{{ _.API_KEY_ID }}\\\",\\\"keySecret\\\":\\\"{{ _.API_KEY_SECRET }}\\\"},\\\"operationName\\\":\\\"GetSpaceliftToken\\\"}\"\n          },\n          \"parameters\": [],\n          \"headers\": [\n              {\n                  \"name\": \"Content-Type\",\n                  \"value\": \"application/json\",\n                  \"id\": \"pair_85e4a9afc2e6491ca59b52f77d94e81f\"\n              }\n          ],\n          \"authentication\": {},\n          \"metaSortKey\": -1656577781496,\n          \"isPrivate\": false,\n          \"settingStoreCookies\": true,\n          \"settingSendCookies\": true,\n          \"settingDisableRenderRequestBody\": false,\n          \"settingEncodeUrl\": true,\n          \"settingRebuildPath\": true,\n          \"settingFollowRedirects\": \"global\",\n          \"_type\": \"request\"\n      },\n      {\n          \"_id\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n          \"parentId\": null,\n          \"modified\": 1656576979763,\n          \"created\": 1656576979763,\n          \"name\": \"Spacelift\",\n          \"description\": \"\",\n          \"scope\": \"collection\",\n          \"_type\": \"workspace\"\n      },\n      {\n          \"_id\": \"req_83de84158a16459fa4bfce6042859df6\",\n          \"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n          \"modified\": 1674497166036,\n          \"created\": 1656577541263,\n          \"url\": \"{{ _.BASE_URL }}/graphql\",\n          \"name\": \"Get Stacks\",\n          \"description\": \"\",\n          \"method\": \"POST\",\n          \"body\": {\n              \"mimeType\": \"application/graphql\",\n              \"text\": \"{\\\"query\\\":\\\"{ \\\\n\\\\tstacks\\\\n\\\\t{\\\\n\\\\t\\\\tid\\\\n\\\\t\\\\tname,\\\\n\\\\t\\\\tadministrative,\\\\n\\\\t\\\\tcreatedAt,\\\\n\\\\t\\\\tdescription\\\\n\\\\t}\\\\n}\\\"}\"\n          },\n          \"parameters\": [],\n          \"headers\": [\n              {\n                  \"name\": \"Content-Type\",\n                  \"value\": \"application/json\",\n                  \"id\": \"pair_80893dda7c0f4266b48bd09d0eaa3222\"\n              }\n          ],\n          \"authentication\": {\n              \"type\": \"bearer\",\n              \"token\": \"{{ _.API_TOKEN }}\"\n          },\n          \"metaSortKey\": -1656577721437.75,\n          \"isPrivate\": false,\n          \"settingStoreCookies\": true,\n          \"settingSendCookies\": true,\n          \"settingDisableRenderRequestBody\": false,\n          \"settingEncodeUrl\": true,\n          \"settingRebuildPath\": true,\n          \"settingFollowRedirects\": \"global\",\n          \"_type\": \"request\"\n      },\n      {\n          \"_id\": \"env_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\",\n          \"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n          \"modified\": 1660646140956,\n          \"created\": 1656576979773,\n          \"name\": \"Base Environment\",\n          \"data\": {},\n          \"dataPropertyOrder\": {},\n          \"color\": null,\n          \"isPrivate\": false,\n          \"metaSortKey\": 1656576979773,\n          \"_type\": \"environment\"\n      },\n      {\n          \"_id\": \"jar_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\",\n          \"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n          \"modified\": 1656576979775,\n          \"created\": 1656576979775,\n          \"name\": \"Default Jar\",\n          \"cookies\": [],\n          \"_type\": \"cookie_jar\"\n      },\n      {\n          \"_id\": \"spc_dbcf993f70b44bb18eee1b2362bb5bdc\",\n          \"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n          \"modified\": 1656576979770,\n          \"created\": 1656576979770,\n          \"fileName\": \"Spacelift\",\n          \"contents\": \"\",\n          \"contentType\": \"yaml\",\n          \"_type\": \"api_spec\"\n      },\n      {\n          \"_id\": \"env_ea5c30c23af449f792c71d160678eff5\",\n          \"parentId\": \"env_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\",\n          \"modified\": 1669716444669,\n          \"created\": 1669716373608,\n          \"name\": \"Spacelift\",\n          \"data\": {\n              \"BASE_URL\": \"https://ACCOUNT_NAME.app.spacelift.io\",\n              \"API_KEY_ID\": \"insert-your-real-api-key-here\",\n              \"API_KEY_SECRET\": \"insert-your-real-api-secret-here\",\n              \"API_TOKEN\": \"{% response 'body', 'req_d7fb83c13cc945da9e21cd9b94722d3d', 'b64::JC5kYXRhLmFwaUtleVVzZXIuand0::46b', 'never', 60 %}\"\n          },\n          \"dataPropertyOrder\": {\n              \"&amp;\": [\n                  \"BASE_URL\",\n                  \"API_KEY_ID\",\n                  \"API_KEY_SECRET\",\n                  \"API_TOKEN\"\n              ]\n          },\n          \"color\": \"#6b84ff\",\n          \"isPrivate\": false,\n          \"metaSortKey\": 828288489886.5,\n          \"_type\": \"environment\"\n        }\n    ]\n}\n</code></pre>"},{"location":"integrations/api.html#paste-the-schema-into-insomnia","title":"Paste the schema into Insomnia","text":"<ol> <li>On Insomnia's home screen, click Import From, then Clipboard.     </li> <li>Click on the Spacelift collection when it appears.</li> <li>In the top left corner, click Spacelift, then Manage Environments.     </li> <li>Fill in the first three variables:     <ul> <li><code>BASE_URL</code>: The URL of your Spacelift account. For example, <code>https://my-account.app.spacelift.io</code>.</li> <li><code>API_KEY_ID</code>: The ID of the API key you created, a 26-character ULID.</li> <li><code>API_KEY_SECRET</code>: Found in the file that was downloaded when you created the API key.</li> <li><code>API_TOKEN</code>: Leave this field as it is.</li> </ul> </li> </ol> <p>That's it! Now you can send an <code>Authentication - Get JWT</code> request, which populates the <code>API_TOKEN</code> environment variable. Then you can send the <code>Get Stacks</code> request to see the list of stacks in your account.</p> <p>If you want to create another request, just right-click on <code>Get Stacks</code> and duplicate it. Then, change the query to whatever you want.</p> <p>Hint</p> <p>Don't forget that the JWT expires after 10 hours. Run the authentication request again to get a new one.</p>"},{"location":"integrations/audit-trail.html","title":"Audit trail","text":""},{"location":"integrations/audit-trail.html#audit-trail","title":"Audit trail","text":"<p>Spacelift supports auditing all operations that change Spacelift resources. We provide a built-in audit log as well as webhook functionality to allow you to optionally store your audit logs in a third party system.</p>"},{"location":"integrations/audit-trail.html#built-in-logs","title":"Built-in logs","text":"<p>As an admin, you can view Audit trail logs by navigating to the Audit trail section of your account settings and choosing the logs tab:</p> <p></p> <p>You can look for specific events using filters (on the left side) and the date picker (in the top-right corner).</p> <p></p> <p>You can see the details:</p> <p> </p> <p>You can also go to the affected resource or apply another filter:</p> <p></p>"},{"location":"integrations/audit-trail.html#retention","title":"Retention","text":"<p>Logs are kept for 30 days.</p>"},{"location":"integrations/audit-trail.html#webhook-setup","title":"Webhook Setup","text":"<p>In order to set up webhooks for audit events, navigate to the Audit trail section of your account settings and choosing the configuration tab, and click the Set up button:</p> <p></p> <p>You will then need to provide a webhook endpoint and an arbitrary secret that you can later use for verifying payload. Optionally you can specify the custom headers that will be added to each HTTP request and enable <code>Include runs</code> option, which controls whether run state change events will be sent to the audit webhook in addition to standard audit events. Let's use ngrok for the purpose of this tutorial:</p> <p></p> <p>If you choose to automatically enable the functionality, clicking the Save button will verify that payloads can be delivered (the endpoint returns a 2xx status code). This gives us an opportunity to look at the payload:</p> <pre><code>{\n  \"account\": \"example\",\n  \"action\": \"audit_trail_webhook.set\",\n  \"actor\": \"github::name\",\n  \"context\": {\n    \"mutation\": \"auditTrailSetWebhook\"\n  },\n  \"data\": {\n    \"args\": {\n      \"Enabled\": true,\n      \"Endpoint\": \"https://example-audithook.com/\",\n      \"SecretSHA\": \"xxxfffdddwww\"\n    }\n  },\n  \"remoteIP\": \"0.0.0.0\",\n  \"timestamp\": 1674124447947\n}\n</code></pre> <p>...and the headers - the interesting ones are highlighted:</p> <p></p>"},{"location":"integrations/audit-trail.html#usage","title":"Usage","text":"<p>Every audit trail payload conforms to the same schema:</p> <ul> <li><code>account</code>: name (subdomain) of the affected Spacelift account;</li> <li><code>action</code>: name of the performed action;</li> <li><code>actor</code>: actor performing the action - the <code>::</code> format shows both the actor identity (second element), and the source of the identity (first element)</li> <li><code>context</code>: some contextual metadata about the request;</li> <li><code>data</code>: action-specific payload showing arguments passed to the request. Any sensitive arguments (like secrets) are sanitized;</li> </ul> <p>Below is a sample:</p> <pre><code>{\n  \"account\": \"example\",\n  \"action\": \"stack.create\",\n  \"actor\": \"github::name\",\n  \"context\": {\n    \"mutation\": \"stackCreate\"\n  },\n  \"data\": {\n    \"ID\": \"audit-trail-demo\",\n    \"args\": {\n      \"Input\": {\n        \"Administrative\": false,\n        \"AfterApply\": [],\n        \"AfterDestroy\": [],\n        \"AfterInit\": [],\n        \"AfterPerform\": [],\n        \"AfterPlan\": [],\n        \"AfterRun\": [],\n        \"Autodeploy\": false,\n        \"Autoretry\": false,\n        \"BeforeApply\": [],\n        \"BeforeDestroy\": [],\n        \"BeforeInit\": [],\n        \"BeforePerform\": [],\n        \"BeforePlan\": [],\n        \"Branch\": \"showcase\",\n        \"Description\": \"\",\n        \"GithubActionDeploy\": true,\n        \"IsDisabled\": null,\n        \"Labels\": [],\n        \"LocalPreviewEnabled\": false,\n        \"Name\": \"audit-trail-demo\",\n        \"Namespace\": \"spacelift-io\",\n        \"ProjectRoot\": \"\",\n        \"ProtectFromDeletion\": false,\n        \"Provider\": \"SHOWCASE\",\n        \"Repository\": \"terraform-starter\",\n        \"RunnerImage\": null,\n        \"Space\": \"legacy\",\n        \"TerraformVersion\": null,\n        \"VendorConfig\": {\n          \"Ansible\": null,\n          \"CloudFormation\": null,\n          \"Kubernetes\": null,\n          \"Pulumi\": null,\n          \"Terraform\": {\n            \"use_smart_sanitization\": null,\n            \"version\": \"1.3.7\",\n            \"workspace\": null\n          }\n        },\n        \"WorkerPool\": null\n      },\n      \"ManageState\": true,\n      \"Slug\": null,\n      \"StackObjectID\": null\n    }\n  },\n  \"remoteIP\": \"0.0.0.0\",\n  \"timestamp\": 1674124447947\n}\n</code></pre>"},{"location":"integrations/audit-trail.html#disabling-and-deleting-the-audit-trail","title":"Disabling and deleting the audit trail","text":"<p>The audit trail can be disabled and deleted at any point, but for both events we will send the appropriate payload. We suggest that you always treat these at least as important security signals, if not alerting conditions:</p> <pre><code>{\n  \"account\": \"example\",\n  \"action\": \"audit_trail_webhook.delete\",\n  \"actor\": \"github::user\",\n  \"context\": {\n    \"mutation\": \"auditTrailDeleteWebhook\"\n  },\n  \"data\": {},\n  \"remoteIP\": \"0.0.0.0\",\n  \"timestamp\": 1674124447947\n}\n</code></pre>"},{"location":"integrations/audit-trail.html#verifying-payload","title":"Verifying payload","text":"<p>Spacelift uses the same similar verification mechanism as GitHub. With each payload we send 2 headers, <code>X-Signature</code> and <code>X-Signature-256</code>. <code>X-Signature</code> header contains the SHA1 hash of the payload, while <code>X-Signature-256</code> contains the SHA256 hash. We're using the exact same mechanism as GitHub to generate signatures, please refer to this article for details.</p>"},{"location":"integrations/audit-trail.html#sending-logs-to-aws","title":"Sending logs to AWS","text":"<p>We provide a reference implementation for sending the Audit Trail logs to an AWS S3 bucket.</p> <p>It works as-is but can also be tweaked to route the logs to other destinations with minimal effort.</p>"},{"location":"integrations/audit-trail.html#failures","title":"Failures","text":"<p>Audit trail deliveries are retried on failure.</p>"},{"location":"integrations/docker.html","title":"Docker","text":""},{"location":"integrations/docker.html#docker","title":"Docker","text":"<p>Every job in Spacelift is processed inside a fresh, isolated Docker container. This approach provides reasonable isolation and resource allocation and - let's face it - is a pretty standard approach these days.</p>"},{"location":"integrations/docker.html#standard-runner-image","title":"Standard runner image","text":"<p>By default, Spacelift uses the latest version of the<code>public.ecr.aws/spacelift/runner-terraform</code> image, a simple Alpine image with a small bunch of universally useful packages. Feel free to refer to the Dockerfile that builds this image.</p> <p>Info</p> <p>Given that we use Continuous Deployment on our backend and Terraform provider, we explicitly don't want to version the runner image. Feature previews are available under a <code>future</code> tag, but we'd advise against using these as the API might change unexpectedly.</p>"},{"location":"integrations/docker.html#standard-runner-image-flavors","title":"Standard runner image flavors","text":"<ul> <li><code>runner-terraform:latest</code> (default) - includes <code>aws</code> CLI</li> <li><code>runner-terraform:gcp-latest</code> - includes <code>gcloud</code> CLI</li> <li><code>runner-terraform:azure-latest</code> - includes <code>az</code> CLI</li> </ul> <p>Note</p> <p>The reason we have separate images for cloud providers is that the <code>gcloud</code> and <code>az</code> CLIs are enormous and we don't want to bloat the default image with them.</p>"},{"location":"integrations/docker.html#allowed-registries-on-public-worker-pools","title":"Allowed registries on public worker pools","text":"<p>On public worker pools, only Docker images from the following registries are allowed to be used for runner images:</p> <ul> <li>azurecr.io (Azure Container Registry)</li> <li>dkr.ecr.&lt;region&gt;.amazonaws.com (All regions are supported)</li> <li>docker.io</li> <li>docker.pkg.dev</li> <li>gcr.io (Google Cloud Container Registry)</li> <li>ghcr.io (GitHub Container Registry)</li> <li>public.ecr.aws</li> <li>quay.io</li> </ul> <ul> <li>registry.gitlab.com</li> <li>registry.hub.docker.com</li> </ul>"},{"location":"integrations/docker.html#customizing-the-runner-image","title":"Customizing the runner image","text":"<p>The best way to customizing your Terraform execution environment is to build a custom runner image and use runtime configuration to tell Spacelift to use it instead of the standard runner. If you're not using Spacelift provider with Terraform 0.12, you can use any image supporting (by far the most popular) AMD64 architecture and add your dependencies to it.</p> <p>If you want our tooling in your image, there are two possible approaches. The first approach is to build on top of our image. We'd suggest doing that only if your customizations are relatively simple. For example, let's add a custom CircleCI provider to your image. They have a releases page allowing you to just <code>curl</code> the right version of the binary and put it in the <code>/bin</code> directory:</p> Dockerfile<pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\n\nWORKDIR /tmp\n\n# Temporarily elevating permissions\nUSER root\n\nRUN curl -O -L https://github.com/mrolla/terraform-provider-circleci/releases/download/v0.3.0/terraform-provider-circleci-linux-amd64 \\\n  &amp;&amp; mv terraform-provider-circleci-linux-amd64 /bin/terraform-provider-circleci \\\n  &amp;&amp; chmod +x /bin/terraform-provider-circleci\n\n# Back to the restricted \"spacelift\" user\nUSER spacelift\n</code></pre> <p>For more sophisticated use cases it may be cleaner to use Docker's multistage build feature to build your image and add our tooling on top of it. As an example, here's the case of us building a Terraform sops provider from source using a particular version. We want to keep our image small so we'll use a separate builder stage.</p> <p>The following approach works for Terraform version 0.12 and below, where custom Terraform providers colocated with the Terraform binary are automatically used.</p> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest as spacelift\nFROM golang:1.13-alpine as builder\n\nWORKDIR /tmp\n\n# Note how we don't bother building the provider statically because\n# we're using Alpine for our final runner image, too.\nRUN git clone https://github.com/carlpett/terraform-provider-sops.git \\\n  &amp;&amp; cd terraform-provider-sops \\\n  &amp;&amp; git checkout c5ffe6ebfac0a56fd60d5e7d77e0f2a73c34c3b7 \\\n  &amp;&amp; go build -o /terraform-provider-sops\n\nFROM alpine:3.10\nCOPY --from=spacelift /bin/terraform-provider-spacelift /bin/terraform-provider-spacelift\nCOPY --from=builder /terraform-provider-sops /bin/terraform-provider-sops\n\nRUN adduser --disabled-password --no-create-home --uid=1983 spacelift\n</code></pre> <p>An additional requirement is the presence of <code>ps</code> command in the image. The Spacelift worker periodically checks the status of the container image and it uses <code>ps</code> to do so.</p> <p>Info</p> <p>Note the <code>adduser</code> bit. Spacelift runs its Docker workflows as user <code>spacelift</code> with UID 1983, so make sure that:</p> <ul> <li>this user exists and has the right UID, otherwise you won't have access to your files;</li> <li>whatever you need accessed and executed in your custom image has the right ownership and/or permissions;</li> </ul> <p>Depending on your image flavor, the exact command to add the user may be different.</p> <p>Tip</p> <p>Any <code>ENTRYPOINT</code> and <code>CMD</code> customization will be ignored because the Spacelift worker binary must be the root process in the container.</p> <p>If you need to customize the shell (e.g., dynamically set environment variables or export functions), you can do so in a <code>before_init</code> hook.</p>"},{"location":"integrations/docker.html#custom-providers-from-terraform-013-onwards","title":"Custom providers from Terraform 0.13 onwards","text":"<p>Since Terraform 0.13, custom providers require a slightly different approach. You will build them the same way as described above, but the path now will be different. In order to work with the new API, we require that you put the provider binaries in the <code>/plugins</code> directory and maintain a particular naming scheme. The above <code>sops</code> provider example will work with Terraform 0.13 if the following stanza is added to the <code>Dockerfile</code>.</p> <pre><code>COPY --from=builder /terraform-provider-sops /plugins/registry.myorg.io/myorg/sops/1.0.0/linux_amd64/terraform-provider-sops\n</code></pre> <p>In addition, the custom provider must be explicitly required in the Terraform code, like this:</p> <pre><code>terraform {\n  required_providers {\n    spacelift = {\n      source = \"registry.myorg.io/myorg/sops\"\n    }\n  }\n}\n</code></pre> <p>Note that the source as defined above and the plugin path as defined in the Dockerfile are entirely arbitrary but must match. You can read more in the official Terraform 0.13 upgrade documentation.</p>"},{"location":"integrations/docker.html#using-private-docker-images","title":"Using private Docker images","text":"<p>If you're using Spacelift's default public worker pool, you're required to use public images. This is by design - if we allowed using private images, they would be cached by the Docker daemon and accessible to all customers using the same shared worker.</p> <p>Hence, only private workers support private Docker images. To enable private image support, first, execute <code>docker login</code> command with the proper registry credentials. Spacelift agent will read the credentials from Docker's configuration directory, but you will need to point it to the correct location by setting the <code>SPACELIFT_DOCKER_CONFIG_DIR</code> environment variable.</p>"},{"location":"integrations/docker.html#special-case-ecr","title":"Special case: ECR","text":"<p>ECR is a special case because those credentials tend to expire pretty quickly, and you'd need to add a mechanism to refresh them periodically if you wanted to maintain live access to the registry (cached images would not be affected by expired credentials). Given that many of our customers use EC2 to host their worker pools, we implemented a special mechanism to support private images hosted in ECR.</p> <p>This access is seamless - if the launcher detects that a runner image is hosted in ECR, it tries to use the existing credentials (e. g. EC2 instance role credentials) to generate the registry access token automatically on each job execution. With ECR images you don't even need to execute <code>docker login</code>.</p>"},{"location":"integrations/docker.html#best-practices","title":"Best practices","text":"<p>Here's a bunch of things we consider essential to keep your Docker usage relatively safe.</p>"},{"location":"integrations/docker.html#if-unsure-build-from-source","title":"If unsure, build from source","text":"<p>Building from the source is generally safer than using a pre-built binary, especially if you can review the code beforehand and make sure you're always building the code you've reviewed. You can use a Git commit hash, like we did above.</p>"},{"location":"integrations/docker.html#use-well-known-bases","title":"Use well-known bases","text":"<p>If you're building an image from a source other than <code>public.ecr.aws/spacelift/runner-terraform</code>, please prefer well-known and well-supported base images. Official images are generally safe, so choose something like <code>golang:1.13-alpine</code> over things like <code>imtotallylegit/notascamipromise:best-golang-image</code>. There's a bunch of services out there offering Docker image vulnerability scanning, so that's an option as well.</p>"},{"location":"integrations/docker.html#limit-push-access","title":"Limit push access","text":"<p>Your stack is only as safe as the runner image you're using for it. A malicious actor is able to doctor your runner image in a way that will allow them to take over your stack and all its associated cloud provider accounts in a snap. Please always review the code, and only allow <code>docker push</code> access to your most trusted associates.</p> <p>Info</p> <p>In our default public worker pool, we only support publicly available Docker images. If you need private Docker images, you can log in to any Docker registry from a worker in a private worker pool.</p>"},{"location":"integrations/plugins.html","title":"Plugins","text":""},{"location":"integrations/plugins.html#plugins","title":"Plugins","text":"<p>Warning</p> <p>Plugins support is currently in closed beta while we ensure stability of the platform and API's. Reach out to your Spacelift CSM for access.</p> <p>Plugins are a way to extend the functionality of Spacelift. They allow you to integrate with third-party services, automate tasks, and enhance your workflows. Spacelift supports a variety of plugins, which can be used to perform actions such as sending notifications, managing resources, or integrating with external systems. You can also develop your own custom plugins using our plugin SDK spaceforge</p>"},{"location":"integrations/plugins.html#available-plugins","title":"Available Plugins","text":"<p>Navigate to the Plugins -&gt; Templates section in the Spacelift UI to see the available plugins.</p> <p></p> <p>You're able to search for plugins on this screen and install it into your account. You can also create plugins yourself using the <code>Create Template</code> button.</p>"},{"location":"integrations/plugins.html#infracost","title":"Infracost","text":"<p>The Infracost plugin helps you estimate the cost impact of your Terraform infrastructure changes before they're deployed.</p> <p>Use Cases:</p> <ul> <li>Get cost estimates for pull requests and plan changes</li> <li>Set budget alerts and cost thresholds</li> <li>Compare costs across different infrastructure configurations</li> <li>Generate cost reports for stakeholders</li> </ul> <p>Prerequisites:</p> <ul> <li>Infracost API key from your Infracost account</li> <li>Terraform configurations with supported cloud providers (AWS, Azure, GCP)</li> </ul> <p>Troubleshooting:</p> <ul> <li>API key issues: Ensure your Infracost API key is valid and has sufficient quota</li> <li>Unsupported resources: Check Infracost's supported resources list</li> <li>Network connectivity: Ensure your workers can reach Infracost API endpoints</li> </ul>"},{"location":"integrations/plugins.html#sops","title":"SOPS","text":"<p>The SOPS plugin enables secure secret management by encrypting/decrypting files during your Terraform runs.</p> <p>Use Cases:</p> <ul> <li>Decrypt encrypted Terraform variable files</li> <li>Manage secrets across multiple environments</li> <li>Integrate with cloud KMS services (AWS KMS, Azure Key Vault, GCP KMS)</li> <li>Maintain encrypted secrets in version control</li> </ul> <p>Prerequisites:</p> <ul> <li>SOPS-encrypted files in your repository</li> <li>Appropriate cloud credentials for your chosen encryption backend</li> <li>SOPS configuration file (<code>.sops.yaml</code>) in your repository</li> </ul> <p>Sample .sops.yaml:</p> <pre><code>creation_rules:\n  - path_regex: \\.dev\\.tf$\n    kms: 'arn:aws:kms:us-east-1:123456789012:key/dev-key-id'\n  - path_regex: \\.prod\\.tf$\n    kms: 'arn:aws:kms:us-east-1:123456789012:key/prod-key-id'\n</code></pre> <p>Troubleshooting:</p> <ul> <li>Permission errors: Verify IAM roles have KMS decrypt permissions</li> <li>File format issues: Ensure encrypted files are in supported formats (YAML, JSON, dotenv)</li> <li>Key rotation: Update SOPS configuration when rotating encryption keys</li> </ul>"},{"location":"integrations/plugins.html#wiz","title":"WIZ","text":"<p>The WIZ plugin integrates cloud security scanning into your infrastructure deployment pipeline.</p> <p>Use Cases:</p> <ul> <li>Scan infrastructure configurations for security vulnerabilities</li> <li>Enforce security policies before deployment</li> <li>Generate security compliance reports</li> <li>Identify misconfigurations and policy violations</li> </ul> <p>Prerequisites:</p> <ul> <li>WIZ platform account and API credentials</li> <li>Cloud resources configured for WIZ scanning</li> <li>Security policies defined in WIZ platform</li> </ul> <p>Plan Policy Integration:</p> <p>The plugin can be configured to fail runs based on:</p> <ul> <li>Critical security findings</li> <li>Policy compliance violations</li> <li>Risk score thresholds</li> <li>Specific vulnerability types</li> </ul> <p>Simply write your plan policy to use the <code>input.third_party_metadata.custom.wiz</code> object.</p> <p>Troubleshooting:</p> <ul> <li>Authentication failures: Verify WIZ API credentials are correct and active</li> <li>Scan timeouts: Increase timeout values for large infrastructure scans</li> <li>Policy conflicts: Review WIZ policy configurations if scans are failing unexpectedly</li> </ul>"},{"location":"integrations/plugins.html#changing-the-plugin-template","title":"Changing the plugin template","text":"<p>Some plugin templates come with default values that you might want to change. Note that when a template is installed, the resulting policies, webhooks, contexts, etc will be locked in the UI. Management of these resources is done via the plugins screen.</p> <p>So to change a plugin template, you can click the \"...\" button next to the plugin template and select \"New template from this\". </p> <p>After that, you can create a new template with the desired changes and install that into your account.</p>"},{"location":"integrations/plugins.html#installing-plugins","title":"Installing Plugins","text":"<p>To install a plugin, navigate to the Plugins -&gt; Templates section in the Spacelift UI and click \"...\" -&gt; Install. When installing a plugin you will be provided a number of options.</p> <ul> <li><code>Installation Name</code> is the name of the plugin installation. This is used to identify the plugin in the Spacelift UI.</li> <li><code>Stack Label</code> is the <code>autoattach</code> label that will be used to automatically attach the plugin to stacks.</li> <li><code>Space</code> is the space where the plugin will be installed. This is used to scope the plugin to a specific space.</li> <li><code>labels</code> are arbitrary labels that can be used to filter in the Spacelift UI.</li> </ul> <p>Below the options, you will see dynamically configured options that are pulled in from the template. For instance, the Infracost plugin template has the <code>Infracost API Key</code> option, which is required to use the plugin. If you need more information about what a specific paramenter is doing hover over the information icon.</p> <p></p> <p>After a plugin is installed, it will move to the <code>Account Plugins</code> tab where you can see details about that specific plugin.</p>"},{"location":"integrations/plugins.html#using-a-plugin","title":"Using a plugin","text":"<p>Using a plugin is very simple, plugins can only attach via the <code>autoattach</code> label. The value you provided in the <code>Stack Label</code> field will be used as the <code>autoattach</code> label. If you installed Infracost with the <code>infracost</code> stack label, you can attach it to a stack by adding the <code>infracost</code> label to the stack. You can also use the <code>*</code> wildcard in the stack label to attach the plugin to all stacks in a space.</p>"},{"location":"integrations/plugins.html#plugin-outputs","title":"Plugin Outputs","text":"<p>Plugins may output information that is useful for you to see. If a plugin produces outputs, they will be visible in the <code>Plugin Outputs</code> tab of the run details page. An example of the <code>Wiz</code> plugin output is shown below:</p> <p></p>"},{"location":"integrations/plugins.html#plugin-development","title":"Plugin Development","text":""},{"location":"integrations/plugins.html#getting-started-with-custom-plugins","title":"Getting Started with Custom Plugins","text":"<p>Custom plugins allow you to extend Spacelift's functionality to meet your specific needs. The spaceforge SDK provides templates and tools for creating, testing, and publishing plugins.</p>"},{"location":"integrations/plugins.html#plugin-architecture","title":"Plugin Architecture","text":"<p>Spacelift plugins are packaged as yaml files. Each plugin defines:</p> <ul> <li>Execution phases: When the plugin runs (e.g., before_init, after_plan, after_apply)</li> <li>Input parameters: Configuration options exposed during installation</li> <li>Output artifacts: Files, logs, or data produced by the plugin</li> <li>Dependencies: Required tools, libraries, or external services</li> </ul>"},{"location":"integrations/plugins.html#plugin-lifecycle","title":"Plugin Lifecycle","text":"<ol> <li>Installation: Plugin template is installed into your account with specific configuration</li> <li>Attachment: Plugin automatically attaches to stacks via autoattach labels</li> <li>Execution: Plugin runs during appropriate run phases based on its configuration</li> <li>Output: Plugin generates logs, artifacts, or external integrations</li> <li>Cleanup: Temporary resources are cleaned up after execution</li> </ol>"},{"location":"integrations/plugins.html#contributing-to-the-plugin-ecosystem","title":"Contributing to the Plugin Ecosystem","text":""},{"location":"integrations/plugins.html#community-contributions","title":"Community Contributions","text":"<p>The Spacelift plugin ecosystem thrives on community contributions. Here's how you can get involved:</p> <p>Plugin Templates:</p> <ul> <li>Contribute new plugin templates to the spaceforge repository</li> <li>Improve existing plugin documentation and examples</li> <li>Submit bug fixes and feature enhancements</li> </ul> <p>Contribution Process:</p> <ol> <li>Fork the repository: Create your own fork of the spaceforge repo</li> <li>Create feature branch: <code>git checkout -b feature/my-awesome-plugin</code></li> <li>Develop and test: Follow the development guidelines and test thoroughly</li> <li>Submit pull request: Include clear description and test results</li> <li>Review process: Core maintainers will review and provide feedback</li> </ol>"},{"location":"integrations/plugins.html#plugin-submission-guidelines","title":"Plugin Submission Guidelines","text":"<p>Quality Standards:</p> <ul> <li>Documentation: Comprehensive README with setup instructions</li> <li>Security: Follow security best practices, no hardcoded secrets</li> <li>Compatibility: Test with supported Spacelift features and IaC tool versions</li> </ul>"},{"location":"integrations/plugins.html#plugin-categories","title":"Plugin Categories","text":"<p>Popular Plugin Categories:</p> <ul> <li>Security &amp; Compliance: Vulnerability scanning, policy enforcement, compliance reporting</li> <li>Cost Management: Cost estimation, budget alerts, resource optimization</li> <li>Notifications: Slack, Discord, email, webhooks for run status updates</li> <li>Monitoring &amp; Observability: Metrics collection, log aggregation, alerting</li> <li>CI/CD Integration: GitHub Actions, GitLab CI, Jenkins pipeline triggers</li> <li>Cloud Services: Provider-specific integrations and automation</li> <li>Testing: Infrastructure testing, compliance validation, performance testing</li> </ul>"},{"location":"integrations/teleport.html","title":"Teleport","text":""},{"location":"integrations/teleport.html#teleport","title":"Teleport","text":"<p>Teleport is a global provider of modern access platforms for infrastructure, improving efficiency of engineering teams, fortifying infrastructure against bad actors or error, and simplifying compliance and audit reporting. The Teleport Access Platform delivers on-demand, least privileged access to infrastructure on a foundation of cryptographic identity and zero trust, with built-in identity security and policy governance.</p> <p>You can use Spacelift with the Teleport Terraform provider to manage dynamic configuration resources via GitOps and infrastructure as code. This gives you an audit trail of changes to your Teleport configuration and a single source of truth for operators to examine.</p> <p>This guide shows you how to configure the Teleport Terraform Provider to authenticate to a Teleport cluster using Machine ID when running on Spacelift.</p> <p>In this setup, the Teleport Terraform Provider proves its identity to the Teleport Auth Service by presenting an ID token signed by Spacelift. This allows it to authenticate with the Teleport cluster without the need for a long-lived shared secret.</p> <p>While following this guide, you will create a Teleport user and role with no privileges in order to show you how to use Spacelift to create dynamic resources.</p>"},{"location":"integrations/teleport.html#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Access to an Enterprise edition of Teleport running in your environment.</p> </li> <li> <p>The Enterprise <code>tctl</code> admin tool and <code>tsh</code> client tool version &gt;= 17.3.4.</p> <p>You can verify the tools you have installed by running the following commands:</p> <pre><code>$ tctl version\n# Teleport Enterprise v17.3.4 git:v15.1.3-0-gc9d69ba go1.21.8\n\n$ tsh version\n# Teleport v17.3.4 git:v15.1.3-0-gc9d69ba go1.21.8\n</code></pre> <p>You can download these tools by following the appropriate installation instructions for your environment and Teleport edition.</p> </li> <li> <p>To check that you can connect to your Teleport cluster, sign in with <code>tsh login</code>, then   verify that you can run <code>tctl</code> commands using your current credentials.   <code>tctl</code> is supported on macOS and Linux machines.</p> <p>For example:</p> <pre><code>$ tsh login --proxy=name=\"teleport.example.com\" --user=name=\"email@example.com\"\n$ tctl status\n# Cluster  teleport.example.com\n# Version  17.3.4\n# CA pin   sha256:abdc1245efgh5678abdc1245efgh5678abdc1245efgh5678abdc1245efgh5678\n</code></pre> <p>If you can connect to the cluster and run the <code>tctl status</code> command, you can use your current credentials to run subsequent <code>tctl</code> commands from your workstation. If you host your own Teleport cluster, you can also run <code>tctl</code> commands on the computer that hosts the Teleport Auth Service for full permissions.</p> </li> <li> <p>A GitHub repository where you will store your Terraform configuration and a   Spacelift stack linked to this repository.</p> </li> <li>A paid Spacelift account. This is required to use the <code>spacelift</code> join method.</li> <li>Your Teleport user should have the privileges to create token resources.</li> </ul>"},{"location":"integrations/teleport.html#step-13-create-a-join-token-for-spacelift","title":"Step 1/3. Create a join token for Spacelift","text":"<p>In order to allow your Spacelift stack to authenticate with your Teleport cluster, you'll first need to create a join token. A join token sets out criteria by which the Teleport Auth Service decides whether to allow a bot or node to join a cluster.</p> <p>In this example, you will create a join token that grants access to any execution within a specific Spacelift stack.</p> <p>Create a file named <code>bot-token.yaml</code>:</p> <pre><code>kind: token\nversion: v2\nmetadata:\n  name: example-bot\nspec:\n  # The Bot role indicates that this token grants access to a bot user, rather\n  # than allowing a node to join. This role is built in to Teleport.\n  roles: [Bot]\n  join_method: spacelift\n  # The bot_name indicates which bot user this token grants access to. This\n  # should match the name of the bot that you will create in the next step.\n  bot_name: example\n  spacelift:\n    # hostname should be the hostname of your Spacelift tenant.\n    hostname: example.app.spacelift.io\n    # allow specifies rules that control which Spacelift executions will be\n    # granted access. Those not matching any allow rule will be denied.\n    allow:\n    # space_id identifies the space that the module or stack resides within.\n    - space_id: root\n      # caller_type is the type of caller_id. This must be `stack` or `module`.\n      caller_type: stack\n      # caller_id is the id of the caller. e.g the name of the stack or module.\n      caller_id: my-stack\n</code></pre> <p>Replace:</p> <ul> <li><code>example.app.spacelift.io</code> with the hostname of your Spacelift tenant.</li> <li><code>my-stack</code> with the name of the Spacelift stack.</li> <li><code>root</code> with the ID of the space that the stack resides within. The   \"space details\" panel on the \"Spaces\" page of the Spacelift UI shows the ID.</li> </ul> <p>Once the resource file has been written, create the token with <code>tctl</code>:</p> <pre><code>$ tctl create -f bot-token.yaml\n# token \"example-bot\" has been created\n</code></pre> <p>Check that token <code>example-bot</code> has been created with the following command:</p> <pre><code>$ tctl tokens ls\nToken       Type Labels Expiry Time (UTC)\n----------- ---- ------ ----------------------------------------------\nexample-bot Bot\n</code></pre>"},{"location":"integrations/teleport.html#step-23-create-a-role-and-machine-id-bot","title":"Step 2/3. Create a role and Machine ID bot","text":"<p>Next, we'll create a Machine ID Bot for our Spacelift job to act as. We'll grant it the <code>terraform-provider</code> role, which automatically grants access to every resource supported by the Teleport terraform provider.</p> <p>Create the bot, specifying the role and token that you have created:</p> <pre><code>$ tctl bots add example --roles=terraform-provider --token=example-bot\n# bot \"example\" has been created\n</code></pre>"},{"location":"integrations/teleport.html#step-33-configure-your-spacelift-stack","title":"Step 3/3. Configure your Spacelift stack","text":"<p>While following this step, you will modify your git repo to:</p> <ul> <li>Configure Spacelift to authenticate the Teleport Terraform provider as a bot   user using credentials generated by Machine ID.</li> <li>Create dynamic Teleport resources using your git repo.</li> </ul> <p>Before continuing, clone your GitHub repository. In the clone, check out a branch from your main branch.</p>"},{"location":"integrations/teleport.html#configure-the-terraform-provider","title":"Configure the Terraform Provider","text":"<p>Add the following to a file called <code>main.tf</code> to configure the Teleport Terraform provider and declare two dynamic resources, a user and role:</p> <pre><code>terraform {\n  required_providers {\n    teleport = {\n      source  = \"terraform.releases.teleport.dev/gravitational/teleport\"\n      version = \"&gt;= (=teleport.plugin.version=)\"\n    }\n  }\n}\n\nprovider \"teleport\" {\n  addr        = \"teleport.example.com:443\"\n  join_method = \"spacelift\"\n  join_token  = \"example-bot\"\n}\n\nresource \"teleport_role\" \"terraform_test\" {\n  version = \"v7\"\n  metadata = {\n    name        = \"terraform-test\"\n    description = \"Terraform test role\"\n    labels = {\n      test = \"true\"\n    }\n  }\n}\n\nresource \"teleport_user\" \"terraform-test\" {\n  metadata = {\n    name        = \"terraform-test\"\n    description = \"Terraform test user\"\n\n    labels = {\n      test = \"true\"\n    }\n  }\n\n  spec = {\n    roles = [teleport_role.terraform_test.id]\n  }\n}\n</code></pre> <p>In the <code>provider</code> block, change:</p> <ul> <li><code>teleport.example.com:443</code> to the host and HTTPS port of your Teleport Proxy   Service.</li> <li><code>example-bot</code> to the name of the join token you created earlier.</li> </ul> <p>Commit your changes and push the branch to GitHub, then open a pull request against the <code>main</code> branch. (Do not merge it just yet.)</p>"},{"location":"integrations/teleport.html#verify-that-the-setup-is-working","title":"Verify that the setup is working","text":"<p>In the Spacelift UI, navigate to your stack, then to PRs. Click the name of the PR you opened.</p> <p>You should see a Terraform plan that includes the user and role you defined earlier:</p> <p> </p> <p>When running <code>terraform plan</code>, the Teleport Terraform Provider uses Machine ID to generate the short-lived credentials necessary to authenticate to the Teleport cluster.</p> <p>Merge the PR, then navigate to your stack and click Runs. Click the status of the first run, which corresponds to merging your PR, to visit the page for the run. Click Confirm to begin applying your Terraform plan.</p> <p>You should see output indicating success:</p> <p> </p> <p>Verify that Spacelift has created the new user and role by running the following commands, which should return YAML data for each resource:</p> <pre><code>$ tctl get roles/terraform-test\n# ---\n# kind: role\n# metadata:\n    name: terraform-test\n# ...\n\n$ tctl get users/terraform-test\n# ---\n# kind: user\n# metadata:\n    name: terraform-test\n# ...\n</code></pre>"},{"location":"integrations/webhooks.html","title":"Webhooks","text":""},{"location":"integrations/webhooks.html#webhooks","title":"Webhooks","text":"<p>Spacelift can be configured to send webhook notifications for various events to an HTTP endpoint of your choice.</p>"},{"location":"integrations/webhooks.html#set-up-webhooks","title":"Set up webhooks","text":"<p>Webhooks can be created or modified by Spacelift administrators in the Webhooks section of the Integrations page.</p> <p></p>"},{"location":"integrations/webhooks.html#fill-required-fields","title":"Fill required fields","text":"<ol> <li>On the Webhooks section of the Integrations page, click View.</li> <li>Click Create webhook.</li> <li>Fill in the webhook details:<ol> <li>Name: Enter a name for the webhook integration.</li> <li>Endpoint URL: Enter the endpoint URL the webhook will send information to.</li> <li>Space: Select the space that can access the webhook.</li> <li>Secret (optional): Enter the secret, if needed, to validate the received payload.</li> <li>Enable webhook: Enable or disable the webhook.</li> <li>Retry on failure: Enable to automatically retry webhook delivery (up to 3 times) when receiving 5xx HTTP responses.</li> <li>Labels (optional): Enter a label or labels to help sort your webhooks if needed.</li> <li>Headers: Enable, then enter the Key and Value pair to add to the webhook.</li> </ol> </li> <li>Click Create.</li> </ol>"},{"location":"integrations/webhooks.html#reference-webhooks-in-policy-rules","title":"Reference webhooks in policy rules","text":"<p>Webhook messages are delivered using the notification policy. When defining rules, the policy expects you to reference the webhook by its <code>ID</code> which you can copy from the webhook list view:</p> <p></p>"},{"location":"integrations/webhooks.html#webhook-deliveries","title":"Webhook deliveries","text":"<p>Webhook deliveries and their response statuses are stored and can be explored by selecting a specific webhook and viewing its details. You'll be presented with a list of deliveries, their status codes and when they happened.</p> <p>You can also click on each delivery to view more details about it:</p> <p></p>"},{"location":"integrations/webhooks.html#default-webhook-payloads","title":"Default webhook payloads","text":"<p>These are the default webhook payloads sent for each event type. If required, webhook payloads can be customized via a notification policy.</p>"},{"location":"integrations/webhooks.html#run-events","title":"Run events","text":"<p>Here's an example of the default webhook payload for a notification about a finished tracked run:</p> <pre><code>{\n    \"account\": \"spacelift-io\",\n    \"state\": \"FINISHED\",\n    \"stateVersion\": 4,\n    \"timestamp\": 1596979684,\n    \"run\": {\n        \"id\": \"01EF9PFPNFFM2MQXTJKHK1B869\",\n        \"branch\": \"master\",\n        \"commit\": {\n            \"authorLogin\": \"marcinwyszynski\",\n            \"authorName\": \"Marcin Wyszynski\",\n            \"hash\": \"0ee3a3b7266daf5a1d44a193a0f48ce995fa75eb\",\n            \"message\": \"Update demo.tf\",\n            \"timestamp\": 1596705932,\n            \"url\": \"https://github.com/spacelift-io/demo/commit/0ee3a3b7266daf5a1d44a193a0f48ce995fa75eb\"\n        },\n        \"createdAt\": 1596979665,\n        \"delta\": {\n            \"added\": 0,\n            \"changed\": 0,\n            \"deleted\": 0,\n            \"resources\": 1\n        },\n        \"triggeredBy\": \"marcinw@spacelift.io\",\n        \"type\": \"TRACKED\"\n    },\n    \"stack\": {\n        \"id\": \"spacelift-demo\",\n        \"name\": \"Spacelift demo\",\n        \"description\": \"\",\n        \"labels\": []\n    }\n}\n</code></pre> <p>The payload consists of a few fields:</p> <ul> <li><code>account</code>: The name (subdomain) of the account generating the webhook. This is useful for pointing webhooks from various accounts at the same endpoint.</li> <li><code>state</code>: A string representation of the run state at the time of the notification being triggered.</li> <li><code>stateVersion</code>: The ordinal number of the state, which can be used to ensure that notifications that may be sent or received out-of-order are correctly processed.</li> <li><code>timestamp</code>: The unix timestamp of the state transition.</li> <li><code>run</code>: Contains information about the run, its associated commit and delta (if any).</li> <li><code>stack</code>: Contains some basic information about the parent Stack of the <code>run</code>.</li> </ul>"},{"location":"integrations/webhooks.html#internal-error-events","title":"Internal error events","text":"<pre><code>{\n  \"title\": \"Invalid Stack Slug Triggered\",\n  \"body\": \"policy tried to trigger Stack 'this-is-not-a-stack' which either doesn't exist or this policy doesn't have access to\",\n  \"error\": \"policy triggered for Stack that doesn't exist\",\n  \"severity\": \"ERROR\",\n  \"account\": \"spacelift-io\"\n}\n</code></pre> <p>Internal errors will always have the same fields set and some of them will be static for an event:</p> <ul> <li><code>title</code>: The title (summary) of the error.</li> <li><code>body</code>: The full explanation of what went wrong.</li> <li><code>error</code>: A description of the error that occurred.</li> <li><code>severity</code>: One of three different constants: <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>.</li> <li><code>account</code>: The account for which the error happened.</li> </ul>"},{"location":"integrations/webhooks.html#validate-payload","title":"Validate payload","text":"<p>To validate the incoming payload, you will need the secret you generated when creating or updating the webhook.</p> <p>The following section provides different instructions for validating the payload depending on whether your Spacelift account is in our FedRAMP environment or not:</p> StandardFedRAMP <p>Every webhook payload comes with two signature headers generated from the combination of the secret and payload. <code>X-Signature</code> header contains the SHA1 hash of the payload, while <code>X-Signature-256</code> contains the SHA256 hash. We're using the exact same mechanism as GitHub to generate signatures, please refer to GitHub docs for details.</p> <p>Only the SHA-256 signature will be used for webhook payload validation. The <code>X-Signature</code> header containing the SHA1 hash will not be provided, ensuring compliance with FIPS requirements that prohibit the use of SHA-1 for cryptographic purposes.</p> <p>We're using the exact same mechanism as GitHub to generate signatures, please refer to GitHub docs for details.</p> <p>Spacelift is using the exact same mechanism as GitHub to generate signatures, so you can refer to GitHub's docs for details.</p>"},{"location":"integrations/webhooks.html#attach-webhooks-to-stacks","title":"Attach webhooks to stacks","text":"<p>Warning</p> <p>We recommend that you use notification policies to route stack events to your webhooks. Stack webhook integrations are provided for backwards compatibility.</p> <p>Webhooks can be set up by Spacelift administrators on per-stack basis.</p> <ol> <li>On the Stacks page, click the three dots beside the stack you would like to attach a webhook to.</li> <li>Click Settings, then Integrations.</li> <li>Click Webhooks.</li> <li>Either select an existing webhook or click Create webhook.</li> </ol> <p></p> <p>Info</p> <p>You can set up as many webhooks as you need for a Stack, though each one must have a unique endpoint.</p>"},{"location":"integrations/webhooks.html#fill-in-webhook-details","title":"Fill in webhook details","text":"<ol> <li>Endpoint: Enter the endpoint URL the webhook will send information to.</li> <li>Secret (optional): Enter the secret, if needed, to validate the received payload. It is up to you to decide on a non-obvious secret.</li> <li>Enable webhook: Enable or disable the webhook.</li> <li>Retry on failure: Enable to automatically retry webhook delivery (up to 3 times) when receiving 5xx HTTP responses.</li> </ol> <p>Once saved, the webhook will appear on the list of integrations:</p> <p></p> <p>Info</p> <p>Unlike some other secrets in Spacelift, the webhook secret can be viewed by anyone with read access to the stack. If you suspect foul play, consider regenerating your secret.</p>"},{"location":"integrations/webhooks.html#disable-existing-webhook","title":"Disable existing webhook","text":"<p>Webhooks are enabled by default, so they are triggered every time there's a run state change event on the stack they are attached to. If you want to temporarily disable some of the endpoints, you can do that without having to delete the whole integration.</p> <ol> <li>Navigate to the Integrations page and click View in the Webhooks card.<ul> <li>You can also navigate to the Stacks page, click the three dots beside the stack name, then click Settings &gt; Integrations &gt; Webhooks.</li> </ul> </li> <li>Click Edit webhook.</li> <li>Click the toggle beside Enable webhook. When disabled, the toggle will be gray.</li> </ol> <p></p> <p>Enable the webhook again by repeating steps 1-3 and clicking the toggle to turn it blue:</p> <p></p>"},{"location":"integrations/chatops/msteams.html","title":"Microsoft Teams","text":""},{"location":"integrations/chatops/msteams.html#microsoft-teams","title":"Microsoft Teams","text":"<p>Microsoft Teams is a Slack alternative and a part of the Microsoft Office 365 suite. It's a chat-based workspace where teams can organize and discuss their work. Many DevOps teams use it to communicate and collaborate on infrastructure and application deployments. Hence, Spacelift has a first-class integration with Microsoft Teams.</p> <p>The integration creates a webhook in Spacelift that will send notifications to a Microsoft Teams channel when:</p> <ul> <li>a tracked run needs confirmation;</li> <li>a tracked run or a task finishes;</li> <li>a module version succeeds or fails;</li> </ul> <p>Based on this configuration, the module will send notifications that look like these:</p> <p></p> <p></p> <p></p>"},{"location":"integrations/chatops/msteams.html#prerequisites","title":"Prerequisites","text":"<p>In order to set up the integration, you'll to perform some manual steps in Microsoft Teams. The Spacelift end of the integration is handled programmatically, by a Terraform module.</p>"},{"location":"integrations/chatops/msteams.html#in-microsoft-teams","title":"In Microsoft Teams","text":"<p>In order to set up the integration, you'll need to create a Microsoft Teams workflow and copy its URL. You can do this by following these steps:</p> <ol> <li>Open the Workflows section.</li> <li>Start a new workflow.</li> <li>Use the Post to a channel when a webhook request is received template to set up the workflow.</li> <li>Pick the Team and Channel where you want to receive the notifications.</li> <li>Create the workflow.</li> <li>Copy the webhook URL, you'll need it in the next step.</li> </ol>"},{"location":"integrations/chatops/msteams.html#in-spacelift","title":"In Spacelift","text":"<p>The Teams integration is based on our notification policy feature, which requires at least an active Cloud tier subscription. While building a notification-based Teams integration from scratch is possible, we've created a Terraform module that will set up all the necessary integration elements for you.</p> <p>This module will only create Spacelift assets:</p> <ul> <li>a notification policy that will send data to Microsoft Teams;</li> <li>a webhook endpoint that serve as a notification target for the policy;</li> </ul>"},{"location":"integrations/chatops/msteams.html#monitoring-and-troubleshooting","title":"Monitoring and troubleshooting","text":"<p>Once the integration is set up, you can monitor the notifications in the <code>My channel</code> channel in Microsoft Teams. You can also monitor the notifications in the corresponding notification policy and its webhook endpoint in Spacelift.</p>"},{"location":"integrations/chatops/slack.html","title":"Slack","text":""},{"location":"integrations/chatops/slack.html#slack","title":"Slack","text":"<p>At Spacelift, we're using Slack for internal communication. And we know that other tech companies do the same, so we've created a first-class integration that we ourselves enjoy using.</p> <p>Here are examples of messages the Spacelift application sends to Slack;</p> <p></p> <p></p>"},{"location":"integrations/chatops/slack.html#configuring-a-slack-app","title":"Configuring a Slack app","text":"<p>In a Self-Hosted installation, you need to create your own Slack app to allow Spacelift to integrate with your Slack workspace. This Slack app provides credentials needed by Spacelift to communicate with Slack, and also contains configuration information like the webhooks URL that Slack should send requests to.</p> <p>There are two options for configuring the Slack app:</p> <ol> <li>Via environment variables.</li> <li>Dynamically via the Spacelift frontend.</li> </ol> <p>This section walks you through the steps to create and configure the Slack app dynamically via the Spacelift frontend. If you want to configure it via the environment please see our configuration reference.</p>"},{"location":"integrations/chatops/slack.html#create-slack-app","title":"Create Slack app","text":"<p>To start the process, go to Integrations &gt; Slack, and choose the Set up option next to the Slack integration:</p> <p></p> <p>The side bar is split into two sections. The top section contains the manifest you should use when creating your Slack app, and the bottom section contains fields to enter the credentials for your Slack app.</p> <p>Copy the app manifest then click on the Create Slack app button. This will take you to Slack and start the app creation process. Choose the From a manifest option when the dialog appears:</p> <p></p> <p>On the next step, choose the Slack workspace you want to connect Spacelift to:</p> <p></p> <p>Next, paste the manifest that your copied from the Spacelift frontend:</p> <p></p> <p>Finally, click Create on the summary page to create your app:</p> <p></p>"},{"location":"integrations/chatops/slack.html#add-slack-app-details-to-spacelift","title":"Add Slack app details to Spacelift","text":"<p>Once your app has been created, go to its Basic Information configuration section and find the Client ID, Client Secret and Signing Secret:</p> <p></p> <p>Enter these details into Spacelift and click the Save &amp; Connect button to continue:</p> <p></p> <p>You will now be redirected to Slack, and asked to grant the app you just created access to your Slack workspace. Click on Allow to continue:</p> <p></p> <p>You should now be redirected to Spacelift, and the status of your integration should be marked as active:</p> <p></p> <p>At this point you can skip the next section and go straight to finding out how to manage access to Stacks.</p>"},{"location":"integrations/chatops/slack.html#connecting-your-spacelift-account-to-the-slack-workspace","title":"Connecting your Spacelift account to the Slack workspace","text":"<p>As a Spacelift and Slack admin, you can connect your Spacelift account to the Slack workspace via the Integrations &gt; Slack page:</p> <p></p> <p>When you click on Connect, it performs an OAuth2 exchange which installs Slack Spacelift app in your workspace.</p> <p>Once the connection is complete, the Slack integration should be marked as active:</p> <p></p> <p>Installing the Slack app doesn't automatically cause Spacelift to flood your Slack channels with torrents of notifications. These are set up on a per-stack basis using Notification policies.</p> <p>Though before that happens, you need to allow requests coming from Slack to access Spacelift stacks.</p>"},{"location":"integrations/chatops/slack.html#managing-access-to-stacks-with-policies","title":"Managing access to Stacks with policies","text":"<p>Our Slack integration allows users in the Slack workspace to interact with stacks by adding the ability to change their run state or view changes that are planned or were applied.</p> <p>Similar to regular requests to our HTTP APIs, requests and actions coming from Slack are subject to the policy-based access validation. If you haven't had a chance to review the policy and Spaces documentation yet, please do it now before proceeding any further - you're risking a chance of getting lost.</p>"},{"location":"integrations/chatops/slack.html#available-actions","title":"Available actions","text":"<p>Currently, we allow:</p> <ul> <li>Confirming and discarding tracked runs.</li> <li>Viewing planned and actual changes.</li> </ul> <p>Both of these actions require specific permissions to be configured using the login policy. Confirming or discarding runs requires Write level permissions while viewing changes requires Read level permissions. The documentation sections about policies below describe how to setup and manage these permissions.</p> <p>Info</p> <p>The default login policy decision for Slack requests is to deny all access.</p>"},{"location":"integrations/chatops/slack.html#login-policy","title":"Login policy","text":"<p>Using login policies is the preferred way to control access for the Slack integration. Using them you can control who can access stacks which are in a specific Space.</p> <p>They allow for granular space access control using the provided policy data such as slack workspace details, Slack team information and user which interacted with the message data. Using the Login policy you can define rules which would allow to have Read or Write level permissions for certain actions.</p> <p>Login policies also don't need to be attached to a specific stack in order to work but are instead evaluated during every stack mutation or read attempt from the integration.</p> <p>Warning</p> <p>It's important to know that if you have multiple login policies, failing to evaluate one of them or having at least one of them result in a deny decision after the evaluation is done, will result in the overall decision being a <code>deny all</code>.</p> <p>Here is an example of data which the login policy receives when evaluating stack access for the integration:</p> <pre><code>{\n  \"request\": {\n    \"timestamp_ns\": \"&lt;int&gt; - a unix timestamp for when this request happened\"\n  },\n  \"slack\": {\n    \"channel\": {\n      \"id\": \"&lt;string&gt; - a channel ID, example: C042YPN0000\",\n      \"name\": \"&lt;string&gt; - a channel name, example: spc-finished\"\n    },\n    \"command\": \"&lt;string&gt;\",\n    \"team\": {\n      \"id\": \"&lt;string&gt; - the workspace ID for which this user belongs, example: T0431750000\",\n      \"name\": \"&lt;string&gt; - the workspace name represented as string, example: slack-workspace-name\"\n    },\n    \"user\": {\n      \"deleted\": \"&lt;boolean&gt;\",\n      \"display_name\": \"&lt;string&gt;\",\n      \"enterprise\": {\n        \"enterprise_id\": \"&lt;string&gt;\",\n        \"enterprise_name\": \"&lt;string&gt;\",\n        \"id\": \"&lt;string&gt;\",\n        \"is_admin\": \"&lt;boolean&gt;\",\n        \"is_owner\": \"&lt;boolean&gt;\",\n      },\n      \"teams\": {\n         \"id\": \"&lt;string&gt;\",\n         \"name\": \"&lt;string&gt;\"\n      },\n      \"id\": \"&lt;string&gt; - a user which initially request ID, example: C042YPN1111\",\n      \"is_admin\": \"&lt;boolean&gt; - is the user an admin\",\n      \"is_owner\": \"&lt;boolean&gt; - is the workspace owner\",\n      \"is_primary_owner\": \"&lt;boolean&gt;\",\n      \"is_restricted\": \"&lt;boolean&gt;\",\n      \"is_stranger\": \"&lt;boolean&gt;\",\n      \"is_ultra_restricted\": \"&lt;boolean&gt;\",\n      \"has_2fa\": \"&lt;boolean&gt;\"\n      \"real_name\": \"&lt;string&gt;\",\n      \"tz\": \"&lt;string&gt;\"\n    }\n  },\n  \"spaces\": [{\n    \"id\": \"&lt;string&gt; - an ID for a Space in spacelift\",\n    \"labels\": \"&lt;stringArray&gt; - a list of labels attached to this space\",\n    \"name\": \"&lt;string&gt; - name for a Space in spacelift\"\n  }, {\n    \"id\": \"&lt;string&gt;\",\n    \"labels\": \"&lt;stringArray&gt;\",\n    \"name\": \"&lt;string&gt;\"\n  }]\n}\n</code></pre> <p>Info</p> <p>The <code>slack</code> object in the policy input data is built using Slack provided data. For example, \"team\" is the term that is used by Space for a workspace - so it is kept as this in the input data for the sake of clarity and consistency with their API. See their official documentation for always up-to-date and full explanation of the <code>slack</code> object fields.</p> <p>Using the above data we can write policies which only allow for a specific user or slack team to access specific spaces in which your stacks reside.</p> <p>For example here is a policy which would allow anyone from a specific slack team to alter stacks in a particular space:</p> <pre><code>package spacelift\n\n# Allow access for anyone in team X\nallow {\n  input.slack.team.id == \"X\"\n}\n\n# Deny access for everyone except team X\ndeny {\n  input.slack.team.id != \"\"\n  input.slack.team.id != \"X\"\n}\n\n# Grant write access to stacks in Space Y for anyone in team X\nspace_write[\"Y\"] {\n  input.slack.team.id == \"X\"\n}\n</code></pre>"},{"location":"integrations/cloud-providers.html","title":"Spacelift cloud provider integrations","text":""},{"location":"integrations/cloud-providers.html#spacelift-cloud-provider-integrations","title":"Spacelift cloud provider integrations","text":"<p>Infrastructure-as-code automation tools such as Terraform, AWS CloudFormation, or Pulumi require powerful credentials to execute. Typically, you'd provide static credentials (such as AWS credentials, GCP service keys, etc.), which goes against security best practices. Spacelift's cloud integrations manage your resources without the need for long-lived static credentials, dynamically generating short-lived access tokens to connect cloud providers with IaC providers.</p> <p>Spacelift currently supports AWS natively. A generic OpenID Connect integration is also available to work with any compatible service provider.</p> <p>Public vs private workers</p> <p>This feature is designed for customers using the shared public worker pool. When hosting Spacelift workers on your own infrastructure, you can use your cloud providers' ambient credentials (e.g. EC2 instance role or EKS worker role on AWS).</p>"},{"location":"integrations/cloud-providers.html#set-up-your-cloud-provider-integration","title":"Set up your cloud provider integration","text":"<ul> <li>Configure Amazon Web Services (AWS).</li> </ul> <p>You can also use OIDC for available cloud providers.</p>"},{"location":"integrations/cloud-providers/aws.html","title":"Amazon Web Services (AWS)","text":""},{"location":"integrations/cloud-providers/aws.html#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<p>The AWS integration allows Spacelift runs or tasks to automatically assume an IAM role in your AWS account, and in the process, generate a set of temporary credentials. These credentials are then exposed as computed environment variables during the run/task that takes place on the Spacelift stack where the integration is attached.</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_SECURITY_TOKEN</code></li> <li><code>AWS_SESSION_TOKEN</code></li> </ul> <p>These temporary credentials are enough for both the AWS Terraform provider and the Amazon S3 state backend to generate a fully authenticated AWS session without further configuration.</p> <p>To use the AWS integration, you need to set it up and attach it to any stacks that need it.</p>"},{"location":"integrations/cloud-providers/aws.html#spacelift-ui-setup","title":"Spacelift UI setup","text":"<p>See Integrate Spacelift with Amazon Web Services to configure your AWS IAM role, trust policy, and the integration within Spacelift.</p>"},{"location":"integrations/cloud-providers/aws.html#programmatic-setup","title":"Programmatic setup","text":"<p>You can also use the Spacelift Terraform provider in order to create an AWS Cloud integration from an administrative stack, including the trust relationship. Note that in order to do that, your administrative stack will require AWS credentials itself, and ones powerful enough to be able to deal with IAM.</p> <p>Here's an example of what it might look like to create an AWS cloud integration programmatically:</p> <pre><code>data \"aws_caller_identity\" \"current\" {}\n\nlocals {\n  role_name = \"example-role\"\n  role_arn  = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\"\n}\n\nresource \"spacelift_stack\" \"this\" {\n  name         = \"Example Stack\"\n  repository   = \"your-awesome-repo\"\n  branch       = \"main\"\n}\n\nresource \"spacelift_aws_integration\" \"this\" {\n  name = local.role_name\n\n  # We need to set this manually rather than referencing the role to avoid a circular dependency\n  role_arn                       = local.role_arn\n  generate_credentials_in_worker = false\n}\n\n# The spacelift_aws_integration_attachment_external_id data source is used to help generate a trust policy for the integration\ndata \"spacelift_aws_integration_attachment_external_id\" \"this\" {\n  integration_id = spacelift_aws_integration.this.id\n  stack_id       = spacelift_stack.this.id\n  read           = true\n  write          = true\n}\n\nresource \"aws_iam_role\" \"this\" {\n  name = local.role_name\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      jsondecode(data.spacelift_aws_integration_attachment_external_id.this.assume_role_policy_statement),\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"this\" {\n  policy_arn = \"arn:aws:iam::aws:policy/PowerUserAccess\"\n  role       = aws_iam_role.this.name\n}\n\nresource \"spacelift_aws_integration_attachment\" \"this\" {\n  integration_id = spacelift_aws_integration.this.id\n  stack_id       = spacelift_stack.this.id\n  read           = true\n  write          = true\n\n  # The role needs to exist before we attach since we test role assumption during attachment.\n  depends_on = [\n    aws_iam_role.this\n  ]\n}\n</code></pre> <p>Info</p> <p>Please always refer to the provider documentation for the most up-to-date documentation.</p>"},{"location":"integrations/cloud-providers/aws.html#attach-a-role-to-multiple-stacks","title":"Attach a role to multiple stacks","text":"<p>The previous example explained how to use the <code>spacelift_aws_integration_attachment_external_id</code> data-source to generate the assume role policy for using the integration with a single stack, but what if you want to attach the integration to multiple stacks? The simplest option would be to create multiple instances of the data-source (one for each stack) but you can also use a Terraform <code>for_each</code> condition to reduce the amount of code required:</p> <pre><code>locals {\n  role_name        = \"multi-stack-integration\"\n  role_arn         = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\"\n\n  # Define the stacks we want to attach the integration to\n  stacks_to_attach = [\"stack-1\", \"stack-2\", \"stack-3\"]\n}\n\ndata \"aws_caller_identity\" \"current\" {}\ndata \"spacelift_account\" \"current\" {}\n\nresource \"spacelift_aws_integration\" \"integration\" {\n  name = local.role_name\n  role_arn                       = local.role_arn\n  generate_credentials_in_worker = false\n}\n\n# Generate the External IDs required for creating our AssumeRole policy\ndata \"spacelift_aws_integration_attachment_external_id\" \"integration\" {\n  for_each = toset(local.stacks_to_attach)\n\n  integration_id = spacelift_aws_integration.integration.id\n  stack_id       = each.key\n  read           = true\n  write          = true\n}\n\nresource \"aws_iam_role\" \"role\" {\n  name = local.role_name\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\",\n        \"Principal\" = {\n          \"AWS\" : data.spacelift_account.current.aws_account_id\n        },\n        \"Action\" = \"sts:AssumeRole\",\n        \"Condition\" = {\n          \"StringEquals\" = {\n            # Allow the external ID for any of the stacks to assume our role\n            \"sts:ExternalId\" = [for i in values(data.spacelift_aws_integration_attachment_external_id.integration) : i.external_id],\n          }\n        }\n      }\n    ],\n  })\n}\n</code></pre> <p>This will generate a trust relationship that looks something like this:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;principal&gt;:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": [\n                        \"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-1@write\",\n                        \"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-2@write\",\n                        \"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-3@write\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"integrations/cloud-providers/aws.html#are-my-credentials-safe","title":"Are my credentials safe?","text":"<p>Assuming roles and generating credentials on private worker is perfectly safe. Those credentials are never leaked to us in any shape or form.</p> <p>As for the trust relationship established between the Spacelift account and your AWS account for the purpose of dynamically generating short-lived credentials, this is considered much safer than storing static credentials in your stack environment. Unlike user keys that you'd normally have to use, role credentials are dynamically created and short-lived. We use the default expiration which is 1 hour, and do not store them anywhere. Leaking them accidentally through the logs is not an option either because we mask AWS credentials.</p> <p>The most tangible safety feature of the AWS integration is the breadcrumb trail it leaves in CloudTrail. Every resource change can be mapped to an individual Terraform run or task whose ID automatically becomes the username as the <code>sts:AssumeRole</code> API call with that ID as <code>RoleSessionName</code>. In conjunction with AWS tools like Config, it can be a very powerful compliance tool.</p> <p>Let's have a look at a CloudTrail event showing an IAM role being created by what seems to be a Spacelift run:</p> <p></p> <p><code>01DSJ63P40BAZY4VW8BXXG7M4K</code> is a run ID we can then trace back even further.</p>"},{"location":"integrations/cloud-providers/aws.html#roles-assuming-other-roles","title":"Roles assuming other roles","text":"<p>The AWS Terraform provider allows you to assume an IAM role during setup, effectively doing the same thing over again. This approach is especially useful if you want to control resources in multiple AWS accounts from a single Spacelift stack. This is totally fine; in IAM, roles can assume other roles, though you need to set up the trust relationship between the role you have Spacelift assume and the role for each provider instance to assume.</p> <p>One thing you want to watch is the guaranteed ability to map the change to a particular run or task that we described in the previous section. One way of fixing this would be to use the <code>TF_VAR_spacelift_run_id</code> computed environment variable available to each Spacelift workflow. It's already a Terraform variable, so you can set it up like this:</p> <pre><code>variable \"spacelift_run_id\" {}\n\n# That's our default provider with credentials generated by Spacelift.\nprovider \"aws\" {}\n\n# That's where Terraform needs to run sts:AssumeRole with your\n# Spacelift-generated credentials to obtain ones for the second account.\nprovider \"aws\" {\n  alias = \"second-account\"\n\n  assume_role {\n    role_arn     = \"&lt;up-to-you&gt;\"\n    session_name = var.spacelift_run_id\n    external_id  = \"&lt;up-to-you&gt;\"\n  }\n}\n</code></pre>"},{"location":"integrations/cloud-providers/oidc.html","title":"OpenID Connect (OIDC)","text":""},{"location":"integrations/cloud-providers/oidc.html#openid-connect-oidc","title":"OpenID Connect (OIDC)","text":"<p>OpenID Connect is a federated identity technology that allows you to exchange short-lived Spacelift credentials for temporary credentials valid for external service providers like AWS, GCP, Azure, HashiCorp Vault etc. With OIDC, Spacelift can manage your infrastructure on these cloud providers without static credentials.</p> <p>OIDC is also an attractive alternative to our native AWS integration in that it implements a common protocol, requires no additional configuration on the Spacelift side, supports a wider range of external service providers and empowers the user to construct more sophisticated access policies based on JWT claims.</p> <p>It is not the purpose of this document to explain the details of the OpenID Connect protocol. If you are not familiar with it, we recommend you read the OpenID Connect specification or GitHub's excellent introduction to security hardening with OpenID Connect.</p>"},{"location":"integrations/cloud-providers/oidc.html#considerations-when-self-hosting","title":"Considerations when self-hosting","text":"<p>For this feature to work, the service you are integrating with needs to be able to verify that tokens issued by Spacelift are valid by accessing the JWKs used by Spacelift.</p> <p>The simplest option is allowing the service you are integrating with to access your Spacelift server directly. In this case it will access the <code>/.well-known/openid-configuration</code> and <code>/.well-known/jwks</code> endpoints on your Spacelift server during the token exchange.</p> <p>Another option is to manually upload the JWKs (that you can get from the <code>/.well-known/jwks</code> endpoint on your Spacelift server) to the service you are integrating with if they support doing so.</p>"},{"location":"integrations/cloud-providers/oidc.html#overriding-issuer-and-jwks-endpoints-self-hosted","title":"Overriding issuer and JWKS endpoints (Self-Hosted)","text":"<p>For Self-Hosted installations, you can override the <code>issuer</code> and <code>jwks_uri</code> values in the OpenID Connect Discovery Document by adding the following environment variables to your deployment:</p> <pre><code>FEDERATION_CUSTOM_ISSUER=https://id.example.com\nFEDERATION_CUSTOM_JWKS_URI=https://id.example.com/.well-known/jwks.json\n</code></pre> <p>Alternatively, if you use CloudFormation, you can use the following configuration values:</p> <pre><code>{\n  \"federation_config\": {\n    \"issuer\": \"https://id.example.com\",\n    \"jwks_uri\": \"https://id.example.com/.well-known/jwks.json\"\n  }\n}\n</code></pre> <p>The behavior follows this precedence:</p> <ul> <li>If only the issuer is provided \u2192 <code>jwks_uri</code> is automatically derived from it</li> <li>If both are provided \u2192 both override values are used</li> <li>If neither is provided \u2192 fallback to current behavior (account URL)</li> </ul> <p>Warning</p> <p>The <code>issuer</code> value must exactly match the <code>iss</code> claim in the tokens for proper validation.</p>"},{"location":"integrations/cloud-providers/oidc.html#about-the-spacelift-oidc-token","title":"About the Spacelift OIDC token","text":"<p>The Spacelift OIDC token is a JSON Web Token that is signed by Spacelift and contains a set of claims that can be used to construct a set of temporary credentials for the external service provider. The token is valid for an hour and is available to every run in any paid Spacelift account. The token is available in the <code>SPACELIFT_OIDC_TOKEN</code> environment variable and in the <code>/mnt/workspace/spacelift.oidc</code> file.</p>"},{"location":"integrations/cloud-providers/oidc.html#standard-claims","title":"Standard claims","text":"<p>The token contains the following standard claims:</p> <ul> <li><code>iss</code>: The issuer of the token. This is the URL of your Spacelift account, for example <code>https://demo.app.spacelift.io</code>, and is unique for each Spacelift account.</li> <li><code>sub</code>: The subject of the token, which includes some information about the Spacelift run that generated this token.<ul> <li>The subject claim is constructed as follows: <code>space:&lt;space_id&gt;:(stack|module):&lt;stack_id|module_id&gt;:run_type:&lt;run_type&gt;:scope:&lt;read|write&gt;</code>. For example, <code>space:legacy:stack:infra:run_type:TRACKED:scope:write</code>. Individual values are also available as separate custom claims.</li> </ul> </li> <li><code>aud</code>: The audience of the token. This is the hostname of your Spacelift account, for example <code>demo.app.spacelift.io</code>, and is unique for each Spacelift account.</li> <li><code>exp</code>: The expiration time of the token, in seconds since the Unix epoch. The token is valid for one hour.</li> <li><code>iat</code>: The time at which the token was issued, in seconds since the Unix epoch.</li> <li><code>jti</code>: The unique identifier of the token.</li> <li><code>nbf</code>: The time before which the token is not valid, in seconds since the Unix epoch. This is always set to the same value as <code>iat</code>.</li> </ul>"},{"location":"integrations/cloud-providers/oidc.html#custom-claims","title":"Custom claims","text":"<p>The token also contains the following custom claims:</p> <ul> <li><code>spaceId</code>: The ID of the space in which the run that owns the token was executed.</li> <li><code>callerType</code>: The type of the caller, i.e. the entity that owns the run, either <code>stack</code> or <code>module</code>.</li> <li><code>callerId</code>: The ID of the caller, i.e. the stack or module that generated the run.</li> <li><code>runType</code>: The type of the run, such as <code>PROPOSED</code>, <code>TRACKED</code>, <code>TASK</code>, <code>TESTING</code> or <code>DESTROY</code>.</li> <li><code>runId</code>: The ID of the run that owns the token.</li> <li><code>scope</code>: The scope of the token, either <code>read</code> or <code>write</code>.</li> </ul>"},{"location":"integrations/cloud-providers/oidc.html#about-scopes","title":"About scopes","text":"<p>Whether the token is given <code>read</code> or <code>write</code> scope depends on the type of the run that generated the token:</p> <ul> <li><code>read</code>: Proposed runs.</li> <li><code>write</code>: Tracked, testing, and destroy runs as well as tasks.</li> </ul> <p>The only exceptions are tracked runs whose stack is not set to autodeploy. In that case, the token will have a <code>read</code> scope during the planning phase, and a <code>write</code> scope during the apply phase. This is because a tracked run requiring a manual approval should not perform write operations before human confirmation.</p> <p>The scope claim, as well as other claims presented by the Spacelift token, are merely advisory. It depends on you whether you want to control access to your external service provider based on the scope of the token or on some other claim like space, caller, or run type. In other words, Spacelift gives you the data and it's up to you to decide how to use it.</p>"},{"location":"integrations/cloud-providers/oidc.html#custom-claims-mapping-for-groups","title":"Custom Claims Mapping for Groups","text":"<p>Some identity providers use non-standard claim names for user group membership. For example:</p> <ul> <li>AWS Cognito uses <code>cognito:groups</code> instead of the standard <code>groups</code> claim</li> <li>Google Workspace does not include group membership in OIDC tokens by default</li> </ul> <p>Spacelift allows you to map custom claims from your identity provider to the standard <code>groups</code> claim that Spacelift expects. This enables group-based access control even when your IdP doesn't follow the standard OIDC groups claim convention.</p>"},{"location":"integrations/cloud-providers/oidc.html#configuring-custom-claims-mapping","title":"Configuring Custom Claims Mapping","text":"<p>To configure custom claims mapping:</p> <ol> <li>Navigate to Organization settings \u2192 Single Sign-On</li> <li>In your OIDC configuration, look for the Custom claims mapping section</li> <li>Add a mapping for the <code>groups</code> claim:</li> <li>Claim name in IdP: Enter the custom claim name from your identity provider (e.g., <code>cognito:groups</code>)</li> <li>Claim name in Spacelift: Enter <code>groups</code></li> </ol> <p></p>"},{"location":"integrations/cloud-providers/oidc.html#use-the-spacelift-oidc-token","title":"Use the Spacelift OIDC token","text":"<p>You can follow our guidelines to see how to use the Spacelift OIDC token to authenticate with:</p> <ul> <li>AWS</li> <li>GCP</li> <li>Azure</li> <li>HashiCorp Vault</li> </ul> <p>In particular, we will focus on setting up the integration and using it from these services' respective Terraform providers.</p>"},{"location":"integrations/cloud-providers/oidc/aws-oidc.html","title":"Amazon Web Services (AWS)","text":""},{"location":"integrations/cloud-providers/oidc/aws-oidc.html#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<p>Warning</p> <p>Until Terraform and OpenTofu versions 1.6.0, the AWS S3 state backend did not support authenticating with OIDC.</p> <p>If you need to use the AWS S3 state backend with older versions, you can use the following workaround:</p> <ul> <li>Add the following command as a <code>before_init</code> hook (make sure to replace <code>&lt;ROLE ARN&gt;</code> with your IAM role ARN).</li> </ul> <pre><code>export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" $(aws sts assume-role-with-web-identity --web-identity-token \"$(cat /mnt/workspace/spacelift.oidc)\" --role-arn &lt;ROLE ARN&gt; --role-session-name spacelift-run-${TF_VAR_spacelift_run_id} --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" --output text))\n</code></pre> <ul> <li>Comment out the <code>role_arn</code> argument in the <code>backend</code> block.</li> <li>Comment out the <code>assume_role_with_web_identity</code> section in the AWS provider block.</li> </ul> <p>Alternatively, you can use the dedicated AWS Cloud Integration that uses AWS STS to obtain temporary credentials.</p>"},{"location":"integrations/cloud-providers/oidc/aws-oidc.html#configure-spacelift-as-an-identity-provider","title":"Configure Spacelift as an identity provider","text":"<p>You need to set up Spacelift as a valid identity provider for your AWS account. This is done by creating an OpenID Connect identity provider. You can do it declaratively using any of the IaC providers, programmatically using the AWS CLI, or with the console.</p> <p>For illustrative purposes, we will use the console:</p> <ol> <li>Go to the AWS console and select the IAM service.</li> <li>Click Identity providers in the left-hand menu.</li> <li>Click Add provider in the top bar.     </li> <li>Select OpenID Connect as the provider type.     </li> <li>Click Get thumbprint. This is required by AWS and protects you from a certain class of MitM attacks.</li> </ol> <p>Hint</p> <p>Add iss to Provider URL and you will need to add aud to Audience.</p> <p>Replace <code>demo.app.spacelift.io</code> with the hostname of your Spacelift account.</p> <p>Once created, the identity provider will be listed in the \"Identity providers\" table.</p>"},{"location":"integrations/cloud-providers/oidc/aws-oidc.html#add-spacelift-oidc-as-the-role-provider","title":"Add Spacelift OIDC as the role provider","text":"<p>You can click on the provider name to see the details. From here, you will also be able to assign an IAM role to this new identity provider:</p> <p></p> <ol> <li>Click Assign role, and choose to create a new role.</li> <li>Click Web identity and select the new Spacelift OIDC provider as the trusted entity.</li> <li>Select the audience from the dropdown (there should only be one option).     </li> <li>The rest of the process is the same as for any other role creation. Select the policies you want to attach to the role, and add tags and a description.</li> <li>Once you're done, click Create role.</li> </ol> <p>If you go to your new role's details page, in the Trust relationships section you will notice that it is now associated with the Spacelift OIDC provider:</p> <p></p> <p>This trust relationship is very relaxed and will allow any stack or module in the <code>demo</code> Spacelift account to assume this role. If you want to be more restrictive, you will want to add more conditions. For example, we can restrict the role to be only assumable by stacks in the <code>production</code> space by adding the following condition:</p> <pre><code>\"StringLike\": {\n  \"demo.app.spacelift.io:sub\": \"space:production:*\"\n}\n</code></pre> <p>Hint</p> <p>You will need to replace <code>demo.app.spacelift.io</code> with the hostname of your Spacelift account.</p> <p>You can also restrict the role so only a specific stack can assume it, using the stack ID:</p> <pre><code>\"StringLike\": {\n  \"demo.app.spacelift.io:sub\": \"*:stack:oidc-is-awesome:*\"\n}\n</code></pre> <p>You can mix and match these to get the exact constraints you need. You can learn more about the intricacies of AWS IAM conditions in the official docs. Remember that AWS does not seem to support custom claims, so you will need to use the standard ones to do the matching (primarily <code>sub</code>, as shown above).</p>"},{"location":"integrations/cloud-providers/oidc/aws-oidc.html#configure-the-terraform-provider","title":"Configure the Terraform provider","text":"<p>Once the Spacelift-AWS OIDC integration is set up, the Terraform provider can be configured without the need for any static credentials. The <code>aws_role_arn</code> variable should be set to the ARN of the role that you want to assume:</p> <pre><code>provider \"aws\" {\n  assume_role_with_web_identity {\n    role_arn = var.aws_role_arn\n    web_identity_token_file = \"/mnt/workspace/spacelift.oidc\"\n  }\n}\n</code></pre>"},{"location":"integrations/cloud-providers/oidc/azure-oidc.html","title":"Microsoft Azure","text":""},{"location":"integrations/cloud-providers/oidc/azure-oidc.html#microsoft-azure","title":"Microsoft Azure","text":""},{"location":"integrations/cloud-providers/oidc/azure-oidc.html#configure-workload-identity-federation","title":"Configure workload identity federation","text":"<p>Set up Spacelift as a valid identity provider for your account to allow Spacelift runs access to Azure resources. This is done using workload identity federation.</p> <p>The setup process involves creating an App Registration, and then adding federated credentials that tell Azure which Spacelift runs should be able to use which App Registrations. This process can be completed via the Azure Portal, Azure CLI or Terraform. For illustrative purposes we will use the Azure Portal.</p> <p>Info</p> <p>These instructions show you how to setup federation using a Microsoft Entra App, but the same approach can also be used with a user-assigned managed identity.</p>"},{"location":"integrations/cloud-providers/oidc/azure-oidc.html#create-an-app-registration","title":"Create an App registration","text":"<ol> <li>Navigate to the Azure AD section of the Azure Portal.</li> <li>Go to App registrations, then click New registration.     </li> <li>Enter a name for your registration, select the Accounts in this organizational directory only option, and click Register.     </li> <li>On the overview page, take note of the Application (client) ID and Directory (tenant) ID for configuring the Terraform provider later.     </li> <li>Go to the Certificates &amp; secrets section, select the Federated credentials tab and click Add credential.     </li> <li>Federated credential scenario: Select Other issuer.     </li> </ol>"},{"location":"integrations/cloud-providers/oidc/azure-oidc.html#configure-the-trust-relationship","title":"Configure the trust relationship","text":"<p>The next step is to configure the trust relationship between Spacelift and Azure. Fill out the following pieces of information:</p> <ul> <li>Issuer: The URL of your Spacelift account, for example <code>https://myaccount.app.spacelift.io</code>.</li> <li>Subject identifier: The subject that a token must contain to be able to get credentials for your App. This uses the format mentioned in the standard claims section.</li> <li>Name: A name for this credential.</li> <li>Audience: The hostname of your Spacelift account, for example <code>myaccount.app.spacelift.io</code>.</li> </ul> <p>Take a look at the following screenshot for an example allowing a proposed run to use our App:</p> <p></p> <p>Workload federation in Azure requires the subject claim of the OIDC token to exactly match the federated credential, and doesn't allow wildcards. Because of this you will need to repeat the same process and add a number of different federated credentials in order to support all the different types of runs for your Stack or module.</p> <p>For example, for a stack called <code>azure-oidc-test</code> in the <code>legacy</code> space you need to add credentials for the following subjects:</p> <pre><code>space:legacy:stack:azure-oidc-test:run_type:TRACKED:scope:read\nspace:legacy:stack:azure-oidc-test:run_type:TRACKED:scope:write\nspace:legacy:stack:azure-oidc-test:run_type:PROPOSED:scope:read\nspace:legacy:stack:azure-oidc-test:run_type:TASK:scope:write\nspace:legacy:stack:azure-oidc-test:run_type:DESTROY:scope:write\n</code></pre> <p>And for a module called <code>my-module</code> in the <code>development</code> space you need to add the following:</p> <pre><code>space:development:stack:my-module:run_type:TESTING:scope:read\nspace:development:stack:my-module:run_type:TESTING:scope:write\n</code></pre> <p>After adding all the credentials for a stack, it should look something like this:</p> <p></p> <p>Info</p> <p>Please see the Standard claims section for more information about the subject format.</p>"},{"location":"integrations/cloud-providers/oidc/azure-oidc.html#configure-the-terraform-provider","title":"Configure the Terraform provider","text":"<p>Once workload identity federation is set up, the AzureRM provider can be configured without the need for any static credentials.</p> <ol> <li>Enable the <code>use_oidc</code> feature of the provider.</li> <li> <p>Use the <code>oidc_token_file_path</code> setting to tell the provider where to find the token:</p> <pre><code>provider \"azurerm\" {\nfeatures {}\nuse_oidc             = true\noidc_token_file_path = \"/mnt/workspace/spacelift.oidc\"\n}\n</code></pre> </li> <li> <p>Next, add the following environment variables to your stack:</p> </li> <li><code>ARM_CLIENT_ID</code>: The client ID of the App registration created in the previous section.</li> <li><code>ARM_TENANT_ID</code>: The tenant ID of the App registration created in the previous section.</li> <li><code>ARM_SUBSCRIPTION_ID</code>: The ID of the Azure subscription you want to use.</li> </ol> <p>Before you can use your App registration to manage Azure resources, you need to assign the correct RBAC permissions to it.</p>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html","title":"Google Cloud Platform (GCP)","text":""},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<p>Spacelift's GCP integration via OIDC allows Spacelift to manage your Google Cloud resources without the need for long-lived static credentials by creating a service account inside the project dedicated to your Stack.</p> <p>With the service account already created, Spacelift generates temporary OAuth token for the service account as a <code>GOOGLE_OAUTH_ACCESS_TOKEN</code> variable in the environment of your runs and tasks. This is one of the configuration options for the Google Terraform provider, so you can define it like this:</p> <pre><code>provider \"google\" {}\n</code></pre> <p>Many GCP resources require the <code>project</code> identifier too, so if you don't specify a default in your provider, you will need to pass it to each individual resource that requires it.</p>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#set-up-the-google-cloud-platform-integration","title":"Set up the Google Cloud Platform integration","text":"<p>In order to enable Spacelift runs to access GCP resources, you need to set up Spacelift as a valid identity provider for your account within GCP.</p>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#set-spacelift-as-a-valid-identity-provider","title":"Set Spacelift as a valid identity provider","text":"<ol> <li>Navigate to the GCP console and select the IAM &amp; Admin service.</li> <li>Click Workload Identity Federation in the left-hand menu.</li> <li>If this is your first time creating a Workload Identity Pool, click Get Started, then Create Pool.     <ul> <li>If you have already created a Workload Identity Pool before, click Create Pool.    </li> </ul> </li> <li>Enter a name for your new identity pool and optionally set a description.</li> <li>Fill in the identity provider details:     <ol> <li>Select a provider: Select OpenID Connect (OIDC).</li> <li>Provider name: Enter the email address linked to your Spacelift account.</li> <li>Issuer (URL): The URL of your Spacelift account, including the scheme. Ensure you add <code>iss</code> to the URL.</li> <li>Audiences: Select Allowed audiences, then enter the hostname of your Spacelift account (e.g. <code>demo.app.spacelift.io</code>). Ensure you add <code>aud</code> to the hostname.</li> </ol> </li> <li>Fill in the provider attributes to configure mappings between Spacelift token claims (assertions) and Google attributes:     <ol> <li>Google 1: This is filled in automatically with <code>google.subject</code>.</li> <li>OIDC 1: Enter <code>assertion.sub</code>.</li> <li>Google 2: Enter <code>attribute.space</code>.</li> <li>OIDC 2: Enter <code>assertion.spaceId</code>. Custom claims like this can be mapped to custom attributes, which need to start with the <code>attribute.</code> prefix.</li> </ol> </li> <li>Attribute conditions: Specify extra conditions using Google's Common Expression Language to restrict which identities can authenticate using your workload identity pool.</li> <li>Finish creating the workload identity pool.</li> </ol> <p>Warning</p> <p>If your Stack ID is too long, it may exceed the threshold set by Google for the <code>google.subject</code> mapping. In that case, you can use a different custom claim to create the mapping.</p>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#grant-access-to-service-account","title":"Grant access to service account","text":"<p>Once the workload identity pool has been created, you need to grant it access impersonate the service account we will be using.</p> <ol> <li>Ensure you have a Spacelift service account ready to use.</li> <li>In the workload identity pool details, click Grant access.</li> <li>Service account: Select the Spacelift service account from the list.</li> <li>Select principals: Select space in the attribute name dropdown, then enter the full SpaceId (from Spacelift) in the text box.</li> <li>Click Save.</li> </ol> <p>In this example, any token claiming to originate from our Spacelift account's <code>prod</code> space can impersonate the service account: </p>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#download-the-configuration-file","title":"Download the configuration file","text":"<p>After you give the workload identity pool access to impersonate the service account, you will be able to Configure your application.</p> <ol> <li>Provider: Select your Spacelift service account name in the dropdown.</li> <li>OIDC ID token path: Enter <code>/mnt/workspace/spacelift.oidc</code>.</li> <li>Format type: Select json.</li> <li>Subject token field name: Leave as <code>access_token</code>.</li> <li>Click Download config.</li> </ol> <p></p> <p>The downloaded file will include the format type in <code>credential_source</code>. You can remove this so your <code>credential_source</code> section only contains:</p> <pre><code> \"credential_source\": {\n    \"file\": \"/mnt/workspace/spacelift.oidc\"\n  }\n</code></pre>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#internal-only-load-balancer-configuration","title":"Internal-only load balancer configuration","text":"<p>GCP needs information about the Spacelift OIDC provider to enable trust between Spacelift and GCP. Generally, GCP will gather this information itself via a JWKS endpoint hosted by Spacelift.</p> <p>However, if you're using an internal only load balancer, GCP will not have access to that endpoint and you will need to provide the JWKS details manually.</p> <ol> <li>Download your JWKS from <code>https://{your-spacelift-url}/.well-known/jwks</code>.</li> <li>Follow this guide on GCP to upload the JWKS to GCP manually.</li> </ol> <p>Once the JWKS is uploaded, OIDC between Spacelift and GCP should work as expected.</p>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#connect-with-specific-iac-providers","title":"Connect with specific IaC providers","text":""},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#opentofu-terraform-and-pulumi","title":"OpenTofu, Terraform, and Pulumi","text":"<p>Once the Spacelift-GCP OIDC integration is set up, the Google Cloud Terraform provider and Pulumi GCP provider can be configured without the need for any static credentials.</p> <p>You will need to provide a configuration file telling the provider how to authenticate. The configuration file can be created manually or generated by the <code>gcloud</code> utility and looks like this:</p> <pre><code>{\n  \"type\": \"external_account\",\n  \"audience\": \"//iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${WORKER_POOL_ID}/providers/${IDENTITY_PROVIDER_ID}\",\n  \"subject_token_type\": \"urn:ietf:params:oauth:token-type:jwt\",\n  \"token_url\": \"https://sts.googleapis.com/v1/token\",\n  \"credential_source\": {\n    \"file\": \"/mnt/workspace/spacelift.oidc\"\n  },\n  \"service_account_impersonation_url\": \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/${SERVICE_ACCOUNT_EMAIL}:generateAccessToken\",\n  \"service_account_impersonation\": {\n    \"token_lifetime_seconds\": 3600\n  }\n}\n</code></pre> <p>Your Spacelift run needs to have access to this file, so check it in, then mount it on a stack directly or in a context that is attached to the stack.</p> <p>You will also need to tell the provider how to find this configuration file. Create a <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable, and set its value as the path to your credentials file.</p> <p>Here is an example of using a Spacelift context to mount the file and configure the provider to be attached to an arbitrary number of stacks:</p> <p></p> <p>For more information about configuring the OpenTofu/Terraform provider, please see the Google Cloud Terraform provider docs. The Pulumi configuration follows the same steps as OpenTofu/Terraform.</p>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#iamserviceaccountsgetaccesstoken-permission_denied","title":"iam.serviceAccounts.getAccessToken PERMISSION_DENIED","text":"<p>If your Spacelift stack does not have permission to impersonate your Service Account, you may receive an error message in your run logs like the following:</p> <pre><code>\"error\": {\n  \"code\": 403,\n  \"message\": \"Permission 'iam.serviceAccounts.getAccessToken' denied on resource (or it may not exist).\",\n  \"status\": \"PERMISSION_DENIED\",\n  \"details\": [\n    {\n      \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n      \"reason\": \"IAM_PERMISSION_DENIED\",\n      \"domain\": \"iam.googleapis.com\",\n      \"metadata\": {\n        \"permission\": \"iam.serviceAccounts.getAccessToken\"\n      }\n    }\n  ]\n}\n</code></pre> <p>If this happens, check the <code>service_account_impersonation_url</code> property in your configuration file and make sure it points at the service account you are trying to use. For example if you are trying to use a service account called <code>spacelift@my-gcp-org.iam.gserviceaccount.com</code>, you should have a value like the following:</p> <pre><code>https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/spacelift@my-gcp-org.iam.gserviceaccount.com:generateAccessToken\n</code></pre> <p>Next, check the conditions about who is allowed to impersonate your service account in your workflow identity pool. For example, in the following screenshot, only stacks in the <code>development-01JS1ZCWC4VYKR20SBRDAAFX6D</code> space are allowed to impersonate your service account:</p> <p></p>"},{"location":"integrations/cloud-providers/oidc/vault-oidc.html","title":"HashiCorp Vault","text":""},{"location":"integrations/cloud-providers/oidc/vault-oidc.html#hashicorp-vault","title":"HashiCorp Vault","text":""},{"location":"integrations/cloud-providers/oidc/vault-oidc.html#configure-spacelift-as-an-identity-provider","title":"Configure Spacelift as an Identity Provider","text":"<p>Set up Spacelift as a valid identity provider for your Vault instance to allow Spacelift runs access to Vault resources. This is done using Vault's OIDC auth method.</p> <p>The setup process involves creating a role in Vault that tells Vault which Spacelift runs should be access which Vault secrets. This process can be completed via the Vault CLI or Terraform. For illustrative purposes we will use the Vault CLI.</p> <p>First, if you haven't enabled the JWT auth method in your Vault instance yet, run the following command:</p> <pre><code>vault auth enable jwt\n</code></pre>"},{"location":"integrations/cloud-providers/oidc/vault-oidc.html#add-spacelift-as-an-identity-provider","title":"Add Spacelift as an identity provider","text":"<ol> <li> <p>Run the following command:</p> <pre><code>vault write auth/jwt/config \\\n  bound_issuer=\"https://demo.app.spacelift.io\" \\\n  oidc_discovery_url=\"https://demo.app.spacelift.io\"\n</code></pre> </li> <li> <p>The <code>bound_issuer</code> parameter is the URL of your Spacelift account, which is used as the issuer claim in the OIDC token you receive from Spacelift.</p> </li> <li>The <code>oidc_discovery_url</code> parameter is the URL of the OIDC discovery endpoint for your Spacelift account, which is (in this case) identical to the <code>bound_issuer</code> parameter.</li> <li> <p>Create a policy to determine which Spacelift runs can access which Vault secrets. For example, the following policy allows all Spacelift runs to read any secret in the <code>secrets/preprod</code> path:</p> <pre><code>vault policy write infra-preprod - &lt;&lt;EOF\npath \"secrets/preprod/*\" {\n  capabilities = [\"read\"]\n}\nEOF\n</code></pre> </li> </ol>"},{"location":"integrations/cloud-providers/oidc/vault-oidc.html#create-role-to-bind-policy-to-identity-provier","title":"Create role to bind policy to identity provier","text":"<ol> <li> <p>The following command creates a role called <code>infra-preprod</code> that binds the <code>infra-preprod</code> policy to the JWT identity provider:</p> <pre><code>vault write auth/jwt/role/infra-preprod -&lt;&lt;EOF\n{\n  \"role_type\": \"jwt\",\n  \"user_claim\": \"iss\",\n  \"bound_audiences\": \"demo.app.spacelift.io\",\n  \"bound_claims\": { \"spaceId\": \"preprod\" },\n  \"policies\": [\"infra-preprod\"],\n  \"ttl\": \"10m\"\n}\nEOF\n</code></pre> </li> <li> <p>The <code>bound_audiences</code> parameter is the hostname of your Spacelift account, which is used as the audience claim in the OIDC token you receive from Spacelift.</p> </li> <li>The <code>bound_claims</code> parameter is a JSON object that contains the claims that the OIDC token must contain in order to be able to access the Vault secrets. How you scope this will very much depend on your use case.</li> </ol> <p>In the above example, only runs belonging to a stack or module in the <code>spaceId</code> claim can assume the \"infra-preprod\" Vault role. You can refer to our documentation to see available standard and custom claims presented by the Spacelift OIDC token.</p>"},{"location":"integrations/cloud-providers/oidc/vault-oidc.html#configure-the-terraform-provider","title":"Configure the Terraform Provider","text":"<p>Once the Vault setup is complete, you need to configure the Terraform Vault provider to use the Spacelift OIDC JWT token to assume a particular role.</p> <ol> <li>Provide the <code>auth_login_jwt</code> configuration block to the provider.</li> <li> <p>Set the <code>role</code> parameter to the name of the role you created in the previous section:</p> <pre><code>provider \"vault\" {\n  # ... other configuration\n  skip_child_token = true\n  auth_login_jwt {\n    role = \"infra-preprod\"\n  }\n}\n</code></pre> </li> <li> <p>Set the <code>TERRAFORM_VAULT_AUTH_JWT</code> environment variable to <code>${SPACELIFT_OIDC_TOKEN}</code>, either directly on your stack, or on one of the attached contexts.</p> <ul> <li>This approach uses interpolation to dynamically set the value of the variable the provider is looking for to the value of the environment variable that Spacelift provides.</li> </ul> </li> </ol>"},{"location":"integrations/external-integrations/backstage.html","title":"Backstage","text":""},{"location":"integrations/external-integrations/backstage.html#backstage","title":"Backstage","text":""},{"location":"integrations/external-integrations/backstage.html#about-the-integration","title":"About the integration","text":"<p>The Backstage integration with Spacelift creates a bridge between your infrastructure as code workflows and Backstage. This integration allows Backstage to check the current status of your team's stacks and execute actions on them.</p>"},{"location":"integrations/external-integrations/backstage.html#benefits","title":"Benefits","text":"<ul> <li>Unified visibility: View the status of all your Spacelift stacks directly within your Backstage developer portal</li> <li>Operational efficiency: Trigger stack runs without leaving Backstage</li> <li>Streamlined workflows: Integrate infrastructure management into your developer portal experience</li> </ul>"},{"location":"integrations/external-integrations/backstage.html#setup-guide","title":"Setup Guide","text":""},{"location":"integrations/external-integrations/backstage.html#create-an-integration-key-in-spacelift","title":"Create an Integration key in Spacelift","text":"<p>To handle the communication between Spacelift and Backstage, we will use a Spacelift Integration key. An integration key is an API key with admin role that will not consume API key seats. The tokens issued with an integration key are short-lived, improving security for the integration communications.</p> <ol> <li>Navigate to Integrations &gt; Backstage</li> <li>Click on Set up integration</li> <li>Select the Space where the integration will have access.</li> <li>Click Create</li> </ol> <p>A file with the API secret will be downloaded automatically.</p> <p>Warning</p> <p>Note that the integration key will provide Admin access to the selected Space. Use the appropriate space to limit the resources that a user can access in Backstage.</p>"},{"location":"integrations/external-integrations/backstage.html#install-the-backstage-plugins","title":"Install the Backstage plugins","text":"<p>To install the Backstage plugins, follow the instructions that you will find in our plugins repository. The README files contain detailed steps to install each plugin. Here's an overview of the steps you have to take:</p> <p>Info</p> <p>These plugins are compatible with Backstage v1.17.0 and above. For older versions, please check the plugin repository for specific compatibility information.</p> <ol> <li> <p>From the root of your project, install the Backend and Frontend plugins:</p> <pre><code>    yarn --cwd packages/backend add @spacelift-io/backstage-integration-backend\n    yarn --cwd packages/app add @spacelift-io/backstage-integration-frontend\n</code></pre> </li> <li> <p>Create your environment variables in <code>app-config.yaml</code></p> <p>You will have to provide the following variables:</p> <pre><code>    spacelift:\n      hostUrl: '&lt;your-selfhosted-spacelift-domain&gt;' # Your Spacelift instance URL (WITHOUT https://)\n      apiKey: ${SPACELIFT_API_KEY} # Your Spacelift integration Key ID\n      apiSecret: ${SPACELIFT_API_SECRET} # Your Spacelift API Key Secret\n</code></pre> <p>Setting environment variables</p> <p>For production environments, we recommend using appropriate secrets management solutions to securely provide these environment variables to your Backstage instance. For local development, you can add the values to <code>app-config.local.yaml</code>, to override production values.</p> </li> <li> <p>Connect the Backend and Frontend plugins to your Backstage code.     You can find more details on how to do that in our Backend README and Frontend README files.</p> </li> <li> <p>Run your Backstage instance and check that everything is working as expected.</p> </li> </ol>"},{"location":"integrations/external-integrations/backstage.html#important-note-on-security-and-permissions","title":"Important note on security and permissions","text":"<p>The plugins operate using the permissions granted to the Spacelift Integration Key configured in Spacelift. They do not implement any additional user-level permission handling within Backstage.</p> <p>It is the responsibility of the Backstage instance administrator to ensure that appropriate Backstage permissions are configured to control access to the Spacelift plugin pages and features, thereby preventing unauthorized actions or information exposure.</p> <p>We recommend configuring the integration key to the top level Space that the Backstage users will be able to see, limiting access within Backstage to any users that lack the appropriated permissions to trigger runs on the associated subspaces.</p>"},{"location":"integrations/external-integrations/backstage.html#integration-capabilities","title":"Integration Capabilities","text":""},{"location":"integrations/external-integrations/backstage.html#stacks-list","title":"Stacks list","text":"<p>The plugin will show a list of all the stacks visible based on the integration key Space. It will display the status of the stacks and allow users to trigger a rerun when the stack allows it.</p> <p></p>"},{"location":"integrations/external-integrations/backstage.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/external-integrations/backstage.html#common-issues","title":"Common issues","text":"<ol> <li> <p>Plugin not appearing in Backstage: Ensure you've correctly configured the plugin in your <code>app-config.yaml</code> and properly imported and registered the plugin in your Backstage app.</p> </li> <li> <p>Authentication errors: Verify that your integration key <code>apiKey</code> and <code>apiSecret</code> environment variables are properly set.</p> </li> <li> <p>Stack listing is empty: Check that the integration key has access to the appropriate spaces containing your stacks.</p> </li> </ol> <p>If you encounter issues not covered here, please check our GitHub repository issues or contact our support.</p>"},{"location":"integrations/external-integrations/backstage.html#removing-integration","title":"Removing Integration","text":"<p>To remove the integration, you need to delete the integration key associated with it.</p> <ol> <li>Go to Integrations &gt; Backstage</li> <li>Find the integration key used to integrate with Backstage</li> <li>From the three dots menu, select Delete</li> </ol> <p>After deleting the integration key, the integration won't be able to access Spacelift resources any more. You can safely uninstall the packages and remove the code associated to them.</p> <pre><code>    yarn --cwd packages/backend remove @spacelift-io/backstage-integration-backend\n    yarn --cwd packages/app remove @spacelift-io/backstage-integration-frontend\n</code></pre>"},{"location":"integrations/external-integrations/servicenow.html","title":"ServiceNow","text":""},{"location":"integrations/external-integrations/servicenow.html#servicenow","title":"ServiceNow","text":""},{"location":"integrations/external-integrations/servicenow.html#about-the-integration","title":"About the integration","text":"<p>The ServiceNow integration with Spacelift creates a bridge between your infrastructure as code workflows and ServiceNow's enterprise platform. This integration enables teams to automate infrastructure provisioning and management tasks directly through ServiceNow, maintaining compliance and governance while leveraging Spacelift's powerful infrastructure management capabilities.</p>"},{"location":"integrations/external-integrations/servicenow.html#setup-guide","title":"Setup Guide","text":""},{"location":"integrations/external-integrations/servicenow.html#setup-a-user-in-servicenow","title":"Setup a User in ServiceNow","text":"<p>To enable Spacelift to create resources on the ServiceNow side, you need to set up a dedicated user account. Follow these steps:</p> <ol> <li>Navigate to System Security &gt; Users and Groups &gt; Users.</li> <li>Click on New to create a new user.</li> <li>Fill out the form. The only required field is the User ID. Once completed, click Submit.</li> <li>Go to the details of the newly created user and generate a password. You can uncheck the Password needs reset option in the user details or go through the reset flow.</li> <li>Navigate to the Roles tab and add the following roles:<ul> <li><code>web_service_admin</code>: Allows the creation of \"REST Message\".</li> <li><code>business_rule_admin</code>: Allows the creation of \"Business Rule\".</li> <li><code>catalog_admin</code>: Allows the creation of \"Catalog Item\".</li> </ul> </li> <li>Save the changes.</li> </ol>"},{"location":"integrations/external-integrations/servicenow.html#create-integration-in-spacelift","title":"Create Integration in Spacelift","text":"<p>To create an integration with ServiceNow in Spacelift, follow these steps:</p> <ol> <li> <p>Navigate to Integrations &gt; ServiceNow &gt; Create Integration.</p> <p></p> </li> <li> <p>Fill in the details in the form:</p> <ul> <li>Name of the Integration: Provide a name for the integration.</li> <li>Space: Select the space where the integration will be created. This will determine which blueprints can be attached to this integration.</li> <li>Description: Optionally, provide a description for the integration.</li> <li>Integration Base URL: Enter the base URL of your ServiceNow instance, e.g., <code>https://{id}.service-now.com</code>.</li> <li>Username: Enter the username for the account created in the previous step.</li> <li>Password: Enter the password for the account created in the previous step.</li> </ul> </li> <li> <p>Click Create to finalize the integration setup.</p> <p></p> </li> </ol> <p>After completing these steps, Spacelift will create a \"REST Message\" on the ServiceNow side (found under System Web Services &gt; Outbound &gt; REST Message) with the authorization details, which will be used for authentication in Spacelift.</p>"},{"location":"integrations/external-integrations/servicenow.html#integration-capabilities","title":"Integration Capabilities","text":""},{"location":"integrations/external-integrations/servicenow.html#blueprints","title":"Blueprints","text":"<p>The Blueprint integration creates ServiceNow catalog items that correspond to your Spacelift Blueprints. When a user orders such a catalog item in ServiceNow, it triggers the creation of a new Spacelift stack based on the associated blueprint.</p>"},{"location":"integrations/external-integrations/servicenow.html#attaching-integration-to-blueprint","title":"Attaching Integration to Blueprint","text":"<p>To create a service catalog item using Blueprints, follow these steps:</p> <ol> <li>Navigate to Blueprints.</li> <li>Select the blueprint you want to use to create a service catalog item.</li> <li> <p>Go to the Integrations tab.</p> <p></p> </li> <li> <p>Click Attach Integration.</p> </li> <li>Select the integration created in the previous step.</li> <li> <p>Click Attach.</p> <p></p> </li> </ol> <p>After completing these steps, Spacelift will create the following resources on the ServiceNow side:</p> <ul> <li>Service Catalog Item: Found under Service Catalog &gt; Catalog Definition &gt; Maintain Items, along with variables that need to be passed to the blueprint to create a stack.</li> <li>Business Rule: Found under System Definition &gt; Business Rules, with a custom script that transforms ServiceNow variables into blueprint input and calls the Spacelift API to create a stack based on the blueprint.</li> </ul> <p>Feel free to adjust any of these resources to suit your needs.</p> <p>This image shows an example of a ServiceNow service catalog item ordering interface after a successful integration with Spacelift.</p> <p></p>"},{"location":"integrations/external-integrations/servicenow.html#removing-integration","title":"Removing Integration","text":"<p>To remove the integration, you need to first detach it from all blueprints.</p> <p></p> <p>Then, navigate to the ServiceNow Integration tab and remove the integration itself.</p> <p></p> <p>When detaching and deleting the integration, Spacelift will also attempt to remove resources created on the ServiceNow side.</p>"},{"location":"integrations/observability.html","title":"Observability","text":""},{"location":"integrations/observability.html#observability","title":"Observability","text":"<p>Spacelift can integrate with observability tools by sending information to a webhook endpoint, here are some of the observability tools we can integrate with:</p> <ul> <li>Datadog</li> <li>Prometheus</li> </ul>"},{"location":"integrations/observability/datadog.html","title":"Datadog integration","text":""},{"location":"integrations/observability/datadog.html#datadog-integration","title":"Datadog integration","text":"<p>Spacelift can send data to Datadog to help you monitor your infrastructure and Spacelift stacks using Datadog's excellent monitoring and analytics tools. Our integration with Datadog focuses primarily on runs and lets you create dashboards and alerts to answer questions like:</p> <ul> <li>How many runs are failing?</li> <li>Which stacks see the most activity?</li> <li>How long does it take to plan a given stack?</li> <li>How long does it take to apply a stack?</li> <li>What is the load on my Spacelift private workers?</li> <li>How many resources am I changing?</li> <li>...and many more!</li> </ul> <p>Here's a very simple dashboard we've created based on this integration that shows the performance of our continuous regression tests:</p> <p></p> <p>If you'd like to use the same dashboard, the JSON is provided below.</p> Click to expand <pre><code>{\n  \"title\": \"Spacelift CI performance\",\n  \"description\": \"\",\n  \"widgets\": [\n    {\n      \"id\": 1204455947489370,\n      \"definition\": {\n        \"title\": \"Overall time spent\",\n        \"title_size\": \"16\",\n        \"title_align\": \"left\",\n        \"show_legend\": true,\n        \"legend_layout\": \"auto\",\n        \"legend_columns\": [\"avg\", \"min\", \"max\", \"value\", \"sum\"],\n        \"type\": \"timeseries\",\n        \"requests\": [\n          {\n            \"formulas\": [{ \"formula\": \"query1\" }],\n            \"queries\": [\n              {\n                \"name\": \"query1\",\n                \"data_source\": \"metrics\",\n                \"query\": \"sum:spacelift.integration.run.timing{$Space AND $Stack AND $Environment AND $WorkerPool AND state NOT IN (queued,confirmed,unconfirmed)} by {state}\"\n              }\n            ],\n            \"response_format\": \"timeseries\",\n            \"style\": {\n              \"palette\": \"dog_classic\",\n              \"line_type\": \"solid\",\n              \"line_width\": \"normal\"\n            },\n            \"display_type\": \"bars\"\n          }\n        ]\n      },\n      \"layout\": { \"x\": 0, \"y\": 0, \"width\": 6, \"height\": 4 }\n    },\n    {\n      \"id\": 4165236413630572,\n      \"definition\": {\n        \"title\": \"Resource changes\",\n        \"title_size\": \"16\",\n        \"title_align\": \"left\",\n        \"show_legend\": true,\n        \"legend_layout\": \"auto\",\n        \"legend_columns\": [\"avg\", \"min\", \"max\", \"value\", \"sum\"],\n        \"type\": \"timeseries\",\n        \"requests\": [\n          {\n            \"formulas\": [{ \"formula\": \"query1\" }],\n            \"queries\": [\n              {\n                \"name\": \"query1\",\n                \"data_source\": \"metrics\",\n                \"query\": \"sum:spacelift.integration.run.resources{$Space,$Stack,$Environment,run_type:tracked} by {change_type}.as_count()\"\n              }\n            ],\n            \"response_format\": \"timeseries\",\n            \"style\": {\n              \"palette\": \"dog_classic\",\n              \"line_type\": \"solid\",\n              \"line_width\": \"normal\"\n            },\n            \"display_type\": \"bars\"\n          }\n        ]\n      },\n      \"layout\": { \"x\": 6, \"y\": 0, \"width\": 6, \"height\": 4 }\n    },\n    {\n      \"id\": 7535326510979494,\n      \"definition\": {\n        \"title\": \"Run outcomes\",\n        \"title_size\": \"16\",\n        \"title_align\": \"left\",\n        \"show_legend\": true,\n        \"legend_layout\": \"auto\",\n        \"legend_columns\": [\"avg\", \"min\", \"max\", \"value\", \"sum\"],\n        \"type\": \"timeseries\",\n        \"requests\": [\n          {\n            \"formulas\": [{ \"formula\": \"query1\" }],\n            \"queries\": [\n              {\n                \"name\": \"query1\",\n                \"data_source\": \"metrics\",\n                \"query\": \"sum:spacelift.integration.run.count{$Space,$Stack,$Environment} by {final_state}.as_count()\"\n              }\n            ],\n            \"response_format\": \"timeseries\",\n            \"style\": {\n              \"palette\": \"dog_classic\",\n              \"line_type\": \"solid\",\n              \"line_width\": \"normal\"\n            },\n            \"display_type\": \"bars\"\n          }\n        ]\n      },\n      \"layout\": { \"x\": 0, \"y\": 4, \"width\": 6, \"height\": 4 }\n    },\n    {\n      \"id\": 3687311929209224,\n      \"definition\": {\n        \"title\": \"Worker pool usage\",\n        \"title_size\": \"16\",\n        \"title_align\": \"left\",\n        \"show_legend\": true,\n        \"legend_layout\": \"auto\",\n        \"legend_columns\": [\"avg\", \"min\", \"max\", \"value\", \"sum\"],\n        \"type\": \"timeseries\",\n        \"requests\": [\n          {\n            \"formulas\": [{ \"formula\": \"query1\" }],\n            \"queries\": [\n              {\n                \"name\": \"query1\",\n                \"data_source\": \"metrics\",\n                \"query\": \"sum:spacelift.integration.run.count{$Environment,$Space} by {worker_pool}.as_count()\"\n              }\n            ],\n            \"response_format\": \"timeseries\",\n            \"style\": {\n              \"palette\": \"dog_classic\",\n              \"line_type\": \"solid\",\n              \"line_width\": \"normal\"\n            },\n            \"display_type\": \"bars\"\n          }\n        ]\n      },\n      \"layout\": { \"x\": 6, \"y\": 4, \"width\": 6, \"height\": 4 }\n    },\n    {\n      \"id\": 3469740549844082,\n      \"definition\": {\n        \"title\": \"Drift detection load\",\n        \"title_size\": \"16\",\n        \"title_align\": \"left\",\n        \"show_legend\": true,\n        \"legend_layout\": \"auto\",\n        \"legend_columns\": [\"avg\", \"min\", \"max\", \"value\", \"sum\"],\n        \"type\": \"timeseries\",\n        \"requests\": [\n          {\n            \"formulas\": [{ \"formula\": \"query1\" }],\n            \"queries\": [\n              {\n                \"name\": \"query1\",\n                \"data_source\": \"metrics\",\n                \"query\": \"sum:spacelift.integration.run.count{$Environment,$Space,$WorkerPool,$Stack} by {drift_detection}.as_count()\"\n              }\n            ],\n            \"response_format\": \"timeseries\",\n            \"style\": {\n              \"palette\": \"dog_classic\",\n              \"line_type\": \"solid\",\n              \"line_width\": \"normal\"\n            },\n            \"display_type\": \"bars\"\n          }\n        ]\n      },\n      \"layout\": { \"x\": 0, \"y\": 8, \"width\": 6, \"height\": 3 }\n    },\n    {\n      \"id\": 2802853783337572,\n      \"definition\": {\n        \"title\": \"Plan policy outcomes\",\n        \"title_size\": \"16\",\n        \"title_align\": \"left\",\n        \"show_legend\": true,\n        \"legend_layout\": \"auto\",\n        \"legend_columns\": [\"avg\", \"min\", \"max\", \"value\", \"sum\"],\n        \"type\": \"timeseries\",\n        \"requests\": [\n          {\n            \"formulas\": [{ \"formula\": \"query1\" }],\n            \"queries\": [\n              {\n                \"name\": \"query1\",\n                \"data_source\": \"metrics\",\n                \"query\": \"sum:spacelift.integration.run.policies{$Environment,$Space,$WorkerPool,$Stack,policy_type:plan} by {policy_outcome}\"\n              }\n            ],\n            \"response_format\": \"timeseries\",\n            \"style\": {\n              \"palette\": \"dog_classic\",\n              \"line_type\": \"solid\",\n              \"line_width\": \"normal\"\n            },\n            \"display_type\": \"bars\"\n          }\n        ]\n      },\n      \"layout\": { \"x\": 6, \"y\": 8, \"width\": 6, \"height\": 3 }\n    }\n  ],\n  \"template_variables\": [\n    {\n      \"name\": \"Environment\",\n      \"prefix\": \"env\",\n      \"available_values\": [\"preprod\", \"prod\"],\n      \"default\": \"*\"\n    },\n    {\n      \"name\": \"Space\",\n      \"prefix\": \"space\",\n      \"available_values\": [],\n      \"default\": \"*\"\n    },\n    {\n      \"name\": \"WorkerPool\",\n      \"prefix\": \"worker_pool\",\n      \"available_values\": [],\n      \"default\": \"*\"\n    },\n    {\n      \"name\": \"Stack\",\n      \"prefix\": \"stack\",\n      \"available_values\": [],\n      \"default\": \"*\"\n    }\n  ],\n  \"layout_type\": \"ordered\",\n  \"notify_list\": [],\n  \"reflow_type\": \"fixed\",\n  \"tags\": []\n}\n</code></pre>"},{"location":"integrations/observability/datadog.html#prerequisites","title":"Prerequisites","text":"<p>The Datadog integration is based on our notification policy feature, which requires at least an active Cloud tier subscription. While building a notification-based Datadog integration from scratch is possible, we've created a Terraform module that will set up all the necessary integration elements for you.</p> <p>This module will only create Spacelift assets:</p> <ul> <li>a notification policy that will send data to Datadog;</li> <li>a webhook endpoint that serve as a notification target for the policy;</li> <li>a webhook secret header that will securely authenticate the payload with Datadog;</li> </ul>"},{"location":"integrations/observability/datadog.html#setting-up-the-integration","title":"Setting up the integration","text":"<p>To set up the integration, you'll need to have a Datadog account and a Datadog API key. If you don't have an administrative stack declaratively manage your Spacelift resources, we suggest you create one, and add module instantiation to it according to its usage instructions. We suggest that you pass the Datadog API key as a stack secret, or - even better - retrieve it from a remote secret store (eg. AWS Parameter Store) using Terraform.</p> <p>If you intend to monitor your entire account (our suggested approach), we suggest that the module is installed in the root space of your Spacelift account. If you only want to monitor a subset of your stacks, you can install the module in their respective space.</p>"},{"location":"integrations/observability/datadog.html#metrics","title":"Metrics","text":"<p>The following metrics are sent:</p> <ul> <li><code>spacelift.integration.run.count</code> (counter) - a simple count of runs;</li> <li><code>spacelift.integration.run.timing</code> (counter, nanoseconds) - the duration of different run states. In addition to common tags, this metric is also tagged with the state name, eg. <code>state:planning</code>, <code>state:applying</code>, etc.;</li> <li><code>spacelift.integration.run.resources</code> (counter) - the resources affected by a run. In addition to common tags, this metric is also tagged with the change type, eg. <code>change_type:added</code>, <code>change_type:changed</code>, etc.;</li> </ul>"},{"location":"integrations/observability/datadog.html#common-tags","title":"Common tags","text":"<p>Common tags for all metrics are the following:</p> <ul> <li><code>account</code> (string) : name of the Spacelift account generating the metric;</li> <li><code>branch</code> (string): name of the Git branch the run was triggered from;</li> <li><code>drift_detection</code> (boolean): whether the run was triggered by drift detection;</li> <li><code>run_type</code> (string): type of a run, eg. \"tracked\", \"proposed\", etc.;</li> <li><code>run_url</code> (string): link to the run that generated the metric;</li> <li><code>final_state</code> (string): the terminal state of the run, eg. \"finished\", \"failed\", etc.;</li> <li><code>space</code> (string): name of the Spacelift space the run belongs to;</li> <li><code>stack</code> (string): name of the Spacelift stack the run belongs to;</li> <li><code>worker_pool</code> (string): name of the Spacelift worker pool the run was executed on - for the public worker pool this value is always <code>public</code>;</li> </ul>"},{"location":"integrations/observability/datadog.html#extending-the-integration","title":"Extending the integration","text":"<p>The benefit of building this integration on top of a notification policy is that you can easily extend it to send additional data to Datadog, change the naming of your metrics, change the tags, etc. To do so, you'll need to edit the policy body generated by the module. You can do so by editing the policy in the Spacelift UI, or by forking the module and editing the policy body in the module's source code.</p> <p>Note that this is an advanced feature, and we recommend that you only do so if you're already familiar with Spacelift's notification policy feature and Datadog's API, or are willing to learn about them.</p>"},{"location":"integrations/observability/prometheus.html","title":"Prometheus integration","text":""},{"location":"integrations/observability/prometheus.html#prometheus-integration","title":"Prometheus integration","text":"<p>The Prometheus exporter allows you to monitor various metrics about your Spacelift account over time. You can then use tools like Grafana to visualize those changes and Alertmanager to take actions based on account metrics. Several metrics are available, and you can find the complete list of available metrics here.</p>"},{"location":"integrations/observability/prometheus.html#how-it-works","title":"How it works","text":"<p>The Prometheus exporter is an adaptor between Prometheus and the Spacelift GraphQL API. Whenever Prometheus asks for the current metrics, the exporter makes a GraphQL request and converts it into the metrics format Prometheus expects.</p> <p>Read more on our blog: Monitoring Your Spacelift Account via Prometheus.</p>"},{"location":"integrations/single-sign-on.html","title":"Single Sign-On","text":""},{"location":"integrations/single-sign-on.html#single-sign-on","title":"Single Sign-On","text":"<p>By default, Spacelift supports logging in using GitHub, GitLab, or Google. Some organizations however prefer a Single Sign-On approach, where access to resources is centralized. To accommodate this use-case, Spacelift supports Single Sign-On using SAML 2.0 or OIDC.</p>"},{"location":"integrations/single-sign-on.html#managed-identity-provider-vs-sso","title":"Managed Identity Provider vs SSO","text":"<p>Tip</p> <p>The SSO integration can only be configured once the Spacelift account has been created using one of the supported Identity Providers.</p> <p>To create a Spacelift account, a user needs to choose one of the supported managed identity providers. That user then becomes the \"Managed IdP Admin\".</p> <p>If SSO is configured, the managed identity provider used to create the account and the associated admin are disabled and the first user to successfully log in becomes the \"SSO Admin\".</p> <p>Login policies are not evaluated for Managed IdP and SSO admins so that they cannot lock themselves out. As a side effect, there won\u2019t be any Login policy samples for them in the Policy Workbench.</p> <p>If SSO is disabled later, the managed identity provider and associated admin are re-enabled automatically.</p>"},{"location":"integrations/single-sign-on.html#backup-credentials","title":"Backup Credentials","text":"<p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on.html#managing-integrations","title":"Managing integrations","text":"<p>In order to manage Single Sign-On integrations on your Spacelift account, please go to the Settings section of your account view. Next, navigate to the Single Sign-On tab. If SSO is not enabled for your account, all you're going to see is instructions on how to get started. The first steps are always taken in your identity provider (Google Workspace, Okta, Auth0, ActiveDirectory, etc.). Navigate to your identity provider and create a dedicated SSO application filled with appropriate URLs taken from the Spacelift settings page presented below.</p> <p></p>"},{"location":"integrations/single-sign-on.html#setting-up-saml","title":"Setting up SAML","text":"<p>When setting up Spacelift on your identity provider, you may want to add three attribute mappings:</p> <ul> <li><code>FirstName</code> is used to build human-friendly user name;</li> <li><code>LastName</code> is used to build human-friendly user name;</li> <li><code>Teams</code> can be used by login and stack access policies to determine the level access to the Spacelift account and/or individual Stacks;</li> </ul> <p>Depending on your identity provider and your use case, your mapping may be different. Especially with regards to <code>Teams</code>, some identity providers (eg. Okta) will support an arbitrary list of memberships similar to GitHub teams out of the box, some will need extra customizations like (eg. Google Workspace) and as a courtesy, we will flush your login history.</p> <p>Some identity providers (eg. Okta) will allow you to provide a custom per-user SAML 2.0 Subject for SAML assertions. You could use this feature to map GitHub usernames to your identity provider users and thus get the exact same experience as when using GitHub as your identity provider.</p>"},{"location":"integrations/single-sign-on.html#custom-attribute-mapping","title":"Custom Attribute Mapping","text":"<p>Some identity providers use non-standard attribute names that differ from the standard SAML attributes Spacelift expects. Spacelift allows you to map custom attributes from your identity provider to the standard attributes that Spacelift uses. This enables you to integrate with any identity provider without needing to reconfigure it to match Spacelift's expected attribute names.</p>"},{"location":"integrations/single-sign-on.html#configuring-custom-attribute-mapping","title":"Configuring Custom Attribute Mapping","text":"<p>To configure custom attribute mapping:</p> <ol> <li>Navigate to Organization settings \u2192 Single Sign-On</li> <li>In your SAML configuration, look for the Custom attribute mapping section</li> <li>Add a mapping by specifying:</li> <li>Attribute name in IdP: Enter the custom attribute name from your identity provider (e.g., <code>Groups</code> instead of <code>Teams</code>, <code>GivenName</code> instead of <code>FirstName</code>)</li> <li>Attribute name in Spacelift: Enter the standard Spacelift attribute name (e.g., <code>Teams</code>, <code>FirstName</code>, <code>LastName</code>)</li> </ol> <p>For example, if your identity provider uses <code>Groups</code> to represent team membership, you can map it to <code>Teams</code> so that Spacelift can properly use it in login and stack access policies.</p> <p></p> <p>Warning</p> <p>When setting up SSO without this GitHub mapping, your future logins will appear as new users since Spacelift has no way of mapping those without your assistance. New users will count against your seat quota and you may run out of seats. If you run into this problem, you can contact us.</p> <p>Info</p> <p>Spacelift uses both HTTP-Redirect and HTTP-POST bindings for SAML 2.0. Most of the IdPs enable both by default, but if you run into any issues, please check your application settings.</p>"},{"location":"integrations/single-sign-on.html#nameid-format","title":"NameID format","text":"<p>The NameID format specifies the format that Spacelift requests user identifiers from your identity provider. The user identifier is used as the Spacelift login, and each unique identifier will count against your seat quota. Some identity providers allow you to configure this format, but certain providers do not.</p> <p>If your identity provider does not allow the NameID format to be configured at their end, you can choose from one of the following options:</p> <ul> <li>Transient - an opaque identifier that is not guaranteed to remain the same between logins.</li> <li>Email Address - an email address.</li> <li>Persistent - an opaque identifier that remains the same between logins.</li> </ul>"},{"location":"integrations/single-sign-on.html#saml-setup-guides","title":"SAML Setup Guides","text":"<p>The following are links to example implementations you can use as a reference/guide for setting up your own SAML integration.</p> <ul> <li>AWS IAM Identity Center (formerly known as AWS SSO)</li> </ul> <p>If you can't find your SAML provider in the list above, don't worry - we do support all SAML 2.0 providers.</p>"},{"location":"integrations/single-sign-on.html#setting-up-oidc","title":"Setting up OIDC","text":"<p>When setting up Spacelift on your identity provider, you must make sure it supports the <code>email</code> scope and returns the corresponding <code>email</code></p>"},{"location":"integrations/single-sign-on.html#additional-claims","title":"Additional claims","text":"<p>Spacelift dynamically checks integrated Identity Provider's Well-Known OpenID configuration for a list of supported scopes and, optionally, asks for <code>profile</code> and <code>groups</code> scopes if those are available.</p> <p>Warning</p> <p>In order to populate the <code>input.session.teams</code> value in the Login Policies Spacelift tries to fetch the <code>groups</code> claim. For many Identity Providers, this claim has to be manually set and configured. Bear in mind that some providers such as Google Workspace do not support retrieving groups of given users.</p>"},{"location":"integrations/single-sign-on.html#oidc-setup-guides","title":"OIDC Setup Guides","text":"<p>The following are links to example implementations you can use as a reference/guide for setting up your own OIDC integration.</p> <ul> <li>GitLab</li> <li>Okta</li> <li>OneLogin</li> <li>Microsoft Entra ID (formerly Azure Active Directory)</li> </ul> <p>If you can't find your OIDC provider in the list above, don't worry - we do support all OIDC providers as long as they support the email scope and return the user's email. Fortunately, most OIDC providers do.</p>"},{"location":"integrations/single-sign-on.html#idp-initiated-sso","title":"IdP-initiated SSO","text":"<p>While certainly more convenient, IdP-initiated SSO lacks some of the protections awarded by SP-initiated SSO and is thus inherently less safe. Since Spacelift manages some of your most valuable resources, we decided against supporting this feature.</p> <p>If our server detects an IdP-initiated SSO session, it simply redirects the browser using 303 See other HTTP status code to the endpoint that triggers a regular SP-initiated SSO flow. As a result, you can still access Spacelift by clicking on the link in your IdP catalog, but are not exposed to the vulnerabilities of the IdP-initiated SSO.</p>"},{"location":"integrations/single-sign-on.html#disabling-sso","title":"Disabling SSO","text":"<p>In order to disable SSO integration for your Spacelift account, or change the IdP provider, please click the Disable button to delete the integration. This change takes effect immediately for new logins, and will invalidate existing sessions. New sessions will be created using the new SSO identity provider or - if none is set up - Spacelift will utilize the default identity provider that was used to create the account originally.</p> <p>Warning</p> <p>Again, please note that new usernames will occupy new seats, even if they're the same users registered with a different identity provider.</p> <p></p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html","title":"AWS IAM Identity Center SAML Setup Guide","text":""},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#aws-iam-identity-center-saml-setup-guide","title":"AWS IAM Identity Center SAML Setup Guide","text":"<p>Info</p> <p>Note that SSO Integration with SAML 2.0 is an Enterprise plan feature.</p> <p>If you'd like to set up the ability to sign in to your Spacelift account using a SAML 2.0 integration with AWS IAM Identity Center (formerly known as AWS SSO), you've come to the right place. This example will walk you through the steps to get this set up, and you'll have Single Sign-On running in no time!</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Spacelift account, with access to admin permissions.</li> <li>AWS account which is a member of an AWS Organization, with permission to create AWS IAM Identity applications.</li> </ul>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#configure-the-aws-iam-identity-application","title":"Configure the AWS IAM Identity application","text":"<p>Log into the AWS account, go to the IAM Identity Center home page and finally, click on the \"Applications\" link.</p> <p></p> <p>On that screen, click on the \"Add a new application\" button.</p> <p></p> <p>Finally, click on the \"Add a custom SAML 2.0 application\" button.</p> <p></p> <p>Set the \"Display name\" field to \"Spacelift\". Then, copy the URL for the \"IAM Identity Center SAML metadata file\" and head to the settings in your Spacelift account.</p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#configure-spacelift-saml-integration","title":"Configure Spacelift SAML integration","text":"<p>From the navigation side bar menu, select \"Settings.\"</p> <p></p> <p>Next, you'll want to click the Set Up button underneath the \"SAML Settings\" section.</p> <p></p> <p>In the SAML settings:</p> <ul> <li>Set the value for \"NameID Format\" to \"Persistent\".</li> <li>Enable the \"Dynamic configuration\".</li> <li>Paste the URL you just copied in AWS in the \"IdP metadata URL\" field.</li> </ul> <p></p> <p>Danger</p> <p>Do not click on the \"Save\" button yet, otherwise Spacelift will try to activate SAML integration right away and we are not completely done with the setup yet.</p> <p>If you clicked on the button anyway, you will be presented with an AWS login page and you will likely be unable to log in at this time. Don't worry. Just open another tab in your browser and go to your Spacelift account. As an administrator, you will be able to log in via the Identity Provider your used to create the account. From there, you will be able to activate the SAML integration once you have completed all the remaining steps documented below.</p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#configure-the-aws-iam-identity-application-continued","title":"Configure the AWS IAM Identity application (Continued)","text":"<p>Go back to the AWS console. In the \"Application metadata\" section, click on the \"If you don't have a metadata file, you can manually type your metadata values.\" link.</p> <p></p> <p>Copy/paste the values for \"Single Sign-On URL\" and \"Entity ID (audience)\" from Spacelift to \"Application ACS URL\" and \"Application SAML audience\", respectively.</p> <p></p> <p></p> <p>Finally, click on the \"Save changes\" button.</p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#set-the-attribute-mappings","title":"Set the attribute mappings","text":"<p>Go to the \"Attribute mappings\" tab, set the values as described below and click on the \"Save changes\" button.</p> User attribute in the application Maps to this string value or user attribute in IAM Identity Center Format Subject ${user:subject} persistent FirstName ${user:givenName} basic LastName ${user:familyName} basic Teams ${user:groups} basic <p>Warning</p> <p>Please note that while available, <code>${user:groups}</code> is not officially supported by AWS IAM Identity Center and it will return the group ID (GUID) and not the group name. There is currently no way to get the group name. </p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#assign-users-and-groups-to-the-application","title":"Assign users and groups to the application","text":"<p>Make sure to assign users and/or groups to the SAML application in the \"Assigned users\" tab.</p> <p></p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#activate-the-spacelift-saml-integration","title":"Activate the Spacelift SAML integration","text":"<p>Back to Spacelift for the final step. You can finally click on the \"Save\" button on the SAML integration page.</p> <p>The page will reload and the AWS login page will be displayed. Use the credentials for a user that has access to the SAML application and you should be able to log into Spacelift.</p>"},{"location":"integrations/single-sign-on/backup-credentials.html","title":"Backup Credentials","text":""},{"location":"integrations/single-sign-on/backup-credentials.html#backup-credentials","title":"Backup Credentials","text":"<p>Spacelift is SSO-first. This means that you can only create a Spacelift account using one of the supported identity providers, and can then further configure custom SAML or OIDC integrations.</p> <p>However, usually you'd also like to set up a set of backup credentials that can be safely stored, in case you ever need to access Spacelift without going through your SSO provider. This might be useful in situations such as accidentally misconfiguring the SSO provider and locking oneself out, or the SSO provider having an outage.</p>"},{"location":"integrations/single-sign-on/backup-credentials.html#creating-backup-credentials","title":"Creating Backup Credentials","text":"<p>In order to create a set of backup credentials, just create an API Key with admin permissions, as described in Spacelift API Key &gt; Token. Then, securely store the API Key ID, as well as the first secret token in the downloaded secret file, the one right after <code>Please use the following API secret when communicating with Spacelift programmatically:</code>.</p>"},{"location":"integrations/single-sign-on/backup-credentials.html#logging-in-with-backup-credentials","title":"Logging in with Backup Credentials","text":"<p>In order to log in using an API Key open <code>https://&lt;your-account&gt;.app.spacelift.io/apikeytoken</code>. There, you'll be able to provide the previously stored API key ID and secret.</p> <p> </p> <p>After clicking EXCHANGE, you'll be logged into the Spacelift UI.</p>"},{"location":"integrations/single-sign-on/backup-credentials.html#additional-use-cases","title":"Additional Use-Cases","text":"<p>Since API Keys go through the login policy and you can specify the teams an API Key should be on, you can use the above functionality with a non-admin key to easily debug how other users see your account, for the purposes of i.e. debugging your login policy.</p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html","title":"GitLab OIDC Setup Guide","text":""},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#gitlab-oidc-setup-guide","title":"GitLab OIDC Setup Guide","text":"<p>If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with GitLab, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time!</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Spacelift account, with access to admin permissions</li> <li>GitLab account, with permission to create GitLab Applications</li> </ul> <p>Info</p> <p>Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.</p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#configure-account-settings","title":"Configure Account Settings","text":"<p>Open Organization settings for your Spacelift account. You can find this panel at the bottom left by clicking the arrow next to your name.</p> <p></p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#setup-oidc","title":"Setup OIDC","text":"<p>Select Single Sign-On under Authorization. Click Set up under the OIDC section.</p> <p>The drawer that opens contains the Authorized redirect URL, which you will need to copy for your login provider. The input fields will be filled later with information from your provider.</p> <p></p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#gitlab-create-gitlab-application","title":"GitLab: Create GitLab Application","text":"<p>Within your GitLab account, visit the Applications section of your account.</p> <p></p> <p>Create your GitLab Application as shown, the application's Name can be whatever you'd like. Spacelift sounds like a great name to use though.</p> <p>Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Redirect URI input as shown.</p> <p>Ensure that the openId, profile and email scopes are check'd.</p> <p>Click Save Application.</p> <p></p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#configure-oidc-settings","title":"Configure OIDC Settings","text":"<p>Now that we have the GitLab Application setup, we'll need to take the Application ID and Secret to configure the Spacelift OIDC Settings.</p> <p>Application ID = Spacelift's Client ID</p> <p>Secret = Spacelift's Secret</p> <p>In Spacelift, the Provider URL depends on where you are using GitLab, if you are using GitLab.com this value can be set as <code>https://gitlab.com</code></p> <p>Info</p> <p>When setting your Provider URL within Spacelift, do not include a trailing slash \"/\" at the end of your URL or you may receive an error.</p> <p></p> <p></p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#gitlab-oidc-setup-completed","title":"GitLab OIDC Setup Completed","text":"<p>That's it! Your OIDC integration with GitLab should now be fully configured.</p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html","title":"Microsoft Entra ID OIDC Setup Guide","text":""},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#microsoft-entra-id-oidc-setup-guide","title":"Microsoft Entra ID OIDC Setup Guide","text":"<p>This guide provides step-by-step instructions to set up Single Sign-On (SSO) with Microsoft Entra ID (formerly Azure Active Directory) for Spacelift. The process includes creating an app registration in Azure, configuring token claims, and finalizing the setup within Spacelift.</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account. These can be used in case of SSO misconfiguration or for other emergency procedures. You can find more details in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Spacelift account with admin permissions</li> <li>Azure account with permissions to create an \"App registration\" within Microsoft Entra ID</li> </ul>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#configure-account-settings","title":"Configure Account Settings","text":"<p>Open Organization settings for your Spacelift account. You can find this panel at the bottom left by clicking the arrow next to your name.</p> <p></p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#set-up-oidc","title":"Set Up OIDC","text":"<p>Select Single Sign-On under Authorization. Click Set up under the OIDC section.</p> <p>The drawer that opens contains the Authorized redirect URL, which you will need to copy for your login provider. The input fields will be filled later with information from your provider.</p> <p></p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#azure-navigate-to-microsoft-entra-id","title":"Azure: Navigate to Microsoft Entra ID","text":"<p>In Microsoft Entra ID, make sure you are in the directory where you want to set up the application. Select App registrations from the left-side panel, then click New registration.</p> <p></p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#azure-register-the-application","title":"Azure: Register the Application","text":"<p>The application name is for your reference only \u2014 pick something that fits your organization. For supported account types, choose either single-tenant or multi-tenant, depending on your organization's setup. Select \"Web\" as the type for the redirect URL, and paste the Authorized redirect URL you copied from Spacelift.</p> <p>Click Register.</p> <p></p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#azure-add-upn-claim","title":"Azure: Add UPN Claim","text":"<p>Under the newly created application, go to Token configuration and add the <code>upn</code> optional claim to the ID token. Click the Add button and make sure the Turn on the Microsoft Graph profile permission checkbox is selected in the popup.</p> <p> </p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#azure-add-groups-claim","title":"Azure: Add Groups Claim","text":"<p>This step is not strictly required, but you will likely want to send user group information to Spacelift. This allows you to assign permissions to user groups in Spacelift using the IdP Group Mapping feature. To enable this, add the <code>groups</code> optional claim to the ID token. Most likely, you will want to choose Security or Groups assigned to the application.</p> <p>Warning</p> <p>The number of groups in the ID token cannot exceed 200 (Azure limit related to HTTP header size). You may want to use Application Groups to avoid hitting the limit. That requires a paid Microsoft Entra ID plan and will be discussed later.</p> <p></p> <p>Info</p> <p>To use Spacelift's IdP Group Mapping feature, navigate to Spacelift &gt; Organization Settings &gt; IdP Group Mapping and select Map IdP group. Microsoft Entra ID sends Group IDs to Spacelift, instead of group names. When setting up your group mapping, ensure you map the Group IDs, not the group names. You can add a human-readable name in the description field for easier identification.</p> <p></p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#azure-configure-application-credentials","title":"Azure: Configure Application Credentials","text":"<p>Create a new Client secret and copy it into Spacelift's OIDC setup panel, within the Secret input.</p> <p>Warning</p> <p>You will need to generate a new client secret before it expires.</p> <p>Info</p> <p>Don't click Save in Spacelift just yet. We still need to get the Client ID and Provider URL for your application.</p> <p> </p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#azure-client-id-and-provider-url","title":"Azure: Client ID and Provider URL","text":"<p>To complete your configuration, you need two more pieces of information. You can find both in the application's Overview section.</p> <ul> <li>Copy the Application (client) ID into the Client ID field in Spacelift.</li> <li>Under Endpoints, copy the OpenID Connect metadata document URL into the Provider URL field.</li> </ul> <p></p> <p>Your Spacelift OIDC configuration should now look similar to the example below. Click Save to continue \u2014 this will redirect you to perform the first login. If the login succeeds, SSO will become active.</p> <p></p> <p>If you selected Application Groups for the <code>groups</code> claim, consider completing the next optional section. Otherwise, you are done.</p>"},{"location":"integrations/single-sign-on/microsoft-entra-id-oidc-setup-guide.html#azure-assign-application-groups-optional","title":"Azure: Assign Application Groups (optional)","text":"<p>If you are part of a large organization where users may belong to more than 200 groups, the group list will not fit into the ID token. Fortunately, Microsoft Entra ID provides an option to assign (\"bind\") groups to an application.</p> <p>If you previously configured the <code>groups</code> claim to include only assigned application groups, all that remains is to complete the group assignment. The ID token will then contain only the intersection of the user's groups and the application's assigned groups. This way, you have the option to send only relevant groups to Spacelift.</p> <p>Go to the Enterprise Applications service in the Azure portal and find your application. From there, complete the group assignment.</p> <p> </p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html","title":"Okta OIDC Setup Guide","text":""},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-oidc-setup-guide","title":"Okta OIDC Setup Guide","text":"<p>If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with Okta, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time!</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Spacelift account, with access to admin permissions</li> <li>Okta account, with permission to create Okta App Integrations</li> </ul> <p>Info</p> <p>Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.</p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#configure-account-settings","title":"Configure Account Settings","text":"<p>Open Organization settings for your Spacelift account. You can find this panel at the bottom left by clicking the arrow next to your name.</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#setup-oidc","title":"Setup OIDC","text":"<p>Select Single Sign-On under Authorization. Click Set up under the OIDC section.</p> <p>The drawer that opens contains the Authorized redirect URL, which you will need to copy for your login provider. The input fields will be filled later with information from your provider.</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-select-applications","title":"Okta: Select Applications","text":"<p>In a new browser tab, open your Okta account. Select the Applications link from the navigation.</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-create-app-integration","title":"Okta: Create App Integration","text":"<p>Click the \"Create App Integration\" button. For the sign in type, ensure you select OIDC - for the application type, select Web Application.</p> <p></p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-configure-app-integration","title":"Okta: Configure App Integration","text":"<p>Give your app integration a name - Spacelift sounds like a good one.</p> <p>Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Sign-in redirect URIs input as shown.</p> <p>As far as the assignments for this app integration, that's up to you at the end of the day. This determines what users from your Okta account will be able to access Spacelift. Click Save.</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-configure-group-claim","title":"Okta: Configure Group Claim","text":"<p>Click on the Sign On tab within your newly created Okta App Integration,</p> <p>You'll need to edit the groups claim type to return groups you consider useful in Spacelift Login Policies. For testing purposes, you could set it to Matches regex with .* for the regex value.</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#configure-oidc-settings","title":"Configure OIDC Settings","text":"<p>Switch back to the General tab. Now that we have the Okta App Integration setup, we'll need to take the Client ID, Client Secret, and Okta domain, to configure the Spacelift OIDC Settings.</p> <p>Info</p> <p>The Okta Domain will be set as the \"Provider URL\" in your Spacelift OIDC settings. Ensure that you prefix this URL with <code>https://</code>.</p> <p></p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-oidc-setup-completed","title":"Okta OIDC Setup Completed","text":"<p>That's it! Your OIDC integration with Okta should now be fully configured. Feel free to make any changes to your liking within your Okta App Integration configuration for the app that you just created.</p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html","title":"OneLogin OIDC Setup Guide","text":""},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-oidc-setup-guide","title":"OneLogin OIDC Setup Guide","text":"<p>If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with OneLogin, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time!</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Spacelift account, with access to admin permissions</li> <li>OneLogin account, with permission to create OneLogin Applications</li> </ul> <p>Info</p> <p>Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.</p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#configure-account-settings","title":"Configure Account Settings","text":"<p>Open Organization settings for your Spacelift account. You can find this panel at the bottom left by clicking the arrow next to your name.</p> <p></p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#setup-oidc","title":"Setup OIDC","text":"<p>Select Single Sign-On under Authorization. Click Set up under the OIDC section.</p> <p>The drawer that opens contains the Authorized redirect URL, which you will need to copy for your login provider. The input fields will be filled later with information from your provider.</p> <p></p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-select-applications","title":"OneLogin: Select Applications","text":"<p>In a new browser tab, open your OneLogin account and visit the Administration page. Select the Applications link from the navigation.</p> <p></p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-add-application","title":"OneLogin: Add Application","text":"<p>Click the Add App button.</p> <p></p> <p>Search for OpenId Connect and select the result as shown.</p> <p></p> <p>Give your new OneLogin App a name, Spacelift sounds like a good one.</p> <p>In regards to \"Visible in portal\" this is a OneLogin configuration decision that's up to you. In this example, we are enabled this value.</p> <p></p> <p>In the app navigation, navigate to the Configuration section. Input your Login URL.</p> <p>Next, paste the previously copied authorized redirect URL into the Redirect URI's input field. Once done, remember to click on the Save button.</p> <p></p> <p>In the app navigation, click on the SSO section. Now that we have the OneLogin App setup, we'll need to take the Client ID, Client Secret, and Issuer URL, to configure the Spacelift OIDC Settings</p> <p></p> <p></p> <p>Info</p> <p>Important: You'll need to ensure your OneLogin user has access to the OneLogin App you just created, or else you will receive an unauthorized error when clicking save.</p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-oidc-setup-completed","title":"OneLogin OIDC Setup Completed","text":"<p>That's it! OIDC integration with OneLogin should now be fully configured. Feel free to make any changes to your liking within your OneLogin App configuration.</p> <p>You'll of course need to configure any users/groups within your OneLogin account to have access to this newly created app.</p>"},{"location":"integrations/source-control.html","title":"Source Control","text":""},{"location":"integrations/source-control.html#source-control","title":"Source Control","text":"<p>For everything but raw Git, integrate your source code with Spacelift on the Source code tab.</p> <p></p> <ol> <li>Click Set up integration.</li> <li>Select your VCS from the dropdown.</li> <li>Follow the wizard to configure the integration.</li> </ol> <p>Your source code can be stored on any of the supported version control systems (VCS):</p> <ul> <li>GitHub</li> <li>GitLab</li> <li>Bitbucket<ul> <li>Cloud</li> <li>Datacenter/Server</li> </ul> </li> <li>Azure DevOps</li> <li>Raw Git</li> </ul>"},{"location":"integrations/source-control/azure-devops.html","title":"Azure DevOps","text":""},{"location":"integrations/source-control/azure-devops.html#azure-devops","title":"Azure DevOps","text":"<p>Spacelift supports Azure DevOps as the code source for your stacks and modules.</p> <p>You can set up multiple Space-level and one default Azure DevOps integration per account.</p>"},{"location":"integrations/source-control/azure-devops.html#create-the-azure-devops-integration","title":"Create the Azure DevOps integration","text":""},{"location":"integrations/source-control/azure-devops.html#initial-setup","title":"Initial setup","text":"<ol> <li>On the Integrations screen, click View on the Azure DevOps card, then Set up Azure DevOps.     </li> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol>"},{"location":"integrations/source-control/azure-devops.html#find-your-organization-url","title":"Find your organization URL","text":"<p>You will need your Azure DevOps organization URL, which usually follows this format: <code>https://dev.azure.com/{my-organization-name}</code>.</p> <p>Tip</p> <p>Depending on when your Azure DevOps organization was created, it may use a different format, for example: <code>https://{my-organization-name}.visualstudio.com</code>.</p> <p></p> <ol> <li>Navigate to your main organization page in Azure DevOps.</li> <li>Copy the Azure DevOps organization URL.</li> </ol>"},{"location":"integrations/source-control/azure-devops.html#create-a-personal-access-token","title":"Create a personal access token","text":"<p>You need to create a personal access token in Azure DevOps to create the integration with Spacelift.</p> <ol> <li> <p>Navigate to User settings &gt; Personal access tokens in the top-right section of the Azure DevOps page.</p> <p></p> </li> <li> <p>Click New Token.</p> </li> <li>Fill in the details to create a new personal access token:     <ol> <li>Name: Enter a descriptive name for the token.</li> <li>Organization: Select the organization to connect to Spacelift.</li> <li>Expiration: Select an expiration date for the token.</li> <li>Scopes: Select Custom defined, then check Read &amp; write in the Code section.</li> </ol> </li> <li>Click Create.</li> <li>Copy the token details to finish the integration in Spacelift.     </li> </ol>"},{"location":"integrations/source-control/azure-devops.html#copy-details-into-spacelift","title":"Copy details into Spacelift","text":"<p>Now that your personal access token has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>Organization URL: Paste your Azure DevOps organization URL.</li> <li>User facing host URL: Enter the URL that will be shown to the user and displayed in the Spacelift UI. This will be the same as the API host URL unless you are using VCS Agents, in which case it should be <code>private://&lt;vcs-agent-pool-name&gt;/&lt;azure-organization-name&gt;</code>.</li> <li>Personal access token: Paste the personal access token that Spacelift will use to access your Azure DevOps organization.</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Use Git checkout: Toggle that defines if integration should use git checkout to download source code, otherwise source code will be downloaded as archive through API. This is required for sparse checkout to work.</li> <li>Click Set up to save your integration details.</li> </ol>"},{"location":"integrations/source-control/azure-devops.html#set-up-webhooks","title":"Set up webhooks","text":"<p>For every Azure DevOps repository being used in Spacelift stacks or modules, you will need to set up a webhook to notify Spacelift about project changes.</p> <p>Note</p> <p>Default integrations are visible to all users of the account, but only root Space admins can see their details.</p> <p>Space-level integrations will be listed to users with read access to the integration Space. Integration details, however, contain sensitive information (such as the webhook secret) and are only visible to those with admin access.</p> <ol> <li>On the Integrations &gt; Azure DevOps page, click the three dots next to the integration name.</li> <li>Click See details to find the webhook endpoint and webhook secret.     </li> </ol>"},{"location":"integrations/source-control/azure-devops.html#configure-webhooks-in-azure-devops","title":"Configure webhooks in Azure DevOps","text":"<ol> <li>In Azure DevOps, select the project you are connecting to Spacelift.</li> <li>Navigate to Project settings &gt; Service hooks.</li> <li>Click Create subscription, then select Web Hooks and click Next.     </li> <li>On the Trigger page of the New Service Hooks Subscription window:<ol> <li>Trigger on this type of event: Select Code pushed, then click Next. </li> </ol> </li> <li>In the Settings section of the Action page:     <ol> <li>URL: Enter the webhook endpoint from Spacelift.</li> <li>Basic authentication username: Leave blank.</li> <li>Basic authentication password: Enter the webhook secret from Spacelift.</li> </ol> </li> <li>Click Finish.</li> <li>Repeat steps 3 through 6 for the following event triggers:<ul> <li>Pull request created.</li> <li>Pull request merge attempted.</li> <li>Pull request updated.</li> <li>Pull request commented on.</li> </ul> </li> </ol> <p>Once all hooks are created, you should see them on the Service Hooks page.</p> <p></p>"},{"location":"integrations/source-control/azure-devops.html#use-the-integration","title":"Use the integration","text":"<p>When creating a stack, you will now be able to choose the Azure DevOps provider and a repository inside of it:</p> <p></p>"},{"location":"integrations/source-control/azure-devops.html#using-spacelift-checks-to-protect-branches","title":"Using Spacelift checks to protect branches","text":"<p>Use commit statuses to protect your branches tracked by Spacelift stacks by ensuring that proposed runs succeed before merging their Pull Requests.</p>"},{"location":"integrations/source-control/azure-devops.html#aggregated-checks","title":"Aggregated checks","text":"<p>If you have multiple stacks tracking the same repository, you can enable the Aggregate VCS checks feature in the integration's settings. This will group all the checks from the same commit into a predefined set of checks, making it easier to see the overall status of the commit.</p> <p></p> <p>When the aggregated option is enabled, Spacelift will post the following checks:</p> <ul> <li>spacelift/tracked: Groups all checks from tracked runs</li> <li>spacelift/proposed: Groups all checks from proposed runs</li> <li>spacelift/modules: Groups all checks from module runs</li> </ul> <p>The summary will look like this:</p> <p></p>"},{"location":"integrations/source-control/azure-devops.html#delete-the-integration","title":"Delete the integration","text":"<p>If you no longer need the integration, delete it by clicking the 3 dots next to the integration name on the Integrations &gt; Azure DevOps page, and then clicking Delete. You need admin access to the integration Space to be able to delete it.</p> <p></p> <p>Warning</p> <p>You can delete integrations while stacks are still using them, which will have consequences.</p>"},{"location":"integrations/source-control/azure-devops.html#consequences","title":"Consequences","text":"<p>When a stack has a detached integration, it will no longer be able to receive webhooks from Azure DevOps and you won't be able to trigger runs manually either.</p> <p>To fix the issue, click the stack name on the Stacks tab, navigate to the Settings tab, and choose a new integration.</p> <p>Tip</p> <p>You can save a little time if you create the new integration with the exact same name as the old one. This way, the webhook URL will remain the same and you won't have to update it in Azure DevOps. You will still need to update the webhook secret though.</p>"},{"location":"integrations/source-control/bitbucket-cloud.html","title":"Bitbucket Cloud","text":""},{"location":"integrations/source-control/bitbucket-cloud.html#bitbucket-cloud","title":"Bitbucket Cloud","text":"<p>Spacelift supports Bitbucket Cloud as the code source for your stacks and modules.</p> <p>You can set up multiple Space-level and one default Bitbucket Cloud integration per account.</p>"},{"location":"integrations/source-control/bitbucket-cloud.html#create-the-bitbucket-cloud-integration","title":"Create the Bitbucket Cloud integration","text":""},{"location":"integrations/source-control/bitbucket-cloud.html#initial-setup","title":"Initial setup","text":"<ol> <li>On the Integrations screen, click View on the Bitbucket Cloud card, then Set up Bitbucket Cloud.     </li> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol> <p>Migration from App Passwords to API Tokens</p> <p>Previously, this integration used Bitbucket App Passwords for authentication. Atlassian has deprecated App Passwords and replaced them with API tokens.</p>"},{"location":"integrations/source-control/bitbucket-cloud.html#create-your-api-token","title":"Create your API token","text":"<p>You will need to create an API token for this integration on the Bitbucket Cloud site.</p> <ol> <li>Navigate to Atlassian account settings &gt; Security &gt; Create API token with scopes.</li> <li> <p>Fill in the details to create a new API token:</p> <ol> <li>Choose a name for your API token.</li> <li>Set an expiration date.</li> <li> <p>Select Bitbucket as the app.</p> <p></p> </li> <li> <p>Select read permissions for:</p> <ul> <li>Repository</li> <li>Pull requests</li> </ul> <p></p> </li> </ol> </li> <li> <p>Click Create.</p> </li> <li>Copy the API token details to finish the integration in Spacelift.</li> </ol>"},{"location":"integrations/source-control/bitbucket-cloud.html#copy-details-into-spacelift","title":"Copy details into Spacelift","text":"<p>Now that your API token has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>Email: Enter your Bitbucket Cloud email address.</li> <li>API token: Paste the API token that Spacelift will use to access your Bitbucket Cloud repository.</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Click Set up to save your integration details.     </li> </ol>"},{"location":"integrations/source-control/bitbucket-cloud.html#set-up-webhooks","title":"Set up webhooks","text":"<p>For every Bitbucket Cloud repository being used in Spacelift stacks or modules, you will need to set up a webhook to notify Spacelift about project changes.</p> <p>Note</p> <p>Default integrations are visible to all users of the account, but only root Space admins can see their details.</p> <p>Space-level integrations will be listed to users with read access to the integration Space. Integration details, however, contain sensitive information (such as the webhook secret) and are only visible to those with admin access.</p> <ol> <li>On the Integrations &gt; Bitbucket Cloud page, click the three dots next to the integration name.</li> <li>Click See details to find the webhook endpoint and webhook secret.     </li> </ol>"},{"location":"integrations/source-control/bitbucket-cloud.html#configure-webhooks-in-bitbucket-cloud","title":"Configure webhooks in Bitbucket Cloud","text":"<p>For each repository you want to use with Spacelift, you now need to add webhooks in Bitbucket Cloud.</p> <ol> <li>In Bitbucket Cloud, select the repository you are connecting to Spacelift.</li> <li>Navigate to Repository settings &gt; Webhooks.</li> <li>Click Add webhook.     </li> <li>Title: Enter a name for the webhook.</li> <li>URL: Paste the webhook endpoint from Spacelift.</li> <li>Secret: Paste the webhook secret from Spacelift.</li> <li>Status: Check Active.</li> <li>Triggers:<ol> <li>Under Repository, check Push.</li> <li>Under Pull Request, check:<ul> <li>Created</li> <li>Updated</li> <li>Approved</li> <li>Approval removed</li> <li>Merged</li> <li>Comment created</li> </ul> </li> </ol> </li> <li>Click Save.</li> </ol>"},{"location":"integrations/source-control/bitbucket-cloud.html#install-pull-request-commit-links-app","title":"Install Pull Request Commit Links app","text":"<p>Finally, you should install the Pull Request Commit Links app to be able to use this API. The app is installed automatically when you go to the commit's details and click Pull requests.</p> <p></p>"},{"location":"integrations/source-control/bitbucket-cloud.html#use-the-bitbucket-cloud-integration","title":"Use the Bitbucket Cloud integration","text":"<p>When creating a stack, you will now be able to choose the Bitbucket Cloud provider and a repository inside of it:</p> <p></p>"},{"location":"integrations/source-control/bitbucket-cloud.html#troubleshooting","title":"Troubleshooting","text":"<p>If you're receiving a 401 error, use this command to check the username and password:</p> <pre><code>curl -v -u your_username:some_app_password \"https://api.bitbucket.org/2.0/workspaces/workspace_id\"\n</code></pre> <p>And this to check if some repositories may not be showing up:</p> <pre><code>curl -s -u your_username:some_app_password \"https://api.bitbucket.org/2.0/repositories\" | jq\n</code></pre>"},{"location":"integrations/source-control/bitbucket-cloud.html#use-spacelift-checks-to-protect-branches","title":"Use Spacelift checks to protect branches","text":"<p>You can use commit statuses to protect your branches tracked by Spacelift stacks by ensuring that proposed runs succeed before merging their Pull Requests.</p>"},{"location":"integrations/source-control/bitbucket-cloud.html#aggregated-checks","title":"Aggregated checks","text":"<p>If you have multiple stacks tracking the same repository, you can enable the Aggregate VCS checks feature in the integration's settings. This will group all the checks from the same commit into a predefined set of checks, making it easier to see the overall status of the commit.</p> <p></p> <p>When the aggregated option is enabled, Spacelift will post the following checks:</p> <ul> <li>spacelift/tracked: Groups all checks from tracked runs</li> <li>spacelift/proposed: Groups all checks from proposed runs</li> <li>spacelift/modules: Groups all checks from module runs</li> </ul> <p>The summary will look like this:</p> <p></p>"},{"location":"integrations/source-control/bitbucket-cloud.html#delete-the-integration","title":"Delete the Integration","text":"<p>If you no longer need the integration, you can delete it by clicking the 3 dots next to the integration name on the Integrations &gt; Bitbucket Cloud page, and then clicking Delete. You need admin access to the integration Space to be able to delete it.</p> <p></p> <p>Warning</p> <p>You can delete source code integrations while stacks are still using them, which will have consequences.</p>"},{"location":"integrations/source-control/bitbucket-cloud.html#consequences","title":"Consequences","text":"<p>When a stack has a detached integration, it will no longer be able to receive webhooks from Bitbucket and you won't be able to trigger runs manually either.</p> <p>To fix the issue, click the stack name on the Stacks tab, navigate to the Settings tab, and choose a new integration.</p> <p>Tip</p> <p>You can save a little time if you create the new integration with the exact same name as the old one. This way, the webhook URL will remain the same and you won't have to update it in Bitbucket.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html","title":"Bitbucket Datacenter/Server","text":""},{"location":"integrations/source-control/bitbucket-datacenter-server.html#bitbucket-datacenterserver","title":"Bitbucket Datacenter/Server","text":"<p>Spacelift supports Bitbucket Data Center (on-premise) as the code source for your stacks and modules.</p> <p>You can set up multiple Space-level and one default Bitbucket Data Center integration per account.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#create-the-bitbucket-data-center-integration","title":"Create the Bitbucket Data Center integration","text":""},{"location":"integrations/source-control/bitbucket-datacenter-server.html#initial-setup","title":"Initial setup","text":"<ol> <li>On the Integrations screen, click View on the Bitbucket Data Center card, then Set up Bitbucket Data Center.     </li> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#create-an-access-token","title":"Create an access token","text":"<p>You will need to create an access token in Bitbucket to use with Spacelift. The token requires the following access:</p> <ul> <li>Read access to any projects Spacelift needs to be able to access.</li> <li>Read access to the repositories within those projects.</li> </ul> <p></p> <ol> <li>Navigate to Manage account &gt; Personal access tokens.</li> <li>Click Create.</li> <li>Name: Enter a descriptive name for the token.</li> <li>Permissions &gt; Projects: Select Read.</li> <li>Permissions &gt; Repositories: Select Read.</li> <li>Automated expiry: Select No.</li> <li>Click Create.     </li> <li>Copy the token details to finish the integration in Spacelift.</li> </ol>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#copy-details-into-spacelift","title":"Copy details into Spacelift","text":"<p>Now that your Bitbucket Data Center access token has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>API host URL: Enter the URL of your Bitbucket server. This will likely use a format like: <code>https://bitbucket.&lt;myorganization&gt;.com</code></li> <li>User facing host URL: Enter the URL that will be shown to the user and displayed in the Spacelift UI. This will be the same as the API host URL unless you are using VCS Agents, in which case it should be <code>private://&lt;vcs-agent-pool-name&gt;</code>.</li> <li>Username: Enter the username for the Bitbucket account where you created the access token.</li> <li>Access token: Enter the access token that Spacelift will use to access your Bitbucket.</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Use Git checkout: Toggle that defines if integration should use git checkout to download source code, otherwise source code will be downloaded as archive through API. This is required for sparse checkout to work.</li> <li>Click Set up to save your integration settings.</li> </ol> <p></p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#set-up-webhooks","title":"Set up webhooks","text":"<p>For every Bitbucket Data Center repository being used in Spacelift stacks or modules, you will need to set up a webhook to notify Spacelift about project changes.</p> <p>Note</p> <p>Default integrations are visible to all users of the account, but only root Space admins can see their details.</p> <p>Space-level integrations will be listed to users with read access to the integration Space. Integration details, however, contain sensitive information (such as the webhook secret) and are only visible to those with admin access.</p> <ol> <li>On the Integrations &gt; Bitbucket Data Center page, click the three dots next to the integration name.</li> <li>Click See details to find the webhook endpoint and webhook secret.     </li> </ol>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#configure-webhooks-in-bitbucket-data-center","title":"Configure webhooks in Bitbucket Data Center","text":"<p>For each repository you want to use with Spacelift, you now need to add webhooks in Bitbucket Data Center.</p> <ol> <li>In Bitbucket Data Center, select the repository you are connecting to Spacelift.</li> <li>Navigate to Repository settings &gt; Webhooks.</li> <li>Click Add webhook.     </li> <li>Title: Enter a name for the webhook.</li> <li>URL: Paste the webhook endpoint from Spacelift.</li> <li>Secret: Paste the webhook secret from Spacelift.</li> <li>Status: Check Active.</li> <li>Triggers:<ol> <li>Under Repository, check Push.</li> <li>Under Pull Request, check:<ul> <li>Opened</li> <li>Source branch updated</li> <li>Modified</li> <li>Approved</li> <li>Unapproved</li> <li>Merged</li> <li>Comment added</li> </ul> </li> </ol> </li> <li>Click Save.</li> </ol> <p>Warning</p> <p>Don't forget to enter a secret when configuring your webhook. Bitbucket will allow you to create your webhook with no secret specified, but any webhook requests to Spacelift will fail without one configured.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#install-pull-request-commit-links-app","title":"Install Pull Request Commit Links app","text":"<p>Finally, you should install the Pull Request Commit Links app to be able to use this API. The app is installed automatically when you go to the commit's details and click Pull requests.</p> <p></p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#use-the-integration","title":"Use the integration","text":"<p>When creating a stack, you will now be able to choose the Bitbucket Data Center provider and a repository inside of it:</p> <p></p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#using-spacelift-checks-to-protect-branches","title":"Using Spacelift checks to protect branches","text":"<p>You can use commit statuses to protect your branches tracked by Spacelift stacks by ensuring that proposed runs succeed before merging their Pull Requests.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#aggregated-checks","title":"Aggregated checks","text":"<p>If you have multiple stacks tracking the same repository, you can enable the Aggregate VCS checks feature in the integration's settings. This will group all the checks from the same commit into a predefined set of checks, making it easier to see the overall status of the commit.</p> <p></p> <p>When the aggregated option is enabled, Spacelift will post the following checks:</p> <ul> <li>spacelift/tracked: Groups all checks from tracked runs</li> <li>spacelift/proposed: Groups all checks from proposed runs</li> <li>spacelift/modules: Groups all checks from module runs</li> </ul> <p>The summary should look like this:</p> <p></p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#delete-the-integration","title":"Delete the integration","text":"<p>If you no longer need the integration, you can delete it by clicking the 3 dots next to the integration name on the Integrations &gt; Bitbucket Data Center page, and then clicking Delete. You need admin access to the integration Space to be able to delete it.</p> <p></p> <p>Warning</p> <p>You can delete integrations while stacks are still using them, which will have consequences.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#consequences","title":"Consequences","text":"<p>When a stack has a detached integration, it will no longer be able to receive webhooks from Bitbucket Data Center and you won't be able to trigger runs manually either.</p> <p></p> <p>To fix the issue, click the stack name on the Stacks tab, navigate to the Settings tab, and choose a new integration.</p> <p>Tip</p> <p>You can save a little time if you create the new integration with the exact same name as the old one. This way, the webhook URL will remain the same and you won't have to update it in Bitbucket Data Center. You will still need to update the webhook secret though.</p>"},{"location":"integrations/source-control/github.html","title":"GitHub","text":""},{"location":"integrations/source-control/github.html#github","title":"GitHub","text":"<p>Spacelift is deeply integrated with GitHub, providing organizations a simple way to manage IaC versioned in GitHub.</p> <p>You can set up multiple Space-level and one default GitHub integration per account.</p> <p>Using multiple GitHub accounts</p> <p>If you want to use multiple GitHub accounts or organizations, or connect Spacelift to your GitHub Enterprise instance, you will need to set up a custom GitHub integration via a GitHub App.</p>"},{"location":"integrations/source-control/github.html#create-and-link-a-custom-application","title":"Create and link a custom application","text":"<p>You will need to create a GitHub application to link it to Spacelift.</p>"},{"location":"integrations/source-control/github.html#create-the-github-application","title":"Create the GitHub application","text":"<ol> <li>On the Integrations screen, click View on the GitHub card, then Set up GitHub.</li> <li>Click Set up via wizard (recommended) or Set up manually.</li> </ol> <p>Warning</p> <p>Manual application setup is more prone to errors and should only be used if other methods will not work.</p>"},{"location":"integrations/source-control/github.html#set-up-via-wizard","title":"Set up via wizard","text":"<ol> <li>Select whether you're integrating with GitHub.com or a self-hosted installation, then click Continue.</li> <li>Select whether the GitHub integration should be owned by a personal or organization account, then click Continue.</li> <li>Click Continue to create the application on GitHub.com.<ol> <li>Enter a name for your integration. This can be changed later.</li> <li>Click Create GitHub app. You will be redirected back to Spacelift.</li> </ol> </li> <li>Fill in the additional information:     <ol> <li>Integration name: Must be unique, and cannot be changed after app creation because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> <li>VCS checks: Individual checks (one per stack) or aggregated checks (summarized checks across all affected stacks).</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Use Git checkout: Toggle that defines if integration should use git checkout to download source code, otherwise source code will be downloaded as archive through API. This is required for sparse checkout to work.</li> </ol> </li> <li>Click Set up. Once the application is created, you will automatically be redirected to install it in GitHub.</li> </ol>"},{"location":"integrations/source-control/github.html#set-up-manually","title":"Set up manually","text":"<p>After selecting the option to enter your details manually, you should see the following form:</p> <p></p> <ol> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol> <p>Once the integration name and the type are chosen, a webhook endpoint and a webhook secret will be generated for the GitHub app in the middle of the form.</p>"},{"location":"integrations/source-control/github.html#create-app-in-github","title":"Create app in GitHub","text":""},{"location":"integrations/source-control/github.html#initial-setup","title":"Initial setup","text":"<ol> <li>Open GitHub, navigate to the GitHub Apps page in the Developer Settings for your account/organization, and click New GitHub App.</li> <li>You can either create the App in an individual user account or within an organization account:    </li> <li>Give your app a name and homepage URL (these are only used for informational purposes within GitHub):    </li> <li>Paste your Webhook URL and secret from Spacelift:    </li> <li> <p>Set the following Repository permissions:</p> Permission Access Checks Read &amp; write Commit statuses Read &amp; write Contents Read-only Deployments Read &amp; write Metadata Read-only Pull requests Read &amp; write Webhooks Read &amp; write </li> <li> <p>Set the following Organization permissions:</p> <ul> <li>Check run</li> <li>Issue comment</li> <li>Organization</li> <li>Pull request</li> <li>Pull request review</li> <li>Push</li> <li>Repository</li> <li>Choose whether you want to allow the App to be installed on any account or only the current account, then click Create GitHub App: </li> </ul> </li> </ol>"},{"location":"integrations/source-control/github.html#generate-key","title":"Generate key","text":"<ol> <li>Copy the App ID in the About section:     </li> <li>Scroll down to the Private keys section of the page and click Generate a private key:          This will download the private key file for your GitHub app named <code>&lt;app-name&gt;.&lt;date&gt;.private-key.pem</code> (for example: <code>spacelift.2025-05-11.private-key.pem</code>).</li> </ol>"},{"location":"integrations/source-control/github.html#copy-api-details-into-spacelift","title":"Copy API details into Spacelift","text":"<p>Now that your GitHub App has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>API host URL: Enter the URL to your GitHub server, which should be https://api.github.com.</li> <li>User facing host URL: Enter the URL that will be shown to the user and displayed in the Spacelift UI. This will be the same as the API host URL unless you are using VCS Agents, in which case it should be <code>private://&lt;vcs-agent-pool-name&gt;</code>.</li> <li>App ID: Enter the App ID you copied before generating the private key.</li> <li>Private key: Paste the contents of your private key file.     </li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Use Git checkout: Toggle that defines if integration should use git checkout to download source code, otherwise source code will be downloaded as archive through API. This is required for sparse checkout to work.</li> <li>Click Set up to save your integration settings.</li> </ol>"},{"location":"integrations/source-control/github.html#install-the-github-application","title":"Install the GitHub application","text":"<p>Once your GitHub app has been created and configured in Spacelift, you can install it on one or more accounts or organizations you have access to.</p> Via Spacelift UIVia GitHub UI <ol> <li> <p>On the Integrations &gt; GitHub page, click Install the app:</p> <p></p> </li> <li> <p>On GitHub, click Install.</p> </li> <li> <p>Choose whether you want to allow Spacelift access to all repositories or only specific ones in the account:</p> <p></p> </li> <li> <p>Click Install to link your GitHub account to Spacelift.</p> </li> </ol> <ol> <li>Find your Spacelift app on the GitHub Apps page in your account settings, and click Edit:     </li> <li>In the Install App section, click Install next to the account you want Spacelift to access:     </li> <li> <p>Choose whether you want to allow Spacelift access to all repositories or only specific ones in the account:</p> <p></p> </li> <li> <p>Click Install to link your GitHub account to Spacelift.</p> </li> </ol>"},{"location":"integrations/source-control/github.html#access-controls","title":"Access controls","text":""},{"location":"integrations/source-control/github.html#space-level-integrations","title":"Space-level integrations","text":"<p>You can use the Spaces to control what can access your integrations. For example, if you have a Space called <code>Rideshare</code>, you can create a GitHub integration in that Space, and that can only be attached to those stacks and modules that are in the same Space (or inherit permissions through a parent Space).</p>"},{"location":"integrations/source-control/github.html#integration-details-visibility","title":"Integration details visibility","text":"<p>Only Space admins will be able to see the webhook URLs and secrets of Space-level integrations. Space readers will only be able to see the name, description, and labels of the integration.</p> <p>The details of default integrations are only visible to root Space admins.</p>"},{"location":"integrations/source-control/github.html#legacy-method","title":"Legacy method","text":"<p>You can also use GitHub's native teams. If you're using GitHub as your identity provider (which is the default), upon login, Spacelift uses the                   GitHub API to determine organization membership level and team membership within an organization and persists it in the session token which is valid for one hour. Based on that token, you can set up login policies to determine who can log in to your Spacelift account, and stack access policies that can grant an appropriate level of access to individual Stacks.</p> <p>Info</p> <p>The list of teams is empty for individual/private GitHub accounts.</p>"},{"location":"integrations/source-control/github.html#notifications","title":"Notifications","text":""},{"location":"integrations/source-control/github.html#commit-status-notifications","title":"Commit status notifications","text":"<p>Commit status notifications are triggered for proposed runs to provide feedback on the proposed changes to your stack. You can trigger a proposed run using a preview command (e.g. <code>terraform plan</code> for Terraform) with the source code of a short-lived feature branch with the state and config of the stack that's pointing to another, long-lived branch. Here's what commit status notifications looks like:</p> <p>1. When the run is in progress (initializing):</p> <p></p> <p>2. When it succeeds without changes:</p> <p></p> <p>3. When it succeeds with changes:</p> <p></p> <p>4. And when it fails:</p> <p></p> <p>In each case, clicking on the Details link will take you to the GitHub check view showing more details about the run:</p> <p></p> <p>The check view provides high-level information about the changes introduced by the push, including the list of changing resources and cost data if Infracost is set up.</p> <p>From this view you can also perform two types of Spacelift actions:</p> <ul> <li>Preview: Execute a proposed run against the tested commit.</li> <li>Deploy: Execute a tracked run against the tested commit.</li> </ul>"},{"location":"integrations/source-control/github.html#pr-pre-merge-deployments","title":"PR (Pre-merge) Deployments","text":"<p>The Deploy functionality has been introduced in response to customers used to the Atlantis approach, where the deployment happens from within a Pull Request itself rather than on merge, which we see as the default and most typical workflow.</p> <p>If you want to prevent users from deploying directly from GitHub, you can add a simple plan policy to that effect, based on the fact that the run trigger always indicates GitHub as the source (the exact format is <code>github/$username</code>).</p> <pre><code>package spacelift\n\ndeny[\"Do not deploy from GitHub\"] {\n  input.spacelift.run.type == \"TRACKED\"\n  startswith(input.spacelift.run.triggered_by, \"github/\")\n}\n</code></pre> <p>The effect is this:</p> <p></p>"},{"location":"integrations/source-control/github.html#using-spacelift-checks-to-protect-branches","title":"Using Spacelift checks to protect branches","text":"<p>You can use commit statuses to protect your branches tracked by Spacelift stacks by ensuring that proposed runs succeed before merging their Pull Requests:</p> <p></p> <p>This is an important part of our proposed workflow.</p>"},{"location":"integrations/source-control/github.html#aggregated-checks","title":"Aggregated checks","text":"<p>If you have multiple stacks tracking the same repository, you can enable the Aggregate VCS checks feature in the integration's settings.</p> <p>This will group all the checks from the same commit into a predefined set of checks, making it easier to see the overall status of the commit.</p> <p></p> <p>When the aggregated option is enabled, Spacelift will post the following checks:</p> <ul> <li>spacelift/tracked: Groups all checks from tracked runs.</li> <li>spacelift/proposed: Groups all checks from proposed runs.</li> <li>spacelift/modules: Groups all checks from module runs.</li> </ul> <p>Here's how the summary looks:</p> <p></p> <p>In each case, clicking on the Details link will take you to the GitHub check view showing more details about stacks or modules included in the aggregated check:</p> <p></p>"},{"location":"integrations/source-control/github.html#deployment-status-notifications","title":"Deployment status notifications","text":"<p>Deployments and their associated statuses are created by tracked runs to indicate that changes are being made to the Terraform state. A GitHub deployment is created and marked as Pending when the planning phase detects changes and a tracked run either transitions to the Unconfirmed state or automatically starts applying the diff:</p> <p></p> <p>If the user does not like the proposed changes and discards the tracked run during the manual review, its associated GitHub deployment is immediately marked as a Failure. The same thing happens when the user confirms the tracked run but the Applying phase fails:</p> <p></p> <p>If the Applying phase succeeds, the deployment is marked as Active:</p> <p></p> <p>Your repository's Environments section displays the entire deployment history broken down by stack:</p> <p></p> <p>That's what it looks like for our test repo, with a single stack pointing at it:</p> <p></p> <p>GitHub deployment environment names are derived from their respective stack names. This can be customized by setting the <code>ghenv:</code> label on the stack. For example, if you have a stack named <code>Production</code> and you want to name the deployment environment <code>I love bacon</code>, you can set the <code>ghenv:I love bacon</code> label on the stack. You can also disable the creation of a GitHub deployments by setting the <code>ghenv:-</code> label on the stack.</p> <p>Info</p> <p>The Deployed links lead to their corresponding Spacelift tracked runs.</p>"},{"location":"integrations/source-control/github.html#pull-requests","title":"Pull Requests","text":"<p>In order to help you keep track of all the pending changes to your infrastructure, Spacelift also has a PRs tab that lists all the active Pull Requests against your tracked branch. Each of the entries shows the current status of the change as determined by Spacelift, and a link to the most recent Run responsible for determining that status:</p> <p></p> <p>Note that this view is read-only. You can't change a Pull Request through the Spacelift UI, but clicking on the PR name will take you to GitHub where you can make changes.</p> <p>Once a Pull Request is closed, either with or without merging, it disappears from this list.</p>"},{"location":"integrations/source-control/github.html#proposed-workflow","title":"Proposed workflow","text":"<p>This proposed workflow has been effective for us and many other DevOps professionals working with infrastructure-as-code. Its simplest version is based on a single stack tracking a long-lived branch like main, and short-lived feature branches temporarily captured in Pull Requests. A more sophisticated version can involve multiple stacks and a process like GitFlow.</p> <p>Tip</p> <p>These are simply suggestions and Spacelift will fit almost any Git workflow. Feel free to experiment and find what works best for you.</p>"},{"location":"integrations/source-control/github.html#single-stack-version","title":"Single stack version","text":"<p>You have a single stack named Infra tracking the default <code>master</code> branch in your repository, also called <code>infra</code>. If you want to introduce some changes, for example define an Amazon S3 bucket, we suggest this workflow:</p> <ol> <li>Open a short-lived feature branch.</li> <li>Make your changes on the branch.</li> <li>Open a Pull Request from that branch to <code>master</code>.</li> </ol> <p>At this point, a proposed run is triggered by the push notification, and the result of running <code>terraform plan</code> with the new code (but existing state and configuration) is reported to the Pull Request.</p> <ol> <li>Ensure that the Pull Request does not get merged to <code>master</code> without a successful run by requiring a successful status check from your stack.</li> <li>Decide whether you should require a manual review before merging the Pull Request on top of Spacelift's automated checks.</li> <li>If coworkers are also modifying the branch, require branches be up-to-date before merging.</li> </ol> <p>If the current feature branch is behind the PR target branch, it needs to be rebased, which triggers a fresh Spacelift run that will ultimately produce the newest and most relevant commit status.</p>"},{"location":"integrations/source-control/github.html#multi-stack-version","title":"Multi-stack version","text":"<p>A common setup involves two similar (or even identical) environments, for example, staging and production. One approach would be to have them in a single repository but different directories, setting the <code>project_root</code> runtime configuration accordingly. This approach means changing the staging directory often and using as much or little duplication as necessary to keep things moving. Many commits will be no-ops for the production stack. This is a very flexible approach, but it leaves Git history messy.</p> <p>If you prefer a cleaner Git history:</p> <ol> <li>Create two long-lived Git branches, each linked to a different stack: the default <code>staging</code> branch linked to the staging stack, and a <code>production</code> branch linked to the production stack.</li> <li>Develop and perfect the code on the <code>staging</code> branch.</li> <li>Open a Pull Request from the <code>staging</code> to <code>production</code> branch, incorporating all the changes.</li> </ol> <p>We've seen many teams use this workflow to implement GitFlow. This approach keeps the history of the <code>production</code> branch clean and allows plenty of experimentation in the <code>staging</code> branch.</p> <p>With this GitFlow-like setup, we propose protecting both <code>staging</code> and <code>production</code> branches in GitHub. To maximize flexibility, the <code>staging</code> branch may require a green commit status from its associated stack but not necessarily a manual review. In the meantime, the <code>production</code> branch should require both a manual approval and a green commit status from its associated stack.</p>"},{"location":"integrations/source-control/github.html#webhook-integrations","title":"Webhook integrations","text":"<p>We subscribe to many GitHub webhooks:</p> Webhook How Spacelift uses it Push events Any time we receive a repository code push notification, we match it against Spacelift repositories and, if necessary, create runs. We'll also stop proposed runs that have been superseded by a newer commit on their branch. App installation creation When the Spacelift GitHub app is installed on an account, we create a corresponding Spacelift account. Organization renamed If a GitHub organization name is changed, we change the name of the corresponding account in Spacelift. (Only applicable for accounts that were created using GitHub originally.) Pull request events Whenever a Pull Request is opened or reopened, we generate a record in our database to show it on the Stack's PRs page. When it's closed, we delete that record. When it's synchronized (eg. new push) or renamed, we update the record accordingly. This way, what you see in Spacelift should be consistent with what you see in GitHub. Pull request review events Whenever a review is added or dismissed from a Pull Request, we check whether a new run should be triggered based on any push policies attached to your stacks. This allows you to make decisions about whether or not to trigger runs based on the approval status of your Pull Request. Repository renamed If a GitHub repository is renamed, we update its name in all the stacks pointing to it. GitHub Action You can use the Setup Spacectl GitHub Action to install our spacectl CLI tool to easily interact with Spacelift. Git checkout support By default Spacelift uses the GitHub API to download a tarball containing the source code for your stack or module. We are introducing experimental support for downloading the code using a standard Git checkout. To enable this feature for your stacks/modules, either add a label called <code>feature:enable_git_checkout</code> to each stack or module that you want to use Git checkout on (which allows you to test without switching all stacks over), or contact our support team and ask us to enable the feature for all stacks/modules in your account."},{"location":"integrations/source-control/github.html#unlinking-github-and-spacelift","title":"Unlinking GitHub and Spacelift","text":"Uninstalling the Marketplace applicationUninstalling the custom application <p>To uninstall the Spacelift application you installed on the GitHub Marketplace:</p> <ol> <li>Go to your GitHub account settings and select Applications.     </li> <li>Click Configure for the spacelift.io application.     </li> <li>Click Uninstall.     </li> </ol> <ol> <li>Go to the Developer settings of your GitHub account.</li> <li>In the GitHub Apps section, click Edit for the Spacelift application.     </li> <li>On the page for the Spacelift application, go to the Advanced section and click Delete GitHub App. Confirm by typing the name of the application.     </li> <li>You can now remove the integration via Delete on the Integrations &gt; GitHub page in Spacelift:     </li> </ol> <p>Warning</p> <p>Please note that you can delete integrations while stacks are still using them. As a consequence, when a stack has a detached integration, it will no longer be able to receive webhooks from Github and you won't be able to trigger runs manually either.</p> <p>To fix it, you'll need to open the stack, go to the Settings tab and choose a new integration.</p>"},{"location":"integrations/source-control/gitlab.html","title":"GitLab","text":""},{"location":"integrations/source-control/gitlab.html#gitlab","title":"GitLab","text":"<p>Spacelift supports GitLab as the code source for your stacks and modules.</p> <p>You can set up multiple Space-level and one default GitLab integration per account.</p> <p>Using multiple GitLab accounts</p> <p>If you want to use multiple GitLab accounts, teams, or groups, or connect Spacelift to your GitLab Enterprise instance, you will need to set up separate GitLab integrations (with different access tokens) for each different team or group in GitLab.</p>"},{"location":"integrations/source-control/gitlab.html#create-the-gitlab-integration","title":"Create the GitLab integration","text":""},{"location":"integrations/source-control/gitlab.html#initial-setup","title":"Initial setup","text":"<ol> <li>On the Integrations screen, click View on the GitLab card, then Set up GitLab.     </li> <li>Integration name: Enter a name for your integration. It cannot be changed later because the Spacelift webhook endpoint is generated based on this name.</li> <li>Integration type: Default (all spaces) or Space-specific. Each Spacelift account can only support one default integration per VCS provider, which is available to all stacks and modules in the same Space as the integration.</li> </ol>"},{"location":"integrations/source-control/gitlab.html#create-an-access-token","title":"Create an access token","text":"<p>Assuming you don't already have an access token at the ready, navigate to your GitLab server (we'll just use <code>gitlab.com</code>) to create one from the Access Tokens section of your User Settings page:</p> <p></p> <ol> <li>Name: Enter a descriptive name for the token.</li> <li>Expires at: We recommend leaving this blank. If set and the token expires before being replaced, Spacelift won't be able to access your GitLab environment.</li> <li>Scopes: Check the <code>api</code> box. While Spacelift will only write commit statuses, merge request comments, and environment deployments, GitLab's permissions require us to take write access on everything.</li> <li>Create the token and copy its details to finish the integration in Spacelift.</li> </ol> <p>Required user access level</p> <p>When creating tokens bound to a GitLab user, the user is required to have \"Maintainer\" level access to any projects you require Spacelift to access.</p>"},{"location":"integrations/source-control/gitlab.html#copy-details-into-spacelift","title":"Copy details into Spacelift","text":"<p>Now that your GitLab access token has been created, return to the integration configuration screen in Spacelift.</p> <ol> <li>API host URL: Enter the URL of your GitLab server. For SaaS GitLab, this is <code>https://gitlab.com</code>.</li> <li>User facing host URL: Enter the URL that will be shown to the user and displayed in the Spacelift UI. This will be the same as the API host URL unless you are using VCS Agents, in which case it should be <code>private://&lt;vcs-agent-pool-name&gt;</code>.</li> <li>API token: Enter the access token that Spacelift will use to access your GitLab.</li> <li>Labels: Organize integrations by assigning labels to them.</li> <li>Description: A markdown-formatted free-form text field to describe the integration.</li> <li>Use Git checkout: Toggle that defines if integration should use git checkout to download source code, otherwise source code will be downloaded as archive through API. This is required for sparse checkout to work.</li> <li>Click Set up to save your integration settings.</li> </ol> <p>Warning</p> <p>Unlike GitHub credentials (which are organization-specific), the GitLab integration uses personal credentials, which makes it more fragile in situations where an individual leaves the organization and deletes the access token. This is a general concern across your environment, not one specific to Spacelift.</p> <p>We recommend you create \"virtual\" (machine) users in GitLab as a source of more stable credentials.</p>"},{"location":"integrations/source-control/gitlab.html#set-up-webhooks","title":"Set up webhooks","text":"<p>For every GitLab project being used in Spacelift stacks or modules, you will need to set up a webhook to notify Spacelift about the project changes.</p> <p>Note</p> <p>Default integrations are visible to all users of the account, but only root Space admins can see their details.</p> <p>Space-level integrations will be listed to users with read access to the integration Space. Integration details, however, contain sensitive information (such as the webhook secret) and are only visible to those with admin access.</p> <ol> <li>On the Integrations &gt; GitLab page, click the three dots next to the integration name.</li> <li>Click See details to find the webhook endpoint and webhook secret.     </li> <li>In GitLab, navigate to Settings &gt; Webhooks to create a new webhook.     <ol> <li>URL: Enter the webhook endpoint from Spacelift.</li> <li>Secret Token: Enter the webhook secret from Spacelift.</li> <li>Trigger: Check the Push events, Tag push events, and Merge request events boxes.</li> </ol> </li> <li>Complete the webhook setup in GitLab.</li> </ol> <p>Warning</p> <p>You only need to set up one hook for each repository used by Spacelift, regardless of how many stacks use it. Setting up multiple hooks for a single repo may lead to unintended behavior.</p> <p>You can also set up GitLab webhooks automatically using GitLab's Terraform provider.</p> <p>Regardless of whether you've created it manually or programmatically, once your project webhook is set up, your GitLab-powered stack or module is ready to use.</p>"},{"location":"integrations/source-control/gitlab.html#use-gitlab-with-stacks-and-modules","title":"Use GitLab with stacks and modules","text":"<p>If your Spacelift account is integrated with GitLab, the creation and editing forms for stacks and modules will display GitLab as a source code option:</p> <p></p> <p>The rest of the process is exactly the same as with creating a GitHub-backed stack or module, so we won't be going into further details.</p>"},{"location":"integrations/source-control/gitlab.html#namespaces","title":"Namespaces","text":"<p>When using the Terraform provider to provision Spacelift stacks for GitLab, you are required to specify a <code>namespace</code>.</p> <p>The <code>namespace</code> value should be set to the group that your project (repository) is within. For example, if you are simply referencing a project (repository) within your GitLab account that is not within any group, then the namespace value should be set to your GitLab account username.</p> <p>If your project lives within a group, then the namespace should be set to the project group's slug. For example, if you have <code>project-a</code> within <code>group-1</code> the namespace would be <code>group-1</code>. When using subgroups, you will also need to include these within your namespace references.</p> <p>GitLab provides a Namespaces API which you can use to find information about your project's namespace. Reference the <code>full_url</code> attribute value as this namespace for a given project.</p>"},{"location":"integrations/source-control/gitlab.html#spacelift-in-gitlab","title":"Spacelift in GitLab","text":"<p>Spacelift provides feedback to GitLab in a number of ways.</p>"},{"location":"integrations/source-control/gitlab.html#commits-and-merge-requests","title":"Commits and merge requests","text":"<p>When a webhook containing a push or tag event is received by Spacelift, it may trigger a test run. Test runs provide feedback though GitLab's pipeline functionality. When viewed from a merge request, the pipeline looks like this:</p> <p></p> <p>Click through to a pipeline's dedicated view to see all the Spacelift jobs executed as part of it:</p> <p></p> <p>As you can see, the test job passed and gave some brief information about the proposed change that, if applied, would add a single resource.</p> <p>Also, for every merge request affected by the commit there will be a comment showing the exact change:</p> <p></p>"},{"location":"integrations/source-control/gitlab.html#use-spacelift-checks-to-protect-branches","title":"Use Spacelift checks to protect branches","text":"<p>You can use commit statuses to protect your branches tracked by Spacelift stacks by ensuring that proposed runs succeed before merging their Merge Requests.</p>"},{"location":"integrations/source-control/gitlab.html#aggregated-checks","title":"Aggregated checks","text":"<p>If you have multiple stacks tracking the same repository, you can enable the Aggregate VCS checks feature in the integration's settings. This will group all the checks from the same commit into a predefined set of checks, making it easier to see the overall status of the commit.</p> <p></p> <p>When the aggregated option is enabled, Spacelift will post the following checks:</p> <ul> <li>spacelift/tracked: Groups all checks from tracked runs</li> <li>spacelift/proposed: Groups all checks from proposed runs</li> <li>spacelift/modules: Groups all checks from module runs</li> </ul> <p>The summary will look like this:</p> <p></p>"},{"location":"integrations/source-control/gitlab.html#environments","title":"Environments","text":"<p>Each Spacelift stack creates an Environment in GitLab where we report the status of each tracked run.</p> <p>For example, this successful run is reflected in its respective GitLab environment:</p> <p> </p> <p>This functionality allows you to track Spacelift history directly from GitLab.</p>"},{"location":"integrations/source-control/gitlab.html#delete-the-github-integration","title":"Delete the GitHub integration","text":"<p>If you no longer need the integration, you can delete it by clicking the 3 dots next to the integration name on the Integrations &gt; GitLab page, and then clicking Delete. You need admin access to the integration Space to be able to delete it.</p> <p></p> <p>Warning</p> <p>You can delete integrations while stacks are still using them, which will have consequences.</p>"},{"location":"integrations/source-control/gitlab.html#consequences","title":"Consequences","text":"<p>When a stack has a detached integration, it will no longer be able to receive webhooks from GitLab and you won't be able to trigger runs manually either.</p> <p></p> <p>To fix the issue, click the stack name on the Stacks tab, navigate to the Settings tab, and choose a new integration.</p> <p>Tip</p> <p>You can save a little time if you create the new integration with the exact same name as the old one. This way, the webhook URL will remain the same and you won't have to update it in GitLab. You will still need to update the webhook secret though.</p>"},{"location":"integrations/source-control/raw-git.html","title":"Raw Git","text":""},{"location":"integrations/source-control/raw-git.html#raw-git","title":"Raw Git","text":"<p>Spacelift supports a range of version control systems that all require setup. With the raw Git integration, you can create stacks straight from a publicly accessible Git repository, including GitHub gists, without additional setup.</p> <p>Info</p> <p>Using Raw Git with Spacelift gives you a one-way connection to the repository. Pushes, PRs, etc. are not supported.</p> <p>This integration is best for consuming public content.</p>"},{"location":"integrations/source-control/raw-git.html#use-raw-git-with-stacks-and-modules","title":"Use Raw Git with stacks and modules","text":"<p>To get started, click Create stack on the Stacks page or Create module on the Terraform registry page. When creating a module instead of a stack, you will connect to source code first, then enter module details.</p> <p></p>"},{"location":"integrations/source-control/raw-git.html#stack-or-module-details","title":"Stack or module details","text":"<p>Fill in required stack details or module details.</p> <p></p> <ol> <li>Name: Enter a unique, descriptive name for your stack or module.</li> <li>Space: Select the space to create the stack or module in.</li> <li>Labels (optional): Add labels to help sort and filter your stacks or modules.</li> <li>Description (optional): Enter a (markdown-supported) description of the stack or module and the resources it manages.</li> <li>Click Continue.</li> </ol>"},{"location":"integrations/source-control/raw-git.html#connect-to-source-code","title":"Connect to source code","text":"<ol> <li>Integration: Click the Raw Git integration.</li> <li>URL: Enter the repository URL you would like to use.</li> <li>Repository: Ensure the name of the repository is correct when autofilled from the URL.</li> <li>Branch: Select the branch of the repository to manage with this stack.</li> <li>Project root (optional): If the entrypoint of the stack is different than the root of the repo, enter its path here.</li> <li>Project globs (optional): Enter additional files and directories that should be managed by the stack.</li> <li>Use Git checkout: Toggle that defines if integration should use git checkout to download source code, otherwise source code will be downloaded as archive through API. This is required for sparse checkout to work.</li> <li>Click Continue.</li> </ol> <p>The rest of the process is exactly the same as creating a GitHub-backed stack or module.</p>"},{"location":"integrations/source-control/raw-git.html#use-github-gists","title":"Use GitHub gists","text":"<p>To create a stack backed by a GitHub gist, enter the gist's URL into the URL field.</p> <p></p>"},{"location":"integrations/source-control/raw-git.html#trigger-runs-on-updates","title":"Trigger runs on updates","text":"<p>This integration gives you a one-way connection to the repository. New runs aren't automatically triggered on updates to the repository. However, you can trigger runs manually.</p> <p></p> <ol> <li>Click Sync to check for updates.</li> <li>If there are any updates since the last sync or creation of the stack, the tracked commit will change.</li> <li>Click Trigger to invoke a run.</li> </ol>"},{"location":"legal/DPA.html","title":"Data Processing Agreement","text":""},{"location":"legal/DPA.html#data-processing-agreement","title":"Data Processing Agreement","text":"<p>Last updated: November 4, 2024</p> <p>This Data Processing Agreement (\u201cDPA\u201d) is incorporated by reference into Terms and Conditions, Master Services Agreement, or any other written agreement (the \u201cServices Agreement\u201d) between Customer and Spacelift, Inc. (\u201cSpacelift\u201d) for the purchase of services from Spacelift (as defined below) to reflect the parties\u2019 agreement concerning the Processing of Personal Data.</p> <p>This DPA does not apply if Customer and Spacelift have executed a separate Data Protection Addendum or Data Protection Agreement.</p>"},{"location":"legal/DPA.html#1-definitions","title":"1. DEFINITIONS","text":"<p>1.1. \u201cApplicable Data Protection Laws\u201d means all data protection and privacy laws applicable to the respective party in its role in the Processing of Personal Data under the Services Agreement, as amended, suspended or replaced from time to time, including but not limited to: (a) Regulation (EU) 2016/679 (the \u201cEU GDPR\u201d); (b) the EU GDPR as saved into UK law by virtue of Section 3 of the UK\u2019s European Union Act 2018 and the UK Data Protection Act 2018 (\u201cUK GDPR\u201d); (c) Swiss Federal Act on Data Protection of 19 June 1992 and its corresponding ordinances (\u201cFADP\u201d); (d) Canadian Personal Information Protection and Electronic Documents Act (\u201cPIPEDA\u201d) and (e) California Consumer Privacy Act, as amended by the California Privacy Rights Act (\u201cCCPA\u201d).</p> <p>1.2. \u201cAuthorized Person\u201d means any person who is required to access or otherwise Process Customer Personal Data on Spacelift\u2019s behalf to enable Spacelift to perform its obligations under the Services Agreement and this DPA, including but not limited to Spacelift\u2019s staff, officers, partners, and Subprocessors.</p> <p>1.3. \u201cCustomer\u201d means a) the party to the Services Agreement subscribing to Services provided by Spacelift and b) said party\u2019s affiliates. In respect of any obligation(s) which are required to be performed by Customer, Customer will ensure that Customer, or as applicable, its affiliates will perform such obligation(s).</p> <p>1.4. \u201dData Subject\u201d means the identified or identifiable natural person who is the subject of Customer Personal Data.</p> <p>1.5. \u201cPersonal Data\u201d means \u201cpersonal data\u201d, \u201cpersonal information\u201d, \u201cpersonally identifiable information\u201d or similar information defined in and/or governed by Applicable Data Protection Laws.</p> <p>1.6. \u201cPersonal Data Breach\u201d means any breach of security that leads to the accidental or unlawful destruction, loss, alteration, unauthorized disclosure of, or access to Personal Data transmitted, stored, or otherwise Processed by Spacelift and/or its Subprocessors in connection with the provision of the Services.</p> <p>1.7. \u201cProcessing\u201d means any operation or set of operations that is performed on Personal Data or sets of Personal Data, whether or not by automated means, such as collection, recording, organization, structuring, storage, adaptation, or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.</p> <p>1.8. \u201cServices\u201d means the services provided by Spacelift to Customer under the Services Agreement.</p> <p>1.9. \u201cServices Agreement\u201d means the agreement between Spacelift and Customer for the provision of the Services, consisting of Terms and Conditions, Master Services Agreement, or any other written agreement.</p> <p>1.10. \u201cSubprocessor\u201d means any authorized third party that Processes Personal Data to assist Spacelift in fulfilling its obligations under the Services Agreement and this DPA.</p> <p>1.11. \u201cTrust Center\u201d means Spacelift\u2019s website at trust.spacelift.io providing insight into Spacelift\u2019s information security posture, listing Subprocessors and Security Measures (as defined in Clause 4.2.);</p> <p>1.12. Other. Capitalized terms, or any other terms, used in this DPA that are not defined in this Section 1 (Definitions) will have the meaning given to them elsewhere in this DPA and/or the Services Agreement and/or in Applicable Data Protection Laws unless otherwise specified.</p>"},{"location":"legal/DPA.html#2-personal-data-processing","title":"2. PERSONAL DATA PROCESSING","text":"<p>2.1. Roles. The parties acknowledge and agree that with regard to the Processing of Personal Data, Customer acts as a Controller or Processor (as applicable) and Spacelift acts as a Processor or sub-processor (as applicable). Where Customer is itself a Processor of Personal Data, acting on behalf of a Controller, Customer will serve as the sole point of contact for Spacelift and Spacelift will not interact directly with (including to seek any authorizations directly from) any such Controller, other than through the regular provision of the Services to the extent required under the Services Agreement.</p> <p>2.2. Scope. The subject matter of Processing of Personal Data by Spacelift, the duration of the Processing, the nature and purpose of the Processing, the types of Personal Data, and categories of Data Subjects Processed under this DPA, are further specified in Annex 1 to this DPA - Subject Matter &amp; Details of Processing. Spacelift may make reasonable amendments to Annex 1 from time to time as Spacelift reasonably considers necessary to meet the requirements of Applicable Data Protection Laws.</p> <p>2.3. Instructions. Spacelift will Process Customer Personal Data solely under and following Customer\u2019s documented instructions and to provide the Services and as otherwise necessary to (a) perform its obligations or exercise its rights under the Services Agreement and (b) to perform its legal obligations and to establish, exercise or defend legal claims in respect of the Services Agreement (\u201cPermitted Purpose\u201d). For the Permitted Purpose, Customer\u2019s instructions include (i) instructions as set out in the Services Agreement and/or this DPA; and (ii) any additional reasonable instructions provided by Customer where such instructions are consistent with the terms of the Services Agreement and/or Applicable Data Protection Laws. Spacelift will promptly inform Customer if, in Spacelift\u2019s opinion, any instruction infringes Applicable Data Protection Laws.</p> <p>2.4. Compliance with Law. The parties will comply with their obligations under Applicable Data Protection Laws concerning the Processing of Customer Personal Data. Each party will promptly notify the other party if it is unable to comply with its obligations under Applicable Data Protection Laws and/or the terms of the Services Agreement (including this DPA) as they relate to or govern the Processing of Customer Personal Data for any reason. In the event of any such non-compliance, and without prejudice to any other right or remedy available to the other party under the Services Agreement, such notifying party will take all reasonable and appropriate steps to remediate any non-compliance.</p> <p>2.5. Cooperation and Assistance. Upon each party\u2019s request, the other party will provide the requesting party with reasonable cooperation and assistance needed to fulfill its obligations under Applicable Data Protection Laws.</p>"},{"location":"legal/DPA.html#3-subprocessors","title":"3. SUBPROCESSORS","text":"<p>3.1. Authorization. Customer specifically authorizes Spacelift to use its Subprocessors as listed in Spacelift\u2019s Trust Center, and generally authorizes Spacelift to engage any new Subprocessors to Process Customer Personal Data.</p> <p>3.2. Obligations. While using Subprocessors, Spacelift:</p> <ul> <li> <p>3.2.1. will enter into a written agreement with each Subprocessor, imposing data protection obligations substantially similar to those set out in this DPA; and</p> </li> <li> <p>3.2.2. remains liable for compliance with the obligations of this DPA and for any acts or omissions of the Subprocessor that cause Spacelift to breach any of its obligations under this DPA.</p> </li> </ul> <p>3.3. New Subprocessors. When any new Subprocessor is engaged, Spacelift will notify Customer of the engagement, which notice may be given via email and/or by updating the Subprocessor list available at Trust Center (Customer can subscribe to receive notifications about changes in the Trust Center). Spacelift will give such notice at least fifteen (15) days before the new Subprocessor Processes any Customer Personal Data, except that if Spacelift reasonably believes engaging a new Subprocessor on an expedited basis is necessary to protect the confidentiality, integrity or availability of Customer Personal Data or avoid material disruption to the Services, Spacelift will give such notice as soon as reasonably practicable.</p> <p>3.4. Objections. Customer may object to an engagement of a new Subprocessor by informing Spacelift in writing within ten (10) days of receipt of the aforementioned notice by Customer, provided such objection is in writing and based on reasonable grounds relating to data protection. Should Customer express in writing its objection to Spacelift's appointment of a new Subprocessor on valid data protection grounds, the parties will engage in a good-faith discussion to address and resolve these concerns. Customer acknowledges that certain Subprocessors are essential to providing the Services and that objecting to the use of a Subprocessor may prevent Spacelift from offering the Services to Customer. If the parties are unable to reach a mutually agreeable resolution within a reasonable period of time, which will not exceed thirty (30) days, Customer may discontinue the use of the affected Services by providing written notice to Spacelift and Spacelift will refund a prorated amount of any prepaid fees. Except for the prorated refund, such discontinuation will not relieve Customer of any fees owed to Spacelift under the Services Agreement.</p>"},{"location":"legal/DPA.html#4-security","title":"4. SECURITY","text":"<p>4.1. Personnel. Spacelift will take reasonable steps to ensure the reliability of any Authorized Persons who may have access to Customer Personal Data, ensuring in each case that access is strictly limited to those individuals who need to know and/or access the relevant Customer Personal Data, as necessary for Permitted Purpose. Spacelift will ensure that Authorized Persons are informed of the confidential nature of Customer Personal Data and that they receive appropriate training regarding their responsibilities. Spacelift will impose appropriate contractual obligations on Authorized Persons, including relevant obligations regarding confidentiality, data protection, and data security.</p> <p>4.2. Security Measures. Taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of Processing, as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, Spacelift will maintain and implement appropriate technical and organizational measures for protection of the security, confidentiality and integrity of Customer Personal Data, as presented in Spacelift\u2019s Trust Center  (the \u201cSecurity Measures\u201d). Customer acknowledges that the Security Measures may be updated from time to time to reflect process improvements or changing practices, but the modifications will not materially decrease Spacelift\u2019s obligations as compared to those reflected in such terms as of the Effective Date of the Services Agreement and will be proportionate to the identified risks.</p> <p>4.3. Customer Responsibility. Customer must thoroughly review the information provided by Spacelift regarding data security. It is Customer's responsibility to independently assess whether the Services comply with its requirements and legal obligations under Applicable Data Protection Laws. Customer acknowledges that, notwithstanding Spacelift's obligations outlined in this DPA, Customer is solely accountable for utilizing the Services. This includes (a) ensuring the Services are appropriately used to maintain a level of security appropriate to the risk associated with Customer Personal Data; (b) safeguarding the authentication credentials, systems, and devices used to access the Services; (c) securing Customer's systems and devices used in conjunction with the Services and (d) configuring, setting up, and operating the Services to align with Customer\u2019s security and operational needs.</p> <p>4.4. Personal Data Breach. Upon becoming aware of a confirmed Personal Data Breach, Spacelift will notify Customer without undue delay about details of such Personal Data Breach, unless prohibited by Applicable Data Protection Laws, provided that (i) in case of SaaS Services - Customer indicated Customer\u2019s contact data in Spacelift\u2019s SaaS solution under the following address: https://.app.spacelift.io/settings/security (being the domain name chosen by Customer to access Services) or (ii) in case of any other Services - Customer provided Spacelift with contact details regarding Personal Data Breaches. A delay in giving such notice requested by law enforcement and/or in light of Spacelift\u2019s legitimate needs to investigate or remediate the matter before providing notice will not constitute an undue delay. Such notices will describe, to the extent possible, details of the Personal Data Breach. Without prejudice to Spacelift\u2019s obligations under this Clause 4.4., Customer is solely responsible for complying with Personal Data Breach notification laws applicable to Customer and fulfilling any third party notification obligations related to any Personal Data Breaches. Spacelift\u2019s notification of or response to a Personal Data Breach under this Clause 4.4 will not be construed as an acknowledgement of any fault or liability with respect to the Personal Data Breach.</p>"},{"location":"legal/DPA.html#5-audit-rights","title":"5. AUDIT RIGHTS","text":"<p>The parties recognize that Customer must be able to evaluate Spacelift's adherence to its obligations under Applicable Data Protection Laws and this DPA, specifically given Spacelift is acting as a Processor or subprocessor. At Customer's request, Spacelift will present information concerning its compliance with the obligations outlined in this DPA to Customer and/or an independent third-party auditor appointed by Customer, including completion of audit questionnaires, provision of security policies and summaries of assessments of compliance with any industry standards (such as SOC II report), and /or penetration testing. Spacelift assures Customer that (a) any information provided in response to such requests is accurate to the best of Spacelift's knowledge and (b) the individual supplying this information is authorized to do so and possesses knowledge about Spacelift's information Security Measures.</p>"},{"location":"legal/DPA.html#6-data-subject-rights","title":"6. DATA SUBJECT RIGHTS","text":"<p>Upon Customer\u2019s request, Spacelift will provide Customer with such assistance as it may reasonably require to comply with its obligations under Applicable Data Protection Laws to respond to requests from Data Subjects to exercise their rights under Applicable Data Protection Laws in cases where Customer cannot reasonably fulfill such requests independently. If Spacelift receives a request from a Data Subject in relation to their Personal Data, Spacelift will advise the Data Subject to submit their request to Customer, and Customer will be responsible for handling any such request.</p>"},{"location":"legal/DPA.html#7-data-privacy-impact-assessment","title":"7. DATA PRIVACY IMPACT ASSESSMENT","text":"<p>Upon Customer\u2019s request, Spacelift will provide Customer with reasonable cooperation needed to fulfill Customer\u2019s obligation under the Applicable Data Protection Laws to carry out a data protection impact assessment or handle prior consultation with the applicable data protection authority related to Customer\u2019s use of the Services, to the extent Customer does not otherwise have access to the relevant information, and to the extent such information is available to Spacelift.</p>"},{"location":"legal/DPA.html#8-deletion-of-data","title":"8. DELETION OF DATA","text":"<p>Spacelift will delete all copies of Customer Personal Data in its possession or control upon the termination or expiry of the Services Agreement, according to its data retention scheme. Notwithstanding the foregoing, Customer acknowledges that Spacelift may retain Customer Personal Data if required by Applicable Data Protection Laws, and such data will remain subject to the requirements of this DPA.</p>"},{"location":"legal/DPA.html#9-liability","title":"9. LIABILITY.","text":"<p>Unless specifically agreed otherwise in the Services Agreement, each party\u2019s liability arising out of or related to this DPA and its Annexes, whether in contract, tort or under any other theory of liability, is subject to any \u201cLimitation of Liability\u201d section of the Services Agreement, and any reference in such section to the liability of a party means the aggregate liability of that party under the Services Agreement and the DPA together, subject to any exclusions in accordance with Applicable Data Protection Laws and provisions of the Services Agreement.</p>"},{"location":"legal/DPA.html#10-international-provisions","title":"10. INTERNATIONAL PROVISIONS","text":"<p>10.1. Processing Activities. Customer acknowledges that Spacelift Processes Customer Personal Data primarily in Europe and the United States. Customer authorizes Spacelift and its Sub-processors to make international data transfers of Customer Personal Data in accordance with this DPA so long as Applicable Privacy Laws for such transfers are respected.</p> <p>10.2. Jurisdiction-Specific Annexes. To the extent that Spacelift Processes Customer Personal Data originating from and protected by Applicable Data Protection Laws in one of the jurisdictions listed in Annex 2 (Jurisdiction Specific Terms), then the terms specified therein with respect to the applicable jurisdiction(s) will apply in addition to the terms of this DPA. In the event of a conflict between the Services Agreement or this DPA and an Annex, the Annex applicable to Customer Personal Data from the relevant jurisdiction will control with respect to Customer Personal Data from that relevant jurisdiction, and solely with regard to the portion of the provision in conflict.</p>"},{"location":"legal/DPA.html#11-miscellaneous","title":"11. MISCELLANEOUS","text":"<p>11.1. Services Agreement. This DPA forms part of the Services Agreement and except as expressly set forth in this DPA, the Services Agreement remains unchanged and in full force and effect. If there is any conflict between this DPA and the Services Agreement, this DPA will govern.</p> <p>11.2. Applicable Data Protection Laws Changes. In the event of changes to Applicable Data Protection Laws, including, but not limited to, the amendment, revision or introduction of new laws, regulations, or other legally binding requirements to which either party is subject, the parties agree to revisit the terms of this DPA, and negotiate any appropriate or necessary updates in good faith, including the addition, amendment, or replacement of any Annexes.</p> <p>11.3. Termination. This DPA will automatically terminate upon expiration or termination of the Services Agreement.  However, for the avoidance of doubt, the provisions of the DPA will in all cases continue to apply for as long as the Spacelift Processes Customer Personal Data on behalf of Customer.</p> <p>11.4. Governing Law and Jurisdiction. Except for the provisions of the Standard Contractual Clauses included in the Annex 2 - Jurisdiction Specific Terms, if applicable:</p> <ul> <li> <p>11.4.1. the parties to this DPA hereby agree to abide by the jurisdiction specified in the Services Agreement for the resolution of any disputes or claims arising under this DPA. This includes disputes related to its existence, validity, termination, or the consequences of its nullity.</p> </li> <li> <p>11.4.2. the laws governing this DPA and all non-contractual or other obligations arising from or in connection with it are determined by the country or territory designated for this purpose in the Services Agreement.</p> </li> </ul> <p>11.5. Severability. If any provision of this DPA is deemed unlawful or unenforceable, such provision will be stricken from this DPA to the extent of such illegality or unenforceability, and the remainder will remain in full force and effect.</p> <p>11.6. Annexes. For the avoidance of doubt, each reference to the DPA means this DPA including its Annexes (including the Standard Contractual Clauses, if the Standard Contractual Clauses have been entered into in accordance with the Services Agreement or this DPA), consisting in:</p> <ul> <li> <p>11.6.1. Annex 1: SUBJECT MATTER AND DETAILS OF PROCESSING</p> </li> <li> <p>11.6.2. Annex 2: JURISDICTION SPECIFIC TERMS</p> </li> </ul>"},{"location":"legal/DPA.html#annex-1-subject-matter-and-details-of-processing","title":"ANNEX 1: SUBJECT MATTER AND DETAILS OF PROCESSING","text":""},{"location":"legal/DPA.html#1-list-of-parties","title":"1. LIST OF PARTIES","text":"Customer Spacelift Name: Customer as identified in the Services Agreement Spacelift, Inc. Address: As listed by Customer in the website purchase portal or as identified on the Services Agreement 541 Jefferson Ave. Suite 100, Redwood City CA 94063, USA Contact Person: As listed by Customer in the website purchase portal or as identified on the Services Agreement privacy@spacelift.io Role: Included in Clause 2.1 of the DPA Included in Clause 2.1 of the DPA Signatures: By entering into the Services Agreement, Data Exporter is deemed to have signed the DPA, including its Annexes By entering into the Services Agreement, Data Importer is deemed to have signed the DPA, including its Annexes"},{"location":"legal/DPA.html#2-description-of-processing-and-transfer-if-applicable","title":"2. DESCRIPTION OF PROCESSING AND TRANSFER, IF APPLICABLE","text":"Description Details Categories of data subjects whose personal data is processed / transferred: Users of the software provided by Spacelift, in particular staff including volunteers, agents, temporary and casual workers Categories of personal data processed / transferred: Name, logins, and e-mail addresses, data concerning Services usage Are sensitive data processed / transferred? No The frequency of processing / transfer: Continuous, as required for the provision of Services under the Services Agreement. Nature and purpose of processing / transfer: Spacelift will process Personal Data as necessary to provide the Services under the Services Agreement. The period for which the personal data will be retained: Specified in the Services Agreement (duration of the Services Agreement). For transfers to (sub-) processors - the subject matter, nature, and duration of the processing: Where Spacelift engages Subprocessors it will do so in compliance with the terms of the DPA. The subject matter, nature, and duration of the Processing activities carried out by the Subprocessor will not exceed the subject matter, nature and duration of the Processing activities as described in the DPA."},{"location":"legal/DPA.html#annex-2-jurisdiction-specific-terms","title":"ANNEX 2: JURISDICTION SPECIFIC TERMS","text":""},{"location":"legal/DPA.html#1-european-economic-area-eea-and-united-kingdom-uk","title":"1. EUROPEAN ECONOMIC AREA (EEA) AND UNITED KINGDOM (UK)","text":""},{"location":"legal/DPA.html#11-definitions","title":"1.1. Definitions.","text":"<p>1.1.1. The definition of \u201cApplicable Data Protection Laws\u201d includes the General Data Protection Regulation (EU 2016/679) (\u201cGPDR\u201d) and the EU GDPR as saved into UK law by virtue of Section 3 of the UK\u2019s European Union Act 2018 and the UK Data Protection Act 2018 (\u201cUK GDPR\u201d).</p> <ul> <li> <p>1.1.2. \"Restricted Transfer\" means (i) where the EU GDPR applies, a transfer of personal data from the European Economic Area to a country outside of the European Economic Area which is not subject to an adequacy determination by the European Commission; and (ii) where the UK GDPR applies, a transfer of personal data from the United Kingdom to any other country which is not based on adequacy regulations pursuant to Section 17A of the United Kingdom Data Protection Act 2018.</p> </li> <li> <p>1.1.3. \u201cStandard Contractual Clauses\u201d means (i) where the EU GDPR applies, the contractual clauses annexed to the European Commission's Implementing Decision 2021/914 of 4 June 2021 on standard contractual clauses for the transfer of personal data to third countries pursuant to Regulation (EU) 2016/679 of the European Parliament and of the Council (\"EU SCCs\"); and (ii) where the UK GDPR applies, the United Kingdom International Data Transfer Addendum to the European Commission\u2019s Standard Contractual Clauses for international data transfers version B1.0 issued by the UK Information Commissioner under Section 119A of the UK Data Protection Act of 2018 and entering into force on 21 March 2022, as updated, amended, or replaced from time to time (\"UK IDTA\").</p> </li> </ul> <p>1.2. SCCs. The parties agree that when the transfer of Personal Data from Customer to Spacelift is a Restricted Transfer, it will be subject to the appropriate Standard Contractual Clauses, being EU SCCs or UK IDTA, which are incorporated herein by reference.</p> <p>1.3. EU SCCs. In relation to Personal Data that is protected by the EU GDPR, the EU SCCs will apply as follows:</p> <ul> <li> <p>1.3.1. Module Two will apply to the extent that Customer is a controller of the Personal Data, and Module Three will apply to the extent that Customer is a processor of the Personal Data on behalf of a third-party controller;</p> </li> <li> <p>1.3.2. For both Modules Two and Three, Customer is the Data Exporter and Spacelift is the Data Importer.</p> </li> <li> <p>1.3.3. In Clause 7, the optional docking clause will apply;</p> </li> <li> <p>1.3.4. In Clause 9, Option 2 (General Authorization) will apply, and the period for prior notice of Sub-processor changes will be as set out in Clause 3.3. of this DPA;</p> </li> <li> <p>1.3.5. In Clause 11, the optional language will not apply;</p> </li> <li> <p>1.3.6. In Clause 17, Option 1 will apply, and the EU SCCs will be governed by Polish law;</p> </li> <li> <p>1.3.7. In Clause 18(b), disputes will be resolved before the courts of Poland;</p> </li> <li> <p>1.3.8. Annex I of the EU SCCs will be deemed complete with (as to Part A and Part B) information set out in Annex 1 to this DPA and (as to Part C) with the Polish supervisory authority;</p> </li> <li> <p>1.3.9. Annex II of the EU SCCs will be deemed completed with the information set out in the Trust Center;</p> </li> <li> <p>1.3.10. Annex III of the EU SCCs will be deemed completed with the information set out the Trust Center;</p> </li> </ul> <p>1.4. UK IDTA. In relation to Controller Personal Data that is protected by the UK GDPR, the UK IDTA will apply completed as follows:</p> <ul> <li> <p>1.4.1. The EU SCCs, completed as set out above in Clause 1.3 of this Annex will also apply to transfers of such Personal Data, subject to Sub-clause 1.4.2 below;</p> </li> <li> <p>1.4.2. Tables 1 to 3 of the UK Addendum will be deemed completed with relevant information from the EU SCCs, completed as set out above, and the options \"either party\" will be deemed checked in Table 4. The start date of the UK IDTA (as set out in Table 4) will be the Services Agreement Effective Date.</p> </li> <li> <p>1.4.3. The parties confirmed that they adopt the following wording of the Part II of the UK IDTA: Mandatory Clauses of the Approved Addendum, being the template Addendum B.1.0 issued by the ICO and laid before Parliament in accordance with section 119A of the Data Protection Act 2018 on 2 February 2022, as it is revised under Section 18 of those Mandatory Clauses.</p> </li> </ul> <p>1.5. GDPR Penalties. Notwithstanding anything to the contrary in this DPA or in the Services Agreement (including, without limitation, either party\u2019s indemnification obligations), neither party will be responsible for any GDPR or UK GDPR fines issued or levied against the other party by a regulatory authority or governmental body in connection with such other party\u2019s violation of the GDPR or UK GDPR.</p>"},{"location":"legal/DPA.html#2-switzerland","title":"2. SWITZERLAND","text":"<p>2.1. The definition of \u201cApplicable Data Protection Laws\u201d includes the Federal Act on Data Protection of 19 June 1992 (the \u201cFADP\u201d).</p> <p>2.2. With respect to Personal Data transferred from Switzerland for which Swiss law (and not the law in any European Economic Area jurisdiction) governs the international nature of the transfer, (i) references to the GDPR in Clause 4 of the EU SCCs are, to the extent legally required, amended to refer to the FADP or its successor instead, and the concept of supervisory authority will include the Swiss Federal Data Protection and Information Commissioner; and (ii) as so amended and updated by Clause 1.3 above, the EU SCCs are incorporated herein by reference and will apply, form a part of this DPA, and take precedence over the rest of this DPA to the extent of conflict.</p>"},{"location":"legal/DPA.html#3-california","title":"3. CALIFORNIA","text":"<p>3.1. The definition of \u201cApplicable Data Protection Laws\u201d includes the California Consumer Privacy Act, as amended by the California Privacy Rights Act (\u201cCCPA\u201d).</p> <p>3.2. The terms \u201cbusiness\u201d, \u201ccommercial purpose, \u201cservice provider\u201d, \u201csell\u201d and \u201cpersonal information\u201d have the meanings given in the CCPA.</p> <p>3.3. With respect to Customer Personal Data, Spacelift is a service provider under the CCPA.</p> <p>3.4. Spacelift will not (a) sell Customer Personal Data; (b) retain, use, or disclose any Customer Personal Data for any purpose other than for the specific purpose of providing the Services, including retaining, using, or disclosing Customer Personal Data for a commercial purpose other than providing the Services; or (c) retain, use or disclose Customer Personal Data outside of the direct business relationship between Spacelift and Customer.</p> <p>3.5. The parties acknowledge and agree that the Processing of Customer Personal Data authorized by Customer\u2019s instructions described in the DPA is integral to and encompassed by Spacelift\u2019s provision of the Services and the direct business relationship between the parties. 3.6. Notwithstanding anything in the Services Agreement or any written agreement entered in connection therewith, the parties acknowledge and agree that Spacelift\u2019s access to Customer Personal Data does not constitute part of the consideration exchanged by the parties in respect of the Services Agreement.</p> <p>3.7. Spacelift agrees that it will provide Customer with reasonable assistance and cooperate with Customer\u2019s obligations under CCPA to ensure that Spacelift is: (a) Processing Personal Data in a manner consistent with Spacelift\u2019s obligations and (b) stop and remediate any unauthorized use of Personal Data.</p>"},{"location":"legal/ai-terms.html","title":"Terms of use of AI Services","text":""},{"location":"legal/ai-terms.html#terms-of-use-of-ai-services","title":"Terms of use of AI Services","text":"<ol> <li> <p>What is SaturnHead Run Summarization (\"AI Feature\")?</p> <p>SaturnHead Run Summarization is employed to help users rapidly interpret log summaries and deliver technical solutions to effectively diagnose and fix failed runs.</p> <p>The SaturnHead Run Summarization feature is designed for core and power users of Spacelift who actively manage infrastructure runs through the Spacelift UI or APIs.</p> <p>The AI Feature is powered by AI models provided by Amazon Web Services, Inc. (\u201cAWS\u201d) and/or Google LLC (\u201cGoogle\u201d) (each, an \u201cAI Model Provider\u201d). Customer agrees to be bound by the Terms &amp; Conditions of the applicable AI Model Provider:</p> <ul> <li>For AWS Bedrock, see AWS Terms and Conditions</li> <li>For Google Gemini API, see Google Terms &amp; Conditions. Regarding the Google services, the terms applicable to Paid Services shall apply.</li> </ul> <p>SaturnHead Run Summarization is disabled in the Customer account by default, and the Customer admin, if authorized by the Customer, might enable the feature for use in the Customer organization by explicitly opting in and accepting these AI Terms.</p> <p>The Customer understands and acknowledges that the use of the AI Feature is not mandatory or integrated by default and Spacelift Services may be used without enabling the AI Feature.</p> </li> <li> <p>Acceptance of AI Terms</p> <p>These Terms of Use of SaturnHead Run Summarization (\"AI Terms\") supplement the applicable Master Services Agreement and/or general Terms and Conditions between the entity you represent, or, if you do not indicate an entity in connection with the Services, you individually (\"Customer\", \"you\" or \"your\") and Spacelift, Inc. (\"Agreement\"), and are incorporated by reference into the Agreement.</p> <p>In case of any conflict between the Agreement and these AI Terms, as it relates specifically to the AI Feature, these AI Terms shall govern.</p> <p>THESE AI TERMS, TOGETHER WITH THE AGREEMENT SET FORTH AND SETTLE THE TERMS ACCORDING TO WHICH THE CUSTOMER MAY USE THE AI FEATURE.</p> <p>By proceeding with enabling the use of the AI Feature for the Customer, you are indicating that you have read, understood, and accepted these AI Terms and agree to be bound by them. If the Customer does not agree with all the terms of these AI Terms, the Customer should not accept the Terms and use the AI Feature.</p> </li> <li> <p>Intellectual Property (IP) Ownership and Right to Use AI Feature</p> <p>Customer acknowledges that Spacelift and AI Model Provider retain ownership of all intellectual property rights related to the AI Feature, including but not limited to the underlying AI models, algorithms, and infrastructure. However, all Customer Data submitted by the Customer while using the AI Feature remains the property of the Customer.</p> <p>Spacelift hereby grants the Customer a limited, revocable, non-exclusive, non-transferable, and non-sublicensable right to use the AI Feature. The use of the AI Feature is subject to the Customer's compliance with any and all applicable laws, these AI Terms, and the terms of the Agreement.</p> </li> <li> <p>Acceptable Use Policy</p> <p>Without limiting the restrictions outlined in the Agreement, the Customer represents and warrants that neither they nor anyone acting on their behalf, including any authorized user, will, including but not limited to the following:</p> <ul> <li>Modify, distribute, prepare derivative works of, reverse engineer, reverse assemble, disassemble, decompile, or otherwise attempt to decipher any code in connection with AI Feature or any other aspect of Spacelift's technology, unless given written permission from Spacelift;</li> <li>Use the AI Feature for illegal or fraudulent purposes;</li> <li>Exploit or misuse personal or sensitive data in violation of privacy laws;</li> <li>Falsely represent identity or affiliation;</li> <li>Attempt to gain unauthorized access to systems or networks;</li> <li>Violate third party intellectual property rights;</li> <li>Generate excessive or repetitive requests that degrade service performance.</li> </ul> </li> <li> <p>Data Training</p> <p>Spacelift does not claim any ownership rights over Customer Data. We do not and will not permit third parties to use Customer Data to improve or train the AI models. We do not and will not use Customer Data to improve or train the AI models without your expressed permission. Your data will only be processed as necessary to provide the AI Feature, in accordance with our Privacy Policy and applicable laws.</p> </li> <li> <p>Accuracy</p> <p>Spacelift does not guarantee the quality or accuracy of the output generated by the AI Feature. Due to the nature of AI models and machine learning, which can be prone to errors and misinterpret prompts, generate inaccurate responses, or introduce bias, the use of the AI Feature may occasionally produce incorrect results or unintended actions. The Customer is fully responsible for the code execution, and Spacelift accepts no responsibility for any damages or issues resulting from your use of the AI Feature.</p> </li> <li> <p>Usage of AI Feature</p> <p>Access to the AI Feature is at the sole discretion of Spacelift and may be revoked at any time, including upon termination of the AI Feature. The Customer acknowledges that Spacelift is under no obligation to continue providing this feature and may choose to discontinue the AI Feature at any time. Spacelift reserves the right to modify or terminate the AI Feature and the AI Terms at any time, at its sole discretion, for any reason, with or without notice, and without liability to the Customer.</p> </li> <li> <p>Warranties and liabilities</p> <p>ANY DISCLAIMERS OF WARRANTIES AND/OR LIMITATIONS OF LIABILITY OUTLINED IN THE AGREEMENT SHALL ALSO APPLY TO THE USE OF THIS AI FEATURE.</p> </li> </ol>"},{"location":"legal/cookie-policy.html","title":"Cookie Policy","text":""},{"location":"legal/cookie-policy.html#cookie-policy","title":"Cookie Policy","text":"<p>Last updated: June 10, 2022</p> <p>This Cookies Policy explains what Cookies are and how We use them. You should read this policy so You can understand what type of cookies We use, the information We collect using Cookies, and how that information is used.</p> <p>Cookies do not typically contain any information that personally identifies a user, but personal information that we store about You may be linked to the information stored in and obtained from Cookies. For further information on how We use, store and keep your personal data secure, see our Privacy Policy.</p> <p>We do not store sensitive personal information, such as mailing addresses, account passwords, etc., in the Cookies We use.</p>"},{"location":"legal/cookie-policy.html#interpretation-and-definitions","title":"Interpretation and Definitions","text":""},{"location":"legal/cookie-policy.html#interpretation","title":"Interpretation","text":"<p>The words in which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in the plural.</p>"},{"location":"legal/cookie-policy.html#definitions","title":"Definitions","text":"<p>For the purposes of this Cookies Policy:</p> <ul> <li>Company (referred to as either \"the Company\", \"We\", \"Us\", or \"Our\" in this Cookies Policy) refers to SPACELIFT, INC., 541 Jefferson Ave. Suite 100, Redwood City, CA 94063.</li> <li>Cookies are small files that are placed on Your computer, mobile device, or any other device by a website, containing details of your browsing history on that website among its many uses.</li> <li>Website refers to Spacelift.io, accessible from https://spacelift.io</li> <li>You mean the individual accessing or using the Website, or a company, or any legal entity on behalf of which such individual is accessing or using the Website, as applicable.</li> </ul>"},{"location":"legal/cookie-policy.html#the-use-of-the-cookies","title":"The use of the Cookies","text":""},{"location":"legal/cookie-policy.html#type-of-cookies-we-use","title":"Type of Cookies We Use","text":"<p>Cookies can be \"Persistent\" or \"Session\" Cookies. Persistent Cookies remain on your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close your web browser.</p> <p>We use both session, and persistent Cookies for the purposes set out below:</p> <ul> <li> <p>Necessary / Essential Cookies</p> <p>Type: Session Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.</p> </li> <li> <p>Cookies Policy / Notice Acceptance Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies identify if users have accepted the use of cookies on the Website.</p> </li> <li> <p>Functionality Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.</p> </li> <li> <p>Tracking and Performance Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Third-Parties</p> <p>Purpose: These Cookies are used to track information about traffic to the Website and how users use the Website. The information gathered via these Cookies may directly or indirectly identify you as an individual visitor. This is because the information collected is typically linked to a pseudonymous identifier associated with the device you use to access the Website. We may also use these Cookies to test new advertisements, pages, features or new functionality of the Website to see how our users react to them.</p> </li> <li> <p>Targeting and Advertising Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Third-Parties</p> <p>Purpose: These Cookies track your browsing habits to enable Us to show advertising which is more likely to be of interest to You. These Cookies use information about your browsing history to group You with other users who have similar interests. Based on that information, and with Our permission, third party advertisers can place Cookies to enable them to show adverts that We think will be relevant to your interests while You are on third-party websites.</p> </li> <li> <p>Social Media Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Third-Parties</p> <p>Purpose: In addition to Our own Cookies, We may also use various third parties Cookies to report usage statistics of the Website, deliver advertisements on and through the Website, and so on. These Cookies may be used when You share information using a social media networking website such as Facebook, Instagram, Twitter, or Google+.</p> </li> </ul>"},{"location":"legal/cookie-policy.html#your-choices-regarding-cookies","title":"Your Choices Regarding Cookies","text":"<p>If You prefer to avoid using Cookies on the Website, first, You must disable the use of Cookies in your browser and then delete the Cookies saved in your browser associated with this website. You may use this option to prevent Cookies use at any time.</p> <p>If You do not accept Our Cookies, You may experience some inconvenience in your use of the Website, and some features may not function properly.</p> <p>If You'd like to delete Cookies or instruct your web browser to delete or refuse Cookies, please visit the help pages of your web browser.</p> <ul> <li>For the Chrome web browser, please visit this page from Google: https://support.google.com/accounts/answer/32050</li> <li>For the Internet Explorer web browser, please visit this page from Microsoft: http://support.microsoft.com/kb/278835</li> <li>For the Firefox web browser, please visit this page from Mozilla: https://support.mozilla.org/en-US/kb/delete-cookies-remove-info-websites-stored</li> <li>For the Safari web browser, please visit this page from Apple: https://support.apple.com/guide/safari/manage-cookies-and-website-data-sfri11471/mac</li> </ul> <p>For any other web browser, please visit your web browser's official web pages.</p>"},{"location":"legal/cookie-policy.html#more-information-about-cookies","title":"More Information about Cookies","text":"<p>You can learn more about cookies: Cookies: What Do They Do?.</p>"},{"location":"legal/cookie-policy.html#contact-us","title":"Contact Us","text":"<p>If you have any questions about this Cookies Policy, You can contact us by email at privacy@spacelift.io.</p>"},{"location":"legal/dpa.html","title":"Data Processing Agreement","text":""},{"location":"legal/dpa.html#data-processing-agreement","title":"Data Processing Agreement","text":"<p>Last updated: November 4, 2024</p> <p>This Data Processing Agreement (\u201cDPA\u201d) is incorporated by reference into Terms and Conditions, Master Services Agreement, or any other written agreement (the \u201cServices Agreement\u201d) between Customer and Spacelift, Inc. (\u201cSpacelift\u201d) for the purchase of services from Spacelift (as defined below) to reflect the parties\u2019 agreement concerning the Processing of Personal Data.</p> <p>This DPA does not apply if Customer and Spacelift have executed a separate Data Protection Addendum or Data Protection Agreement.</p>"},{"location":"legal/dpa.html#1-definitions","title":"1. DEFINITIONS","text":"<p>1.1. \u201cApplicable Data Protection Laws\u201d means all data protection and privacy laws applicable to the respective party in its role in the Processing of Personal Data under the Services Agreement, as amended, suspended or replaced from time to time, including but not limited to: (a) Regulation (EU) 2016/679 (the \u201cEU GDPR\u201d); (b) the EU GDPR as saved into UK law by virtue of Section 3 of the UK\u2019s European Union Act 2018 and the UK Data Protection Act 2018 (\u201cUK GDPR\u201d); (c) Swiss Federal Act on Data Protection of 19 June 1992 and its corresponding ordinances (\u201cFADP\u201d); (d) Canadian Personal Information Protection and Electronic Documents Act (\u201cPIPEDA\u201d) and (e) California Consumer Privacy Act, as amended by the California Privacy Rights Act (\u201cCCPA\u201d).</p> <p>1.2. \u201cAuthorized Person\u201d means any person who is required to access or otherwise Process Customer Personal Data on Spacelift\u2019s behalf to enable Spacelift to perform its obligations under the Services Agreement and this DPA, including but not limited to Spacelift\u2019s staff, officers, partners, and Subprocessors.</p> <p>1.3. \u201cCustomer\u201d means a) the party to the Services Agreement subscribing to Services provided by Spacelift and b) said party\u2019s affiliates. In respect of any obligation(s) which are required to be performed by Customer, Customer will ensure that Customer, or as applicable, its affiliates will perform such obligation(s).</p> <p>1.4. \u201dData Subject\u201d means the identified or identifiable natural person who is the subject of Customer Personal Data.</p> <p>1.5. \u201cPersonal Data\u201d means \u201cpersonal data\u201d, \u201cpersonal information\u201d, \u201cpersonally identifiable information\u201d or similar information defined in and/or governed by Applicable Data Protection Laws.</p> <p>1.6. \u201cPersonal Data Breach\u201d means any breach of security that leads to the accidental or unlawful destruction, loss, alteration, unauthorized disclosure of, or access to Personal Data transmitted, stored, or otherwise Processed by Spacelift and/or its Subprocessors in connection with the provision of the Services.</p> <p>1.7. \u201cProcessing\u201d means any operation or set of operations that is performed on Personal Data or sets of Personal Data, whether or not by automated means, such as collection, recording, organization, structuring, storage, adaptation, or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.</p> <p>1.8. \u201cServices\u201d means the services provided by Spacelift to Customer under the Services Agreement.</p> <p>1.9. \u201cServices Agreement\u201d means the agreement between Spacelift and Customer for the provision of the Services, consisting of Terms and Conditions, Master Services Agreement, or any other written agreement.</p> <p>1.10. \u201cSubprocessor\u201d means any authorized third party that Processes Personal Data to assist Spacelift in fulfilling its obligations under the Services Agreement and this DPA.</p> <p>1.11. \u201cTrust Center\u201d means Spacelift\u2019s website at trust.spacelift.io providing insight into Spacelift\u2019s information security posture, listing Subprocessors and Security Measures (as defined in Clause 4.2.);</p> <p>1.12. Other. Capitalized terms, or any other terms, used in this DPA that are not defined in this Section 1 (Definitions) will have the meaning given to them elsewhere in this DPA and/or the Services Agreement and/or in Applicable Data Protection Laws unless otherwise specified.</p>"},{"location":"legal/dpa.html#2-personal-data-processing","title":"2. PERSONAL DATA PROCESSING","text":"<p>2.1. Roles. The parties acknowledge and agree that with regard to the Processing of Personal Data, Customer acts as a Controller or Processor (as applicable) and Spacelift acts as a Processor or sub-processor (as applicable). Where Customer is itself a Processor of Personal Data, acting on behalf of a Controller, Customer will serve as the sole point of contact for Spacelift and Spacelift will not interact directly with (including to seek any authorizations directly from) any such Controller, other than through the regular provision of the Services to the extent required under the Services Agreement.</p> <p>2.2. Scope. The subject matter of Processing of Personal Data by Spacelift, the duration of the Processing, the nature and purpose of the Processing, the types of Personal Data, and categories of Data Subjects Processed under this DPA, are further specified in Annex 1 to this DPA - Subject Matter &amp; Details of Processing. Spacelift may make reasonable amendments to Annex 1 from time to time as Spacelift reasonably considers necessary to meet the requirements of Applicable Data Protection Laws.</p> <p>2.3. Instructions. Spacelift will Process Customer Personal Data solely under and following Customer\u2019s documented instructions and to provide the Services and as otherwise necessary to (a) perform its obligations or exercise its rights under the Services Agreement and (b) to perform its legal obligations and to establish, exercise or defend legal claims in respect of the Services Agreement (\u201cPermitted Purpose\u201d). For the Permitted Purpose, Customer\u2019s instructions include (i) instructions as set out in the Services Agreement and/or this DPA; and (ii) any additional reasonable instructions provided by Customer where such instructions are consistent with the terms of the Services Agreement and/or Applicable Data Protection Laws. Spacelift will promptly inform Customer if, in Spacelift\u2019s opinion, any instruction infringes Applicable Data Protection Laws.</p> <p>2.4. Compliance with Law. The parties will comply with their obligations under Applicable Data Protection Laws concerning the Processing of Customer Personal Data. Each party will promptly notify the other party if it is unable to comply with its obligations under Applicable Data Protection Laws and/or the terms of the Services Agreement (including this DPA) as they relate to or govern the Processing of Customer Personal Data for any reason. In the event of any such non-compliance, and without prejudice to any other right or remedy available to the other party under the Services Agreement, such notifying party will take all reasonable and appropriate steps to remediate any non-compliance.</p> <p>2.5. Cooperation and Assistance. Upon each party\u2019s request, the other party will provide the requesting party with reasonable cooperation and assistance needed to fulfill its obligations under Applicable Data Protection Laws.</p>"},{"location":"legal/dpa.html#3-subprocessors","title":"3. SUBPROCESSORS","text":"<p>3.1. Authorization. Customer specifically authorizes Spacelift to use its Subprocessors as listed in Spacelift\u2019s Trust Center, and generally authorizes Spacelift to engage any new Subprocessors to Process Customer Personal Data.</p> <p>3.2. Obligations. While using Subprocessors, Spacelift:</p> <ul> <li> <p>3.2.1. will enter into a written agreement with each Subprocessor, imposing data protection obligations substantially similar to those set out in this DPA; and</p> </li> <li> <p>3.2.2. remains liable for compliance with the obligations of this DPA and for any acts or omissions of the Subprocessor that cause Spacelift to breach any of its obligations under this DPA.</p> </li> </ul> <p>3.3. New Subprocessors. When any new Subprocessor is engaged, Spacelift will notify Customer of the engagement, which notice may be given via email and/or by updating the Subprocessor list available at Trust Center (Customer can subscribe to receive notifications about changes in the Trust Center). Spacelift will give such notice at least fifteen (15) days before the new Subprocessor Processes any Customer Personal Data, except that if Spacelift reasonably believes engaging a new Subprocessor on an expedited basis is necessary to protect the confidentiality, integrity or availability of Customer Personal Data or avoid material disruption to the Services, Spacelift will give such notice as soon as reasonably practicable.</p> <p>3.4. Objections. Customer may object to an engagement of a new Subprocessor by informing Spacelift in writing within ten (10) days of receipt of the aforementioned notice by Customer, provided such objection is in writing and based on reasonable grounds relating to data protection. Should Customer express in writing its objection to Spacelift's appointment of a new Subprocessor on valid data protection grounds, the parties will engage in a good-faith discussion to address and resolve these concerns. Customer acknowledges that certain Subprocessors are essential to providing the Services and that objecting to the use of a Subprocessor may prevent Spacelift from offering the Services to Customer. If the parties are unable to reach a mutually agreeable resolution within a reasonable period of time, which will not exceed thirty (30) days, Customer may discontinue the use of the affected Services by providing written notice to Spacelift and Spacelift will refund a prorated amount of any prepaid fees. Except for the prorated refund, such discontinuation will not relieve Customer of any fees owed to Spacelift under the Services Agreement.</p>"},{"location":"legal/dpa.html#4-security","title":"4. SECURITY","text":"<p>4.1. Personnel. Spacelift will take reasonable steps to ensure the reliability of any Authorized Persons who may have access to Customer Personal Data, ensuring in each case that access is strictly limited to those individuals who need to know and/or access the relevant Customer Personal Data, as necessary for Permitted Purpose. Spacelift will ensure that Authorized Persons are informed of the confidential nature of Customer Personal Data and that they receive appropriate training regarding their responsibilities. Spacelift will impose appropriate contractual obligations on Authorized Persons, including relevant obligations regarding confidentiality, data protection, and data security.</p> <p>4.2. Security Measures. Taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of Processing, as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, Spacelift will maintain and implement appropriate technical and organizational measures for protection of the security, confidentiality and integrity of Customer Personal Data, as presented in Spacelift\u2019s Trust Center  (the \u201cSecurity Measures\u201d). Customer acknowledges that the Security Measures may be updated from time to time to reflect process improvements or changing practices, but the modifications will not materially decrease Spacelift\u2019s obligations as compared to those reflected in such terms as of the Effective Date of the Services Agreement and will be proportionate to the identified risks.</p> <p>4.3. Customer Responsibility. Customer must thoroughly review the information provided by Spacelift regarding data security. It is Customer's responsibility to independently assess whether the Services comply with its requirements and legal obligations under Applicable Data Protection Laws. Customer acknowledges that, notwithstanding Spacelift's obligations outlined in this DPA, Customer is solely accountable for utilizing the Services. This includes (a) ensuring the Services are appropriately used to maintain a level of security appropriate to the risk associated with Customer Personal Data; (b) safeguarding the authentication credentials, systems, and devices used to access the Services; (c) securing Customer's systems and devices used in conjunction with the Services and (d) configuring, setting up, and operating the Services to align with Customer\u2019s security and operational needs.</p> <p>4.4. Personal Data Breach. Upon becoming aware of a confirmed Personal Data Breach, Spacelift will notify Customer without undue delay about details of such Personal Data Breach, unless prohibited by Applicable Data Protection Laws, provided that (i) in case of SaaS Services - Customer indicated Customer\u2019s contact data in Spacelift\u2019s SaaS solution under the following address: https://.app.spacelift.io/settings/security (being the domain name chosen by Customer to access Services) or (ii) in case of any other Services - Customer provided Spacelift with contact details regarding Personal Data Breaches. A delay in giving such notice requested by law enforcement and/or in light of Spacelift\u2019s legitimate needs to investigate or remediate the matter before providing notice will not constitute an undue delay. Such notices will describe, to the extent possible, details of the Personal Data Breach. Without prejudice to Spacelift\u2019s obligations under this Clause 4.4., Customer is solely responsible for complying with Personal Data Breach notification laws applicable to Customer and fulfilling any third party notification obligations related to any Personal Data Breaches. Spacelift\u2019s notification of or response to a Personal Data Breach under this Clause 4.4 will not be construed as an acknowledgement of any fault or liability with respect to the Personal Data Breach.</p>"},{"location":"legal/dpa.html#5-audit-rights","title":"5. AUDIT RIGHTS","text":"<p>The parties recognize that Customer must be able to evaluate Spacelift's adherence to its obligations under Applicable Data Protection Laws and this DPA, specifically given Spacelift is acting as a Processor or subprocessor. At Customer's request, Spacelift will present information concerning its compliance with the obligations outlined in this DPA to Customer and/or an independent third-party auditor appointed by Customer, including completion of audit questionnaires, provision of security policies and summaries of assessments of compliance with any industry standards (such as SOC II report), and /or penetration testing. Spacelift assures Customer that (a) any information provided in response to such requests is accurate to the best of Spacelift's knowledge and (b) the individual supplying this information is authorized to do so and possesses knowledge about Spacelift's information Security Measures.</p>"},{"location":"legal/dpa.html#6-data-subject-rights","title":"6. DATA SUBJECT RIGHTS","text":"<p>Upon Customer\u2019s request, Spacelift will provide Customer with such assistance as it may reasonably require to comply with its obligations under Applicable Data Protection Laws to respond to requests from Data Subjects to exercise their rights under Applicable Data Protection Laws in cases where Customer cannot reasonably fulfill such requests independently. If Spacelift receives a request from a Data Subject in relation to their Personal Data, Spacelift will advise the Data Subject to submit their request to Customer, and Customer will be responsible for handling any such request.</p>"},{"location":"legal/dpa.html#7-data-privacy-impact-assessment","title":"7. DATA PRIVACY IMPACT ASSESSMENT","text":"<p>Upon Customer\u2019s request, Spacelift will provide Customer with reasonable cooperation needed to fulfill Customer\u2019s obligation under the Applicable Data Protection Laws to carry out a data protection impact assessment or handle prior consultation with the applicable data protection authority related to Customer\u2019s use of the Services, to the extent Customer does not otherwise have access to the relevant information, and to the extent such information is available to Spacelift.</p>"},{"location":"legal/dpa.html#8-deletion-of-data","title":"8. DELETION OF DATA","text":"<p>Spacelift will delete all copies of Customer Personal Data in its possession or control upon the termination or expiry of the Services Agreement, according to its data retention scheme. Notwithstanding the foregoing, Customer acknowledges that Spacelift may retain Customer Personal Data if required by Applicable Data Protection Laws, and such data will remain subject to the requirements of this DPA.</p>"},{"location":"legal/dpa.html#9-liability","title":"9. LIABILITY.","text":"<p>Unless specifically agreed otherwise in the Services Agreement, each party\u2019s liability arising out of or related to this DPA and its Annexes, whether in contract, tort or under any other theory of liability, is subject to any \u201cLimitation of Liability\u201d section of the Services Agreement, and any reference in such section to the liability of a party means the aggregate liability of that party under the Services Agreement and the DPA together, subject to any exclusions in accordance with Applicable Data Protection Laws and provisions of the Services Agreement.</p>"},{"location":"legal/dpa.html#10-international-provisions","title":"10. INTERNATIONAL PROVISIONS","text":"<p>10.1. Processing Activities. Customer acknowledges that Spacelift Processes Customer Personal Data primarily in Europe and the United States. Customer authorizes Spacelift and its Sub-processors to make international data transfers of Customer Personal Data in accordance with this DPA so long as Applicable Privacy Laws for such transfers are respected.</p> <p>10.2. Jurisdiction-Specific Annexes. To the extent that Spacelift Processes Customer Personal Data originating from and protected by Applicable Data Protection Laws in one of the jurisdictions listed in Annex 2 (Jurisdiction Specific Terms), then the terms specified therein with respect to the applicable jurisdiction(s) will apply in addition to the terms of this DPA. In the event of a conflict between the Services Agreement or this DPA and an Annex, the Annex applicable to Customer Personal Data from the relevant jurisdiction will control with respect to Customer Personal Data from that relevant jurisdiction, and solely with regard to the portion of the provision in conflict.</p>"},{"location":"legal/dpa.html#11-miscellaneous","title":"11. MISCELLANEOUS","text":"<p>11.1. Services Agreement. This DPA forms part of the Services Agreement and except as expressly set forth in this DPA, the Services Agreement remains unchanged and in full force and effect. If there is any conflict between this DPA and the Services Agreement, this DPA will govern.</p> <p>11.2. Applicable Data Protection Laws Changes. In the event of changes to Applicable Data Protection Laws, including, but not limited to, the amendment, revision or introduction of new laws, regulations, or other legally binding requirements to which either party is subject, the parties agree to revisit the terms of this DPA, and negotiate any appropriate or necessary updates in good faith, including the addition, amendment, or replacement of any Annexes.</p> <p>11.3. Termination. This DPA will automatically terminate upon expiration or termination of the Services Agreement.  However, for the avoidance of doubt, the provisions of the DPA will in all cases continue to apply for as long as the Spacelift Processes Customer Personal Data on behalf of Customer.</p> <p>11.4. Governing Law and Jurisdiction. Except for the provisions of the Standard Contractual Clauses included in the Annex 2 - Jurisdiction Specific Terms, if applicable:</p> <ul> <li> <p>11.4.1. the parties to this DPA hereby agree to abide by the jurisdiction specified in the Services Agreement for the resolution of any disputes or claims arising under this DPA. This includes disputes related to its existence, validity, termination, or the consequences of its nullity.</p> </li> <li> <p>11.4.2. the laws governing this DPA and all non-contractual or other obligations arising from or in connection with it are determined by the country or territory designated for this purpose in the Services Agreement.</p> </li> </ul> <p>11.5. Severability. If any provision of this DPA is deemed unlawful or unenforceable, such provision will be stricken from this DPA to the extent of such illegality or unenforceability, and the remainder will remain in full force and effect.</p> <p>11.6. Annexes. For the avoidance of doubt, each reference to the DPA means this DPA including its Annexes (including the Standard Contractual Clauses, if the Standard Contractual Clauses have been entered into in accordance with the Services Agreement or this DPA), consisting in:</p> <ul> <li> <p>11.6.1. Annex 1: SUBJECT MATTER AND DETAILS OF PROCESSING</p> </li> <li> <p>11.6.2. Annex 2: JURISDICTION SPECIFIC TERMS</p> </li> </ul>"},{"location":"legal/dpa.html#annex-1-subject-matter-and-details-of-processing","title":"ANNEX 1: SUBJECT MATTER AND DETAILS OF PROCESSING","text":""},{"location":"legal/dpa.html#1-list-of-parties","title":"1. LIST OF PARTIES","text":"Customer Spacelift Name: Customer as identified in the Services Agreement Spacelift, Inc. Address: As listed by Customer in the website purchase portal or as identified on the Services Agreement 541 Jefferson Ave. Suite 100, Redwood City CA 94063, USA Contact Person: As listed by Customer in the website purchase portal or as identified on the Services Agreement privacy@spacelift.io Role: Included in Clause 2.1 of the DPA Included in Clause 2.1 of the DPA Signatures: By entering into the Services Agreement, Data Exporter is deemed to have signed the DPA, including its Annexes By entering into the Services Agreement, Data Importer is deemed to have signed the DPA, including its Annexes"},{"location":"legal/dpa.html#2-description-of-processing-and-transfer-if-applicable","title":"2. DESCRIPTION OF PROCESSING AND TRANSFER, IF APPLICABLE","text":"Description Details Categories of data subjects whose personal data is processed / transferred: Users of the software provided by Spacelift, in particular staff including volunteers, agents, temporary and casual workers Categories of personal data processed / transferred: Name, logins, and e-mail addresses, data concerning Services usage Are sensitive data processed / transferred? No The frequency of processing / transfer: Continuous, as required for the provision of Services under the Services Agreement. Nature and purpose of processing / transfer: Spacelift will process Personal Data as necessary to provide the Services under the Services Agreement. The period for which the personal data will be retained: Specified in the Services Agreement (duration of the Services Agreement). For transfers to (sub-) processors - the subject matter, nature, and duration of the processing: Where Spacelift engages Subprocessors it will do so in compliance with the terms of the DPA. The subject matter, nature, and duration of the Processing activities carried out by the Subprocessor will not exceed the subject matter, nature and duration of the Processing activities as described in the DPA."},{"location":"legal/dpa.html#annex-2-jurisdiction-specific-terms","title":"ANNEX 2: JURISDICTION SPECIFIC TERMS","text":""},{"location":"legal/dpa.html#1-european-economic-area-eea-and-united-kingdom-uk","title":"1. EUROPEAN ECONOMIC AREA (EEA) AND UNITED KINGDOM (UK)","text":""},{"location":"legal/dpa.html#11-definitions","title":"1.1. Definitions.","text":"<p>1.1.1. The definition of \u201cApplicable Data Protection Laws\u201d includes the General Data Protection Regulation (EU 2016/679) (\u201cGPDR\u201d) and the EU GDPR as saved into UK law by virtue of Section 3 of the UK\u2019s European Union Act 2018 and the UK Data Protection Act 2018 (\u201cUK GDPR\u201d).</p> <ul> <li> <p>1.1.2. \"Restricted Transfer\" means (i) where the EU GDPR applies, a transfer of personal data from the European Economic Area to a country outside of the European Economic Area which is not subject to an adequacy determination by the European Commission; and (ii) where the UK GDPR applies, a transfer of personal data from the United Kingdom to any other country which is not based on adequacy regulations pursuant to Section 17A of the United Kingdom Data Protection Act 2018.</p> </li> <li> <p>1.1.3. \u201cStandard Contractual Clauses\u201d means (i) where the EU GDPR applies, the contractual clauses annexed to the European Commission's Implementing Decision 2021/914 of 4 June 2021 on standard contractual clauses for the transfer of personal data to third countries pursuant to Regulation (EU) 2016/679 of the European Parliament and of the Council (\"EU SCCs\"); and (ii) where the UK GDPR applies, the United Kingdom International Data Transfer Addendum to the European Commission\u2019s Standard Contractual Clauses for international data transfers version B1.0 issued by the UK Information Commissioner under Section 119A of the UK Data Protection Act of 2018 and entering into force on 21 March 2022, as updated, amended, or replaced from time to time (\"UK IDTA\").</p> </li> </ul> <p>1.2. SCCs. The parties agree that when the transfer of Personal Data from Customer to Spacelift is a Restricted Transfer, it will be subject to the appropriate Standard Contractual Clauses, being EU SCCs or UK IDTA, which are incorporated herein by reference.</p> <p>1.3. EU SCCs. In relation to Personal Data that is protected by the EU GDPR, the EU SCCs will apply as follows:</p> <ul> <li> <p>1.3.1. Module Two will apply to the extent that Customer is a controller of the Personal Data, and Module Three will apply to the extent that Customer is a processor of the Personal Data on behalf of a third-party controller;</p> </li> <li> <p>1.3.2. For both Modules Two and Three, Customer is the Data Exporter and Spacelift is the Data Importer.</p> </li> <li> <p>1.3.3. In Clause 7, the optional docking clause will apply;</p> </li> <li> <p>1.3.4. In Clause 9, Option 2 (General Authorization) will apply, and the period for prior notice of Sub-processor changes will be as set out in Clause 3.3. of this DPA;</p> </li> <li> <p>1.3.5. In Clause 11, the optional language will not apply;</p> </li> <li> <p>1.3.6. In Clause 17, Option 1 will apply, and the EU SCCs will be governed by Polish law;</p> </li> <li> <p>1.3.7. In Clause 18(b), disputes will be resolved before the courts of Poland;</p> </li> <li> <p>1.3.8. Annex I of the EU SCCs will be deemed complete with (as to Part A and Part B) information set out in Annex 1 to this DPA and (as to Part C) with the Polish supervisory authority;</p> </li> <li> <p>1.3.9. Annex II of the EU SCCs will be deemed completed with the information set out in the Trust Center;</p> </li> <li> <p>1.3.10. Annex III of the EU SCCs will be deemed completed with the information set out the Trust Center;</p> </li> </ul> <p>1.4. UK IDTA. In relation to Controller Personal Data that is protected by the UK GDPR, the UK IDTA will apply completed as follows:</p> <ul> <li> <p>1.4.1. The EU SCCs, completed as set out above in Clause 1.3 of this Annex will also apply to transfers of such Personal Data, subject to Sub-clause 1.4.2 below;</p> </li> <li> <p>1.4.2. Tables 1 to 3 of the UK Addendum will be deemed completed with relevant information from the EU SCCs, completed as set out above, and the options \"either party\" will be deemed checked in Table 4. The start date of the UK IDTA (as set out in Table 4) will be the Services Agreement Effective Date.</p> </li> <li> <p>1.4.3. The parties confirmed that they adopt the following wording of the Part II of the UK IDTA: Mandatory Clauses of the Approved Addendum, being the template Addendum B.1.0 issued by the ICO and laid before Parliament in accordance with section 119A of the Data Protection Act 2018 on 2 February 2022, as it is revised under Section 18 of those Mandatory Clauses.</p> </li> </ul> <p>1.5. GDPR Penalties. Notwithstanding anything to the contrary in this DPA or in the Services Agreement (including, without limitation, either party\u2019s indemnification obligations), neither party will be responsible for any GDPR or UK GDPR fines issued or levied against the other party by a regulatory authority or governmental body in connection with such other party\u2019s violation of the GDPR or UK GDPR.</p>"},{"location":"legal/dpa.html#2-switzerland","title":"2. SWITZERLAND","text":"<p>2.1. The definition of \u201cApplicable Data Protection Laws\u201d includes the Federal Act on Data Protection of 19 June 1992 (the \u201cFADP\u201d).</p> <p>2.2. With respect to Personal Data transferred from Switzerland for which Swiss law (and not the law in any European Economic Area jurisdiction) governs the international nature of the transfer, (i) references to the GDPR in Clause 4 of the EU SCCs are, to the extent legally required, amended to refer to the FADP or its successor instead, and the concept of supervisory authority will include the Swiss Federal Data Protection and Information Commissioner; and (ii) as so amended and updated by Clause 1.3 above, the EU SCCs are incorporated herein by reference and will apply, form a part of this DPA, and take precedence over the rest of this DPA to the extent of conflict.</p>"},{"location":"legal/dpa.html#3-california","title":"3. CALIFORNIA","text":"<p>3.1. The definition of \u201cApplicable Data Protection Laws\u201d includes the California Consumer Privacy Act, as amended by the California Privacy Rights Act (\u201cCCPA\u201d).</p> <p>3.2. The terms \u201cbusiness\u201d, \u201ccommercial purpose, \u201cservice provider\u201d, \u201csell\u201d and \u201cpersonal information\u201d have the meanings given in the CCPA.</p> <p>3.3. With respect to Customer Personal Data, Spacelift is a service provider under the CCPA.</p> <p>3.4. Spacelift will not (a) sell Customer Personal Data; (b) retain, use, or disclose any Customer Personal Data for any purpose other than for the specific purpose of providing the Services, including retaining, using, or disclosing Customer Personal Data for a commercial purpose other than providing the Services; or (c) retain, use or disclose Customer Personal Data outside of the direct business relationship between Spacelift and Customer.</p> <p>3.5. The parties acknowledge and agree that the Processing of Customer Personal Data authorized by Customer\u2019s instructions described in the DPA is integral to and encompassed by Spacelift\u2019s provision of the Services and the direct business relationship between the parties. 3.6. Notwithstanding anything in the Services Agreement or any written agreement entered in connection therewith, the parties acknowledge and agree that Spacelift\u2019s access to Customer Personal Data does not constitute part of the consideration exchanged by the parties in respect of the Services Agreement.</p> <p>3.7. Spacelift agrees that it will provide Customer with reasonable assistance and cooperate with Customer\u2019s obligations under CCPA to ensure that Spacelift is: (a) Processing Personal Data in a manner consistent with Spacelift\u2019s obligations and (b) stop and remediate any unauthorized use of Personal Data.</p>"},{"location":"legal/intent-terms.html","title":"Spacelift Intent Early Access Release","text":""},{"location":"legal/intent-terms.html#spacelift-intent-early-access-release","title":"Spacelift Intent Early Access Release","text":""},{"location":"legal/intent-terms.html#terms-and-conditions","title":"Terms and Conditions","text":"<p>Effective Date: October 8, 2025</p> <p>These Terms and Conditions (\"Agreement\") govern your participation in and use of the beta version of Spacelift Intent commercial implementation (\"Beta Product\"). This Agreement constitutes a binding agreement between Spacelift, Inc. (\"Spacelift,\" \"we,\" \"us,\" or \"our\") and the entity you represent (\"Organization\"), or, if the Beta Product is accessed through an individual account not associated with any organization, you individually (in either case, \"Customer,\" \"you,\" or \"your\").</p> <p>BY ACCESSING, ENABLING, OR USING THE BETA PRODUCT, YOU ACKNOWLEDGE THAT YOU HAVE READ, UNDERSTOOD, AND AGREE TO BE BOUND BY ALL TERMS OF THIS AGREEMENT. IF THE BETA PRODUCT IS ACCESSED THROUGH AN ORGANIZATIONAL SPACELIFT PLATFORM ACCOUNT, YOU FURTHER REPRESENT THAT YOU HAVE THE NECESSARY AUTHORITY TO BIND THE ORGANIZATION TO THIS AGREEMENT ON ITS BEHALF. IF YOU DO NOT AGREE OR DO NOT HAVE SUCH AUTHORITY, YOU MUST NOT ACCESS, ENABLE, OR USE THE BETA PRODUCT.</p> <p>THE BETA PRODUCT, AND ALL RELATED INFORMATION, IS STRICTLY CONFIDENTIAL INFORMATION OF SPACELIFT. SPACELIFT DOES NOT BACK UP YOUR DATA. SPACELIFT HAS NO OBLIGATION OR LIABILITY FOR ANY LOSS, ALTERATION, DESTRUCTION, DAMAGE, CORRUPTION, OR RECOVERY OF YOUR DATA.</p>"},{"location":"legal/intent-terms.html#1-definitions","title":"1. DEFINITIONS","text":"<p>1.1 \"Beta Product\" means the pre-release of the commercial version of Spacelift Intent software and services, including all associated functionality, interfaces, and integrations.</p> <p>1.2 \"Confidential Information\" means all non-public information relating to the Beta Product, its functionality, performance, features, and Documentation.</p> <p>1.3 \"Documentation\" means any user manuals, technical guides, documentation, specifications, and other materials provided by Spacelift relating to the Beta Product.</p> <p>1.4 \"Feedback\" means opinions, suggestions, comments, ideas, reactions, or other information you provide regarding the Beta Product.</p> <p>1.5 \"NDA\" means any existing non-disclosure agreement between you and Spacelift, whether a standalone contract or part of any agreement.</p> <p>1.6 \"Organization\" means your company, organization, or other legal entity entering into this Agreement.</p> <p>1.7 \"Spacelift Platform\" means Spacelift's generally available infrastructure-as-code management platform and related services.</p>"},{"location":"legal/intent-terms.html#2-beta-product-description-and-limitations","title":"2. BETA PRODUCT DESCRIPTION AND LIMITATIONS","text":"<p>2.1 Product Nature. The Beta Product is a separate yet integrated with the Spacelift Platform product that assists in the creation of infrastructure resources through natural language. Users interact with it through external LLM clients, describing desired infrastructure in natural language rather than writing code. Such requests are translated into direct cloud provider API calls while all execution, policy enforcement, and state storage occur within Spacelift's systems. Results, and infrastructure state are visible in the Spacelift Platform UI, allowing users to monitor and manage what was created.</p> <p>2.2 Organization-Wide Access. Upon activation via the feature flag in the Spacelift Platform, the Beta Product will be available to all authorized users within your organization who have appropriate access permissions to the Spacelift Platform.</p> <p>2.3 Free of Charge. The Beta Product is provided free of charge during the beta period. This free access: (i) may be revoked at any time, (ii) does not guarantee free access to any future commercial version, and (iii) creates no obligation for us to provide any services.</p> <p>2.4 Acceptance and Eligibility. Your participation in the Beta Product release is subject to our approval and may be revoked at any time. You must comply with all applicable terms, policies, and guidelines we provide. We may limit the number of participants, geographic availability, or duration of the Beta Product release at our sole discretion.</p>"},{"location":"legal/intent-terms.html#3-beta-product-limitations-and-ai-considerations","title":"3. BETA PRODUCT LIMITATIONS AND AI CONSIDERATIONS","text":"<p>3.1 Pre-Release Status. The Beta Product is pre-release software provided exclusively for testing, evaluation, and feedback purposes. The Beta Product is not a final commercial product and may be subject to changes.</p> <p>3.2 Experimental Nature. You acknowledge that the Beta Product: (i) may contain incomplete or experimental features, bugs, errors, defects, and other problems; (ii) is subject to changes, updates, or complete redesign without notice; (iii) may experience unexpected downtime, data processing errors, or integration failures; (iv) is not subject to any service level agreements (SLAs) or uptime guarantees; (v) may be discontinued, modified, or replaced at any time at Spacelift's sole discretion.</p> <p>3.3 AI-Powered Technology Limitations and Third-Party Models. You acknowledge and agree that: (i) The Beta Product utilizes artificial intelligence and machine learning technologies provided by third-party AI model providers, not by Spacelift; (ii) Spacelift does not provide, control, or guarantee the performance of the underlying AI models; (iii) AI technologies by their nature may generate inaccurate, incomplete, misleading, or fabricated information (commonly known as \"hallucinations\"); (iv) AI-generated outputs may not reflect actual infrastructure configurations, requirements, or best practices; (v) All AI-generated suggestions, recommendations, configurations and code require human review and validation; (vi) The Beta Product should not be relied upon as the sole basis for infrastructure decisions or implementations; (vii) AI systems may exhibit unpredictable behavior or responses to certain inputs or queries.</p> <p>3.4 Customer Responsibility for AI Outputs. You acknowledge that: (i) you are solely responsible for reviewing, validating, and testing all AI-generated outputs before implementation; (ii) you must apply appropriate human oversight and professional judgment to all Beta Product recommendations; (iii) Spacelift makes no representations or warranties regarding the accuracy, completeness, or suitability of AI-generated content or third-party AI model performance; (iv) your use of AI-generated outputs is entirely at your own risk and discretion; (v) you understand that the underlying AI models are provided by third parties and Spacelift has no control over their operation, availability, or performance.</p> <p>3.5 No Production Use Recommendation. The Beta Product is not recommended for production environments or critical business operations due to its experimental and pre-release nature.</p>"},{"location":"legal/intent-terms.html#4-confidentiality-and-non-disclosure","title":"4. CONFIDENTIALITY AND NON-DISCLOSURE","text":"<p>4.1 Applicable Confidentiality Terms.</p> <p>(a) If an NDA Exists: If you or the entity you represent has executed a separate non-disclosure agreement with Spacelift that is currently in effect, the confidentiality provisions of that NDA shall govern the treatment of Confidential Information related to the Beta Product, and shall supersede the confidentiality provisions in this Section 4.</p> <p>(b) If No NDA Exists: If no separate NDA is in effect, the following confidentiality provisions shall apply.</p> <p>4.2 Strict Confidentiality. THE BETA PRODUCT, DOCUMENTATION, AND ALL RELATED INFORMATION ARE STRICTLY CONFIDENTIAL. The features, functionality, and any information about the Beta Product or Documentation constitute Confidential Information that is proprietary to Spacelift and, where applicable, to the respective owners of any third-party components.</p> <p>4.3 Non-Disclosure Obligations. You agree to: (i) not disclose any information about the Beta Product to any third party, (ii) not discuss, mention, or reference the Beta Product in any public or private forum, social media, forums, blogs, or any online platform, without Spacelift's prior written consent, (iii) not share, post, or distribute any screenshots, recordings, descriptions, or information about the Beta Product, (iv) only share information about the Beta Product with other authorized participants, and (v) maintain the confidentiality of all Confidential Information with at least the same degree of care you use to protect your own confidential information, but in no event less than reasonable care.</p> <p>4.4 Confidentiality Period. Your confidentiality obligations shall survive termination of this Agreement and continue for a period of five (5) years from the date of termination, or until such information becomes publicly available through no breach of this Agreement.</p> <p>4.5 Required Disclosure. If you are legally compelled to disclose Confidential Information, you must immediately notify us in writing and cooperate with us to limit such disclosure to the minimum required by law.</p>"},{"location":"legal/intent-terms.html#5-data-use-and-collection","title":"5. DATA USE AND COLLECTION","text":"<p>5.1 Permitted Data. The Beta Product is designed exclusively for assistance with creating infrastructure resources. You may only input, upload, store, or process data that is directly related to infrastructure-as-code configurations, templates, policies, and management activities.</p> <p>5.2 Prohibited Data. You must not input, upload, store, or process any sensitive data, personally identifiable information (PII), protected health information (PHI), financial data, authentication credentials, or any other confidential or regulated data in the Beta Product. This prohibition includes, but is not limited to: (i) any data not directly related to infrastructure-as-code operations; (ii) personally identifiable information of any kind; (iii) protected health information or medical records; (iv) financial records, payment information, or credit card data; (v) data subject to regulatory requirements (GDPR, HIPAA, PCI-DSS, SOX, etc.); (vi) confidential business information unrelated to IaC; or (vii) any data that would cause harm if disclosed, processed by AI systems, or lost.</p> <p>5.4 Data Handling. Spacelift: (i) does not guarantee data retention, (ii) may delete data without notice, (iii) may reset the environment periodically, and (iv) is not responsible for data backup or recovery.</p> <p>5.5 Your Data Responsibility. You acknowledge and agree that: (i) you bear sole responsibility for any data you enter, (ii) we have no obligation to preserve, backup, or protect any data, and (iii) any violation of data restrictions may result in immediate access termination. You are responsible for ensuring all your Organization's authorized users understand and comply with the data provisions and restrictions in this Section 5.</p> <p>5.6 Data Collection. The Beta Product and your use of the Documentation may result in collection of usage data and analytics, performance and error data, or feature interaction data. Collected data will be used for product improvement and development, bug identification and resolution and usage analysis and optimization.</p>"},{"location":"legal/intent-terms.html#6-license-grant-and-restrictions","title":"6. LICENSE GRANT AND RESTRICTIONS","text":"<p>6.1 Limited License. Subject to your compliance with this Agreement, Spacelift grants you a limited, non-exclusive, non-transferable, revocable license to access and use the Beta Product and Documentation solely for internal evaluation and testing purposes.</p> <p>6.2 License Restrictions. You will not, and will not permit any other person to: (i) copy, modify, create derivative works, distribute, sell, transfer, or sublicense the Beta Product or Documentation, (ii) reverse engineer, decompile, or attempt to derive source code from the Beta Product, (iii) bypass security measures or use invalid or another user's credentials, (iv) input harmful, unlawful, or malicious content or code, (v) disrupt, disable, or interfere with the Beta Product operation or system resources, (vi) remove or alter any proprietary notices, trademarks, or disclaimers, (vii) use the Beta Product for illegal purposes or to violate third-party rights, (viii) use the Beta Product or Documentation for competitive analysis or developing competing products, or (ix) perform unauthorized security testing or vulnerability scanning.</p>"},{"location":"legal/intent-terms.html#7-intellectual-property","title":"7. INTELLECTUAL PROPERTY","text":"<p>7.1 Spacelift Ownership. We retain all rights, title, and interest in and to: (i) Documentation and Beta Product and all components, (ii) all intellectual property rights therein, and (iii) all improvements and modifications. If the Beta Product incorporates, integrates, or enables the use of any open-source software or third-party components, all such elements remain the property of their respective owners.</p> <p>7.2 Feedback Rights. You hereby assign to us all rights, title, and interest in and to all Feedback. We may use, share, and commercialize Feedback in any way and for any purpose without: (i) attribution or acknowledgment, (ii) compensation to you, and (iii) any obligation or restriction. You agree that Feedback is not confidential and that we owe no fiduciary or other obligation to you regarding Feedback.</p> <p>7.3. No Rights Granted. Except for the limited license explicitly granted, this Agreement grants you no rights to: (i) our intellectual property, (ii) our trademarks or brand features, (iii) any source code or technical information, or (iv) any future products or services.</p>"},{"location":"legal/intent-terms.html#8-disclaimer-of-warranties","title":"8. DISCLAIMER OF WARRANTIES","text":"<p>THE BETA PRODUCT IS PROVIDED \"AS IS\" AND \"AS AVAILABLE\" WITHOUT WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED. WE SPECIFICALLY DISCLAIM ALL IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT, AND ALL WARRANTIES ARISING FROM THE COURSE OF DEALING, USAGE, OR TRADE PRACTICE. WITHOUT LIMITING THE FOREGOING, WE MAKE NO WARRANTY OF ANY KIND THAT THE BETA PRODUCT OR RESULTS OF THE USE THEREOF, WILL MEET YOUR OR ANY OTHER PERSON'S REQUIREMENTS, OPERATE WITHOUT INTERRUPTION, ACHIEVE ANY INTENDED RESULT, BE COMPATIBLE OR WORK WITH ANY SOFTWARE, SYSTEM, OR OTHER SERVICES, OR BE SECURE, ACCURATE, COMPLETE, FREE OF HARMFUL CODE, OR ERROR-FREE. WE SPECIFICALLY DISCLAIM ANY WARRANTIES REGARDING THE ACCURACY, RELIABILITY, OR COMPLETENESS OF AI-GENERATED CONTENT, RECOMMENDATIONS, OR OUTPUTS PRODUCED BY THE BETA PRODUCT OR ANY THIRD-PARTY AI MODELS INTEGRATED WITH THE BETA PRODUCT.</p>"},{"location":"legal/intent-terms.html#9-limitation-of-liability","title":"9. LIMITATION OF LIABILITY.","text":"<p>9.1 EXCLUSIONS OF LIABILITY. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL SPACELIFT BE LIABLE UNDER OR IN CONNECTION WITH THIS AGREEMENT OR ITS SUBJECT MATTER UNDER ANY LEGAL OR EQUITABLE THEORY, INCLUDING BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, AND OTHERWISE, FOR ANY: (i) LOSS OF PRODUCTION, USE, BUSINESS, REVENUE, OR PROFIT OR DIMINUTION IN VALUE; (ii) IMPAIRMENT, INABILITY TO USE OR LOSS, INTERRUPTION, OR DELAY OF THE BETA PRODUCT; (iii) LOSS, DAMAGE, CORRUPTION, OR RECOVERY OF DATA, OR BREACH OF DATA OR SYSTEM SECURITY; (iv) COST OF REPLACEMENT GOODS OR SERVICES; (v) LOSS OF GOODWILL OR REPUTATION; OR (vi) CONSEQUENTIAL, INCIDENTAL, INDIRECT, EXEMPLARY, SPECIAL, ENHANCED, OR PUNITIVE DAMAGES, REGARDLESS OF WHETHER SUCH PERSONS WERE ADVISED OF THE POSSIBILITY OF SUCH LOSSES OR DAMAGES OR SUCH LOSSES OR DAMAGES WERE OTHERWISE FORESEEABLE, AND NOTWITHSTANDING THE FAILURE OF ANY AGREED OR OTHER REMEDY OF ITS ESSENTIAL PURPOSE.</p> <p>9.2 LIABILITY CAP. SPACELIFT WILL ONLY BE LIABLE FOR DIRECT DAMAGES EXCLUDING ANY SITUATION FOR WHICH WE ARE NOT RESPONSIBLE OR WHICH IS CAUSED BY EVENTS OUTSIDE OUR REASONABLE CONTROL. HOWEVER, EXCEPT AS OTHERWISE PROVIDED IN THIS SECTION, IN NO EVENT WILL THE COLLECTIVE AGGREGATE LIABILITY OF SPACELIFT ARISING OUT OF OR RELATED TO THIS AGREEMENT, WHETHER ARISING UNDER OR RELATED TO BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, OR ANY OTHER LEGAL OR EQUITABLE THEORY, EXCEED THE AMOUNT OF FIVE THOUSAND US DOLLARS ($5,000.00 USD). THE FOREGOING LIMITATIONS APPLY EVEN IF ANY REMEDY FAILS OF ITS ESSENTIAL PURPOSE.</p>"},{"location":"legal/intent-terms.html#10-indemnification","title":"10. INDEMNIFICATION","text":"<p>10.1 Your Indemnification Obligation. You agree to defend, indemnify, and hold harmless Spacelift, its affiliates, officers, directors, employees, agents, licensors, and suppliers from and against all claims, losses, damages, costs, and expenses (including reasonable attorneys' fees) arising from: (i) your use or misuse of the Beta Product or Documentation, (ii) your violation of this Agreement, (iii) your violation of the confidentiality obligations, (iv) your input of prohibited data, or (v) your negligence or intentional misconduct.</p> <p>10.2 Indemnification Process. We will notify you of any claim subject to indemnification. You will cooperate fully in the defense and may not settle any claim without our prior written consent.</p>"},{"location":"legal/intent-terms.html#11-termination","title":"11. TERMINATION","text":"<p>11.1 Term. This Agreement begins upon your first access and continues until terminated by either party or the conclusion of the beta period.</p> <p>11.2 Termination by Spacelift. We reserve the right to terminate, suspend, or restrict your access to the Beta Product at any time, for any reason or no reason, with or without notice.</p> <p>11.3 Termination by You. You may terminate this Agreement by ceasing all use of the Beta Product and notifying us in writing.</p> <p>11.4 Effects of Termination. Upon termination: (i) all access rights and licenses cease immediately, (ii) all your data included in the Beta Product will be deleted, (iii) you must cease all use of Documentation and destroy any copies in your possession, and (iv) sections that by their nature should survive will remain in effect (including, but not limited, to confidentiality provisions).</p>"},{"location":"legal/intent-terms.html#12-general-provisions","title":"12. GENERAL PROVISIONS","text":"<p>12.1 Entire Agreement. This Agreement constitutes the entire agreement between the parties regarding the Beta Product and supersedes all prior agreements and understandings, except for any existing NDA, which shall remain in full force and effect as specified in Section 4.1.</p> <p>12.2 Modifications. We may modify this Agreement at any time. Continued use after modification constitutes acceptance. We are not bound by any additional or different terms you provide.</p> <p>12.3 Governing Law and Jurisdiction. This Agreement is governed by the laws of the State of Delaware, US, without regard to conflict of law principles. You consent to exclusive jurisdiction and venue in the courts of New Castle County, Delaware, US.</p> <p>12.4 Contact Details. For any formal notices or complaints, please contact legal@spacelift.io. In any other matters, including any questions about the use of the Beta Product, please contact us at intent@spacelift.io.</p> <p>12.5 No Waiver. Our failure to exercise or enforce any right or provision of the Agreement will not operate as a waiver of such right or provision.</p> <p>12.6 Assignment. We may assign any or all of our rights and obligations to others at any time. We will notify you of any assignment.</p> <p>12.7 Severability. If any provision of the Agreement is invalid, illegal, or unenforceable in any jurisdiction, such invalidity, illegality, or unenforceability will not affect any other provision of these Terms or invalidate or render unenforceable such provision in any other jurisdiction.</p> <p>12.8 No relationship. There is no joint venture, partnership, employment, or agency relationship created between you and us as a result of the Agreement or your use of the Beta Product.</p> <p>12.9 Force Majeure. Neither party is liable for delays or failures due to causes beyond reasonable control.</p> <p>12.10 Export Compliance. You must comply with all applicable export control laws and regulations. You represent that you are not prohibited from receiving the Beta Product under any applicable laws.</p> <p>12.11 Relationship Between Agreements. If you or your Organization has multiple agreements with Spacelift (including but not limited to service agreements, NDAs, or other beta testing agreements), each agreement shall be interpreted consistently with the others to the extent possible. In case of direct conflict regarding confidentiality obligations, the most restrictive provisions shall apply.</p>"},{"location":"legal/privacy.html","title":"Privacy","text":""},{"location":"legal/privacy.html#privacy","title":"Privacy","text":"<p>Thank you for choosing to be part of our community at Spacelift (\u201ccompany\u201d, \u201cwe\u201d, \u201cus\u201d, or \u201cour\u201d). We are committed to protecting your personal information and your right to privacy. If you have any questions or concerns about our policy, or our practices with regards to your personal information, please contact us at privacy@spacelift.io.</p> <p>When you visit our Sites and use our services, you trust us with your personal information. We take your privacy very seriously. In this privacy notice, we describe our privacy policy. We seek to explain to you in the clearest way possible what information we collect, how we use it and what rights you have in relation to it. We hope you take some time to read through it carefully, as it is important. If there are any terms in this privacy policy that you do not agree with, please discontinue use of our Sites and our services.</p> <p>This privacy policy applies to all information collected through our website and/or any related services, sales, marketing or events (we refer to them collectively in this privacy policy as the \"Sites\").</p> <p>Please read this privacy policy carefully as it will help you make informed decisions about sharing your personal information with us.</p>"},{"location":"legal/privacy.html#1-what-information-do-we-collect","title":"1. What information do we collect?","text":""},{"location":"legal/privacy.html#personal-information-you-disclose-to-us","title":"Personal information you disclose to us","text":"<p>We collect personal information that you voluntarily provide to us when registering at the Sites expressing an interest in obtaining information about us or our products and services, when participating in activities on the Sites or otherwise contacting us.</p> <p>The personal information that we collect depends on the context of your interactions with us and the site, the choices you make and the products and features you use. The personal information we collect can include the following:</p> <p>Name and Contact Data. We may collect your first and last name, email address, and other similar contact data from contact forms.</p> <p>Social Media Login Data. We provide you with the option to register using social media account details, like your GitHub. If you choose to register in this way, we will collect the Information described in the relevant section below.</p> <p>All personal information that you provide to us must be true, complete and accurate, and you must notify us of any changes to such personal information.</p>"},{"location":"legal/privacy.html#information-automatically-collected","title":"Information automatically collected","text":"<p>We automatically collect certain information when you visit, use or navigate the site. This information does not reveal your specific identity (like your name or contact information) but may include device and usage information, such as your IP address, browser and device characteristics, operating system, language preferences, referring URLs, device name, country, location, information about how and when you use our site and other technical information. This information is primarily needed to maintain the security and operation of our site, and for our internal analytics and reporting purposes.</p> <p>Like many businesses, we also collect information through cookies and similar technologies.</p>"},{"location":"legal/privacy.html#2-will-your-information-be-shared-with-anyone","title":"2. Will your information be shared with anyone?","text":"<p>We may process or share data based on the following legal basis:</p> <ul> <li> <p>Consent: We may process your data if you have given us specific consent to use your personal information for a specific purpose.</p> </li> <li> <p>Legitimate Interests: We may process your data when it is reasonably necessary to achieve our legitimate business interests.</p> </li> <li> <p>Performance of a Contract: Where we have entered into a contract with you, we may process your personal information to fulfil the terms of our contract.</p> </li> <li> <p>Legal Obligations: We may disclose your information where we are legally required to do so in order to comply with applicable law, governmental requests, a judicial proceeding, court order, or legal process, such as in response to a court order or a subpoena (including in response to public authorities to meet national security or law enforcement requirements).</p> </li> <li> <p>Vital Interests: We may disclose your information where we believe it is necessary to investigate, prevent, or take action regarding potential violations of our policies, suspected fraud, situations involving potential threats to the safety of any person and illegal activities, or as evidence in litigation in which we are involved.</p> </li> </ul> <p>More specifically, we may need to process your data or share your personal information in the following situations:</p> <ul> <li> <p>Vendors, Consultants and Other Third-Party Service Providers. We may share your data with third party vendors, service providers, contractors or agents who perform services for us or on our behalf and require access to such information to do that work. Examples include: payment processing, data analysis, email delivery, hosting services, customer service and marketing efforts. We may allow selected third parties to use tracking technology on the Sites, which will enable them to collect data about how you interact with them over time. This information may be used to, among other things, analyze and track data, determine the popularity of certain content and better understand online activity. Unless described in this Policy, we do not share, sell, rent or trade any of your information with third parties for their promotional purposes.</p> </li> <li> <p>Business Transfers. We may share or transfer your information in connection with, or during negotiations of, any merger, sale of company assets, financing, or acquisition of all or a portion of our business to another company.</p> </li> </ul>"},{"location":"legal/privacy.html#3-do-we-use-cookies-and-other-tracking-technologies","title":"3. Do we use cookies and other tracking technologies?","text":"<p>We may use cookies and similar tracking technologies (like web beacons and pixels) to access or store information. Specific information about how we use such technologies and how you can refuse certain cookies is set out in our Cookie Policy.</p>"},{"location":"legal/privacy.html#4-how-do-we-handle-your-social-logins","title":"4. How do we handle your social logins?","text":"<p>Our site offers you the ability to register and login using your third party social media account details (like your GitHub login). Where you choose to do this, we will receive certain profile information about you from your social media provider. The profile Information we receive may vary depending on the social media provider concerned, but will often include your name, e-mail address, organization membership, profile picture as well as other information you choose to make public.</p> <p>We will use the information we receive only for the purposes that are described in this privacy policy or that are otherwise made clear to you on the site. Please note that we do not control, and are not responsible for, other uses of your personal information by your third party social media provider. We recommend that you review their privacy policy to understand how they collect, use and share your personal information, and how you can set your privacy preferences on their sites and apps.</p>"},{"location":"legal/privacy.html#5-is-your-information-transferred-internationally","title":"5. Is your information transferred internationally?","text":"<p>Our servers are located in Ireland. If you are accessing our site from outside, please be aware that your information may be transferred to, stored, and processed by us in our facilities and by those third parties with whom we may share your personal information.</p> <p>If you are a resident in the European Economic Area, then these countries may not have data protection or other laws as comprehensive as those in your country. We will however take all necessary measures to protect your personal information in accordance with this privacy policy and applicable law.</p>"},{"location":"legal/privacy.html#6-how-long-do-we-keep-your-information","title":"6. How long do we keep your information?","text":"<p>We will only keep your personal information for as long as it is necessary for the purposes set out in this privacy policy, unless a longer retention period is required or permitted by law (such as tax, accounting or other legal requirements). No purpose in this policy will require us keeping your personal information for longer than 30 days.</p> <p>When we have no ongoing legitimate business need to process your personal information, we will either delete or anonymize it, or, if this is not possible (for example, because your personal information has been stored in backup archives), then we will securely store your personal information and isolate it from any further processing until deletion is possible.</p>"},{"location":"legal/privacy.html#7-how-do-we-keep-your-information-safe","title":"7. How do we keep your information safe?","text":"<p>We have implemented appropriate technical and organisational security measures designed to protect the security of any personal information we process. However, please also remember that we cannot guarantee that the internet itself is 100% secure. Although we will do our best to protect your personal information, transmission of personal information to and from our site is at your own risk. You should only access the services within a secure environment.</p>"},{"location":"legal/privacy.html#8-do-we-collect-information-from-minors","title":"8. Do we collect information from minors?","text":"<p>We do not knowingly solicit data from or market to children under 13 years of age. By using the Sites, you represent that you are at least 13 or that you are the parent or guardian of such a minor and consent to such minor dependent\u2019s use of the site. If we learn that personal information from users less than 18 years of age has been collected, we will deactivate the account and take reasonable measures to promptly delete such data from our records. If you become aware of any data we have collected from children under age 13, please contact us at privacy@spacelift.io.</p>"},{"location":"legal/privacy.html#9-what-are-your-privacy-rights","title":"9. What are your privacy rights?","text":"<p>In some regions (like the European Economic Area), you have certain rights under applicable data protection laws. These may include the right (i) to request access and obtain a copy of your personal information, (ii) to request rectification or erasure; (iii) to restrict the processing of your personal information; and (iv) if applicable, to data portability. In certain circumstances, you may also have the right to object to the processing of your personal information. To make such a request, please use the contact details provided below. We will consider and act upon any request in accordance with applicable data protection laws.</p> <p>If we are relying on your consent to process your personal information, you have the right to withdraw your consent at any time. Please note however that this will not affect the lawfulness of the processing before its withdrawal.</p> <p>If you are resident in the European Economic Area and you believe we are unlawfully processing your personal information, you also have the right to complain to your local data protection supervisory authority. You can find their contact details here: http://ec.europa.eu/justice/data-protection/bodies/authorities/index_en.htm</p>"},{"location":"legal/privacy.html#account-information","title":"Account Information","text":"<p>If you would at any time like to review or change the information in your account or terminate your account, you can:</p> <p>Upon your request to terminate your account, we will deactivate or delete your account and information from our active databases. However, some information may be retained in our files to prevent fraud, troubleshoot problems, assist with any investigations, enforce our Terms of Use and/or comply with legal requirements.</p>"},{"location":"legal/privacy.html#10-do-california-residents-have-specific-privacy-rights","title":"10. Do California residents have specific privacy rights?","text":"<p>California Civil Code Section 1798.83, also known as the \u201cShine The Light\u201d law, permits our users who are California residents to request and obtain from us, once a year and free of charge, information about categories of personal information (if any) we disclosed to third parties for direct marketing purposes and the names and addresses of all third parties with which we shared personal information in the immediately preceding calendar year. If you are a California resident and would like to make such a request, please submit your request in writing to us using the contact information provided below.</p> <p>If you are under 18 years of age, reside in California, and have a registered account with the site, you have the right to request removal of unwanted data that you publicly post on the site. To request removal of such data, please contact us using the contact information provided below, and include the email address associated with your account and a statement that you reside in California. We will make sure the data is not publicly displayed on the site, but please be aware that the data may not be completely or comprehensively removed from our systems.</p>"},{"location":"legal/privacy.html#11-how-can-you-contact-us-about-this-policy","title":"11. How can you contact us about this policy?","text":"<p>If you have questions or comments about this policy, you may email us at privacy@spacelift.io or by post to:</p> <p>Spacelift, Inc. 541 Jefferson Ave. Suite 100 Redwood City CA 94063</p> <p>For GDPR related:</p> <p>Joanna Jastrzab Spacelift ul. Nowowiejska 5/9  30-052 Cracow  POLAND</p>"},{"location":"legal/privacy.html#12-how-can-you-review-update-or-delete-the-data-we-collect-from-you","title":"12. How can you review, update, or delete the data we collect from you?","text":"<p>Based on the laws of some countries, you may have the right to request access to the personal information we collect from you, change that information, or delete it in some circumstances. To request to review, update, or delete your personal information, please email us at privacy@spacelift.io. We will respond to your request within 30 days.</p>"},{"location":"legal/refund-policy.html","title":"Refund Policy","text":""},{"location":"legal/refund-policy.html#refund-policy","title":"Refund Policy","text":""},{"location":"legal/refund-policy.html#general-provisions","title":"GENERAL PROVISIONS","text":"<p>Refunds will be made on the terms and conditions set out in this Policy in the event that the Customer withdraws from the Software purchase Agreement.</p>"},{"location":"legal/refund-policy.html#definitions","title":"DEFINITIONS","text":"<p>The following definitions are set out for the purposes of this Policy:</p> <ol> <li>Policy - these regulations for making refunds.</li> <li>Company \u2013 Spacelift, Inc., a Delaware corporation with offices located at 541 Jefferson Ave. Suite 100, Redwood City, CA 94063.</li> <li>Customer - an entity who purchases software from the Company on the basis of the Agreement.</li> <li>Software - software offered by the Company, purchased by the Customer, which is the subject of the Agreement.</li> <li>Agreement - the agreement between the Company and the Customer concerning the purchase of the Software.</li> <li>Funds - funds paid by the Customer to the Company under the Agreement for the purchase of the Software.</li> </ol>"},{"location":"legal/refund-policy.html#refund-process","title":"REFUND PROCESS","text":"<p>The Customer may withdraw from the Agreement and claim a refund of funds within 14 days after the completion of the purchase of the Software provided that the purchased Software has not been activated during that period.</p> <p>Upon receipt of notification of withdrawal, access to the Software will be blocked and verification of compliance with the conditions described in Section 3 above will begin. The verification process may take up to 10 days.</p> <p>In the case of positive verification, referred to in Section 4 above, the procedure of returning the Funds will be initiated.</p> <p>The Funds will be returned using the original payment method.</p> <p>The time taken to return the Funds may be affected by the operation of the payment provider.</p>"},{"location":"legal/refund-policy.html#final-provisions","title":"FINAL PROVISIONS","text":"<p>The current version of the Policy is available here</p> <p>This Policy is valid from 1st November 2021.</p> <p>In matters not regulated by this Policy, the Terms and Conditions shall apply.</p>"},{"location":"legal/terms.html","title":"Terms and Conditions","text":""},{"location":"legal/terms.html#terms-and-conditions","title":"Terms and Conditions","text":"<p>Last updated: July 14, 2025</p> <p>These Terms and Conditions (\u201cTerms\u201d, \u201cT&amp;C\u201d) are between the entity you represent, or, if you do not indicate an entity in connection with the Services, you individually (\u201cCustomer\u201d, \u201cyou\u201d or \u201cyour\u201d), and Spacelift, Inc. with its principal office at 541 Jefferson Ave. Suite 100, Redwood City CA 94063, United States of America (\u201cSpacelift\u201d, \u201cthe Company\u201d \u201cwe\u201d, \u201cus\u201d, or \u201cour\u201d). These terms incorporate by reference the Data Processing Agreement, the Privacy Policy, the Refund Policy and the Cookie Policy (together, they are the basis for your contractual relationship with Spacelift).</p> <p>The Terms govern the use of:</p> <ul> <li> <p>Services procured directly from Spacelift, except where a separate, mutually agreed master services agreement has been executed (such as MSA or Standard Contract for AWS Marketplace and any amendments, if Services are procured through AWS Marketplace);</p> </li> <li> <p>Services obtained through an authorized Spacelift partner, with the exception of provisions in Sections 5 and 6, which will not apply in such instances.</p> </li> </ul>"},{"location":"legal/terms.html#1-definitions","title":"1. DEFINITIONS","text":"<p>1.1. \u201cAuthorized User\u201d means Customer\u2019s employees, consultants, contractors, agents, and Workers (a) who Customer authorizes to access and use the Services under the rights granted to Customer under these Terms; and (b) for whom access to the Services has been purchased hereunder.</p> <p>1.2. \u201cConfidential Information\u201d means all nonpublic information, including, but not limited to, source code, software, trade secrets, know-how, technical drawings, algorithms, ideas, inventions, other technical, business or sales information, negotiations or proposals, disclosed by us in whatever form and which is known by Customer or its Authorized User to be confidential or under circumstances by which the receiving party should reasonably understand such information is to be treated as confidential, whether or not marked as \u201cConfidential\u201d.</p> <p>1.3. \u201cCustomer Data\u201d means information, data, and other content, in any form or medium, that is collected, downloaded, or otherwise received from Customer or an Authorized User by or through the Services. For the avoidance of doubt, Customer Data does not include Resultant Data or any other information reflecting the access or use of the Services by or on behalf of Customer or any Authorized User.</p> <p>1.4. \u201cDocumentation\u201d means any manuals, instructions, including technical specifications, or other documents or materials describing the features and functionality of the Services, which are located on the Website or provided to you, and may be updated from time to time.</p> <p>1.5. \u201cIntellectual Property Rights\u201d or \u201cIPR\u201d means any registered and unregistered rights granted, applied for, or otherwise now or hereafter in existence under or related to any patent, copyright, trademark, trade secret, database protection, or other intellectual property rights laws in any part of the world.</p> <p>1.6. \u201cMetrics\u201d - means the measurements used for quantifying the Services usage with the following meaning:</p> <ul> <li> <p>\u200b1.6.1. \u201cPrivate Minute(s)\u201d means minute(s) of Private Workers\u2019 Services usage in a given month;</p> </li> <li> <p>1.6.2. \u201cPublic Minutes\u201d means minutes of Public Worker\u2019s Services usage in a given month;</p> </li> <li> <p>1.6.3. \u201cUser(s)\u201d means Authorized User(s) who actively logged in to the Services in a given month and any virtual users, such as API keys;</p> </li> <li> <p>1.6.4. \u201cWorker(s)\u201d means a predefined set(s) of computing resources that are specifically optimized for the development and provisioning and deployment of cloud-based infrastructures based on IaC; a Worker can be either a self-hosted agent performing infrastructure management in a Customer-controlled environment (\u201cPrivate Worker\u201d) or any other software agent, provided and managed by Spacelift in a common secure worker pool (\u201cPublic Worker\u201d).</p> </li> </ul> <p>1.7.  \u201cPatch\u201d   means   a   modification   of   the   self-hosted   version   of   the   Services,   released independently of the latest Version of the Services, aimed at prompt improvement of Services\u2019 security or operation, or prompt resolution of any incidents.</p> <p>1.8. \u201cServices\u201d means the Spacelift\u2019s specialized, continuous integration and deployment (CI/CD) platform for infra-as-code;</p> <p>1.9. \u201cSubscription\u201d means enrollment for Services for a Subscription Plan on Subscription Details as defined in the Terms;</p> <p>1.10. \u201cSubscription Plan(s)\u201d means Subscription offer(s) for use of the Services as described on https://spacelift.io/pricing/, which include paid plans (\u201cPaid Subscription Plan(s)\u201d) and the Free Plan;</p> <p>1.11. \u201cSubscription Details(s)\u201d means the conditions under which a Subscription is made under the Terms, including the Subscription Period, Metrics, and Subscription Fees;</p> <p>1.12.  \u201cVersion\u201d means a version of the self-hosted Services made available to Customer for implementation in Customer Systems, which includes improvements and modifications to the previous version of the Services.</p> <p>1.13. \u201cWebsite means https://spacelift.io website managed by Spacelift.</p>"},{"location":"legal/terms.html#2-general","title":"2. GENERAL","text":"<p>2.1. Execution of the Terms. Accepting these Terms is a condition of using the Services provided by Spacelift. \u200b\u200bBY COMPLETING THE REGISTRATION PROCESS, ACCESSING OR USING THE SERVICES, YOU ACKNOWLEDGE AND AGREE THAT (I) YOU HAVE READ, UNDERSTOOD, AND ACCEPTED THESE TERMS, AND (II) YOU HEREBY REPRESENT AND WARRANT THAT YOU ARE AUTHORIZED TO ENTER OR ACT ON BEHALF OF CUSTOMER, AND BIND TO THESE TERMS. If you do not have the legal authority to enter these Terms, do not understand these Terms or do not agree to these Terms, you should not accept the Terms, or use the Services.</p> <p>2.2. Conflict of Provisions. In the event of any inconsistencies or conflict between the documents incorporated into these Terms, the documents will prevail in the following order: (a) any written document or amendment agreed upon by the parties (such as order forms or quotes); (b) Data Processing Agreement and Privacy Policy; (c) any written NDA executed between the parties: (d) these Terms and (e) the Refund Policy.</p> <p>2.3. Compliance. You are responsible for (a) compliance with the provisions of the Terms by you and your Authorized Users and for any and all acts and omissions of Authorized Users connected with their use and access to the Services and for any breach of these Terms by Authorized Users; and (b) any delay or failure of performance caused in whole or in part by your delay in performing, or failure to perform, any of your obligations under these Terms. Without limiting the foregoing, you are solely responsible for ensuring that your use of the Services is compliant with all applicable laws and regulations, as well as any and all privacy policies, agreements, or other obligations you may maintain or enter into.</p> <p>2.4. Amendments. Any individual amendment to these Terms must be made in writing (expressly stating that it is amending these Terms) and signed by both parties.</p>"},{"location":"legal/terms.html#3-license-intellectual-property-rights-and-ownership","title":"3. LICENSE, INTELLECTUAL PROPERTY RIGHTS, AND OWNERSHIP","text":"<p>3.1. Ownership. The Services, Documentation, and Website, all copies and portions thereof, and all IPR therein, including, but not limited to source code, databases, functionality, software, website designs, audio, video, text, photographs, graphics, or derivative works therefrom, are owned by us or licensed to us. You are not authorized to use (and will not permit any third party to use) the Services, Website, Documentation, or any portion thereof except as expressly authorized by the Terms. Specifically, no part of the Services, Documentation, or Website may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, modified, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission.</p> <p>3.2. Confidential Information. All our Confidential Information and derivations thereof will remain our sole and exclusive property. You should not disclose, use or publish Confidential Information without our prior written consent. You must hold all our Confidential Information in strict confidence and safeguard the Confidential Information from unauthorized use, access, or disclosure using at least the degree of care you use to protect your similarly sensitive information and in no event less than a reasonable degree of care. If parties execute a separate NDA, its provisions will govern the use and disclosure of Confidential Information.</p> <p>3.3. License. Spacelift makes the Services available to you during the Subscription Period, subject to the provisions of the Terms and Subscription Details. Spacelift grants you a limited, non-exclusive, non-sublicensable, non-transferable right to access and use the Services and its Documentation during the Subscription Period, solely for your internal business purposes or your personal use. For self-hosted deployments operated in your own environment (i.e. air-gapped, on- prem), license keys with expiration dates may be issued to enable access to the Services during the  Term.  Renewal  of   such   licenses  may  require  periodic  manual  key  updates  provided   by Spacelift.</p> <p>3.4. Restrictions. You will not, and will not permit any other person to, access or use the Services except as expressly permitted by these Terms. For purposes of clarity and without limiting the generality of the foregoing, you will not, except as the Terms expressly permit: (a) copy, modify, or create derivative works or improvements of the Services or Documentation; (b) rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer, or otherwise make available any Services or Documentation to any person; (c) reverse engineer, disassemble, decompile, decode, adapt, or otherwise attempt to derive or gain access to the source code of the Services, in whole or in part; (d) bypass or breach any security device or protection used by the Services or access or use the Services other than by an Authorized User through the use of his or her own then valid access credentials; (e) input, upload, transmit, or otherwise provide to or through the Services or Documentation, any information or materials that are unlawful, illegal, illicit or injurious, or contain, transmit, or activate any harmful code; (f) damage, destroy, disrupt, disable, impair, interfere with, or otherwise impede or harm in any manner the Services or Documentation, or our provision of services to any third party, in whole or in part; (g) remove, delete, alter, or obscure any trademarks, Documentation, warranties, or disclaimers, or IPR notices from any Services; (h) access or use the Services or Documentation in any manner or for any purpose that is illegal, illicit, unlawful or infringes, misappropriates, or otherwise violates any IPR or other right of any third party or that violates any applicable law; or (i) access or use the Services or Documentation for purposes of competitive analysis of the Services, the development, provision, or use of a competing software service or product.</p> <p>3.5. Customer Data. You are and will remain the sole and exclusive owner of all rights, title, and interest in and to all Customer Data, including all IPR, subject to the rights and permissions granted in the Terms. You have exclusive control and responsibility for determining what data you submit to the Services, for obtaining all necessary consents and permissions for the submission of Customer Data, and for the accuracy, quality, and legality of Customer Data.</p> <p>3.6. Consent to Use Customer Data. You grant all such rights and permissions in or relating to Customer Data as are reasonably necessary or useful to us to enforce these Terms and exercise our rights and perform our obligations hereunder.</p> <p>3.7. Use of Resultant Data. We may collect data and information related to your use of the Services that is used by us in an aggregate manner, including to compile statistical and performance information related to the provision and operation of the Services (\u201cResultant Data\u201d). You unconditionally and irrevocably grant to us an assignment of all rights, title, and interest in and to the Resultant Data, including all IPR relating thereto, if any. For self-hosted deployments   operated   in   Customer\u2019s   own   environment   (i.e.   air-gapped,   on-prem),   certain Resultant Data may not be collected unless Customer voluntarily submits such data via manual export.</p>"},{"location":"legal/terms.html#4-provision-of-services","title":"4. PROVISION OF SERVICES\u00bb","text":"<p>4.1. Metrics. Use of the Services is subject to usage limits reflected in Metrics, as set forth in the Subscription Plan. We will monitor your use of the Services in order to verify whether you comply with the presented limits, and, if you exceed the limits, we reserve the right to calculate additional fees (overages).</p> <p>4.2. Services Modifications. We reserve the right to make any changes to the Services or Documentation (including modifications to the scope of the features available in Subscription Plans) that we deem necessary or useful to: (a) maintain or enhance: (i) the quality or delivery of Services to you and other clients; (ii) the competitive strength of or market for Services; or (iii) the Services' cost efficiency or performance; or (b) to comply with applicable law.</p> <p>4.2.1.  If Customer has purchased a self-hosted offering, Spacelift will notify Customer of any Updates or Patches to the Software and provide reasonable implementation instructions. Customer agrees to implement any such Update or Patch without undue delay and shall, upon request, either confirm successful implementation or notify Spacelift if the Update or Patch will not be implemented, including the reasons for such decision. Customer acknowledges that timely application of Updates and Patches is essential to ensure the proper performance and security of the Software. Spacelift may (a) withhold Support Services if Customer fails to apply a Patch and (b) limit Support Services to Software Versions released within the six (6) months preceding the request for support.</p> <p>4.3. Privacy. When applicable, we will comply with all applicable laws, regulations, and government orders relating to personally identifiable information and data privacy with respect to any such Customer Data that we receive or have access to under the Terms or in connection with the performance of the Services. In particular, regulations for the protection of personally identifiable information are indicated in the Data Processing Agreement https://docs.spacelift.io/legal/dpa and Privacy Policy https://docs.spacelift.io/legal/privacy incorporated herein by reference.</p> <p>4.4. Access and Security. You will employ all physical, administrative, and technical controls, screening, security procedures, and other safeguards necessary to: (a) securely administer the distribution and use of all access credentials and protect against any unauthorized access to or use of the Services; and (b) control the content and use of Customer Data, including the uploading or other provision of Customer Data for processing by the Services.</p> <p>4.5. Security. We maintain industry-standard security and privacy certification, such as a SOC 2 Type II certification. We will use appropriate technical and organizational measures designed to prevent unauthorized access, use, alteration, or disclosure of the Services or Customer Data.</p> <p>4.6. Services and Website Management. We reserve the right at our sole discretion, to (a) monitor the Services for breaches of the Terms; (b) take appropriate legal action against anyone in breach of applicable laws or the Terms; (c) refuse, restrict access to, or availability of, or disable (to the extent technologically feasible) any of Customer Data which infringe any third party rights, applicable laws, or include illegal or illicit content; (d) remove from the Services or Website or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (e) otherwise manage the Website and Services in a manner designed to protect our rights and property and to facilitate the proper functioning of the Website and Services.</p> <p>4.7. Third-Party Materials. The Website and/or Services may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third-party websites or applications or the third-party operator. We are not responsible for and do not endorse any third-party websites or applications or their availability or content.</p> <p>4.8. Access by Third-Party Accounts. You may register and login to the Services using your third-party service providers account details, like a Google or GitHub account (\u201cThird-Party Account\u201d). When you do so, we will receive certain profile information varying on the identity provider and the information you decided to include in your Third-Party Account. You will have the ability to disable the connection between your Services account and your Third-Party Accounts at any time. Please note that your relationship with the third-party service providers associated with your Third-Party Accounts is governed solely by your agreement(s) with such third-party service providers. If a Third-Party Account or associated service becomes unavailable or the access to such Third-Party Account is terminated by the third-party service provider, then your access using such Third-Party Account may no longer be available.</p> <p>4.9. Support Services. During the Subscription Period, we will provide support services depending on the Subscription Plan, as described in https://docs.spacelift.io/product/support/ incorporated to the Terms by reference.</p> <p>4.10. Customer Systems and Cooperation. You will at all times during the Subscription Period: (a) set up, maintain, and operate in good repair and in accordance with the Documentation all your systems (meaning information technology infrastructure, including computers, software, databases, electronic systems, database management systems, and networks) on or through which the Services are accessed or used, (\"Customer Systems\"); (b) provide us with such access to your data or systems as is necessary for us to perform the Services in accordance with the Terms and Documentation; (c) use reasonable measures to prevent and promptly notify us of any unauthorized access to Authorized User accounts of which you become aware of, and (d) provide all cooperation and assistance as we may reasonably request to enable us to exercise our rights and perform our obligations under and in connection with the Terms; (e) retain sole responsibility for: (i) all Customer Data and any personally identifiable information (\u201cPII\u201d), including their content and use; (ii) all information, instructions, and materials provided by or on your behalf or by any Authorized User in connection with the Services; (iii) the security and use of your and your Authorized Users\u2019 access credentials; and (iv) all access to and use of the Services and any Provider materials directly or indirectly by or through the Customer Systems or your Authorized Users\u2019 access credentials, with or without your knowledge or consent, including all results obtained from, and all conclusions, decisions, and actions based on, such access or use.</p> <p>4.11. Quality of Services. You are aware that the quality of the Services and your use of the Services might be affected by a number of factors outside our control, including but not limited to force majeure, technical conditions, hardware or software (including third-party software and network) issues. Any delay or default affecting the availability, functionality, correctness, or timely performance of the Services caused by such circumstances will not constitute a breach of the Terms.</p> <p>4.12. No Data Backup. We do not store or backup any Customer Data. The Services do not replace the need for you to maintain regular data backups or redundant data archives.   WE HAVE NO OBLIGATION OR LIABILITY FOR ANY LOSS, ALTERATION, DESTRUCTION, DAMAGE, CORRUPTION, OR RECOVERY OF CUSTOMER DATA.</p> <p>4.13. Disclaimer. The content on the Website is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Website.</p> <p>4.14. Optional AI Feature. For some Subscription Plans, we may offer an optional SaturnHead Run Summarization Services (\u201cAI Feature\u201d) that you may choose to use at their discretion as part of the Services. If you choose to use the AI Feature, its use will be governed by the Terms of use of AI Services, available at https://docs.spacelift.io/legal/ai-terms (\u201cAI Terms\u201d), supplementing these Terms. By enabling and using the AI Feature, you confirm acknowledgment and acceptance of the AI. Use of the AI Feature is not mandatory and will not affect your access to the core Services provided under these Terms, should you choose not to enable the feature.</p>"},{"location":"legal/terms.html#5-subscription-plans-and-details","title":"5. SUBSCRIPTION PLANS AND DETAILS","text":"<p>5.1. Effective Date and Subscription Period. These Terms commence on the effective date being the day of your registration, access to, or use of the Services, whichever happens first (\u201cEffective Date\u201d). Unless earlier terminated pursuant to the provisions of the Terms, the Terms will continue through the Subscription Period of a chosen Subscription Plan.</p> <p>5.2. Subscription Plans. The Services are available under the Subscription Plans presented on https://spacelift.io/pricing/ (and incorporated to the Terms by reference), describing in detail the relevant Metrics, Subscription Fees, Subscription Period, and the scope of the features included.</p> <p>5.3. Trial. The trial offers access to Services to get to know Services before starting any of the Subscription Plans and it expires 14 days after your registration to the Services. Upon the end of trial, you may: (a) stop using the Services and delete your account thus terminating the Terms, (b) continue to use the Services under one of the available Subscriptions Plans, including the Free Plan or Paid Subscription Plans.</p> <p>5.4. Subscription Period, Renewal and Termination. Subscription Period depends on the chosen Subscription Plan:</p> <ul> <li> <p>5.4.1. the Free Plan is available for an indefinite period of time and might be terminated at any time. You may cancel the Free Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at contact@spacelift.io</p> </li> <li> <p>5.4.2. the Paid Subscription Plans are available for monthly or annual Subscription Periods, depending on the chosen Plan. If you have a self-service Paid Subscription Plan purchased directly through our website, you may cancel or downgrade your plan at any time by updating your account settings in your Spacelift account. Any cancellation or downgrade takes effect immediately on the date you make the change in your account settings. Please note that the option to cancel or downgrade only applies to self-service Paid Subscription Plans purchased independently through our website and does not apply to any Subscription Plan for which an order form was executed.  </p> </li> <li> <p>5.4.3. IF YOU DON\u2019T TERMINATE OR DOWNGRADE THE PAID SUBSCRIPTION PLAN BEFORE THE END OF A GIVEN SUBSCRIPTION PERIOD, IT WILL AUTOMATICALLY RENEW FOR A NEW SUBSCRIPTION PERIOD.</p> </li> </ul> <p>5.5. Billing. If you activate one of the Paid Subscription Plans, you authorize Spacelift to periodically charge all accrued sums on or before the payment due date on a going-forward basis until the recurring payments or your account are canceled/terminated. The \"Subscription Billing Date\" is the date when you purchase your first Paid Subscription Plan or upgrade the Plan. Your account will be automatically charged on the Subscription Billing Date for all applicable fees for the next Subscription Period. The subscription will continue unless and until you cancel your Subscription or we terminate it subject to the provisions in Section 7. You must cancel your Subscription before it renews to avoid billing the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide during registration (or to a different payment method if you change your payment information).</p> <p>5.6. Plan Adjustments and Upgrades. The Subscription Details for each plan, including Subscription Period, Fees, and Metrics may be adjusted by (a) self-service functionalities in your account settings (if available for a given Subscription Plan) or (b) written agreement of the parties.</p> <p>5.6.1. Upgrading the Subscription Plan. If you upgrade the Subscription Plan with the use of the self-service functionalities in your account, the changes to the Subscription Plan will take effect immediately. Your billing will be adjusted accordingly, and you will be charged the prorated fees for the difference between your current Subscription Plan and the upgraded Plan for the remainder of the current billing cycle.</p> <p>5.6.2. Downgrading the Subscription Plan. If you downgrade the Subscription Plan with the use of the self-service functionalities in your account, the changes to the Subscription Plan will take effect immediately, which might result in losing to the selected functionalities, such as add-ons. Your billing will be adjusted accordingly and you will be owed the prorated fees for the difference between your current Subscription Plan and the upgraded Plan for the remainder of the current billing cycle. To receive the refund, please contact ar@spacelift.io.  </p> <p>5.7. Individual Arrangements. Some of the Subscription Details of the Paid Subscription Plans are individually discussed by the parties and bind the parties on the basis of a separately executed agreement. In case a separate agreement is not executed between the parties, the written arrangements (such as order forms) regarding Subscription Details apply and in the remaining scope conditions for the use of the Services will be subject to conditions set forth in these Terms.</p> <p>5.8. Services Usage during Negotiations. If you wish to actively use the Services in the course of negotiating a separate agreement, the parties may agree on the temporary conditions of use of the Services, including the relevant Metrics, period, and fees, and in the remaining scope conditions for the use of the Services will be subject to conditions set forth in these Terms.</p> <p>5.9. Withdrawal and Refund. You may withdraw from the Terms and claim a refund of fees within 14 days after its execution, provided that the Services have not been activated during that period. You can find all the details regarding the refund in our Refund Policy.</p> <p>5.10. Fee Increases. We may increase the Subscription Fee after the end of your initial Subscription Period. Any updated Fee will apply starting from your next Subscription Period, provided we notify you at least 60 days in advance.</p> <p>5.11. Discounts. Any discounts we may offer apply only once, for a given Subscription Period. They will not carry over or apply to any renewal Subscription Period, unless we explicitly agree otherwise in an order form executed between the parties.</p>"},{"location":"legal/terms.html#6-subscription-fees","title":"6. SUBSCRIPTION FEES","text":"<p>6.1. Payment. Unless otherwise agreed by the parties in writing, Subscription Fees will be payable in USD via a credit card on a going-forward basis according to this Section 6.</p> <p>6.2. Taxes. All Subscription Fees and other amounts payable by you under the Terms are exclusive of taxes and similar assessments. Without limiting the foregoing, you are responsible for all sales, use, excise taxes, and any other similar taxes, duties, and charges of any kind, other than any taxes imposed on our income.</p> <p>6.3. Late Payment. If you fail to make any payment when due then, in addition to all other remedies that may be available: (a) we may charge interest on the past due amount at the rate of 1.5% per month calculated daily and compounded monthly or, if lower, the highest rate permitted under applicable law; (b) you will reimburse us for all reasonable costs incurred by us in collecting any late payments or interest, including attorneys' fees, court costs, and collection agency fees; and (c) if such failure continues for fourteen (14) days following written notice, we may suspend performance of the Services until all past due amounts and interest have been paid, without incurring any obligation or liability to you or any other person by reason of such suspension.</p> <p>6.4. Subscription Fees Increases. Separately from any changes in Subscription Fees due to the upgrade of the relevant Metrics, we may increase Fees for any Subscription Period before its start by providing you with a notice prior to the commencement of the next Subscription Period. Your continued use of the Services constitutes your acceptance of such changed Subscription Fees.</p>"},{"location":"legal/terms.html#7-suspension-and-termination","title":"7. SUSPENSION AND TERMINATION","text":"<p>7.1. Suspension. Without limiting any other provision of the Terms, we reserve the right to, in our sole discretion and with or without notice or liability, deny access to and use of the Services (including blocking certain IP addresses or using certain disabling device), to any person for any reason including but not limited to (a) proven or suspected breach of any representation, warranty or covenant contained in the Terms or of any applicable law or regulation; (b) your use of the Services poses a risk to the Services, our other customers, or us (including our infrastructure, security, and third-party relationships); (c) your use of the Services could subject us to liability; (d) you are past due in the payment of Subscription Fees; (e) when we receive a judicial or other governmental demand or order, subpoena, or law enforcement request that expressly or by reasonable implication requires us to do so. We will provide you with prompt notice of any suspension.</p> <p>7.2. Effect of Suspension. If we suspend your access to the Services for any reason set out in the Terms, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.</p> <p>7.3. Termination for Cause. Notwithstanding the termination for convenience as described in Section 5, either party may terminate the Terms, effective on written notice to the other party, if the other party (a) materially breaches the Terms (including any incorporated documents), especially its obligations under Sections 3.2 or 3.4, and such breach: (i) is incapable of cure; or (ii) being capable of cure, remains uncured 30 days after the non-breaching party provides the breaching party with a written notice of such breach; (b) becomes insolvent or is generally unable to pay, or fails to pay, its debts as they become due, , and such failure continues more than 14 days after Provider's delivery of written notice thereof; (c) files, or has filed against it, a petition for voluntary or involuntary bankruptcy or otherwise becomes subject, voluntarily or involuntarily, to any proceeding under any domestic or foreign bankruptcy or insolvency law; (d) makes or seeks to make a general assignment for the benefit of its creditors; or (e) applies for or has appointed a receiver, trustee, custodian, or similar agent appointed by order of any court of competent jurisdiction to take charge of or sell any material portion of its property or business.</p> <p>7.4. Effect of Termination. Upon any termination of the Terms, except as expressly otherwise provided in the Terms: (a) all rights, licenses, consents, and authorizations granted by either party to the other will immediately terminate; (b) we will immediately cease all use of any Customer Data and at your request destroy all documents and tangible materials containing or based on Customer Data and erase all Customer Data from all our systems, provided that, for clarity, our obligations under this Section 7.4 do not apply to any Resultant Data or other data that is required to establish proof of a right or a contract, which will be stored for the duration provided by applicable law; (c) you will immediately cease all use of any Services and destroy all documents and tangible materials containing or based on any our materials, including Documentation and erase all our materials from the systems you directly or indirectly control. You acknowledge and agree that you are responsible to retrieve Customer Data from the Services prior to the termination of the Terms; (d) if we terminate the Terms due to your breach, all Fees that would have become payable had the Terms remained in effect until the expiration of the Term will become immediately due and payable, and you shall pay such Fees, together with all previously-accrued but not yet paid Fees, upon receipt of our invoice therefor.</p> <p>7.5. Surviving Provisions. The provisions set forth in the following sections, and any other right or obligation of the parties in the Terms that, by its nature, should survive termination or expiration of the Terms, will survive any expiration or termination of the Terms: 3.1, 3.2, 3.4, 3.7, 7.4, 7.5, 8.4, 9, 10, 12.</p>"},{"location":"legal/terms.html#8-representations-and-warranties","title":"8. REPRESENTATIONS AND WARRANTIES","text":"<p>8.1. Mutual Representations and Warranties. Each party represents and warrants to the other party that it has the full right, power, and authority to enter into and perform its obligations and grant the rights, licenses, consents, and authorizations it grants or is required to grant under the Terms.</p> <p>8.2. Additional Spacelift Representations, Warranties, and Covenants. We represent, warrant, and covenant to you that (a) we will perform the Services using personnel of required skill, experience, and qualifications and in a professional and workmanlike manner in accordance with generally recognized industry standards and will devote adequate resources to meet our obligations under the Terms; (b) the Services will be performed in accordance with generally recognized industry standards for similar services; (c) we will use all commercially reasonable efforts to ensure that the Services are free from any viruses, worms, malware, or other malicious source code.</p> <p>8.3. Additional Customer Representations, Warranties, and Covenants. You represent, warrant, and covenant to us that (a) you own or otherwise have and will have the necessary rights and consents in and relating to Customer Data so that, as received by us and processed in accordance with the Terms, they do not and will not infringe, misappropriate, or otherwise violate any IPR, or any privacy or other rights of any third party or violate any applicable law; (b) all registration information you submit will be true, accurate, current, complete and relate to you and not a third party; (c) you will maintain the accuracy of such information and promptly update such information as necessary; (d) you will keep your access credentials confidential and will be responsible for all use of your access credentials; (e) you are aware that you may not access or use the Services for any purpose other than that for which we make the Services available, (f) you are at least eighteen years of age and (g) you confirm the warranties set in Section 12.7.</p> <p>8.4. DISCLAIMER OF WARRANTIES. EXCEPT FOR THE EXPRESS WARRANTIES SET FORTH IN THIS SECTION 8, ALL SERVICES, DOCUMENTATION, AND WEBSITE ARE PROVIDED \"AS IS.\" WE SPECIFICALLY DISCLAIM ALL IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT, AND ALL WARRANTIES ARISING FROM THE COURSE OF DEALING, USAGE, OR TRADE PRACTICE. WITHOUT LIMITING THE FOREGOING, WE MAKE NO WARRANTY OF ANY KIND THAT THE SERVICES, DOCUMENTATION, PROVIDER SYSTEMS, ANY PROVIDER MATERIALS OR WEBSITE, OR ANY PRODUCTS OR RESULTS OF THE USE THEREOF, WILL MEET YOUR OR ANY OTHER PERSON'S REQUIREMENTS, OPERATE WITHOUT INTERRUPTION, ACHIEVE ANY INTENDED RESULT, BE COMPATIBLE OR WORK WITH ANY SOFTWARE, SYSTEM, OR OTHER SERVICES, OR BE SECURE, ACCURATE, COMPLETE, FREE OF HARMFUL CODE, OR ERROR-FREE. ALL THIRD-PARTY MATERIALS ARE PROVIDED \"AS IS\" AND ANY REPRESENTATION OR WARRANTY OF OR CONCERNING ANY THIRD-PARTY MATERIALS IS STRICTLY BETWEEN CUSTOMER AND THE THIRD-PARTY OWNER OR DISTRIBUTOR OF THE THIRD-PARTY MATERIALS.</p>"},{"location":"legal/terms.html#9-indemnification","title":"9. INDEMNIFICATION","text":"<p>9.1. Spacelift Indemnification. Subject to the remainder of this Section 9 and the liability limitations set forth in Section 10, we will indemnify, defend, and hold you harmless from and against any and all losses incurred by you resulting from any action by a third party, that your use of the Services (excluding Customer Data and any third-party materials) in accordance with these Terms infringes or misappropriates IPR. The foregoing obligation does not apply to the extent that the alleged infringement arises from (a) any third-party materials or Customer Data; (b) access to or use of the Services in combination with any hardware, system, software, network, or other materials or service not provided by us; (c) failure to timely implement any modifications, upgrades, replacements, or enhancements made available to you by us or on our behalf; or (d) use of the Services other than in accordance with the Terms and the Documentation. THIS SECTION 9 SETS FORTH YOUR SOLE REMEDIES AND SPACELIFT\u2019S SOLE LIABILITY AND OBLIGATION FOR ANY ACTUAL, THREATENED, OR ALLEGED CLAIMS THAT THE SERVICES, DOCUMENTATION, WEBSITE AND ANY OTHER SPACELIFT MATERIALS OR ANY SUBJECT MATTER OF THE TERMS INFRINGE, MISAPPROPRIATE OR OTHERWISE VIOLATE ANY INTELLECTUAL PROPERTY RIGHTS OF ANY THIRD PARTY.</p> <p>9.2. Mitigation. If the Services, Documentation, or any of the other Spacelift\u2019s materials are, or in our opinion are likely to be, claimed to infringe, misappropriate, or otherwise violate any third-party IPR, or if you or your Authorized User's use of the Services, Documentation or other Spacelift\u2019s materials is enjoined or threatened to be enjoined, we may, at our option and sole cost and expense: (a) obtain the right for you to continue to use the Services and said materials materially as contemplated by the Terms, or (b) modify or replace the Services and said materials.</p> <p>9.3. Customer Indemnification. You will indemnify, defend, and hold us harmless from and against any and all losses incurred by us resulting from any action by a third party to the extent that such losses arise out of or result from, or are alleged to arise out of or result from: (a) your use of the Services; (b) Customer Data, including any processing of Customer Data by us or on our behalf in accordance with the Terms; (c) any other materials or information (including any documents, data, or technology) provided by you or on your behalf, (d) your breach of any of its representations, warranties, covenants, or obligations under the Terms; or (e) negligence or more culpable act or omission (including recklessness or willful misconduct) by you, any Authorized User, or any third party acting on your behalf or any Authorized User, in connection with the Terms, provided, that you will have no obligation under this Section 9.3 to the extent the applicable claim arises from Spacelift\u2019s breach of the Terms.</p> <p>9.4. Indemnification Procedure. Each party\u2019s indemnification obligations are conditioned on the indemnified party: (a) promptly giving written notice of the claim to the indemnifying party; (b) giving the indemnifying party sole control of the defense and settlement of the claim; and (c) providing to the indemnifying party all available information and assistance in connection with the claim, at the indemnifying party\u2019s request and expense. The indemnified party may participate in the defense of the claim, at the indemnified party\u2019s sole expense (not subject to reimbursement). Neither party may admit liability for or consent to any judgment or concede or settle or compromise any claim unless such admission or concession or settlement or compromise includes a full and unconditional release of the other party from all liabilities in respect of the such claim.</p>"},{"location":"legal/terms.html#10-liability","title":"10. LIABILITY","text":"<p>10.1. Exclusion of Liability In no event will Spacelift have any obligation or liability arising from (a) use or inability to use any Services if modified or combined with materials not provided by us; (b) statements or conduct of any third party on or in the Services, (c) any Customer Data, (d) any failure by Customer to comply with the Terms; and (e) damages suffered by Customer or Authorized Users, or any other person having arisen due to the third-party claims (other than described in Section 9.1), (f)suspension or termination of the Services, or (g)  any other reasons arising from Customer\u2019s fault or responsibility.</p> <p>10.2. EXCLUSION OF DAMAGES. EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4 AND TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL SPACELIFT OR ANY OF ITS LICENSORS OR SERVICE PROVIDERS BE LIABLE UNDER OR IN CONNECTION WITH THESE TERMS OR ITS SUBJECT MATTER UNDER ANY LEGAL OR EQUITABLE THEORY, INCLUDING BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, AND OTHERWISE, FOR ANY: (a) LOSS OF PRODUCTION, USE, BUSINESS, REVENUE, OR PROFIT OR DIMINUTION IN VALUE; (b) IMPAIRMENT, INABILITY TO USE OR LOSS, INTERRUPTION, OR DELAY OF THE SERVICES; (c) LOSS, DAMAGE, CORRUPTION, OR RECOVERY OF DATA, OR BREACH OF DATA OR SYSTEM SECURITY; (d) COST OF REPLACEMENT GOODS OR SERVICES; (e) LOSS OF GOODWILL OR REPUTATION; OR (f) CONSEQUENTIAL, INCIDENTAL, INDIRECT, EXEMPLARY, SPECIAL, ENHANCED, OR PUNITIVE DAMAGES, REGARDLESS OF WHETHER SUCH PERSONS WERE ADVISED OF THE POSSIBILITY OF SUCH LOSSES OR DAMAGES OR SUCH LOSSES OR DAMAGES WERE OTHERWISE FORESEEABLE, AND NOTWITHSTANDING THE FAILURE OF ANY AGREED OR OTHER REMEDY OF ITS ESSENTIAL PURPOSE.</p> <p>10.3. CAP ON MONETARY LIABILITY. SPACELIFT WILL ONLY BE LIABLE FOR DIRECT DAMAGES EXCLUDING ANY SITUATION FOR WHICH WE ARE NOT RESPONSIBLE OR WHICH ARE CAUSED BY EVENTS OUTSIDE OUR REASONABLE CONTROL. HOWEVER, EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4, IN NO EVENT WILL THE COLLECTIVE AGGREGATE LIABILITY OF SPACELIFT ARISING OUT OF OR RELATED TO THESE TERMS, WHETHER ARISING UNDER OR RELATED TO BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, OR ANY OTHER LEGAL OR EQUITABLE THEORY, EXCEED THE GREATER OF (a) THE TOTAL AMOUNTS PAID TO SPACELIFT UNDER THESE TERMS IN THE 6 MONTH PERIOD PRECEDING THE EVENT GIVING RISE TO THE CLAIM OR (b) THE AMOUNT OF 5000 USD. THE FOREGOING LIMITATIONS APPLY EVEN IF ANY REMEDY FAILS OF ITS ESSENTIAL PURPOSE.</p> <p>10.4. Exceptions. NOTHING IN THIS SECTION 10 WILL BE DEEMED TO LIMIT EITHER PARTY\u2019S LIABILITY FOR WILLFUL MISCONDUCT, GROSS NEGLIGENCE, FRAUD, OR INFRINGEMENT BY ONE PARTY OF THE OTHER\u2019S INTELLECTUAL PROPERTY RIGHTS.</p>"},{"location":"legal/terms.html#11-provisions-relating-to-consumers","title":"11. PROVISIONS RELATING TO CONSUMERS","text":"<p>11.1. Right to Withdraw. If you are a natural person and have your habitual residence within a Member State of the European Union or the European Economic Area and are entering into the Terms as a consumer (i.e. for purposes which are outside your trade, business, craft or profession), you have the right to withdraw from the contract as described below.</p> <p>11.2. Withdrawal Period. You have the right to withdraw from the Terms (concluded under any Subscription Plan) within 14 days without giving any reason. The withdrawal right will expire after 14 days from the day of the conclusion of the Terms.</p> <p>11.3. Exercise of the Right to Withdraw. To exercise the right of withdrawal, you must inform us, Spacelift, Inc., of your decision to withdraw from the Terms by an unequivocal statement (e.g. an e-mail sent to legal@spacelift.io). To meet the withdrawal deadline, it is sufficient for you to send your communication concerning your exercise of the right of withdrawal before the withdrawal period expires.</p> <p>11.4. Model Withdrawal Form. To exercise your right of withdrawal, you may use the model withdrawal form, included in Appendix No. 2 to the Act on Consumer Rights of May 20, 2014, but this is not obligatory.</p> <p>11.5. Effect of the Withdrawal. If you withdraw from the Terms, we will reimburse you all payments received from you, without undue delay and in any event not later than 14 days from the day on which we are informed about your decision to withdraw from the Terms. We will carry out such reimbursement using the same means of payment as you used for the initial transaction unless you have expressly agreed otherwise.</p> <p>11.6. Consumer Rights. Nothing in the Terms will affect your legal rights as a consumer. If any provision of the Terms does not comply with the applicable law for you as a consumer, the applicable law will apply instead of this provision. The severability clause equally applies. In case of any concerns, questions, or doubts, contact us at legalt@spacelift.io</p> <p>11.7. Complaints. If you have a complaint about Services, you should contact us at contact@spacelift.io, providing as much detail as possible about the complaint, together with your name, the Terms\u2019s Effective Date, and expected means of settling a complaint. We will respond by confirming receipt and will investigate the matter. Upon receiving the complaint, we will investigate the complaint internally, taking into account the importance and complexity of the issue raised, and get back to you no later than 30 days from the receipt of the complaint.</p> <p>11.8. ADR. If you are a consumer, you may consider Alternative Dispute Resolution means in the event of a dispute with us, including referring to the trade inspection, a consumer ombudsman, or an organization whose statutory tasks include consumer protection.</p>"},{"location":"legal/terms.html#12-final-provisions","title":"12. FINAL PROVISIONS","text":"<p>12.1. Current Version of the Terms. Usage of the Services is subject to the then-current version of the Terms posted on the Website and we advise you to periodically review the latest currently effective Terms. We reserve the right to update the provisions of the Terms from time to time at our sole discretion. The updated Terms version supersedes all prior versions, as well as is effective and binding immediately after posting on the Website. Your continued use of the Services on or after the date of the updated version of the Terms is effective and constitutes your acceptance of such updated provisions. If you do not agree to our updated Terms, you can terminate the Subscription in accordance with Section 5.</p> <p>12.2. Applicable Law and Jurisdiction. These Terms are governed by and construed in accordance with the Applicable Law (stated below) without giving effect to any choice or conflict of law provision of any jurisdiction. Any legal suit, action, or proceeding arising out of or related to these Terms will be subject to the exclusive jurisdiction of the Applicable Jurisdiction as provided in the following table:</p> Client Applicable Law Applicable Jurisdiction Consumers residing in the Member State of the European Union or the European Economic Area Poland Warsaw, Poland Other Customers State of Delaware, US County New Castle, Delaware, US <p>Each party irrevocably submits to the exclusive jurisdiction of such courts in any such suit, action, or proceeding.</p> <p>12.3. Contact details. For any formal notices or complaints, please contact legal@spacelift.io. In any other matters, including any inquiry about the use of the Services, please contact us at contact@spacelift.io.</p> <p>12.4. Notices. Except as otherwise expressly set forth in the Terms, any notice, request, consent, claim, demand, waiver, or other communications under these Terms have legal effect and will be deemed effectively given: (a) when received, if delivered by hand or with signed confirmation of receipt; (b) when received, if sent by a nationally recognized overnight courier or by certified or registered mail, signature required; or (c) when sent, if by email, if sent during the addressee's normal business hours, and on the next business day, if sent after the addressee's normal business hours.</p> <p>12.5. Feedback. If you provide us with any suggestions, comments, recommendations, opinions, or other information relating to the Services or Website (\u201cFeedback\u201d), you grant us a royalty-free, non-exclusive, irrevocable, perpetual, worldwide right and license to use the Feedback on our websites or in marketing materials. We reserve the right to remove any Feedback posted on the Website if, in our opinion, such Feedback does not comply with the Terms or applicable law.</p> <p>12.6. Logo usage. You grant us the right to use your name and other indicia, such as logo or trademark in our list of current or former clients in promotional materials and on our websites. Any other announcement, statement, press release, or other publicity or marketing materials relating to your use of Services will be subject to your consent.</p> <p>12.7. Export Laws. Each party will comply with the export laws and regulations of the United States and other applicable jurisdictions in providing and using the Services. Without limiting the generality of the foregoing, you represent and warrant that you are: (a) not a resident of a country sanctioned by the Office of Foreign Assets Control, Department of the Treasury (\u201cOFAC\u201d); (b) not currently identified on the Specially Designated Nationals and Blocked Persons List maintained by OFAC and/or on any other similar list maintained by the OFAC pursuant to any authorizing statute, executive order or regulation; (c) not a person or entity with whom a citizen of the United States is prohibited to engage in transactions by any trade embargo, economic sanction, or other prohibition of United States law, regulation, or executive order of the President of the United States; (d) not engaged in any activity or conduct that would breach any anti-corruption laws or anti-money laundering laws; and (e) not currently under investigation by any governmental authority for alleged criminal activity relating to the OFAC, Patriot Act Offenses, anti-corruption laws or anti-money laundering laws.</p> <p>12.8. Non-waiver. Our failure to exercise or enforce any right or provision of the Terms will not operate as a waiver of such right or provision.</p> <p>12.9. Assignment. We may assign any or all of our rights and obligations to others at any time. We will notify you of any assignment.</p> <p>12.0. Severability. If any provision of the Terms is invalid, illegal, or unenforceable in any jurisdiction, such invalidity, illegality, or unenforceability will not affect any other provision of these Terms or invalidate or render unenforceable such provision in any other jurisdiction.</p> <p>12.11. No relationship. There is no joint venture, partnership, employment, or agency relationship created between you and us as a result of the Terms or use of the Services.</p>"},{"location":"legal/archive/terms.html","title":"Terms and Conditions","text":""},{"location":"legal/archive/terms.html#terms-and-conditions","title":"Terms and Conditions","text":"<p>Effective from November 4, 2024 till June 9, 2025</p> <p>These Terms and Conditions (\u201cTerms\u201d, \u201cT&amp;C\u201d) are between the entity you represent, or, if you do not indicate an entity in connection with the Services, you individually (\u201cCustomer\u201d, \u201cyou\u201d or \u201cyour\u201d), and Spacelift, Inc. with its principal office at 541 Jefferson Ave. Suite 100, Redwood City CA 94063, United States of America (\u201cSpacelift\u201d, \u201cthe Company\u201d \u201cwe\u201d, \u201cus\u201d, or \u201cour\u201d). These terms incorporate by reference the Data Processing Agreement, the Privacy Policy, the Refund Policy and the Cookie Policy (together, they are the basis for your contractual relationship with Spacelift).</p> <p>The Terms govern the use of:</p> <ul> <li> <p>Services procured directly from Spacelift, except where a separate, mutually agreed master services agreement has been executed (such as Standard Contract for AWS Marketplace and any amendments, if Services are procured through AWS Marketplace);</p> </li> <li> <p>Services obtained through an authorized Spacelift partner, with the exception of provisions in Sections 5 and 6, which will not apply in such instances.</p> </li> </ul>"},{"location":"legal/archive/terms.html#1-definitions","title":"1. DEFINITIONS","text":"<p>1.1. \u201cAuthorized User\u201d means Customer\u2019s employees, consultants, contractors, agents, and Workers (a) who Customer authorizes to access and use the Services under the rights granted to Customer under these Terms; and (b) for whom access to the Services has been purchased hereunder.</p> <p>1.2. \u201cConfidential Information\u201d means all nonpublic information, including, but not limited to, source code, software, trade secrets, know-how, technical drawings, algorithms, ideas, inventions, other technical, business or sales information, negotiations or proposals, disclosed by us in whatever form and which is known by Customer or its Authorized User to be confidential or under circumstances by which the receiving party should reasonably understand such information is to be treated as confidential, whether or not marked as \u201cConfidential\u201d.</p> <p>1.3. \u201cCustomer Data\u201d means information, data, and other content, in any form or medium, that is collected, downloaded, or otherwise received from Customer or an Authorized User by or through the Services. For the avoidance of doubt, Customer Data does not include Resultant Data or any other information reflecting the access or use of the Services by or on behalf of Customer or any Authorized User.</p> <p>1.4. \u201cDocumentation\u201d means any manuals, instructions, including technical specifications, or other documents or materials describing the features and functionality of the Services, which are located on the Website or provided to you, and may be updated from time to time.</p> <p>1.5. \u201cIntellectual Property Rights\u201d or \u201cIPR\u201d means any registered and unregistered rights granted, applied for, or otherwise now or hereafter in existence under or related to any patent, copyright, trademark, trade secret, database protection, or other intellectual property rights laws in any part of the world.</p> <p>1.6. \u201cMetrics\u201d - means the measurements used for quantifying the Services usage with the following meaning:</p> <ul> <li> <p>\u200b1.6.1. \u201cPrivate Minute(s)\u201d means minute(s) of Private Workers\u2019 Services usage in a given month;</p> </li> <li> <p>1.6.2. \u201cPublic Minutes\u201d means minutes of Public Worker\u2019s Services usage in a given month;</p> </li> <li> <p>1.6.3. \u201cUser(s)\u201d means Authorized User(s) who actively logged in to the Services in a given month and any virtual users, such as API keys;</p> </li> <li> <p>1.6.4. \u201cWorker(s)\u201d means a predefined set(s) of computing resources that are specifically optimized for the development and provisioning and deployment of cloud-based infrastructures based on IaC; a Worker can be either a self-hosted agent performing infrastructure management in a Customer-controlled environment (\u201cPrivate Worker\u201d) or any other software agent, provided and managed by Spacelift in a common secure worker pool (\u201cPublic Worker\u201d).</p> </li> </ul> <p>1.7. \u201cServices\u201d means the Spacelift\u2019s specialized, continuous integration and deployment (CI/CD) platform for infra-as-code;</p> <p>1.8. \u201cSubscription\u201d means enrollment for Services for a Subscription Plan on Subscription Details as defined in the Terms;</p> <p>1.9. \u201cSubscription Plan(s)\u201d means Subscription offer(s) for use of the Services as described on https://spacelift.io/pricing/, which include paid plans (\u201cPaid Subscription Plan(s)\u201d) and the Free Plan;</p> <p>1.10. \u201cSubscription Details(s)\u201d means the conditions under which a Subscription is made under the Terms, including the Subscription Period, Metrics, and Subscription Fees;</p> <p>1.11. \u201cWebsite means https://spacelift.io website managed by Spacelift.</p>"},{"location":"legal/archive/terms.html#2-general","title":"2. GENERAL","text":"<p>2.1. Execution of the Terms. Accepting these Terms is a condition of using the Services provided by Spacelift. \u200b\u200bBY COMPLETING THE REGISTRATION PROCESS, ACCESSING OR USING THE SERVICES, YOU ACKNOWLEDGE AND AGREE THAT (I) YOU HAVE READ, UNDERSTOOD, AND ACCEPTED THESE TERMS, AND (II) YOU HEREBY REPRESENT AND WARRANT THAT YOU ARE AUTHORIZED TO ENTER OR ACT ON BEHALF OF CUSTOMER, AND BIND TO THESE TERMS. If you do not have the legal authority to enter these Terms, do not understand these Terms or do not agree to these Terms, you should not accept the Terms, or use the Services.</p> <p>2.2. Conflict of Provisions. In the event of any inconsistencies or conflict between the documents incorporated into these Terms, the documents will prevail in the following order: (a) any written amendment agreed upon by the parties (such as order forms); (b) Data Processing Agreement and Privacy Policy; (c) these Terms and (d) the Refund Policy.</p> <p>2.3. Compliance. You are responsible for (a) compliance with the provisions of the Terms by you and your Authorized Users and for any and all acts and omissions of Authorized Users connected with their use and access to the Services and for any breach of these Terms by Authorized Users; and (b) any delay or failure of performance caused in whole or in part by your delay in performing, or failure to perform, any of your obligations under these Terms. Without limiting the foregoing, you are solely responsible for ensuring that your use of the Services is compliant with all applicable laws and regulations, as well as any and all privacy policies, agreements, or other obligations you may maintain or enter into.</p> <p>2.4. Amendments. Any individual amendment to these Terms must be made in writing (expressly stating that it is amending these Terms) and signed by both parties.</p>"},{"location":"legal/archive/terms.html#3-license-intellectual-property-rights-and-ownership","title":"3. LICENSE, INTELLECTUAL PROPERTY RIGHTS, AND OWNERSHIP","text":"<p>3.1. Ownership. The Services, Documentation, and Website, all copies and portions thereof, and all IPR therein, including, but not limited to source code, databases, functionality, software, website designs, audio, video, text, photographs, graphics, or derivative works therefrom, are owned by us or licensed to us. You are not authorized to use (and will not permit any third party to use) the Services, Website, Documentation, or any portion thereof except as expressly authorized by the Terms. Specifically, no part of the Services, Documentation, or Website may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, modified, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission.</p> <p>3.2. Confidential Information. All our Confidential Information and derivations thereof will remain our sole and exclusive property. You should not disclose, use or publish Confidential Information without our prior written consent. You must hold all our Confidential Information in strict confidence and safeguard the Confidential Information from unauthorized use, access, or disclosure using at least the degree of care you use to protect your similarly sensitive information and in no event less than a reasonable degree of care.</p> <p>3.3. License. Spacelift makes the Services available to you during the Subscription Period, subject to the provisions of the Terms and Subscription Details. Spacelift grants you a limited, non-exclusive, non-sublicensable, non-transferable right to access and use the Services and its Documentation during the Subscription Period, solely for your internal business purposes or your personal use.</p> <p>3.4. Restrictions. You will not, and will not permit any other person to, access or use the Services except as expressly permitted by these Terms. For purposes of clarity and without limiting the generality of the foregoing, you will not, except as the Terms expressly permit: (a) copy, modify, or create derivative works or improvements of the Services or Documentation; (b) rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer, or otherwise make available any Services or Documentation to any person; (c) reverse engineer, disassemble, decompile, decode, adapt, or otherwise attempt to derive or gain access to the source code of the Services, in whole or in part; (d) bypass or breach any security device or protection used by the Services or access or use the Services other than by an Authorized User through the use of his or her own then valid access credentials; (e) input, upload, transmit, or otherwise provide to or through the Services or Documentation, any information or materials that are unlawful, illegal, illicit or injurious, or contain, transmit, or activate any harmful code; (f) damage, destroy, disrupt, disable, impair, interfere with, or otherwise impede or harm in any manner the Services or Documentation, or our provision of services to any third party, in whole or in part; (g) remove, delete, alter, or obscure any trademarks, Documentation, warranties, or disclaimers, or IPR notices from any Services; (h) access or use the Services or Documentation in any manner or for any purpose that is illegal, illicit, unlawful or infringes, misappropriates, or otherwise violates any IPR or other right of any third party or that violates any applicable law; or (i) access or use the Services or Documentation for purposes of competitive analysis of the Services, the development, provision, or use of a competing software service or product.</p> <p>3.5. Customer Data. You are and will remain the sole and exclusive owner of all rights, title, and interest in and to all Customer Data, including all IPR, subject to the rights and permissions granted in the Terms. You have exclusive control and responsibility for determining what data you submit to the Services, for obtaining all necessary consents and permissions for the submission of Customer Data, and for the accuracy, quality, and legality of Customer Data.</p> <p>3.6. Consent to Use Customer Data. You grant all such rights and permissions in or relating to Customer Data as are reasonably necessary or useful to us to enforce these Terms and exercise our rights and perform our obligations hereunder.</p> <p>3.7. Use of Resultant Data. We may collect data and information related to your use of the Services that is used by us in an aggregate manner, including to compile statistical and performance information related to the provision and operation of the Services (\u201cResultant Data\u201d). You unconditionally and irrevocably grant to us an assignment of all rights, title, and interest in and to the Resultant Data, including all IPR relating thereto, if any.</p>"},{"location":"legal/archive/terms.html#4-provision-of-services","title":"4. PROVISION OF SERVICES\u00bb","text":"<p>4.1. Metrics. Use of the Services is subject to usage limits reflected in Metrics, as set forth in the Subscription Plan. We will monitor your use of the Services in order to verify whether you comply with the presented limits, and, if you exceed the limits, we reserve the right to calculate additional fees (overages).</p> <p>4.2. Services Modifications. We reserve the right to make any changes to the Services or Documentation (including modifications to the scope of the features available in Subscription Plans) that we deem necessary or useful to: (a) maintain or enhance: (i) the quality or delivery of Services to you and other clients; (ii) the competitive strength of or market for Services; or (iii) the Services' cost efficiency or performance; or (b) to comply with applicable law.</p> <p>4.3. Privacy. When applicable, we will comply with all applicable laws, regulations, and government orders relating to personally identifiable information and data privacy with respect to any such Customer Data that we receive or have access to under the Terms or in connection with the performance of the Services. In particular, regulations for the protection of personally identifiable information are indicated in the Data Processing Agreement https://docs.spacelift.io/legal/dpa and Privacy Policy https://docs.spacelift.io/legal/privacy incorporated herein by reference.</p> <p>4.4. Access and Security. You will employ all physical, administrative, and technical controls, screening, security procedures, and other safeguards necessary to: (a) securely administer the distribution and use of all access credentials and protect against any unauthorized access to or use of the Services; and (b) control the content and use of Customer Data, including the uploading or other provision of Customer Data for processing by the Services.</p> <p>4.5. Security. We maintain industry-standard security and privacy certification, such as a SOC 2 Type II certification. We will use appropriate technical and organizational measures designed to prevent unauthorized access, use, alteration, or disclosure of the Services or Customer Data.</p> <p>4.6. Services and Website Management. We reserve the right at our sole discretion, to (a) monitor the Services for breaches of the Terms; (b) take appropriate legal action against anyone in breach of applicable laws or the Terms; (c) refuse, restrict access to, or availability of, or disable (to the extent technologically feasible) any of Customer Data which infringe any third party rights, applicable laws, or include illegal or illicit content; (d) remove from the Services or Website or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (e) otherwise manage the Website and Services in a manner designed to protect our rights and property and to facilitate the proper functioning of the Website and Services.</p> <p>4.7. Third-Party Materials. The Website and/or Services may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third-party websites or applications or the third-party operator. We are not responsible for and do not endorse any third-party websites or applications or their availability or content.</p> <p>4.8. Access by Third-Party Accounts. You may register and login to the Services using your third-party service providers account details, like a Google or GitHub account (\u201cThird-Party Account\u201d). When you do so, we will receive certain profile information varying on the identity provider and the information you decided to include in your Third-Party Account. You will have the ability to disable the connection between your Services account and your Third-Party Accounts at any time. Please note that your relationship with the third-party service providers associated with your Third-Party Accounts is governed solely by your agreement(s) with such third-party service providers. If a Third-Party Account or associated service becomes unavailable or the access to such Third-Party Account is terminated by the third-party service provider, then your access using such Third-Party Account may no longer be available.</p> <p>4.9. Support Services. During the Subscription Period, we will provide support services depending on the Subscription Plan, as described in https://docs.spacelift.io/product/support/ incorporated to the Terms by reference.</p> <p>4.10. Customer Systems and Cooperation. You will at all times during the Subscription Period: (a) set up, maintain, and operate in good repair and in accordance with the Documentation all your systems (meaning information technology infrastructure, including computers, software, databases, electronic systems, database management systems, and networks) on or through which the Services are accessed or used; (b) provide us with such access to your data or systems as is necessary for us to perform the Services in accordance with the Terms and Documentation; (c) use reasonable measures to prevent and promptly notify us of any unauthorized access to Authorized User accounts of which you become aware of, and (d) provide all cooperation and assistance as we may reasonably request to enable us to exercise our rights and perform our obligations under and in connection with the Terms.</p> <p>4.11. Quality of Services. You are aware that the quality of the Services and your use of the Services might be affected by a number of factors outside our control, including but not limited to force majeure, technical conditions, hardware or software (including third-party software and network) issues. Any delay or default affecting the availability, functionality, correctness, or timely performance of the Services caused by such circumstances will not constitute a breach of the Terms.</p> <p>4.12. No Data Backup. We do not store or backup any Customer Data. The Services do not replace the need for you to maintain regular data backups or redundant data archives.   WE HAVE NO OBLIGATION OR LIABILITY FOR ANY LOSS, ALTERATION, DESTRUCTION, DAMAGE, CORRUPTION, OR RECOVERY OF CUSTOMER DATA.</p> <p>4.13. Disclaimer. The content on the Website is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Website.</p>"},{"location":"legal/archive/terms.html#5-subscription-plans-and-details","title":"5. SUBSCRIPTION PLANS AND DETAILS","text":"<p>5.1. Effective Date and Subscription Period. These Terms commence on the effective date being the day of your registration, access to, or use of the Services, whichever happens first (\u201cEffective Date\u201d). Unless earlier terminated pursuant to the provisions of the Terms, the Terms will continue through the Subscription Period of a chosen Subscription Plan.</p> <p>5.2. Subscription Plans. The Services are available under the Subscription Plans presented on https://spacelift.io/pricing/ (and incorporated to the Terms by reference), describing in detail the relevant Metrics, Subscription Fees, Subscription Period, and the scope of the features included.</p> <p>5.3. Trial. The trial offers access to Services to get to know Services before starting any of the Subscription Plans and it expires 14 days after your registration to the Services. Upon the end of trial, you may: (a) stop using the Services and delete your account thus terminating the Terms, (b) continue to use the Services under one of the available Subscriptions Plans, including the Free Plan or Paid Subscription Plans.</p> <p>5.4. Subscription Period, Renewal and Termination. Subscription Period depends on the chosen Subscription Plan:</p> <ul> <li> <p>5.4.1. the Free Plan is available for an indefinite period of time and might be terminated at any time. You may cancel the Free Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at contact@spacelift.io</p> </li> <li> <p>5.4.2. the Paid Subscription Plans are available for monthly or annual Subscription Periods, depending on the chosen Plan. You may cancel the chosen Paid Subscription Plan or downgrade it at any time by accessing your account settings in your Spacelift account. The termination or downgrade will be effective on the date you changed your account settings. If you don\u2019t terminate or downgrade the Paid Subscription Plan before the end of a given Subscription Period, it will automatically renew for a new Subscription Period.</p> </li> </ul> <p>5.5. Billing. If you activate one of the Paid Subscription Plans, you authorize Spacelift to periodically charge all accrued sums on or before the payment due date on a going-forward basis until the recurring payments or your account are canceled/terminated. The \"Subscription Billing Date\" is the date when you purchase your first Paid Subscription Plan or upgrade the Plan. Your account will be automatically charged on the Subscription Billing Date for all applicable fees for the next Subscription Period. The subscription will continue unless and until you cancel your Subscription or we terminate it subject to the provisions in Section 7. You must cancel your Subscription before it renews to avoid billing the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide during registration (or to a different payment method if you change your payment information).</p> <p>5.6. Plan Adjustments and Upgrades. The Subscription Details for each plan, including Subscription Period, Fees, and Metrics may be adjusted by (a) self-service functionalities in your account settings (if available for a given Subscription Plan) or (b) written agreement of the parties.</p> <p>5.6.1. Upgrading the Subscription Plan. If you upgrade the Subscription Plan with the use of the self-service functionalities in your account, the changes to the Subscription Plan will take effect immediately. Your billing will be adjusted accordingly, and you will be charged the prorated fees for the difference between your current Subscription Plan and the upgraded Plan for the remainder of the current billing cycle.</p> <p>5.6.2. Downgrading the Subscription Plan. If you downgrade the Subscription Plan with the use of the self-service functionalities in your account, the changes to the Subscription Plan will take effect immediately, which might result in losing to the selected functionalities, such as add-ons. Your billing will be adjusted accordingly and you will be owed the prorated fees for the difference between your current Subscription Plan and the upgraded Plan for the remainder of the current billing cycle. To receive the refund, please contact ar@spacelift.io.  </p> <p>5.7. Individual Arrangements. Some of the Subscription Details of the Paid Subscription Plans are individually discussed by the parties and bind the parties on the basis of a separately executed agreement. In case a separate agreement is not executed between the parties, the written arrangements (such as order forms) regarding Subscription Details apply and in the remaining scope conditions for the use of the Services will be subject to conditions set forth in these Terms.</p> <p>5.8. Services Usage during Negotiations. If you wish to actively use the Services in the course of negotiating a separate agreement, the parties may agree on the temporary conditions of use of the Services, including the relevant Metrics, period, and fees, and in the remaining scope conditions for the use of the Services will be subject to conditions set forth in these Terms.</p> <p>5.9. Withdrawal and Refund. You may withdraw from the Terms and claim a refund of fees within 14 days after its execution, provided that the Services have not been activated during that period. You can find all the details regarding the refund in our Refund Policy.</p>"},{"location":"legal/archive/terms.html#6-subscription-fees","title":"6. SUBSCRIPTION FEES","text":"<p>6.1. Payment. Unless otherwise agreed by the parties in writing, Subscription Fees will be payable in USD via a credit card on a going-forward basis according to this Section 6.</p> <p>6.2. Taxes. All Subscription Fees and other amounts payable by you under the Terms are exclusive of taxes and similar assessments. Without limiting the foregoing, you are responsible for all sales, use, excise taxes, and any other similar taxes, duties, and charges of any kind, other than any taxes imposed on our income.</p> <p>6.3. Late Payment. If you fail to make any payment when due then, in addition to all other remedies that may be available: (a) we may charge interest on the past due amount at the rate of 1.5% per month calculated daily and compounded monthly or, if lower, the highest rate permitted under applicable law; (b) you will reimburse us for all reasonable costs incurred by us in collecting any late payments or interest, including attorneys' fees, court costs, and collection agency fees; and (c) if such failure continues for fourteen (14) days following written notice, we may suspend performance of the Services until all past due amounts and interest have been paid, without incurring any obligation or liability to you or any other person by reason of such suspension.</p> <p>6.4. Subscription Fees Increases. Separately from any changes in Subscription Fees due to the upgrade of the relevant Metrics, we may increase Fees for any Subscription Period before its start by providing you with a notice prior to the commencement of the next Subscription Period. Your continued use of the Services constitutes your acceptance of such changed Subscription Fees.</p>"},{"location":"legal/archive/terms.html#7-suspension-and-termination","title":"7. SUSPENSION AND TERMINATION","text":"<p>7.1. Suspension. Without limiting any other provision of the Terms, we reserve the right to, in our sole discretion and without notice or liability, deny access to and use of the Services (including blocking certain IP addresses), to any person for any reason including but not limited to (a) proven or suspected breach of any representation, warranty or covenant contained in the Terms or of any applicable law or regulation; (b) your use of the Services poses a risk to the Services, our other customers, or us (including our infrastructure, security, and third-party relationships); (c) your use of the Services could subject us to liability or (d) you are past due in the payment of Subscription Fees. We will provide you with prompt notice of any suspension.</p> <p>7.2. Effect of Suspension. If we suspend your access to the Services for any reason set out in the Terms, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.</p> <p>7.3. Termination for Cause. Notwithstanding the termination for convenience as described in Section 5, either party may terminate the Terms, effective on written notice to the other party, if the other party (a) materially breaches the Terms (including any incorporated documents), and such breach: (i) is incapable of cure; or (ii) being capable of cure, remains uncured 30 days after the non-breaching party provides the breaching party with a written notice of such breach; (b) becomes insolvent or is generally unable to pay, or fails to pay, its debts as they become due; (c) files, or has filed against it, a petition for voluntary or involuntary bankruptcy or otherwise becomes subject, voluntarily or involuntarily, to any proceeding under any domestic or foreign bankruptcy or insolvency law; (d) makes or seeks to make a general assignment for the benefit of its creditors; or (e) applies for or has appointed a receiver, trustee, custodian, or similar agent appointed by order of any court of competent jurisdiction to take charge of or sell any material portion of its property or business.</p> <p>7.4. Effect of Termination. Upon any termination of the Terms, except as expressly otherwise provided in the Terms: (a) all rights, licenses, consents, and authorizations granted by either party to the other will immediately terminate; (b) we will immediately cease all use of any Customer Data and at your request destroy all documents and tangible materials containing or based on Customer Data and erase all Customer Data from all our systems, provided that, for clarity, our obligations under this Section 7.4 do not apply to any Resultant Data or other data that is required to establish proof of a right or a contract, which will be stored for the duration provided by applicable law; (c) you will immediately cease all use of any Services and destroy all documents and tangible materials containing or based on any our materials, including Documentation and erase all our materials from the systems you directly or indirectly control. You acknowledge and agree that you are responsible to retrieve Customer Data from the Services prior to the termination of the Terms.</p> <p>7.5. Surviving Provisions. The provisions set forth in the following sections, and any other right or obligation of the parties in the Terms that, by its nature, should survive termination or expiration of the Terms, will survive any expiration or termination of the Terms: 3.1, 3.2, 3.4, 3.7, 7.4, 7.5, 8.4, 9, 10, 12.</p>"},{"location":"legal/archive/terms.html#8-representations-and-warranties","title":"8. REPRESENTATIONS AND WARRANTIES","text":"<p>8.1. Mutual Representations and Warranties. Each party represents and warrants to the other party that it has the full right, power, and authority to enter into and perform its obligations and grant the rights, licenses, consents, and authorizations it grants or is required to grant under the Terms.</p> <p>8.2. Additional Spacelift Representations, Warranties, and Covenants. We represent, warrant, and covenant to you that (a) we will perform the Services using personnel of required skill, experience, and qualifications and in a professional and workmanlike manner in accordance with generally recognized industry standards and will devote adequate resources to meet our obligations under the Terms; (b) the Services will be performed materially in accordance with the applicable Documentation; (c) to the best of our knowledge, the Services are free from any viruses, worms, malware, or other malicious source code.</p> <p>8.3. Additional Customer Representations, Warranties, and Covenants. You represent, warrant, and covenant to us that (a) you own or otherwise have and will have the necessary rights and consents in and relating to Customer Data so that, as received by us and processed in accordance with the Terms, they do not and will not infringe, misappropriate, or otherwise violate any IPR, or any privacy or other rights of any third party or violate any applicable law; (b) all registration information you submit will be true, accurate, current, complete and relate to you and not a third party; (c) you will maintain the accuracy of such information and promptly update such information as necessary; (d) you will keep your access credentials confidential and will be responsible for all use of your access credentials; (e) you are aware that you may not access or use the Services for any purpose other than that for which we make the Services available, (f) you are at least eighteen years of age and (g) you confirm the warranties set in Section 12.7.</p> <p>8.4. DISCLAIMER OF WARRANTIES. EXCEPT FOR THE EXPRESS WARRANTIES SET FORTH IN THIS SECTION 8, ALL SERVICES, DOCUMENTATION, AND WEBSITE ARE PROVIDED \"AS IS.\" WE SPECIFICALLY DISCLAIM ALL IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT, AND ALL WARRANTIES ARISING FROM THE COURSE OF DEALING, USAGE, OR TRADE PRACTICE. WITHOUT LIMITING THE FOREGOING, WE MAKE NO WARRANTY OF ANY KIND THAT THE SERVICES, DOCUMENTATION OR WEBSITE, OR ANY PRODUCTS OR RESULTS OF THE USE THEREOF, WILL MEET YOUR OR ANY OTHER PERSON'S REQUIREMENTS, OPERATE WITHOUT INTERRUPTION, ACHIEVE ANY INTENDED RESULT, BE COMPATIBLE OR WORK WITH ANY SOFTWARE, SYSTEM, OR OTHER SERVICES, OR BE SECURE, ACCURATE, COMPLETE, FREE OF HARMFUL CODE, OR ERROR-FREE. ALL THIRD-PARTY MATERIALS ARE PROVIDED \"AS IS\" AND ANY REPRESENTATION OR WARRANTY OF OR CONCERNING ANY THIRD-PARTY MATERIALS IS STRICTLY BETWEEN CUSTOMER AND THE THIRD-PARTY OWNER OR DISTRIBUTOR OF THE THIRD-PARTY MATERIALS.</p>"},{"location":"legal/archive/terms.html#9-indemnification","title":"9. INDEMNIFICATION","text":"<p>9.1. Spacelift Indemnification. Subject to the remainder of this Section 9 and the liability limitations set forth in Section 10, we will indemnify, defend, and hold you harmless from and against any and all losses incurred by you resulting from any action by a third party, that your use of the Services (excluding Customer Data and any third-party materials) in accordance with these Terms infringes or misappropriates IPR. The foregoing obligation does not apply to the extent that the alleged infringement arises from (a) any third-party materials or Customer Data; (b) access to or use of the Services in combination with any hardware, system, software, network, or other materials or service not provided by us; (c) failure to timely implement any modifications, upgrades, replacements, or enhancements made available to you by us or on our behalf; or (d) use of the Services other than in accordance with the Terms and the Documentation. THIS SECTION 9 SETS FORTH YOUR SOLE REMEDIES AND SPACELIFT\u2019S SOLE LIABILITY AND OBLIGATION FOR ANY ACTUAL, THREATENED, OR ALLEGED CLAIMS THAT THE SERVICES, DOCUMENTATION, WEBSITE AND ANY OTHER SPACELIFT MATERIALS OR ANY SUBJECT MATTER OF THE TERMS INFRINGE, MISAPPROPRIATE OR OTHERWISE VIOLATE ANY INTELLECTUAL PROPERTY RIGHTS OF ANY THIRD PARTY.</p> <p>9.2. Mitigation. If the Services, Documentation, or any of the other Spacelift\u2019s materials are, or in our opinion are likely to be, claimed to infringe, misappropriate, or otherwise violate any third-party IPR, or if you or your Authorized User's use of the Services, Documentation or other Spacelift\u2019s materials is enjoined or threatened to be enjoined, we may, at our option and sole cost and expense: (a) obtain the right for you to continue to use the Services and said materials materially as contemplated by the Terms, or (b) modify or replace the Services and said materials.</p> <p>9.3. Customer Indemnification. You will indemnify, defend, and hold us harmless from and against any and all losses incurred by us resulting from any action by a third party to the extent that such losses arise out of or result from, or are alleged to arise out of or result from: (a) your use of the Services; (b) Customer Data, including any processing of Customer Data by us or on our behalf in accordance with the Terms; (c) any other materials or information (including any documents, data, or technology) provided by you or on your behalf, (d) your breach of any of its representations, warranties, covenants, or obligations under the Terms; or (e) negligence or more culpable act or omission (including recklessness or willful misconduct) by you, any Authorized User, or any third party acting on your behalf or any Authorized User, in connection with the Terms, provided, that you will have no obligation under this Section 9.3 to the extent the applicable claim arises from Spacelift\u2019s breach of the Terms.</p> <p>9.4. Indemnification Procedure. Each party\u2019s indemnification obligations are conditioned on the indemnified party: (a) promptly giving written notice of the claim to the indemnifying party; (b) giving the indemnifying party sole control of the defense and settlement of the claim; and (c) providing to the indemnifying party all available information and assistance in connection with the claim, at the indemnifying party\u2019s request and expense. The indemnified party may participate in the defense of the claim, at the indemnified party\u2019s sole expense (not subject to reimbursement). Neither party may admit liability for or consent to any judgment or concede or settle or compromise any claim unless such admission or concession or settlement or compromise includes a full and unconditional release of the other party from all liabilities in respect of the such claim.</p>"},{"location":"legal/archive/terms.html#10-liability","title":"10. LIABILITY","text":"<p>10.1. Exclusion of Liability In no event will Spacelift have any obligation or liability arising from (a) use or inability to use any Services if modified or combined with materials not provided by us; (b) statements or conduct of any third party on or in the Services, (c) any Customer Data, (d) any failure by Customer to comply with the Terms; and (e) damages suffered by Customer or Authorized Users, or any other person having arisen due to the third-party claims (other than described in Section 9.1), suspension or termination of the Services, or (f)  any other reasons arising from Customer\u2019s fault or responsibility.</p> <p>10.2. EXCLUSION OF DAMAGES. EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4 AND TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL SPACELIFT OR ANY OF ITS LICENSORS OR SERVICE PROVIDERS BE LIABLE UNDER OR IN CONNECTION WITH THESE TERMS OR ITS SUBJECT MATTER UNDER ANY LEGAL OR EQUITABLE THEORY, INCLUDING BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, AND OTHERWISE, FOR ANY: (a) LOSS OF PRODUCTION, USE, BUSINESS, REVENUE, OR PROFIT OR DIMINUTION IN VALUE; (b) IMPAIRMENT, INABILITY TO USE OR LOSS, INTERRUPTION, OR DELAY OF THE SERVICES; (c) LOSS, DAMAGE, CORRUPTION, OR RECOVERY OF DATA, OR BREACH OF DATA OR SYSTEM SECURITY; (d) COST OF REPLACEMENT GOODS OR SERVICES; (e) LOSS OF GOODWILL OR REPUTATION; OR (f) CONSEQUENTIAL, INCIDENTAL, INDIRECT, EXEMPLARY, SPECIAL, ENHANCED, OR PUNITIVE DAMAGES, REGARDLESS OF WHETHER SUCH PERSONS WERE ADVISED OF THE POSSIBILITY OF SUCH LOSSES OR DAMAGES OR SUCH LOSSES OR DAMAGES WERE OTHERWISE FORESEEABLE, AND NOTWITHSTANDING THE FAILURE OF ANY AGREED OR OTHER REMEDY OF ITS ESSENTIAL PURPOSE.</p> <p>10.3. CAP ON MONETARY LIABILITY. SPACELIFT WILL ONLY BE LIABLE FOR DIRECT DAMAGES EXCLUDING ANY SITUATION FOR WHICH WE ARE NOT RESPONSIBLE OR WHICH ARE CAUSED BY EVENTS OUTSIDE OUR REASONABLE CONTROL. HOWEVER, EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4, IN NO EVENT WILL THE COLLECTIVE AGGREGATE LIABILITY OF SPACELIFT ARISING OUT OF OR RELATED TO THESE TERMS, WHETHER ARISING UNDER OR RELATED TO BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, OR ANY OTHER LEGAL OR EQUITABLE THEORY, EXCEED THE GREATER OF (a) THE TOTAL AMOUNTS PAID TO SPACELIFT UNDER THESE TERMS IN THE 6 MONTH PERIOD PRECEDING THE EVENT GIVING RISE TO THE CLAIM OR (b) THE AMOUNT OF 5000 USD. THE FOREGOING LIMITATIONS APPLY EVEN IF ANY REMEDY FAILS OF ITS ESSENTIAL PURPOSE.</p> <p>10.4. Exceptions. NOTHING IN THIS SECTION 10 WILL BE DEEMED TO LIMIT EITHER PARTY\u2019S LIABILITY FOR WILLFUL MISCONDUCT, GROSS NEGLIGENCE, FRAUD, OR INFRINGEMENT BY ONE PARTY OF THE OTHER\u2019S INTELLECTUAL PROPERTY RIGHTS.</p>"},{"location":"legal/archive/terms.html#11-provisions-relating-to-consumers","title":"11. PROVISIONS RELATING TO CONSUMERS","text":"<p>11.1. Right to Withdraw. If you are a natural person and have your habitual residence within a Member State of the European Union or the European Economic Area and are entering into the Terms as a consumer (i.e. for purposes which are outside your trade, business, craft or profession), you have the right to withdraw from the contract as described below.</p> <p>11.2. Withdrawal Period. You have the right to withdraw from the Terms (concluded under any Subscription Plan) within 14 days without giving any reason. The withdrawal right will expire after 14 days from the day of the conclusion of the Terms.</p> <p>11.3. Exercise of the Right to Withdraw. To exercise the right of withdrawal, you must inform us, Spacelift, Inc., of your decision to withdraw from the Terms by an unequivocal statement (e.g. an e-mail sent to legal@spacelift.io). To meet the withdrawal deadline, it is sufficient for you to send your communication concerning your exercise of the right of withdrawal before the withdrawal period expires.</p> <p>11.4. Model Withdrawal Form. To exercise your right of withdrawal, you may use the model withdrawal form, included in Appendix No. 2 to the Act on Consumer Rights of May 20, 2014, but this is not obligatory.</p> <p>11.5. Effect of the Withdrawal. If you withdraw from the Terms, we will reimburse you all payments received from you, without undue delay and in any event not later than 14 days from the day on which we are informed about your decision to withdraw from the Terms. We will carry out such reimbursement using the same means of payment as you used for the initial transaction unless you have expressly agreed otherwise.</p> <p>11.6. Consumer Rights. Nothing in the Terms will affect your legal rights as a consumer. If any provision of the Terms does not comply with the applicable law for you as a consumer, the applicable law will apply instead of this provision. The severability clause equally applies. In case of any concerns, questions, or doubts, contact us at legalt@spacelift.io</p> <p>11.7. Complaints. If you have a complaint about Services, you should contact us at contact@spacelift.io, providing as much detail as possible about the complaint, together with your name, the Terms\u2019s Effective Date, and expected means of settling a complaint. We will respond by confirming receipt and will investigate the matter. Upon receiving the complaint, we will investigate the complaint internally, taking into account the importance and complexity of the issue raised, and get back to you no later than 30 days from the receipt of the complaint.</p> <p>11.8. ADR. If you are a consumer, you may consider Alternative Dispute Resolution means in the event of a dispute with us, including referring to the trade inspection, a consumer ombudsman, or an organization whose statutory tasks include consumer protection.</p>"},{"location":"legal/archive/terms.html#12-final-provisions","title":"12. FINAL PROVISIONS","text":"<p>12.1. Current Version of the Terms. Usage of the Services is subject to the then-current version of the Terms posted on the Website and we advise you to periodically review the latest currently effective Terms. We reserve the right to update the provisions of the Terms from time to time at our sole discretion. The updated Terms version supersedes all prior versions, as well as is effective and binding immediately after posting on the Website. Your continued use of the Services on or after the date of the updated version of the Terms is effective and constitutes your acceptance of such updated provisions. If you do not agree to our updated Terms, you can terminate the Subscription in accordance with Section 5.</p> <p>12.2. Applicable Law and Jurisdiction. These Terms are governed by and construed in accordance with the Applicable Law (stated below) without giving effect to any choice or conflict of law provision of any jurisdiction. Any legal suit, action, or proceeding arising out of or related to these Terms will be subject to the exclusive jurisdiction of the Applicable Jurisdiction as provided in the following table:</p> Client Applicable Law Applicable Jurisdiction Consumers residing in the Member State of the European Union or the European Economic Area Poland Warsaw, Poland Other Customers State of Delaware, US County New Castle, Delaware, US <p>Each party irrevocably submits to the exclusive jurisdiction of such courts in any such suit, action, or proceeding.</p> <p>12.3. Contact details. For any formal notices or complaints, please contact legal@spacelift.io. In any other matters, including any inquiry about the use of the Services, please contact us at contact@spacelift.io.</p> <p>12.4. Notices. Except as otherwise expressly set forth in the Terms, any notice, request, consent, claim, demand, waiver, or other communications under these Terms have legal effect and will be deemed effectively given: (a) when received, if delivered by hand or with signed confirmation of receipt; (b) when received, if sent by a nationally recognized overnight courier or by certified or registered mail, signature required; or (c) when sent, if by email, if sent during the addressee's normal business hours, and on the next business day, if sent after the addressee's normal business hours.</p> <p>12.5. Feedback. If you provide us with any suggestions, comments, recommendations, opinions, or other information relating to the Services or Website (\u201cFeedback\u201d), you grant us a royalty-free, non-exclusive, irrevocable, perpetual, worldwide right and license to use the Feedback on our websites or in marketing materials. We reserve the right to remove any Feedback posted on the Website if, in our opinion, such Feedback does not comply with the Terms or applicable law.</p> <p>12.6. Logo usage. You grant us the right to use your name and other indicia, such as logo or trademark in our list of current or former clients in promotional materials and on our websites. Any other announcement, statement, press release, or other publicity or marketing materials relating to your use of Services will be subject to your consent.</p> <p>12.7. Export Laws. Each party will comply with the export laws and regulations of the United States and other applicable jurisdictions in providing and using the Services. Without limiting the generality of the foregoing, you represent and warrant that you are: (a) not a resident of a country sanctioned by the Office of Foreign Assets Control, Department of the Treasury (\u201cOFAC\u201d); (b) not currently identified on the Specially Designated Nationals and Blocked Persons List maintained by OFAC and/or on any other similar list maintained by the OFAC pursuant to any authorizing statute, executive order or regulation; (c) not a person or entity with whom a citizen of the United States is prohibited to engage in transactions by any trade embargo, economic sanction, or other prohibition of United States law, regulation, or executive order of the President of the United States; (d) not engaged in any activity or conduct that would breach any anti-corruption laws or anti-money laundering laws; and (e) not currently under investigation by any governmental authority for alleged criminal activity relating to the OFAC, Patriot Act Offenses, anti-corruption laws or anti-money laundering laws.</p> <p>12.8. Non-waiver. Our failure to exercise or enforce any right or provision of the Terms will not operate as a waiver of such right or provision.</p> <p>12.9. Assignment. We may assign any or all of our rights and obligations to others at any time. We will notify you of any assignment.</p> <p>12.0. Severability. If any provision of the Terms is invalid, illegal, or unenforceable in any jurisdiction, such invalidity, illegality, or unenforceability will not affect any other provision of these Terms or invalidate or render unenforceable such provision in any other jurisdiction.</p> <p>12.11. No relationship. There is no joint venture, partnership, employment, or agency relationship created between you and us as a result of the Terms or use of the Services.</p>"},{"location":"legal/archive/terms.html#terms-and-conditions_1","title":"Terms and conditions","text":"<p>Effective from March 7, 2023 till November 3, 2024</p> <p>This Terms and Conditions is between the entity you represent, or, if you do not indicate an entity in connection with the Services, you individually (\u201cClient\u201d, \u201cyou\u201d or \u201cyour\u201d), and Spacelift, Inc. with its principal office at 541 Jefferson Ave. Suite 100, Redwood City CA 94063, United States of America (\u201cSpacelift\u201d, \u201cthe Company\u201d \u201cwe\u201d, \u201cus\u201d, or \u201cour\u201d). It consists of the terms and conditions below, the Privacy Policy, the Refund Policy and the Cookie Policy (together, the \u201cAgreement\u201d).</p> <p>The Agreement does not govern the use of:</p> <ul> <li>the Services under Enterprise Plan provided that a separate and mutually agreed contract has been executed,</li> <li>the Services purchased via AWS Marketplace, which are subject to Standard Contract for AWS Marketplace and any amendments,</li> <li>any self-hosted services provided by Spacelift, Inc. which are subject to a separate agreement.</li> </ul>"},{"location":"legal/archive/terms.html#1-definitions_1","title":"1. DEFINITIONS","text":"<p>1.1. \u201cAuthorized User\u201d means Client\u2019s employees, consultants, contractors, agents, and Workers (a) who Client authorizes to access and use the Services under the rights granted to Client under this Agreement; and (b) for whom access to the Services has been purchased hereunder.</p> <p>1.2. \u201cClient Data\u201d means information, data, and other content, in any form or medium, that is collected, downloaded, or otherwise received from the Client or an Authorized User by or through the Services. For the avoidance of doubt, Client Data does not include Resultant Data or any other information reflecting the access or use of the Services by or on behalf of Client or any Authorized User.</p> <p>1.3. \u201cConfidential Information\u201d means all nonpublic information, including, but not limited to, source code, software, trade secrets, know-how, technical drawings, algorithms, ideas, inventions, other technical, business or sales information, negotiations or proposals, disclosed by us in whatever form and which is known by the Client or its Authorized User to be confidential or under circumstances by which the receiving party should reasonably understand such information is to be treated as confidential, whether or not marked as \u201cConfidential\u201d.</p> <p>1.4. \u201cDocumentation\u201d means any manuals, instructions, including technical specifications, or other documents or materials describing the features and functionality of the Services, which are located on Website or provided to Clients, and may be updated from time to time.</p> <p>1.5. \u201cIntellectual Property Rights\u201d or \u201cIPR\u201d means any registered and unregistered rights granted, applied for, or otherwise now or hereafter in existence under or related to any patent, copyright, trademark, trade secret, database protection, or other intellectual property rights laws in any part of the world.</p> <p>1.6. \u201cMetrics\u201d - means the measurements used for quantifying the Services usage with the following meaning:</p> <p>\u00a0\u00a0\u00a0\u00a0\u200b1.6.1. \u201cPrivate Minute(s)\u201d means minute(s) of Private Workers\u2019 Services usage in a given month;</p> <p>\u00a0\u00a0\u00a0\u00a01.6.2. \u201cPublic Minutes\u201d means minutes of Public Worker\u2019s Services usage in a given month;</p> <p>\u00a0\u00a0\u00a0\u00a016.3. \u201cSeat(s)\u201d means an Authorized User(s) who actively logged in to the Services in a given month;</p> <p>\u00a0\u00a0\u00a0\u00a01.6.4. \u201cWorker(s)\u201d means a predefined set(s) of computing resources that are specifically optimized for the development and provisioning and deployment of cloud-based infrastructures based on IaC; a Worker can be either a self-hosted agent performing infrastructure management in a Client-controlled environment (\u201cPrivate Worker\u201d) or any other software agent, provided and managed by Spacelift in a common secure worker pool (\u201cPublic Worker\u201d).</p> <p>1.7. \u201cServices\u201d means the Spacelift\u2019s specialized, continuous integration and deployment (CI/CD) platform for infra-as-code available at https://spacelift.io as SaaS;</p> <p>1.8. \u201cSubscription\u201d means enrollment for Services for a Subscription Plan on Subscription Terms as defined in the Agreement;</p> <p>1.9. \u201cSubscription Plan(s)\u201d means available Subscription offer(s) for use of the Services as described on the Website, including Trial, Free, Cloud, and Enterprise;</p> <p>1.10. \u201cSubscription Term(s)\u201d means the conditions under which a Subscription is made under the Agreement, including the Subscription Period, Metrics, and Subscription Fees, as described in Section 5;</p> <p>1.11. \u201cWebsite\u201d means https://spacelift.io website managed by Spacelift.</p>"},{"location":"legal/archive/terms.html#2-general_1","title":"2. GENERAL","text":"<p>2.1. Execution of the Agreement. Accepting this Agreement is a condition of using the Services provided by Spacelift. \u200b\u200bBY COMPLETING THE REGISTRATION PROCESS, ACCESSING OR USING THE SERVICES, YOU ACKNOWLEDGE AND AGREE THAT (I) YOU HAVE READ, UNDERSTOOD, AND ACCEPTED THIS AGREEMENT, AND (II) YOU HEREBY REPRESENT AND WARRANT THAT YOU ARE AUTHORIZED TO ENTER OR ACT ON BEHALF OF THE CLIENT, AND BIND TO THIS AGREEMENT. If you do not have the legal authority to enter this Agreement, do not understand this Agreement, or do not agree to the Agreement, you should not accept the Terms and Conditions, or use the Services.</p> <p>2.2. Conflict of Provisions. In the event of any inconsistencies or conflict between the documents that make up this Agreement, the documents will prevail in the following order: (a) any written amendment agreed upon by the parties (such as order forms); (b) Privacy Policy; (c) these Terms and Conditions and (d) the Refund Policy.</p> <p>2.3. Compliance. You are responsible for (a) compliance with the provisions of the Agreement by you and your Authorized Users and for any and all acts and omissions of Authorized Users connected with their use and access to the Services and for any breach of this Agreement by Authorized Users; and (b) any delay or failure of performance caused in whole or in part by your delay in performing, or failure to perform, any of your obligations under this Agreement. Without limiting the foregoing, you are solely responsible for ensuring that your use of the Services is compliant with all applicable laws and regulations, as well as any and all privacy policies, agreements, or other obligations you may maintain or enter into.</p> <p>2.4. Amendments. Any individual amendment to this Agreement must be made in writing (expressly stating that it is amending this Agreement) and signed by both parties.</p>"},{"location":"legal/archive/terms.html#3-license-intellectual-property-rights-and-ownership_1","title":"3. LICENSE, INTELLECTUAL PROPERTY RIGHTS, AND OWNERSHIP","text":"<p>3.1. Ownership. The Services, Documentation, and Website, all copies and portions thereof, and all IPR therein, including, but not limited to source code, databases, functionality, software, website designs, audio, video, text, photographs, graphics, or derivative works therefrom, are owned by us or licensed to us. You are not authorized to use (and will not permit any third party to use) the Services, Website, Documentation, or any portion thereof except as expressly authorized by this Agreement. Specifically, no part of the Services, Documentation, or Website may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, modified, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission.</p> <p>3.2. Confidential Information. All our Confidential Information and derivations thereof will remain our sole and exclusive property. You should not disclose, use or publish Confidential Information without our prior written consent. You must hold all our Confidential Information in strict confidence and safeguard the Confidential Information from unauthorized use, access, or disclosure using at least the degree of care you use to protect your similarly sensitive information and in no event less than a reasonable degree of care.</p> <p>3.3. License. Spacelift makes the Services available to you during the Subscription Period, subject to the terms and conditions of this Agreement and Subscription Terms. Spacelift grants a limited, non-exclusive, non-sublicensable, non-transferable right to access and use the Services and its Documentation during the Subscription Period, solely for your internal business purposes or your personal use.</p> <p>3.4. Restrictions. You will not, and will not permit any other person to, access or use the Services except as expressly permitted by this Agreement. For purposes of clarity and without limiting the generality of the foregoing, you will not, except as this Agreement expressly permits: (a) copy, modify, or create derivative works or improvements of the Services or Documentation; (b) rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer, or otherwise make available any Services or Documentation to any person; (c) reverse engineer, disassemble, decompile, decode, adapt, or otherwise attempt to derive or gain access to the source code of the Services, in whole or in part; (d) bypass or breach any security device or protection used by the Services or access or use the Services other than by an Authorized User through the use of his or her own then valid access credentials; (e) input, upload, transmit, or otherwise provide to or through the Services or Documentation, any information or materials that are unlawful or injurious, or contain, transmit, or activate any harmful code; (f) damage, destroy, disrupt, disable, impair, interfere with, or otherwise impede or harm in any manner the Services or Documentation, or our provision of services to any third party, in whole or in part; (g) remove, delete, alter, or obscure any trademarks, Documentation, warranties, or disclaimers, or IPR notices from any Services; (h) access or use the Services or Documentation in any manner or for any purpose that infringes, misappropriates, or otherwise violates any IPR or other right of any third party or that violates any applicable law; or (i) access or use the Services or Documentation for purposes of competitive analysis of the Services, the development, provision, or use of a competing software service or product.</p> <p>3.5. Client Data. You are and will remain the sole and exclusive owner of all rights, title, and interest in and to all Client Data, including all IPR, subject to the rights and permissions granted in the Agreement. You have exclusive control and responsibility for determining what data you submit to the Services, for obtaining all necessary consents and permissions for the submission of Client Data, and for the accuracy, quality, and legality of Client Data.</p> <p>3.6. Consent to Use Client Data. You irrevocably grant all such rights and permissions in or relating to Client Data as are reasonably necessary or useful to us to enforce this Agreement and exercise our rights and perform our obligations hereunder.</p> <p>3.7. Use of Resultant Data. We may collect data and information related to your use of the Services that is used by us in an aggregate manner, including to compile statistical and performance information related to the provision and operation of the Services (\u201cResultant Data\u201d). You unconditionally and irrevocably grant to us an assignment of all rights, title, and interest in and to the Resultant Data, including all IPR relating thereto, if any.</p>"},{"location":"legal/archive/terms.html#4-provision-of-services_1","title":"4. PROVISION OF SERVICES","text":"<p>4.1. Metrics. Use of the Services is subject to usage limits reflected in Metrics, as set forth in the Subscription Plan. We will monitor your use of the Services in order to verify whether you comply with the presented limits.</p> <p>4.2. Services Modifications. We reserve the right to make any changes to the Services or Documentation that we deem necessary or useful to: (a) maintain or enhance: (i) the quality or delivery of Services to you and other clients; (ii) the competitive strength of or market for Services; or (iii) the Services' cost efficiency or performance; or (b) to comply with applicable law.</p> <p>4.3. Privacy. When applicable, we will comply with all applicable laws, regulations, and government orders relating to personally identifiable information and data privacy with respect to any such Client Data that we receive or have access to under the Agreement or in connection with the performance of the Services. In particular, regulations for the protection of personally identifiable information are indicated in the Privacy Policy incorporated herein by reference.</p> <p>4.4. Access and Security. You will employ all physical, administrative, and technical controls, screening, security procedures, and other safeguards necessary to: (a) securely administer the distribution and use of all access credentials and protect against any unauthorized access to or use of the Services; and (b) control the content and use of Client Data, including the uploading or other provision of Client Data for processing by the Services.</p> <p>4.5. Security. We maintain industry-standard security and privacy certification, such as a SOC II certification. We will use appropriate technical and organizational measures designed to prevent unauthorized access, use, alteration, or disclosure of the Services or Client Data.</p> <p>4.6. Incidents. We will notify you in case of any security incident as soon as possible, provided that you have indicated your contact data in the Services under the address: https://*.app.spacelift.io/settings/security (* being the domain name chosen by you to access Services).</p> <p>4.7. Downtimes. We will use commercially reasonable efforts to give you at least 24 hours prior notice of all scheduled outages of the Services. You can check the current Services\u2019 availability status at https://spacelift.statuspage.io/</p> <p>4.8. Services and Website Management. We reserve the right at our sole discretion, to (a) monitor the Services for breaches of the Agreement; (b) take appropriate legal action against anyone in breach of applicable laws or the Agreement; (c) refuse, restrict access to, or availability of, or disable (to the extent technologically feasible) any of Client Data; (d) remove from the Services or Website or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (e) otherwise manage the Website and Services in a manner designed to protect our rights and property and to facilitate the proper functioning of the Website and Services.</p> <p>4.9. Third-Party Materials. The Website and/or Services may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third-party websites or applications or the third-party operator. We are not responsible for and do not endorse any third-party websites or applications or their availability or content.</p> <p>4.10. Access by Third-Party Accounts. You may register and login to the Services using your third-party service providers account details, like a Google or GitHub account (\u201cThird-Party Account\u201d). When you do so, we will receive certain profile information varying on the identity provider and the information you decided to include in your Third-Party Account. You will have the ability to disable the connection between your Services account and your Third Party Accounts at any time. Please note that your relationship with the third-party service providers associated with your Third-Party Accounts is governed solely by your agreement(s) with such third-party service providers. If a Third Party Account or associated service becomes unavailable or the access to such Third Party Account is terminated by the third-party service provider, then your access using such Third Party Account may no longer be available.</p> <p>4.11. Support Services. During the Subscription Period, we will provide support services depending on the Subscription Plan, as described in https://docs.spacelift.io/product/support/</p> <p>4.12. Client Systems and Cooperation. You will at all times during the period of Subscription: (a) set up, maintain, and operate in good repair and in accordance with the Documentation all your systems (meaning information technology infrastructure, including computers, software, databases, electronic systems, database management systems, and networks) on or through which the Services are accessed or used; (b) provide us with such access to your data or systems as is necessary for us to perform the Services in accordance with the Agreement and Documentation; (c) use reasonable measures to prevent and promptly notify us of any unauthorized access to Authorized User accounts of which you become aware of, and (d) provide all cooperation and assistance as we may reasonably request to enable us to exercise our rights and perform our obligations under and in connection with this Agreement.</p> <p>4.13. Quality of Services. You are aware that the quality of the Services and your use of the Services might be affected by a number of factors outside our control, including but not limited to force majeure, technical conditions, hardware or software (including third-party software and network) issues. Any delay or default affecting the availability, functionality, correctness, or timely performance of the Services caused by such circumstances will not constitute a breach of the Agreement.</p> <p>4.14. No Data Backup. We do not store or backup any Client Data. The Services do not replace the need for you to maintain regular data backups or redundant data archives. We have no obligation or liability for any loss, alteration, destruction, damage, corruption, or recovery of Client Data.</p> <p>4.15. Disclaimer. The content on the Website is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Website.</p>"},{"location":"legal/archive/terms.html#5-subscription-plans-and-terms","title":"5. SUBSCRIPTION PLANS AND TERMS","text":"<p>5.1. Effective Date and Term. This Agreement commences on the effective date being the day of your registration, access to, or use of the Services, whichever happens first (\u201cEffective Date\u201d). Unless earlier terminated pursuant to the terms of this Section, the Agreement will continue through the Subscription Period of a chosen Subscription Plan.</p> <p>5.2. Subscription Plans. The Services are available under the following Subscription Plans with the relevant Subscription Terms:</p> Plan Subscription Period Termination Subscription Fee Trial 14 days Any time None Free Non-definite term Any time None Cloud Default: 1 month, monthly renewal.Individual arrangements may include an annual or  a multi-year Subscription Period. Any time during the Subscription Period, having its effect on the last day of the given Subscription Period. Current fees are set forth in https://spacelift.io/pricing Enterprise As agreed by the parties in a separate agreement or order form As agreed by the parties in a separate agreement or order form As agreed by the parties in a separate agreement or order form <p>5.3. The Trial Plan. The Subscription Terms for the Trial Plan are as follows:</p> <p>\u00a0\u00a0\u00a0\u00a05.3.1. Scope and Metrics. The Trial Plan offers access to Services to get to know Services before starting the Free, Cloud, or Enterprise Plan. Any usage limitations are indicated on https://spacelift.io/pricing.</p> <p>\u00a0\u00a0\u00a0\u00a05.3.2. Subscription Period. The Trial Plan expires 14 days after your registration to the Services.</p> <p>\u00a0\u00a0\u00a0\u00a05.3.3. Termination. You may cancel the Trial Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io.</p> <p>\u00a0\u00a0\u00a0\u00a05.3.4. Next Steps. Upon the end of Trial Plan, you may: (a) stop using the Services and delete your account thus terminating the Agreement, (b) continue to use the Services under Free Plan, (c) subscribe to Cloud Plan if you provide payment details to make a Subscription Fee according to the currently effective rates presented on https://spacelift.io/pricing, or (d) contact sales@spacelift.io to discuss the Enterprise Plan which is subject to separate agreement.</p> <p>5.4. The Free Plan. The Subscription Terms for the Free Plan are as follows:</p> <p>\u00a0\u00a0\u00a0\u00a05.4.1. Scope and Metrics. The Free Plan offers access to the Services with usage limitations indicated on https://spacelift.io/pricing. In order to expand the usage limitations, upgrade to Cloud Plan or Enterprise Plan.</p> <p>\u00a0\u00a0\u00a0\u00a05.4.2. Subscription Period and Termination. The Free Plan is available for an indefinite period of time and might be terminated at any time. You may cancel the Free Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io.</p> <p>5.5. The Cloud Plan. The Subscription Terms for the Cloud Plan are as follows:</p> <p>\u00a0\u00a0\u00a0\u00a05.5.1. Scope and Metrics. The Cloud Plan offers access to the Services with usage limitations indicated on https://spacelift.io/pricing. This plan can be supplemented with additional Seats and/or Workers as required, at an additional cost calculated on the basis of current rates.</p> <p>\u00a0\u00a0\u00a0\u00a05.5.2. Subscription Period and Billing. If you activate the Cloud Plan, you authorize Spacelift to periodically charge, on a going-forward basis and until cancellation of either the recurring payments or your account, all accrued sums on or before the payment due date for the accrued sums. The \"Subscription Billing Date\" is the date when you purchase your first Subscription to the Services. Your account will be charged automatically on the Subscription Billing Date for all applicable fees for the next Subscription Period. The subscription will continue unless and until you cancel your Subscription or we terminate it. You must cancel your Subscription before it renews in order to avoid billing the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide to us during registration (or to a different payment method if you change your payment information).</p> <p>\u00a0\u00a0\u00a0\u00a05.5.3. Termination. You may cancel the Cloud Plan at any time by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io. The termination will be effective on the last day of the given Subscription Period.</p> <p>5.6. Withdrawal and Refund. You may withdraw from the Agreement and claim a refund of funds within 14 days after its execution provided that the Services have not been activated during that period. You can find all the details regarding the refund in our Refund Policy.</p> <p>5.7. Plan Adjustments and Upgrades. The Subscription Terms for each plan, including Subscription Period, Fees, and Metrics may be adjusted by written agreement of the parties. If you wish to upgrade your Subscription Plan, contact sales@spacelift.io to discuss the available options.</p> <p>5.8. The Enterprise Plan. In most cases, Subscription Terms of the Enterprise Plan are individually discussed by the parties and bind the parties on the basis of a separately executed agreement. In case a separate agreement is not executed between the parties, the written arrangements (such as order forms) regarding Subscription Terms apply and in the remaining scope terms for the use of the Services will be subject to conditions set forth in this Agreement.</p> <p>5.9. Services Usage during Negotiations. If you wish to actively use the Services in the course of negotiating the separate agreement, the parties may agree on the temporary terms of use of the Services, including the relevant Metrics, period, and fees, and in the remaining scope terms for the use of the Services will be subject to conditions set forth in this Agreement.</p>"},{"location":"legal/archive/terms.html#6-subscription-fees_1","title":"6. SUBSCRIPTION FEES","text":"<p>6.1. Terms of Payment. Unless otherwise agreed by the parties, Subscription Fees will be payable in USD via a credit card on a going-forward basis and will be subject to this Section 6.</p> <p>6.2. Taxes. All Subscription Fees and other amounts payable by you under this Agreement are exclusive of taxes and similar assessments. Without limiting the foregoing, you are responsible for all sales, use, excise taxes, and any other similar taxes, duties, and charges of any kind, other than any taxes imposed on our income.</p> <p>6.3. Late Payment. If you fail to make any payment when due then, in addition to all other remedies that may be available: (a) we may charge interest on the past due amount at the rate of 1.5% per month calculated daily and compounded monthly or, if lower, the highest rate permitted under applicable law; (b) you will reimburse us for all reasonable costs incurred by us in collecting any late payments or interest, including attorneys' fees, court costs, and collection agency fees; and (c) if such failure continues for fourteen (14) days following written notice, we may suspend performance of the Services until all past due amounts and interest have been paid, without incurring any obligation or liability to you or any other person by reason of such suspension.</p> <p>6.4. Subscription Fees Increases. Separately from any changes in Subscription Fees due to the upgrade of the relevant Metrics, we may increase Fees for any Subscription Period before its start by providing you a notice prior to the commencement of the next Subscription Period. Your continued use of the Services constitutes your acceptance of such changed Subscription Fees.</p>"},{"location":"legal/archive/terms.html#7-suspension-and-termination_1","title":"7. SUSPENSION AND TERMINATION","text":"<p>7.1. Suspension. Without limiting any other provision of the Agreement, we reserve the right to, in our sole discretion and without notice or liability, deny access to and use of the Services (including blocking certain IP addresses), to any person for any reason including but not limited to (a) proven or suspected breach of any representation, warranty or covenant contained in the Agreement or of any applicable law or regulation; (b) your use of the Services poses a risk to the Services, our other customers, or us (including our infrastructure, security, and third-party relationships); (c) your use of the Services could subject us to liability or (d) you are past due in the payment of Subscription Fee. We will provide you with prompt notice of any suspension.</p> <p>7.2. Effect of Suspension. If we suspend your access to the Services for any reason set out in the Agreement, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.</p> <p>7.3. Termination for Cause. Notwithstanding the termination for convenience as described in Section 5, Either Party may terminate this Agreement, effective on written notice to the other party, if the other party (a) materially breaches this Agreement, and such breach: (i) is incapable of cure; or (ii) being capable of cure, remains uncured 30 days after the non-breaching party provides the breaching party with written notice of such breach; (b) becomes insolvent or is generally unable to pay, or fails to pay, its debts as they become due; (c) files, or has filed against it, a petition for voluntary or involuntary bankruptcy or otherwise becomes subject, voluntarily or involuntarily, to any proceeding under any domestic or foreign bankruptcy or insolvency law; (d) makes or seeks to make a general assignment for the benefit of its creditors; or (e) applies for or has appointed a receiver, trustee, custodian, or similar agent appointed by order of any court of competent jurisdiction to take charge of or sell any material portion of its property or business.</p> <p>7.4. Effect of Termination. Upon any termination of this Agreement, except as expressly otherwise provided in this Agreement: (a) all rights, licenses, consents, and authorizations granted by either party to the other will immediately terminate; (b) we will immediately cease all use of any Client Data and at your request destroy, all documents and tangible materials containing or based on Client Data and erase all Client Data from all our systems, provided that, for clarity, our obligations under this Section 7.4 do not apply to any Resultant Data or other data that is required to establish proof of a right or a contract, which will be stored for the duration provided by enforceable law; (c) you will immediately cease all use of any Services and within sixty (60) days destroy, all documents and tangible materials containing or based on any our materials, including Documentation and erase all our materials from the systems you directly or indirectly control. You acknowledge and agree that you are responsible to retrieve Client Data from the Services prior to the termination of this Agreement.</p> <p>7.5. Surviving Terms. The provisions set forth in the following sections, and any other right or obligation of the parties in this Agreement that, by its nature, should survive termination or expiration of this Agreement, will survive any expiration or termination of this Agreement: 3.1, 3.2, 3.4, 3.7, 7.4, 7.5, 8.4, 9, 10, 12.</p>"},{"location":"legal/archive/terms.html#8-representations-and-warranties_1","title":"8. REPRESENTATIONS AND WARRANTIES","text":"<p>8.1. Mutual Representations and Warranties. Each party represents and warrants to the other party that it has the full right, power, and authority to enter into and perform its obligations and grant the rights, licenses, consents, and authorizations it grants or is required to grant under this Agreement.</p> <p>8.2. Additional Spacelift Representations, Warranties, and Covenants. We represent, warrant, and covenant to you that (a) we will perform the Services using personnel of required skill, experience, and qualifications and in a professional and workmanlike manner in accordance with generally recognized industry standards and will devote adequate resources to meet its obligations under this Agreement; (b) the Services will be performed materially in accordance with the applicable Documentation; (c) to the best of our knowledge, the Services is free from any viruses, worms, malware, or other malicious source code.</p> <p>8.3. Additional Client Representations, Warranties, and Covenants. You represent, warrant, and covenant to us that (a) you own or otherwise have and will have the necessary rights and consents in and relating to the Client Data so that, as received by us and processed in accordance with this Agreement, they do not and will not infringe, misappropriate, or otherwise violate any IPR, or any privacy or other rights of any third party or violate any applicable law; (b) all registration information you submit will be true, accurate, current, and complete and relate to you and not a third party; (c) you will maintain the accuracy of such information and promptly update such information as necessary; (d) you will keep your access credentials confidential and will be responsible for all use of your access credentials; (e) you are aware that you may not access or use the Services for any purpose other than that for which we make the Services available and (f) you are at least eighteen years of age.</p> <p>8.4. DISCLAIMER OF WARRANTIES. EXCEPT FOR THE EXPRESS WARRANTIES SET FORTH IN THIS SECTION 8, ALL SERVICES, DOCUMENTATION, AND WEBSITE ARE PROVIDED \"AS IS.\" WE SPECIFICALLY DISCLAIM ALL IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT, AND ALL WARRANTIES ARISING FROM THE COURSE OF DEALING, USAGE, OR TRADE PRACTICE. WITHOUT LIMITING THE FOREGOING, WE MAKE NO WARRANTY OF ANY KIND THAT THE SERVICES, DOCUMENTATION OR WEBSITE, OR ANY PRODUCTS OR RESULTS OF THE USE THEREOF, WILL MEET YOUR OR ANY OTHER PERSON'S REQUIREMENTS, OPERATE WITHOUT INTERRUPTION, ACHIEVE ANY INTENDED RESULT, BE COMPATIBLE OR WORK WITH ANY SOFTWARE, SYSTEM, OR OTHER SERVICES, OR BE SECURE, ACCURATE, COMPLETE, FREE OF HARMFUL CODE, OR ERROR-FREE. ALL THIRD-PARTY MATERIALS ARE PROVIDED \"AS IS\" AND ANY REPRESENTATION OR WARRANTY OF OR CONCERNING ANY THIRD-PARTY MATERIALS IS STRICTLY BETWEEN THE CLIENT AND THE THIRD-PARTY OWNER OR DISTRIBUTOR OF THE THIRD-PARTY MATERIALS.</p>"},{"location":"legal/archive/terms.html#9-indemnification_1","title":"9. INDEMNIFICATION","text":"<p>9.1. Spacelift Indemnification. Subject to the remainder of this Section 9 and the liability limitations set forth in Section 10, we will indemnify, defend, and hold you harmless from and against any and all losses incurred by you resulting from any action by a third party that your use of the Services (excluding Client Data and any third-party materials) in accordance with this Agreement infringes or misappropriates IPR. The foregoing obligation does not apply to the extent that the alleged infringement arises from (a) any third-party materials or Client Data; (b) access to or use of the Services in combination with any hardware, system, software, network, or other materials or service not provided by us; (c) failure to timely implement any modifications, upgrades, replacements, or enhancements made available to you by us or on our behalf; or (d) use of the Services other than in accordance with the terms and conditions of this Agreement and the Documentation. THIS SECTION 9 SETS FORTH THE CLIENT\u2019S SOLE REMEDIES AND SPACELIFT\u2019S SOLE LIABILITY AND OBLIGATION FOR ANY ACTUAL, THREATENED, OR ALLEGED CLAIMS THAT THE SERVICES AND ANY OTHER PROVIDER MATERIALS OR ANY SUBJECT MATTER OF THIS AGREEMENT INFRINGES, MISAPPROPRIATES OR OTHERWISE VIOLATES ANY INTELLECTUAL PROPERTY RIGHTS OF ANY THIRD PARTY.</p> <p>9.2. Mitigation. If the Services or any of the other Spacelift\u2019s materials are, or in our opinion are likely to be, claimed to infringe, misappropriate, or otherwise violate any third-party IPR, or if your or your Authorized User's use of the Services or other Spacelift\u2019s materials is enjoined or threatened to be enjoined, we may, at our option and sole cost and expense: (a) obtain the right for you to continue to use the Services and said materials materially as contemplated by this Agreement, or (b) modify or replace the Services and said materials.</p> <p>9.3. Client Indemnification. You will indemnify, defend, and hold us harmless from and against any and all losses incurred by us resulting from any action by a third party to the extent that such losses arise out of or result from, or are alleged to arise out of or result from: (a) your use of the Services; (b) Client Data, including any processing of Client Data by us or on our behalf in accordance with this Agreement; (c) any other materials or information (including any documents, data, or technology) provided by you or on your behalf, (d) your breach of any of its representations, warranties, covenants, or obligations under this Agreement; or (e) negligence or more culpable act or omission (including recklessness or willful misconduct) by you, any Authorized User, or any third party acting on your behalf or any Authorized User, in connection with this Agreement, provided, that Client will have no obligation under this Section 9.3 to the extent the applicable claim arises from Spacelift\u2019s breach of this Agreement.</p> <p>9.4. Indemnification Procedure. Each party\u2019s indemnification obligations are conditioned on the indemnified party: (a) promptly giving written notice of the claim to the indemnifying party; (b) giving the indemnifying party sole control of the defense and settlement of the claim; and (c) providing to the indemnifying party all available information and assistance in connection with the claim, at the indemnifying party\u2019s request and expense. The indemnified party may participate in the defense of the claim, at the indemnified party\u2019s sole expense (not subject to reimbursement). Neither party may admit liability for or consent to any judgment or concede or settle or compromise any claim unless such admission or concession or settlement or compromise includes a full and unconditional release of the other Party from all liabilities in respect of the such claim.</p>"},{"location":"legal/archive/terms.html#10-liability_1","title":"10. LIABILITY","text":"<p>10.1. Exclusion of Liability In no event will Spacelift have any obligation or liability arising from (a) use or inability to use any Services if modified or combined with materials not provided by us; (b) statements or conduct of any third party on or in the Services, (c) any Client Data, (d) any failure by Client to comply with the Agreement; and (e) damages suffered by the Client or Authorized Users, or any other person having arisen due to the third-party claims (other than described in Section 9.1), suspension or termination of the Services, or for other reasons arising from the Client\u2019s fault.</p> <p>10.2. EXCLUSION OF DAMAGES. EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4 AND TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL SPACELIFT OR ANY OF ITS LICENSORS OR SERVICE PROVIDERS BE LIABLE UNDER OR IN CONNECTION WITH THIS AGREEMENT OR ITS SUBJECT MATTER UNDER ANY LEGAL OR EQUITABLE THEORY, INCLUDING BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, AND OTHERWISE, FOR ANY: (a) LOSS OF PRODUCTION, USE, BUSINESS, REVENUE, OR PROFIT OR DIMINUTION IN VALUE; (b) IMPAIRMENT, INABILITY TO USE OR LOSS, INTERRUPTION, OR DELAY OF THE SERVICES; (c) LOSS, DAMAGE, CORRUPTION, OR RECOVERY OF DATA, OR BREACH OF DATA OR SYSTEM SECURITY; (d) COST OF REPLACEMENT GOODS OR SERVICES; (e) LOSS OF GOODWILL OR REPUTATION; OR (f) CONSEQUENTIAL, INCIDENTAL, INDIRECT, EXEMPLARY, SPECIAL, ENHANCED, OR PUNITIVE DAMAGES, REGARDLESS OF WHETHER SUCH PERSONS WERE ADVISED OF THE POSSIBILITY OF SUCH LOSSES OR DAMAGES OR SUCH LOSSES OR DAMAGES WERE OTHERWISE FORESEEABLE, AND NOTWITHSTANDING THE FAILURE OF ANY AGREED OR OTHER REMEDY OF ITS ESSENTIAL PURPOSE.</p> <p>10.3. CAP ON MONETARY LIABILITY. SPACELIFT WILL ONLY BE LIABLE FOR DIRECT DAMAGES EXCLUDING ANY SITUATION FOR WHICH WE ARE NOT RESPONSIBLE OR WHICH ARE CAUSED BY EVENTS OUTSIDE OUR REASONABLE CONTROL. HOWEVER, EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4, IN NO EVENT WILL THE COLLECTIVE AGGREGATE LIABILITY OF SPACELIFT ARISING OUT OF OR RELATED TO THIS AGREEMENT, WHETHER ARISING UNDER OR RELATED TO BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, OR ANY OTHER LEGAL OR EQUITABLE THEORY, EXCEED THE GREATER OF (a) THE TOTAL AMOUNTS PAID TO SPACELIFT UNDER THIS AGREEMENT IN THE 6 MONTH PERIOD PRECEDING THE EVENT GIVING RISE TO THE CLAIM OR (b) THE AMOUNT OF 5000 USD. THE FOREGOING LIMITATIONS APPLY EVEN IF ANY REMEDY FAILS OF ITS ESSENTIAL PURPOSE.</p> <p>10.4. Exceptions. NOTHING IN THIS SECTION 10 WILL BE DEEMED TO LIMIT EITHER PARTY\u2019S LIABILITY FOR WILLFUL MISCONDUCT, GROSS NEGLIGENCE, FRAUD, OR INFRINGEMENT BY ONE PARTY OF THE OTHER\u2019S INTELLECTUAL PROPERTY RIGHTS.</p>"},{"location":"legal/archive/terms.html#11-provisions-relating-to-consumers_1","title":"11. PROVISIONS RELATING TO CONSUMERS","text":"<p>11.1. Right to Withdraw. If you are a natural person and have your habitual residence within a Member State of the European Union or the European Economic Area and are entering into the Agreement as a consumer (i.e. for purposes which are outside your trade, business, craft or profession), you have the right to withdraw from the contract as described below.</p> <p>11.2. Withdrawal Period. You have the right to withdraw from this Agreement (concluded under any Subscription Plan) within 14 days without giving any reason. The withdrawal right will expire after 14 days from the day of the conclusion of the Agreement.</p> <p>11.3. Exercise of the Right to Withdraw. To exercise the right of withdrawal, you must inform us, Spacelift, Inc, of your decision to withdraw from this Agreement by an unequivocal statement (e.g. an e-mail sent to legal@spacelift.io). To meet the withdrawal deadline, it is sufficient for you to send your communication concerning your exercise of the right of withdrawal before the withdrawal period expires.</p> <p>11.4. Model Withdrawal Form. To exercise your right of withdrawal, you may use the model withdrawal form, included in Appendix No. 2 to the Act on Consumer Rights of May 20, 2014, but this is not obligatory.</p> <p>11.5. Effect of the Withdrawal. If you withdraw from this Agreement, we will reimburse you all payments received from you, without undue delay and in any event not later than 14 days from the day on which we are informed about your decision to withdraw from this Agreement. We will carry out such reimbursement using the same means of payment as you used for the initial transaction unless you have expressly agreed otherwise; in any event, you will not incur any fees as a result of such reimbursement.</p> <p>11.6. Consumer Rights. Nothing in the Agreement will affect your legal rights as a consumer. If any provision of the Agreement does not comply with the relevant law for you as a consumer, the relevant law will apply instead of this provision. The severability clause equally applies. In case of any concerns, questions, or doubts, contact us at legal@spacelift.io.</p> <p>11.7. Complaints. If you have a complaint about Services, you should contact us at contact@spacelift.io, providing as much detail as possible about the complaint, together with your name, date of execution of the Agreement, and expected means of settling a complaint. We will respond by confirming receipt and will investigate the matter. Upon receiving the complaint, we will investigate the complaint internally, taking into account the importance and complexity of the issue raised, and get back to you no later than 30 days from the receipt of the complaint.</p> <p>11.8. ADR. If you are a consumer, you may consider Alternative Dispute Resolution means in the event of a dispute with us, including referring to the trade inspection, a consumer ombudsman, or an organization whose statutory tasks include consumer protection.</p>"},{"location":"legal/archive/terms.html#12-final-provisions_1","title":"12. FINAL PROVISIONS","text":"<p>12.1. Current Version of Agreement. Usage of the Services is subject to the then-current version of the Agreement posted on the Website and we advise you to periodically review the latest currently effective Agreement. We reserve the right to update the provisions of the Agreement from time to time at our sole discretion. The updated Agreement version supersedes all prior versions, as well as is effective and binding immediately after posting on the Website. Your continued use of the Services on or after the date of the updated version of the Agreement is effective and constitutes your acceptance of such updated terms. If you do not agree to our updated Agreement, you can terminate the Subscription in accordance with Section 5.</p> <p>12.2. Applicable Law and Jurisdiction. This Agreement is governed by and construed in accordance with the Applicable Law without giving effect to any choice or conflict of law provision of any jurisdiction. Any legal suit, action, or proceeding arising out of or related to this Agreement will be subject to the exclusive jurisdiction of the Applicable Jurisdiction as provided in the following table:</p> Client Applicable Law Applicable Jurisdiction Consumers residing in the Member State of the European Union or the European Economic Area Poland Warsaw, Poland Other Clients State of Delaware, US County New Castle, Delaware, US <p>Each party irrevocably submits to the exclusive jurisdiction of such courts in any such suit, action, or proceeding.</p> <p>12.3. Contact details. In order to resolve a complaint regarding the Services, receive further information regarding the use of the Services, or send any notice to Spacelift, please contact us by email at contact@spacelift.io.</p> <p>12.4. Notices. Except as otherwise expressly set forth in this Agreement, any notice, request, consent, claim, demand, waiver, or other communications under this Agreement have legal effect and will be deemed effectively given: (a) when received, if delivered by hand or with signed confirmation of receipt; (b) when received, if sent by a nationally recognized overnight courier or by certified or registered mail, signature required; or (c) when sent, if by email, if sent during the addressee's normal business hours, and on the next business day, if sent after the addressee's normal business hours.</p> <p>12.5. Feedback. If you provide us with any suggestions, comments, recommendations, opinions, or other information relating to the Services or Website (\u201cFeedback\u201d), you grant us a royalty-free, non-exclusive, irrevocable, perpetual, worldwide right and license to use the Feedback on our websites or in marketing materials. We reserve the right to remove any Feedback posted on the Website if, in our opinion, such Feedback does not comply with the Agreement or applicable law.</p> <p>12.6. Logo usage. You grant us the right to use your name and other indicia, such as logo or trademark in our list of current or former clients in promotional materials and on our websites. Any other announcement, statement, press release, or other publicity or marketing materials relating to your use of Services will be subject to your consent.</p> <p>12.7. Export Laws. Each Party will comply with the export laws and regulations of the United States and other applicable jurisdictions in providing and using the Services. Without limiting the generality of the foregoing, Client represents that it is not named on any U.S. government denied-party list and will not make the Services available to any user or entity that is located in a country that is subject to a U.S. government embargo, or is listed on any U.S. government list of prohibited or restricted parties.</p> <p>12.8. Non-waiver. Our failure to exercise or enforce any right or provision of the Agreement will not operate as a waiver of such right or provision.</p> <p>12.9. Assignment. We may assign any or all of our rights and obligations to others at any time. We will notify you of any assignment.</p> <p>12.0. Severability. If any term or provision of this Agreement is invalid, illegal, or unenforceable in any jurisdiction, such invalidity, illegality, or unenforceability will not affect any other term or provision of this Agreement or invalidate or render unenforceable such term or provision in any other jurisdiction.</p> <p>12.11. No relationship. There is no joint venture, partnership, employment, or agency relationship created between you and us as a result of the Agreement or use of the Services.</p>"},{"location":"legal/archive/terms.html#terms-and-conditions_2","title":"Terms and conditions","text":"<p>Effective until March 6, 2023</p>"},{"location":"legal/archive/terms.html#1-agreement-to-terms","title":"1. Agreement to Terms","text":"<p>1.1 These Terms and Conditions constitute a legally binding agreement made between you, whether personally or on behalf of an entity (you), and Spacelift Inc., registered at 651 N. Broad Street, Suite 206B Middletown, County of New Castle DE 19709 United States, doing business as Spacelift (we, us), concerning your access to and use of the Spacelift (https://spacelift.io) website as well as any related applications (the Site).</p> <p>The Site provides the following services: a specialized, Terraform-compatible continuous integration and deployment (CI/CD) platform for infra-as-code (Services). You agree that by accessing the Site and/or Services, you have read, understood, and agree to be bound by all of these Terms and Conditions.</p> <p>If you do not agree with all of these Terms and Conditions, then you are prohibited from using the Site and Services and you must discontinue use immediately. We recommend that you print a copy of these Terms and Conditions for future reference.</p> <p>1.2 The supplemental policies set out in Section 1.7 below, as well as any supplemental terms and conditions or documents that may be posted on the Site from time to time, are expressly incorporated by reference.</p> <p>1.3 We may make changes to these Terms and Conditions at any time. The updated version of these Terms and Conditions will be indicated by an updated \u201cRevised\u201d date and the updated version will be effective as soon as it is accessible. You are responsible for reviewing these Terms and Conditions to stay informed of updates. Your continued use of the Site represents that you have accepted such changes.</p> <p>1.4 We may update or change the Site from time to time to reflect changes to our products, our users' needs and/or our business priorities.</p> <p>1.5 The information provided on the Site is not intended for distribution to or use by any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject us to any registration requirement within such jurisdiction or country.</p> <p>1.6 The Site is intended for users who are at least 18 years old.  If you are under the age of 18, you are not permitted to register for the Site or use the Services without parental permission.</p> <p>1.7 Subscription</p> <p>The Services may include automatically recurring payments for periodic charges (\"Subscription Service\"). If you activate a Subscription Service, you authorize Spacelift to periodically charge, on a going-forward basis and until cancellation of either the recurring payments or your account, all accrued sums on or before the payment due date for the accrued sums. The \"Subscription Billing Date\" is the date when you purchase your first subscription to the Service. Your account will be charged automatically on the Subscription Billing Date all applicable fees for the next subscription period. The subscription will continue unless and until you cancel your subscription or we terminate it. You must cancel your subscription before it renews in order to avoid billing of the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide to us during registration (or to a different payment method if you change your payment information). You may cancel the Subscription Service by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io .</p> <p>In Subscription Service:</p> <ul> <li>Seat means each user who actively logged in to the Site in the last month.</li> <li>Private worker is a single worker installed on-premise that allows you to execute Spacelift workflows on your end. You can read more about it here.</li> </ul> <p>1.8 Additional policies which also apply to your use of the Site include:</p> <ul> <li> <p>Our Privacy Notice, which sets out the terms on which we process any personal data we collect from you, or that you provide to us. By using the Site, you consent to such processing and you warrant that all data provided by you is accurate.</p> </li> <li> <p>Our Cookie Policy, which sets out information about the cookies on the Site.</p> </li> </ul>"},{"location":"legal/archive/terms.html#2-acceptable-use","title":"2. Acceptable Use","text":"<p>2.1You may not access or use the Site for any purpose other than that for which we make the site and our services available. The Site may not be used in connection with any commercial endeavors except those that are specifically endorsed or approved by us.</p> <p>2.2 As a user of this Site, you agree not to:</p> <ul> <li> <p>Systematically retrieve data or other content from the Site to a compile database or directory without written permission from us;</p> </li> <li> <p>Make any unauthorized use of the Site, including collecting usernames and/or email addresses of users to send unsolicited email or creating user accounts under false pretenses;</p> </li> <li> <p>Use the Site to advertise or sell goods and services;</p> </li> <li> <p>Circumvent, disable, or otherwise interfere with security-related features of the Site, including features that prevent or restrict the use or copying of any content or enforce limitations on the use;</p> </li> <li> <p>Trick, defraud, or mislead us and other users, especially in any attempt to learn sensitive account information such as user passwords;</p> </li> <li> <p>Make improper use of our support services, or submit false reports of abuse or misconduct;</p> </li> <li> <p>Interfere with, disrupt, or create an undue burden on the Site or the networks and services connected to the Site;</p> </li> <li> <p>Attempt to impersonate another user or person, or use the username of another user;</p> </li> <li> <p>Sell or otherwise transfer your profile;</p> </li> <li> <p>Use any information obtained from the Site in order to harass, abuse, or harm another person;</p> </li> <li> <p>Harass, annoy, intimidate, or threaten any of our employees, agents, or other users;</p> </li> <li> <p>Delete the copyright or other proprietary rights notice from any of the content;</p> </li> <li> <p>Upload or transmit (or attempt to upload or to transmit) viruses, Trojan horses, or other material that interferes with any party\u2019s uninterrupted use and enjoyment of the Site, or any material that acts as a passive or active information collection or transmission mechanism;</p> </li> <li> <p>Use the Site in a manner inconsistent with any applicable laws or regulations;</p> </li> <li> <p>Falsely imply a relationship with us or another company with whom you do not have a relationship;</p> </li> </ul>"},{"location":"legal/archive/terms.html#3-information-you-provide-to-us","title":"3. Information you provide to us","text":"<p>3.1 You represent and warrant that: (a) all registration information you submit will be true, accurate, current, and complete and relate to you and not a third party; (b) you will maintain the accuracy of such information and promptly update such information as necessary; (c) you will keep your password confidential and will be responsible for all use of your password and account; (d) you have the legal capacity and you agree to comply with these Terms and Conditions; and (e) you are not a minor in the jurisdiction in which you reside, or if a minor, you have received parental permission to use the Site.</p> <p>If you know or suspect that anyone other than you knows your user information (such as an identification code or user name) and/or password you must promptly notify us at contact@spacelift.io.</p> <p>3.2 If you provide any information that is untrue, inaccurate, not current or incomplete, we may suspend or terminate your account. We may remove or change a user name you select if we determine that such user name is inappropriate.</p> <p>3.3 As part of the functionality of the Site, you may link your account with online accounts you may have with third party service providers (each such account, a Third Party Account) by either: (a) providing your Third Party Account login information through the Site; or (b) allowing us to access your Third Party Account, as is permitted under the applicable terms and conditions that govern your use of each Third Party Account.</p> <p>You represent that you are entitled to disclose your Third Party Account login information to us and/or grant us access to your Third Party Account without breach by you of any of the terms and conditions that govern your use of the applicable Third Party Account and without obligating us to pay any fees or making us subject to any usage limitations imposed by such third party service providers.</p> <p>3.4 By granting us access to any Third Party Accounts, you understand that (a) we may access, make available and store (if applicable) any content that you have provided to and stored in your Third Party Account (the \u201cSocial Network Content\u201d) so that it is available on and through the Site via your account; and (b) we may submit and receive additional information to your Third Party Account to the extent you are notified when you link your account with the Third Party Account.</p> <p>Depending on the Third Party Accounts you choose and subject to the privacy settings that you have set in such Third Party Accounts, personally identifiable information that you post to your Third Party Accounts may be available on and through your account on the Site. Please note that if a Third Party Account or associated service becomes unavailable or our access to such Third Party Account is terminated by the third party service provider, then Social Network Content may no longer be available on and through the Site.</p> <p>You will have the ability to disable the connection between your account on the Site and your Third Party Accounts at any time. Please note that your relationship with the third party service providers associated with your third party accounts is governed solely by your agreement(s) with such third party service providers. We make no effort to review any Social Network Content for any purpose, including but not limited to, for accuracy, legality or non-infringement, and we are not responsible for any Social Network Content.</p>"},{"location":"legal/archive/terms.html#4-content-you-provide-to-us","title":"4. Content you provide to us","text":"<p>4.1 There may be opportunities for you to post content to the Site or send feedback to us (User Content). You understand and agree that your User Content may be viewed by other users on the Site, and that they may be able to see who has posted that User Content.</p> <p>4.2 In posting User Content, including reviews or making contact with other users of the Site you shall comply with our Acceptable Use Policy.</p> <p>4.3 You warrant that any User Content does comply with our Acceptable Use Policy, and you will be liable to us and indemnify us for any breach of that warranty. This means you will be responsible for any loss or damage we suffer as a result of your breach of this warranty.</p> <p>4.4 We have the right to remove any User Content you put on the Site if, in our opinion, such User Content does not comply with the Acceptable Use Policy.</p> <p>4.5 We are not responsible and accept no liability for any User Content including any such content that contains incorrect information or is defamatory or loss of User Content. We accept no obligation to screen, edit or monitor any User Content but we reserve the right to remove, screen and/or edit any User Content without notice and at any time. User Content has not been verified or approved by us and the views expressed by other users on the Site do not represent our views or values.</p> <p>4.6 If you wish to complain about User Content uploaded by other users please contact us at contact@spacelift.io.</p>"},{"location":"legal/archive/terms.html#5-our-content","title":"5. Our content","text":"<p>5.1 Unless otherwise indicated, the Site and Services including source code, databases, functionality, software, website designs, audio, video, text, photographs, and graphics on the Site (Our Content) are owned or licensed to us, and are protected by copyright and trade mark laws.</p> <p>5.2 Except as expressly provided in these Terms and Conditions, no part of the Site, Services or Our Content may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission.</p> <p>5.3 Provided that you are eligible to use the Site, you are granted a limited licence to access and use the Site and Our Content and to download or print a copy of any portion of the Content to which you have properly gained access solely for your personal, non-commercial use.</p> <p>5.4 You shall not (a) try to gain unauthorised access to the Site or any networks, servers or computer systems connected to the Site; and/or (b) make for any purpose including error correction, any modifications, adaptations, additions or enhancements to the Site or Our Content, including the modification of the paper or digital copies you may have downloaded.</p> <p>5.5 We shall (a) prepare the Site and Our Content with reasonable skill and care; and (b) use industry standard virus detection software to try to block the uploading of content to the Site that contains viruses.</p> <p>5.6 The content on the Site is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Site.</p> <p>5.7 Although we make reasonable efforts to update the information on our site, we make no representations, warranties or guarantees, whether express or implied, that Our Content on the Site is accurate, complete or up to date.</p>"},{"location":"legal/archive/terms.html#6-link-to-third-party-content","title":"6. Link to third party content","text":"<p>6.1 The Site may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third party websites or applications or the third party operator. We are not responsible for and do not endorse any third party websites or applications or their availability or content.</p> <p>6.2 We accept no responsibility for adverts contained within the Site. If you agree to purchase goods and/or services from any third party who advertises in the Site, you do so at your own risk. The advertiser, and not us, is responsible for such goods and/or services and if you have any questions or complaints in relation to them, you should contact the advertiser.</p>"},{"location":"legal/archive/terms.html#7-site-management","title":"7. Site Management","text":"<p>7.1 We reserve the right at our sole discretion, to (1) monitor the Site for breaches of these Terms and Conditions; (2) take appropriate legal action against anyone in breach of applicable laws or these Terms and Conditions; (3) refuse, restrict access to or availability of, or disable (to the extent technologically feasible) any of your Contributions; (4) remove from the Site or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (5) otherwise manage the Site in a manner designed to protect our rights and property and to facilitate the proper functioning of the Site and Services.</p> <p>7.2 We do not guarantee that the Site will be secure or free from bugs or viruses.</p> <p>7.3 You are responsible for configuring your information technology, computer programs and platform to access the Site and you should use your own virus protection software.</p>"},{"location":"legal/archive/terms.html#8-modifications-to-and-availability-of-the-site","title":"8. Modifications to and availability of the Site","text":"<p>8.1 We reserve the right to change, modify, or remove the contents of the Site at any time or for any reason at our sole discretion without notice. We also reserve the right to modify or discontinue all or part of the Services without notice at any time.</p> <p>8.2 We cannot guarantee the Site and Services will be available at all times. We may experience hardware, software, or other problems or need to perform maintenance related to the Site, resulting in interruptions, delays, or errors. You agree that we have no liability whatsoever for any loss, damage, or inconvenience caused by your inability to access or use the Site or Services during any downtime or discontinuance of the Site or Services. We are not obliged to maintain and support the Site or Services or to supply any corrections, updates, or releases.</p> <p>8.3 There may be information on the Site that contains typographical errors, inaccuracies, or omissions that may relate to the Services, including descriptions, pricing, availability, and various other information. We reserve the right to correct any errors, inaccuracies, or omissions and to change or update the information at any time, without prior notice.</p>"},{"location":"legal/archive/terms.html#9-disclaimerlimitation-of-liability","title":"9. Disclaimer/Limitation of Liability","text":"<p>9.1  The Site and Services are provided on an as-is and as-available basis. You agree that your use of the Site and/or Services will be at your sole risk except as expressly set out in these Terms and Conditions. All warranties, terms, conditions and undertakings, express or implied (including by statute, custom or usage, a course of dealing, or common law) in connection with the Site and Services and your use thereof including, without limitation, the implied warranties of satisfactory quality, fitness for a particular purpose and non-infringement are excluded to the fullest extent permitted by applicable law.</p> <p>We make no warranties or representations about the accuracy or completeness of the Site\u2019s content and are not liable for any (1) errors or omissions in content: (2) any unauthorized access to or use of our servers and/or any and all personal information and/or financial information stored on our server; (3) any interruption or cessation of transmission to or from the site or services; and/or (4) any bugs, viruses, trojan horses, or the like which may be transmitted to or through the site by any third party. We will not be responsible for any delay or failure to comply with our obligations under these Terms and Conditions if such delay or failure is caused by an event beyond our reasonable control.</p> <p>9.2 Our responsibility for loss or damage suffered by you:</p> <p>Whether you are a consumer or a business user:</p> <ul> <li> <p>We do not exclude or limit in any way our liability to you where it would be unlawful to do so. This includes liability for death or personal injury caused by our negligence or the negligence of our employees, agents or subcontractors and for fraud or fraudulent misrepresentation;</p> </li> <li> <p>If we fail to comply with these Terms and Conditions, we will be responsible for loss or damage you suffer that is a foreseeable result of our breach of these Terms and Conditions, but we would not be responsible for any loss or damage that were not foreseeable at the time you started using the Site/Services;</p> </li> </ul> <p>Notwithstanding anything to the contrary contained in the Disclaimer/Limitation of Liability section, our liability to you for any cause whatsoever and regardless of the form of the action, will at all times be limited to a total aggregate amount equal to the greater of (a) the sum of PLN 5000 or (b) the amount paid, if any, by you to us for the Services/Site during the six (6) month period prior to any cause of action arising.</p> <p>If you are a business user:</p> <p>We will not be liable to you for any loss or damage, whether in contract, tort (including negligence), breach of statutory duty, or otherwise, even if foreseeable, arising under or in connection with:</p> <ul> <li> <p>use of, or inability to use, our Site/Services; or</p> </li> <li> <p>use of or reliance on any content displayed on our Site.</p> </li> </ul> <p>In particular, we will not be liable for:</p> <ul> <li> <p>loss of profits, sales, business, or revenue;</p> </li> <li> <p>business interruption;</p> </li> <li> <p>loss of anticipated savings;</p> </li> <li> <p>loss of business opportunity, goodwill or reputation; or</p> </li> <li> <p>any indirect or consequential loss or damage.</p> </li> </ul> <p>If you are a consumer user:</p> <ul> <li> <p>Please note that we only provide our Site for domestic and private use. You agree not to use our Site for any commercial or business purposes, and we have no liability to you for any loss of profit, loss of business, business interruption, or loss of business opportunity;</p> </li> <li> <p>If defective digital content that we have supplied, damages a device or digital content belonging to you and this is caused by our failure to use reasonable care and skill, we will either repair the damage or pay you compensation.</p> </li> <li> <p>You have legal rights in relation to goods that are faulty or not as described. Advice about your legal rights is available from your local Citizens' Advice Bureau or Trading Standards office. Nothing in these Terms and Conditions will affect these legal rights.</p> </li> </ul>"},{"location":"legal/archive/terms.html#10-term-and-termination","title":"10. Term and Termination","text":"<p>10.1 These Terms and Conditions shall remain in full force and effect while you use the Site or Services or are otherwise a user of the Site, as applicable. You may terminate your use or participation at any time, for any reason, by following the instructions for terminating user accounts, if available, or by contacting us at contact@spacelift.io.</p> <p>10.2 Without limiting any other provision of these Terms and Conditions, we reserve the right to, in our sole discretion and without notice or liability, deny access to and use of the Site and the Services (including blocking certain IP addresses), to any person for any reason including without limitation for breach of any representation, warranty or covenant contained in these Terms and Conditions or of any applicable law or regulation.</p> <p>If we determine, in our sole discretion, that your use of the Site/Services is in breach of these Terms and Conditions or of any applicable law or regulation, we may terminate your use or participation in the Site and the Services or delete your profile and any content or information that you posted at any time, without warning, in our sole discretion.</p> <p>10.3 If we terminate or suspend your account for any reason set out in this Section 9, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to terminating or suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.</p>"},{"location":"legal/archive/terms.html#11-general","title":"11. General","text":"<p>11.1 Visiting the Site, sending us emails, and completing online forms constitute electronic communications. You consent to receive electronic communications and you agree that all agreements, notices, disclosures, and other communications we provide to you electronically, via email and on the Site, satisfy any legal requirement that such communication be in writing.</p> <p>You hereby agree to the use of electronic signatures, contracts, orders and other records and to electronic delivery of notices, policies and records of transactions initiated or completed by us or via the Site. You hereby waive any rights or requirements under any statutes, regulations, rules, ordinances or other laws in any jurisdiction which require an original signature or delivery or retention of non-electronic records, or to payments or the granting of credits by other than electronic means.</p> <p>11.2 These Terms and Conditions and any policies or operating rules posted by us on the Site or in respect to the Services constitute the entire agreement and understanding between you and us.</p> <p>11.3 Our failure to exercise or enforce any right or provision of these Terms and Conditions shall not operate as a waiver of such right or provision.</p> <p>11.4 We may assign any or all of our rights and obligations to others at any time.</p> <p>11.5 We shall not be responsible or liable for any loss, damage, delay or failure to act caused by any cause beyond our reasonable control.</p> <p>11.6 If any provision or part of a provision of these Terms and Conditions is unlawful, void or unenforceable, that provision or part of the provision is deemed severable from these Terms and Conditions and does not affect the validity and enforceability of any remaining provisions.</p> <p>11.7 There is no joint venture, partnership, employment or agency relationship created between you and us as a result of these Terms and Conditions or use of the Site or Services.</p> <p>11.8 For consumers only  - Please note that these Terms and Conditions, their subject matter and their formation, are governed by Polish law. You and we both agree that the courts of Poland will have exclusive jurisdiction. If you have any complaint or wish to raise a dispute under these Terms and Conditions or otherwise in relation to the Site please follow this link.</p> <p>11.9 For business users only - If you are a business user, these Terms and Conditions, their subject matter and their formation (and any non-contractual disputes or claims) are governed by Polish Law. We both agree to the exclusive jurisdiction of the courts of Poland.</p> <p>11.10 A person who is not a party to these Terms and Conditions shall have no right to enforce any term of these Terms and Conditions.</p> <p>11.11 In order to resolve a complaint regarding the Services or to receive further information regarding use of the Services, please contact us by email at contact@spacelift.io or by post to:</p> <p>Spacelift Inc. 651 N. Broad Street, Suite 206B Middletown, County of New Castle DE 19709, United States</p>"},{"location":"product/bulk-actions.html","title":"Bulk actions","text":""},{"location":"product/bulk-actions.html#bulk-actions","title":"Bulk actions","text":"<p>Spacelift allows you to perform bulk actions on various entities. This is useful when you need to perform the same action on multiple entities at once. To initiate bulk actions, select the entities you want to perform the action on.</p>"},{"location":"product/bulk-actions.html#how-to-use-bulk-actions","title":"How to use bulk actions","text":"<p>After you select one or more entities, a floating action bar will appear at the bottom of the screen. This bar will show the number of selected entities and the available actions.</p> <p></p> <p>Info</p> <p>The UI will only show the actions that can be performed on the selected entities, filtering out the unavailable ones for your convenience.</p> <p>Actions that are not available for all the selected entities will be marked with an icon. You can hover over the icon to see how many entities are going to be affected.</p> <p></p>"},{"location":"product/bulk-actions.html#performing-actions","title":"Performing actions","text":"<p>If you need a more detailed view of the selected entities, click the \"See details\" button, that will open the bulk actions drawer. From the drawer you can also dismiss any entities you deem unnecessary. Use the floating bar for quick actions and use the drawer when you need to be more careful with your selection.</p> <p></p> <p>On the drawer you will see fine-grained details on which actions are available for each of the selected entities.</p> <p></p> <p>Once you select an action, you will be presented with a confirmation step that allows you to add additional details (like a note for the Lock action) and to confirm the action itself.</p> <p></p> <p>The same view is also available on the drawer, where you can review the applicable and not applicable items again.</p> <p></p> <p>Once you confirm the action you'll be presented with the action results drawer, where you can review the status of each item. Please stay on this view until all of your actions are performed, otherwise any in-progress actions will be stopped.</p> <p>When the bulk execution is complete, you can use the \"New action\" button to perform another action on the same selection or a subset of it (from the completed or the failed results).</p> <p></p>"},{"location":"product/bulk-actions.html#stopping-actions","title":"Stopping actions","text":"<p>Note: It is possible to stop the queued actions if you make a mistake by clicking on either \"Stop all\" or the \"Stop\" button available for all queued items.</p> <p></p>"},{"location":"product/bulk-actions.html#available-bulk-actions","title":"Available bulk actions","text":""},{"location":"product/bulk-actions.html#stacks-list-available-actions","title":"Stacks list available actions","text":"<p>When you make a selection on the \"Stacks\" page you can choose the intended action from the following options:</p> <ul> <li>Approve - Adds an approval for stacks that require approval policy reviews.</li> <li>Confirm - Confirms the blocking run for stacks in the <code>Unconfirmed</code> state.</li> <li>Disable - Disables the stacks, so they will not trigger any runs.</li> <li>Discard - Discards the blocking run for stacks in the <code>Unconfirmed</code> state.</li> <li>Enable - Enables the stacks.</li> <li>Lock - Locks the stacks for exclusive use.</li> <li>Reject - Adds a rejection for stacks that require approval policy reviews.</li> <li>Run task - Manually triggers a run with a custom command.</li> <li>Sync commit - Updates stacks to point at the latest HEAD commit for their tracked branches.</li> <li>Trigger - Triggers a tracked run for stacks.</li> <li>Unlock - Unlocks stacks that are currently locked.</li> </ul>"},{"location":"product/bulk-actions.html#stack-runs-list-available-actions","title":"Stack runs list available actions","text":"<p>When you make a selection on the \"Stack runs\" page you can choose the intended action from the following options:</p> <ul> <li>Approve - Adds an approval for runs that require approval policy reviews.</li> <li>Confirm - Confirms the runs in the <code>Unconfirmed</code> state.</li> <li>Discard - Discards the runs in the <code>Unconfirmed</code> state.</li> <li>Reject - Adds a rejection for runs that require approval policy reviews.</li> </ul>"},{"location":"product/bulk-actions.html#runs-view-available-actions","title":"Runs view available actions","text":"<p>On the \"Runs\" page all the filtered runs will be available for bulk actions. Once you filter out the items you will see the bulk actions floating bar (if any of the actions are applicable) from the following options:</p> <ul> <li>Approve - Adds an approval for runs that require approval policy reviews.</li> <li>Confirm - Confirms the runs in the <code>Unconfirmed</code> state.</li> <li>Discard - Discards the runs in the <code>Unconfirmed</code> state.</li> <li>Reject - Adds a rejection for runs that require approval policy reviews.</li> </ul>"},{"location":"product/bulk-actions.html#modules-list-available-actions","title":"Modules list available actions","text":"<p>When you make a selection on the \"Modules\" page you can choose the intended action from the following options:</p> <ul> <li>Disable - Disables the modules, so they will not trigger any runs.</li> <li>Enable - Enables the modules.</li> <li>Favorite - Marks the modules as favorite, so they appear on top of the list.</li> <li>Unfavorite - Unmarks the modules as favorite.</li> </ul>"},{"location":"product/bulk-actions.html#notifications-available-actions","title":"Notifications available actions","text":"<p>When you make a selection on the \"Notifications\" page you can choose the intended action from the following options:</p> <ul> <li>Dismiss - Dismisses the notifications for easier filtering.</li> </ul>"},{"location":"product/bulk-actions.html#private-worker-pool-workers-list-available-actions","title":"Private worker pool workers list available actions","text":"<p>When you make a selection on the \"Private worker pool workers list\" page you can choose the intended action from the following options:</p> <ul> <li>Drain - Disables scheduling future runs on the workers.</li> <li>Undrain - Enables scheduling future runs on the workers.</li> </ul>"},{"location":"product/bulk-actions.html#worker-pool-queued-runs-list-available-actions","title":"Worker pool queued runs list available actions","text":"<p>When you make a selection on the \"Worker pools queued runs\" (public or private) page you can choose the intended action from the following options:</p> <ul> <li>Deprioritize - Removes the run from the priority queue.</li> <li>Discard - Discards the runs in the <code>Ready</code> state.</li> <li>Prioritize - Moves the run to the priority queue, so it's started before the other runs.</li> </ul>"},{"location":"product/bulk-actions.html#run-changes-view-available-actions","title":"Run changes view available actions","text":"<p>When you make a selection on the \"Run changes\" page you can choose the intended action from the following options:</p> <ul> <li>Replan - Triggers a targeted replan with the selected changes.</li> </ul>"},{"location":"product/dashboard.html","title":"Dashboard","text":""},{"location":"product/dashboard.html#dashboard","title":"Dashboard","text":"<p>Info</p> <p>Non-admin users can view most dashboard widgets, with the exception of the Launch Pad and User Activity widgets, which are available only to admins. In all cases, users only see stacks and runs that belong to spaces they have access to. If a user does not have read permissions for a given space, no information from that space is displayed on the dashboard.</p> <p>The dashboard is the first page you see when logging into the app. It provides an overview of your account and the status of your infrastructure.</p>"},{"location":"product/dashboard.html#creating-a-new-view","title":"Creating a new view","text":"<p>The Dashboard comes with two predefined views: <code>Overview</code> and <code>Metrics</code>. You can switch between them by clicking on the tabs at the top of the page.</p> <p>You can also create your own views to customize the dashboard to your needs. To create a new view, click on the <code>+</code> button next to the existing tabs.</p> <p></p> <p>The \"Manage views\" dialog will appear. Here you can customize the layout settings and choose the widgets you want to display.</p> <p></p>"},{"location":"product/dashboard.html#changing-the-landing-page","title":"Changing the landing page","text":"<p>The landing page of the app is the first page you see when you log in. By default, it is set to the dashboard. However, you can change it to a different page in your \"Personal settings\".</p> <p>To change the landing page, click on your profile picture in the bottom left corner of the app and select \"Personal settings\". Next, go to the \"Sidebar customization\" in the \"Personalization\" section.</p> <p></p> <p>By setting your selected page to be the default, you will be redirected to it every time you log in to the app. To select a page to be the default, click on the three dots next to the page name and select \"Set as default\".</p> <p></p>"},{"location":"product/disaster-continuity.html","title":"Disaster Continuity","text":""},{"location":"product/disaster-continuity.html#disaster-continuity","title":"Disaster Continuity","text":""},{"location":"product/disaster-continuity.html#preparation","title":"Preparation","text":"<p>The following preparation items are recommended to be followed to ensure that you are able to continue with your infrastructure as code deployments in the event of a Spacelift outage.</p>"},{"location":"product/disaster-continuity.html#state-management","title":"State Management","text":"<p>Here is a list of best practices that should be followed in regards to managing the state of your infrastructure.</p> <ul> <li>Store your state externally (e.g. Amazon S3 Bucket)</li> <li>Enable versioning on your state file to keep a record of changes</li> <li>Replicate your state file across regions</li> <li>Enable MFA on deletion to prevent accidental loss of your state file</li> </ul>"},{"location":"product/disaster-continuity.html#deployment-roles","title":"Deployment Roles","text":"<p>Within your Spacelift configuration, each Spacelift stack utilizes a given Role for deployment purposes. We will be referring to this Role as the deployment role.</p> <p>In the event of a disaster, Spacelift will presumably not be accessible or usable. You should ensure that you have appropriate access to your deployment role, to provide yourself the ability to assume it for deployment purposes, or have plans to use another role for deployment purposes.</p> <ul> <li>Keep a record of all Roles used by your Spacelift Stacks that are used for deployment purposes</li> <li>Ensure that you've done one of the following:</li> <li>Provided yourself access to the deployment role that Spacelift is using</li> <li>Have a plan to create, or have already created a break-glass role that you can use for disaster purposes</li> </ul>"},{"location":"product/disaster-continuity.html#terraform-break-glass-example-procedure","title":"Terraform Break Glass Example Procedure","text":""},{"location":"product/disaster-continuity.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Access to assume your deployment role(s)</li> <li>Terraform installed locally</li> <li>Managing your state externally (not Spacelift-managed state)</li> </ul>"},{"location":"product/disaster-continuity.html#assume-deployment-role-locally","title":"Assume deployment role locally","text":"<p>Using your favorite cloud provider, generate temporary credentials for your deployment role. With Amazon Web Services for example, this would be done using the following command:</p> <pre><code>aws sts assume-role \\\n  --role-arn &lt;your-deployment-role-arn&gt; \\\n  --role-session-name local-infra-deployment\n</code></pre> <p>Using the output from the assume-role command, set your credentials in your shell.</p> <pre><code>export AWS_ACCESS_KEY_ID=&lt;value for access key id&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;value for secret access key&gt;\nexport AWS_SESSION_TOKEN=&lt;value for session token&gt;\n</code></pre>"},{"location":"product/disaster-continuity.html#run-deployment-commands-as-required","title":"Run deployment commands as required","text":"<p>Initialize your code locally:</p> <pre><code>terraform init\n</code></pre> <p>To preview changes to be deployed:</p> <pre><code>terraform plan\n</code></pre> <p>To deploy changes:</p> <pre><code>terraform apply\n</code></pre>"},{"location":"product/feature-requests.html","title":"Feature Requests","text":""},{"location":"product/feature-requests.html#feature-requests","title":"Feature Requests","text":""},{"location":"product/feature-requests.html#we-value-your-feedback","title":"We value your feedback","text":"<p>At Spacelift we really value your feedback on how to improve our platform. We strive to engage with you at every step of our development journey, whether it\u2019s evaluating feature requests or doing user research (if you\u2019re interested in having more impact, go ahead and check out our Test Pilot Programme).</p> <p>We value your ideas and will always be honest about how feasible or viable they are for our company. We hope you understand!</p> <p>Please also remember that some ideas might be too complex, take too long to implement, or not be the best use of our time for now.</p>"},{"location":"product/feature-requests.html#submitting-a-feature-request","title":"Submitting a Feature Request:","text":"<p>You can submit a feature request by visiting our dashboard, where you can also review other feature requests. To proceed, you'll need to sign up in the tool. This allows us to ask follow-up questions and keep you informed about status changes.</p> <ul> <li> <p>Check if a similar feature request already exists. If yes, add a vote and comment on the idea. If you don\u2019t find anything similar, create a new one.</p> </li> <li> <p>One idea = one feature request.</p> </li> <li> <p>Be clear and concise.</p> </li> <li> <p>Specify not only the solution you wish for but also the problem you are trying to solve with that solution, or provide a use case. This will help us better understand what you need and evaluate if your proposal is the best way to solve it.</p> </li> <li> <p>Let us know if there\u2019s a workaround you are currently using.</p> </li> <li> <p>The title really matters. It makes the idea searchable and enables others to vote on your ideas.</p> </li> <li> <p>Be honest about the priority. Nice to have it doesn\u2019t mean it will not be delivered.</p> </li> </ul>"},{"location":"product/feature-requests.html#our-feature-requests-flow-statuses-and-what-they-mean","title":"Our Feature Requests flow - statuses and what they mean:","text":"<ol> <li> <p>Under review is the status for all newly created feature requests. Voting on those ideas is not possible, as we want to review them first.</p> <ol> <li> <p>If there is already an existing FR, we will merge it.</p> </li> <li> <p>Rejected: Something we will not deliver. Whether it\u2019s because it does not align with our strategy or for another reason, we will always provide a justification for a rejected idea.</p> </li> </ol> </li> <li> <p>Gathering feedback: We evaluated it and believe it makes sense; however, we don\u2019t guarantee any prioritization. We will keep an eye on the vote numbers and similar upcoming feature requests.</p> </li> <li> <p>Planned: We accepted the idea and are committed to delivering it.</p> </li> <li> <p>In progress: Our development team is currently working on it.</p> </li> <li> <p>Completed: Request is live on production.</p> </li> </ol> <p>We aim to evaluate all feature requests within two weeks. However, there may be times when factors like holidays or events extend this timeframe. If your request is urgent, please contact your Customer Success representative.</p>"},{"location":"product/fedramp.html","title":"FedRAMP","text":""},{"location":"product/fedramp.html#fedramp","title":"FedRAMP","text":"<p>Spacelift provides a FedRAMP Moderate Authorized environment for U.S. government agencies and contractors who need to meet federal security requirements. Our FedRAMP environment lets government organizations keep their development velocity while staying compliant with strict security standards.</p>"},{"location":"product/fedramp.html#what-is-fedramp","title":"What is FedRAMP?","text":"<p>FedRAMP (Federal Risk and Authorization Management Program) is a U.S. government-wide program that standardizes security assessment, authorization, and continuous monitoring of cloud products and services. The program ensures federal data hosted in the cloud is consistently protected at the highest security standards.</p> <p>FedRAMP compliance is required for cloud service providers working with U.S. federal agencies. Spacelift meets the FedRAMP Moderate compliance level, which covers the security standards needed for government workloads.</p>"},{"location":"product/fedramp.html#why-choose-spacelifts-fedramp-environment","title":"Why Choose Spacelift's FedRAMP Environment?","text":""},{"location":"product/fedramp.html#security-standards","title":"Security Standards","text":"<p>FedRAMP certification means your infrastructure management meets strict government security requirements:</p> <ul> <li>Rigorous security architecture and controls</li> <li>Strong encryption and approved cryptographic libraries</li> <li>Regular security patching and updates</li> <li>Continuous monitoring and compliance validation</li> </ul>"},{"location":"product/fedramp.html#operational-benefits","title":"Operational Benefits","text":"<ul> <li>Keep your existing development workflows while meeting compliance requirements</li> <li>No need to manage your own Infrastructure as Code platform</li> <li>Get the same Spacelift features you're used to, just in a secure environment</li> <li>Enterprise-level support with government-specific expertise</li> </ul>"},{"location":"product/fedramp.html#eligibility","title":"Eligibility","text":"<p>The FedRAMP environment is exclusively available to eligible organizations, including:</p> <ul> <li>U.S. Federal Government Agencies: Primary users of FedRAMP authorized services.</li> <li>Government Contractors: Organizations working under contract with the federal government handling government data.</li> <li>State and Local Governments: Increasingly adopting FedRAMP to enhance data and system security.</li> <li>Businesses Seeking Government Contracts: Organizations pursuing federal contracts typically required to use FedRAMP authorized services.</li> </ul>"},{"location":"product/fedramp.html#resources-separation-strategies","title":"Resources Separation Strategies","text":"<p>FedRAMP and commercial workloads must be isolated from one another. You can achieve this in two ways:</p>"},{"location":"product/fedramp.html#isolation-delivered-by-spacelift","title":"Isolation Delivered by Spacelift","text":"<p>To achieve full logical separation at the tenant level you need to create a second Spacelift account. The benefits of this approach include having a separate admin account, separate Identity Provider (IdP) configuration, separate audit and billing. This provides complete logical isolation between environments and ensures maximum security and compliance.</p>"},{"location":"product/fedramp.html#isolation-configured-by-the-customer","title":"Isolation Configured by The Customer","text":"<p>Spacelift supports strong multi-project isolation within a single tenant using Spaces, RBAC, private worker pools and Identity Provider (IdP) independent MFA. Each Space functions as a self-contained boundary for stacks, policies, integrations, and contexts, allowing teams to operate independently without interference.</p> <ul> <li>Spaces with role-based access control (RBAC) and resource isolation. Each Space isolates stacks, policies, worker pools, contexts, and integrations. You can restrict which users or groups have access to each Space.</li> <li>Separate Private Worker Pools that run within your own infrastructure, enabling network and execution-level isolation. The temporary run state is encrypted so only your workers can decrypt it.</li> <li>Separate customer-managed encryption keys (Bring Your Own Key / BYOK) per worker pool or per integration for data-level separation, so Spacelift does not hold the decryption key.</li> <li>Policy-driven governance. Use Spacelift\u2019s policy types (Login, Plan, Push, Approval, etc.) to enforce separation rules or guardrails over actions, role assignments, and allowed operations.</li> <li>IdP-independent MFA (multi-factor authentication). You can enforce MFA policies that are not tied exclusively to a particular identity provider, applying additional security layers for users logging in through any IdP.</li> <li>Integration with external secret managers. You can use pre-initialization hooks to fetch secrets directly from your own infrastructure, ensuring that sensitive data never leaves your environment and that Spacelift never has access to those secrets.</li> </ul> <p>Note that with this approach, commercial workloads must meet FedRAMP standards since they are hosted in the FedRAMP environment.</p>"},{"location":"product/fedramp.html#platform-features-and-limitations","title":"Platform Features and Limitations","text":"<p>Before getting started, it's important to understand how the FedRAMP environment differs from our standard SaaS platform. The FedRAMP environment provides identical functionality with two key security-driven limitations:</p> <ul> <li>No Public Workers: For security compliance, public workers are not available</li> <li>Limited Error Collection: Private worker error collection by Spacelift for troubleshooting is restricted. You are free to collect errors the way you see fit</li> </ul> <p>Hint</p> <p>Since public workers aren't available in the FedRAMP environment, you cannot use them to programmatically set up the first private worker pool.</p> <p>You can work around this by running your initial Infrastructure as Code through your favorite CI/CD pipeline (e.g., GitHub Actions) to get your first private worker pool configured.</p>"},{"location":"product/fedramp.html#getting-started","title":"Getting Started","text":"<ol> <li>Contact a Spacelift Account Executive.</li> <li>Sign a Mutual Non-Disclosure Agreement (MNDA).</li> <li>Complete identity verification.</li> <li>Receive the FedRAMP trial sign-up URL.</li> <li>Create your Spacelift account (initially disabled for compliance).</li> <li>Share account URL with Account Executive for activation.</li> <li>Begin configuration once account is enabled.</li> </ol> <p>The identity verification process is secure and follows best practices. Spacelift and their vendors only store verification results, never the ID documents themselves.</p>"},{"location":"product/fedramp.html#infrastructure-and-hosting","title":"Infrastructure and Hosting","text":"<p>Our FedRAMP environment is built on a robust, compliant infrastructure designed for government workloads.</p>"},{"location":"product/fedramp.html#hosting-details","title":"Hosting Details","text":"<ul> <li>Primary Region: AWS <code>us-east-2</code> (commercial partition).</li> <li>Disaster Recovery: Available in AWS <code>us-west-1</code>.</li> <li>Compliance Partner: Knox Systems provides the FedRAMP-compliant AWS environment. Spacelift is listed on Knox Systems' FedRAMP Marketplace listing.</li> </ul>"},{"location":"product/fedramp.html#getting-help","title":"Getting Help","text":"<p>Need to get started with FedRAMP Spacelift or have questions about compliance requirements? Here's how to reach us:</p> <ul> <li>Sales inquiries: Contact our sales team for pricing and trial access.</li> <li>Existing customers: Reach out to your Account Executive or Customer Success Manager.</li> </ul> <p>Our team can help determine if the FedRAMP environment is right for your organization's specific compliance and operational needs.</p>"},{"location":"product/migrating-to-spacelift.html","title":"Migrating to Spacelift","text":""},{"location":"product/migrating-to-spacelift.html#migrating-to-spacelift","title":"Migrating to Spacelift","text":"<p>Migrating from one Infrastructure as Code CI/CD provider to another can feel daunting. This is why we created a migration kit that takes care of the heavy lifting.</p> <p>Edit a few settings, and let it do the hard work. Then review, and possibly tweak, the generated code, and finally have your Spacelift entities created.</p> <p>There is no one-size-fits-all for this kind of migration. This is why we designed this tool to be flexible and easy to hack to meet your specific needs. Feel free to reach out to our support team if you need any help or guidance.</p>"},{"location":"product/migrating-to-spacelift.html#overview","title":"Overview","text":"<p>The migration process is as follows:</p> <ul> <li>Export the definition for your resources at your current vendor.</li> <li>Generate the Terraform code to recreate similar resources at Spacelift using the Terraform provider.</li> <li>Review and possibly edit the generated Terraform code.</li> <li>Commit the Terraform code to a repository.</li> <li>Create a manager Spacelift stack that points to the repository with the Terraform code.</li> </ul> <p>Tip</p> <p>Currently, only Terraform Cloud and Terraform Enterprise are supported as sources. The instructions below apply to both.</p>"},{"location":"product/migrating-to-spacelift.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13.1 or newer</li> <li>Poetry 2.0.1 or newer</li> </ul>"},{"location":"product/migrating-to-spacelift.html#installation","title":"Installation","text":"<ul> <li>Ensure that Python is installed.</li> <li>Download the Migration Kit: <code>git clone git@github.com:spacelift-io/spacelift-migration-kit.git</code> (or other available   methods in GitHub).</li> <li>Go to the Migration Kit folder: <code>spacelift-migration-kit</code>.</li> <li>Install the Python dependencies and the <code>spacemk</code> command in a Python virtual environment: <code>poetry install</code>.</li> <li>Activate the Python virtual environment: <code>$(poetry env activate)</code>.</li> </ul>"},{"location":"product/migrating-to-spacelift.html#usage","title":"Usage","text":""},{"location":"product/migrating-to-spacelift.html#configuration","title":"Configuration","text":"<p>Copy the <code>config.yml.example</code> file to <code>config.yml</code> and edit it as needed.</p> <p>Environment variables can be referenced by their name preceded by the <code>$</code> sign (e.g., <code>$API_TOKEN</code>). This is helpful if you do not want to store sensitive information in the configuration file.</p> <p>If a <code>.env</code> file is present at the root of the Spacelift Migration Kit folder, it will be automatically loaded when running <code>spacemk</code> and the tests, and the environment variables it contains will be available to that process.</p>"},{"location":"product/migrating-to-spacelift.html#audit","title":"Audit","text":"<p>This step is optional but recommended. It will analyze your current setup and display statistics in the terminal. Also, an Excel file with the list of entities to be migrated is created (<code>tmp/report.xlsx</code>).</p> <p>Additionally, it will perform checks and warn you of possible problems. For example, entities cannot be automatically migrated and might need to be handled manually.</p>"},{"location":"product/migrating-to-spacelift.html#migration","title":"Migration","text":"<p>The migration is split into a few different steps that need to be run in order.</p>"},{"location":"product/migrating-to-spacelift.html#export","title":"Export","text":"<p>The <code>spacemk export</code> command exports information about the source provider entities and stores them as a normalized JSON file (<code>tmp/data.json</code>).</p> <p>That file can be reviewed and modified before moving to the next step.</p>"},{"location":"product/migrating-to-spacelift.html#generate","title":"Generate","text":"<p>The <code>spacemk generate</code> command uses the normalized JSON file from the export step and uses a Jinja template to generate Terraform code that uses the Spacelift provider to create Spacelift entities that mimic the behavior of the source provider entities.</p> <p>The generated code can be found in the <code>tmp/code/main.tf</code> file. Feel free to review and edit it as needed.</p>"},{"location":"product/migrating-to-spacelift.html#publish","title":"Publish","text":"<p>Once the Terraform code has been generated, push the <code>tmp/code/main.tf</code> file to a git repository of your choosing that is available to your Spacelift account.</p>"},{"location":"product/migrating-to-spacelift.html#deploy","title":"Deploy","text":"<p>After pushing the generated Terraform has been pushed to a git repository, create a manager stack in Spacelift.</p> <p>Point it to the repository, and possibly folder, where you stored the Terraform code, and make sure to mark it as administrative.</p> <p>Finally, trigger a run to create the Spacelift entities.</p>"},{"location":"product/migrating-to-spacelift.html#set-sensitive-variable-values","title":"Set Sensitive Variable Values","text":"<p>This step can be skipped if there are no sensitive variables defined.</p> <p>To avoid storing sensitive variable values in Terraform code and the state file, the <code>generate</code> command does not set the value for those variables.</p> <p>Once the stacks have been created, set the values for the <code>spacelift</code> section of the <code>config.yml</code> file and run the <code>spacemk set-sensitive-env-vars</code> command to set the value for the sensitive environment variables.</p>"},{"location":"product/migrating-to-spacelift.html#set-terraform-variables-with-invalid-names","title":"Set Terraform Variables with Invalid Names","text":"<p>This step can be skipped if there are no Terraform variables with an invalid name.</p> <p>Among the different ways to pass variable values to Terraform, Spacelift uses environment variables named <code>TF_VAR_</code> followed by the name of a declared variable.</p> <p>However, Terraform allows the use of characters in variable names that are not allowed in environment variable names (e.g., <code>-</code>).</p> <p>To work around this issue, the Spacelift Migration Kit identifies Terraform variables with invalid names and stores them in a mounted file named <code>tf_vars_with_invalid_name.auto.tfvars</code> so that it gets automatically loaded by Terraform.</p> <p>Once the stacks have been created, set the values for the <code>spacelift</code> section of the <code>config.yml</code> file and run the <code>spacemk set-tf-vars-with-invalid-name</code> command to set the values for the Terraform variables with invalid names.</p>"},{"location":"product/migrating-to-spacelift.html#import-state","title":"Import State","text":"<p>This step will connect spacelift to TFC/TFE and import the state of the stacks created by the migration. To do this, simply run the command <code>spacemk import-state-files-to-spacelift</code>. This will create a context in spacelift with your TFC/TFE credentials, attach it to your newly created stacks and start a task to import the state from TFC directly. It does not use the state on disk (in the <code>tmp</code> directory) but rather fetches the state from TFC directly.</p> <p>When doing this step, we authenticate to both TFC/TFE and Spacelift. This means that you need to have the necessary credentials for both providers in your <code>config.yml</code> file.</p>"},{"location":"product/migrating-to-spacelift.html#create-module-versions","title":"Create Module Versions","text":"<p>This step can be skipped if there are no modules defined.</p> <p>Once the modules have been created, set the values for the <code>github</code> section of the <code>config.yml</code> file and run the <code>spacemk create-module-versions</code> command to re-create existing module versions.</p>"},{"location":"product/migrating-to-spacelift.html#cleanup","title":"Cleanup","text":"<p>All temporary local artifacts are stored in the <code>tmp</code> folder. Delete some or all of it to clean up.</p> <p>Additionally, you can destroy the Spacelift resources created by the manager stack and then the manager stack to fully remove the migration artifacts.</p> <p>The source vendor setup is left untouched by the Migration Kit and can be deleted once the migration has been verified to be successful.</p>"},{"location":"product/migrating-to-spacelift.html#customization","title":"Customization","text":"<p>Every migration is different, and while the Spacelift Migration Kit aims at doing most of the heavy lifting, there is often a need for customizing the workflow.</p> <p>Spacelift Migration Kit has been designed to be easily extended and modified. All customizations are stored in the <code>custom</code> folder.</p>"},{"location":"product/migrating-to-spacelift.html#custom-template","title":"Custom Template","text":"<p>The <code>generate</code> command uses a Jinja template that can be overridden partially or entirely by creating a file named <code>main.tf.jinja</code> in the <code>custom/templates</code> folder.</p> <p>To selectively override pieces of the base template, add the following instruction at the top of the custom template:</p> <pre><code>{% extends \"base.tf.jinja\" %}\n</code></pre> <p>Then, override any block by declaring it in the custom template.</p> <p>Here is an example:</p> <pre><code>{% extends \"base.tf.jinja\" %}\n\n{% block stacks %}\n\u2026\ncustom code to generate the Terraform code to define the Spacelift stacks\n\u2026\n{% endblock %}\n</code></pre> <p>Available blocks can be found in the base.tf.jinja template.</p> <p>It should be rarely needed, but if you can override the base template entirely by not including the <code>{% extends \"base.tf.jinja\" %}</code> instruction.</p>"},{"location":"product/migrating-to-spacelift.html#custom-command","title":"Custom Command","text":"<p>You can add a custom command by creating a Python file in the <code>custom/commands</code> folder based on the following code:</p> <pre><code>import click\n\n@click.command(help=\"Custom command.\")\ndef custom():\n    print(\"This is a custom command\")\n</code></pre> <p>The file can have any name, but we recommend naming it after the command name. In the example above, the file would be <code>custom.py</code>.</p> <p>If the custom command needs some configuration settings, they can be added to the <code>config.yml</code> file, and the configuration passed to the command:</p> <pre><code>import click\n\n@click.command(help=\"Custom command.\")\n@click.decorators.pass_meta_key(\"config\")\ndef custom(config):\n    print(f\"This is a custom command with a custom setting {config.get('custom.foo', 'bar')}\")\n</code></pre> <p>The commands are managed by the click Python library. Check its documentation or the Spacelift Migration Kit native commands for examples.</p>"},{"location":"product/migrating-to-spacelift.html#custom-exporter","title":"Custom Exporter","text":"<p>There might be no native exporter for your source provider, or you might need to tweak an existing provider.</p> <p>To do so, you can create a Python file in the <code>custom/exporters</code> folder. It must be named after the exporter and define a class named <code>&lt;CapitalizedExporterName&gt;Exporter</code> that derives from the <code>spacemk.exporters.BaseExporter</code> class for new exporters or a native exporter class when overriding an existing exporter.</p> <p>Here are a few examples:</p> Exporter Name Filename Class Name <code>foo</code> <code>foo.py</code> <code>FooExporter</code> <code>foo_bar</code> <code>foo_bar.py</code> <code>FooBarExporter</code> <p>Here is an example:</p> <pre><code>from spacemk.exporters import BaseExporter\n\nclass CustomExporter(BaseExporter):\n    def _extract_data(self) -&gt; list[dict]:\n        data = []\n\n        \u2026\n        custom code to extract data\n        \u2026\n\n        return data\n\n    def _map_data(self, src_data: dict) -&gt; dict:\n        data = []\n\n        \u2026\n        custom code to map data source data to Spacelift normalized data definitions\n        \u2026\n\n        return data\n</code></pre>"},{"location":"product/migrating-to-spacelift.html#custom-python-packages","title":"Custom Python Packages","text":"<p>If the custom Python code requires packages not included in Spacelift Migration Kit, you can create a <code>requirements.txt</code> file in the <code>custom</code> folder and install those dependencies with the following command:</p> <pre><code>pip install -r custom/requirements.txt\n</code></pre>"},{"location":"product/migrating-to-spacelift.html#storing-customizations","title":"Storing Customizations","text":""},{"location":"product/migrating-to-spacelift.html#simple-use-case","title":"Simple Use Case","text":"<p>Customizations live in the <code>custom</code> folder. This is fine for most use cases but could be a problem if more than one engineer works on the migration or if you need to collaborate with Spacelift engineers on advanced migrations.</p>"},{"location":"product/migrating-to-spacelift.html#advanced-use-case","title":"Advanced Use Case","text":"<p>For those advanced use cases, the proposed approach is to create a private clone of this repository and version your customizations there.</p> <p>Here are the steps to create the private clone:</p> <ol> <li>Create a bare clone of the repository. This is temporary and will be removed, so just do it wherever.</li> </ol> <pre><code>git clone --bare git@github.com:spacelift-io/spacelift-migration-kit.git\n</code></pre> <ol> <li> <p>Create a new private repository in your VCS provider and name it <code>spacelift-migration-kit</code>.</p> </li> <li> <p>Mirror-push your bare clone to your new <code>spacelift-migration-kit</code> repository.</p> </li> </ol> <pre><code>cd spacelift-migration-kit.git\ngit push --mirror git@github.com:&lt;ACCOUNT NAME&gt;/spacelift-migration-kit.git\n</code></pre> <ol> <li>Remove the temporary local clone you created in step 1.</li> </ol> <pre><code>cd ..\nrm -rf spacelift-migration-kit.git\n</code></pre> <ol> <li>You can now clone your <code>spacelift-migration-kit</code> repository on your machine where you see fit.</li> </ol> <pre><code>git clone git@github.com:&lt;ACCOUNT NAME&gt;/spacelift-migration-kit.git\n</code></pre> <ol> <li>Add the original <code>spacelift-migration-kit</code> repository as <code>upstream</code> to fetch updates. The example below uses GitHub but you can use any git VCS provider information.</li> </ol> <pre><code>git remote add upstream git@github.com:spacelift-io/spacelift-migration-kit.git\ngit remote set-url --push upstream DISABLE\n</code></pre> <ol> <li>The git remotes for your local clone, listed with <code>git remote -v</code> should look like this:</li> </ol> <pre><code>origin git@github.com:&lt;ACCOUNT NAME&gt;/spacelift-migration-kit.git (fetch)\norigin git@github.com:&lt;ACCOUNT NAME&gt;/spacelift-migration-kit.git (push)\nupstream git@github.com:spacelift-io/spacelift-migration-kit.git (fetch)\nupstream DISABLE (push)\n</code></pre> <ol> <li>Interact with your <code>origin</code> remote as usual. You can pull changes from the original repository by fetching from the <code>upstream</code> remote and rebasing on top of your local branch.</li> </ol> <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre> <p>There should not be any conflicts if you keep your modifications in the <code>custom</code> folder, but if there are, solve them as usual.</p>"},{"location":"product/migrating-to-spacelift.html#using-nonvcs-workspaces","title":"Using NonVCS Workspaces","text":"<p>Spacelift requires a VCS provider to create a stack. However, you can migrate a non-VCS (cli driven) workspace to spacelift using the <code>spacemk update-vcs-config</code> command. This command will read a CSV that tells spacemk \"this workspace is a non-VCS workspace and in spacelift should point at this repository\".</p> <p>Before using the <code>generate</code> command, you need to create the csv that will tie workspaces to repositories in git. In the migration kit repo, there is a <code>vcs_config.csv</code> file that you can use as a template. Fill this CSV out for all non-vcs repositories and then run <code>spacemk update-vcs-config</code> to update the stack configurations. Once this is complete, you can run the <code>generate</code> command as usual and you will see that the stacks are now pointing to the provided repository.</p>"},{"location":"product/migrating-to-spacelift.html#vcs_configcsv","title":"vcs_config.csv","text":"<p>The VCS Config CSV file must include the following headers:</p> <ul> <li><code>WorkspaceName</code> is the name of the workspace in your source TACOS provider.</li> <li><code>ProjectRoot</code> is the path to the project root your workspace should use.</li> <li><code>Branch</code> is the branch that the workspace should use.</li> <li><code>Namespace</code> is, generally, the organization or user that owns the repository. For example, if im using a repository in the spacelift-io GitHub organization, the namespace would be <code>spacelift-io</code>.</li> <li><code>RepoName</code> is the name of the repository.</li> <li><code>VCSProvider</code> is one of <code>gitlab</code>, <code>bitbucket</code> or <code>github_enterprise</code> depending on the provider you are using. Can also be unset if using the default VCS integration.</li> </ul>"},{"location":"product/migrating-to-spacelift.html#uninstallation","title":"Uninstallation","text":"<ul> <li>Delete the Python virtual environment: <code>poetry env remove --all</code>.</li> <li>Delete the <code>spacelift-migration-kit</code> folder.</li> </ul>"},{"location":"product/migrating-to-spacelift.html#support","title":"Support","text":"<p>If you found a bug or want to submit a feature request, please use the repository issues.</p> <p>If you need help or guidance, please reach out to your Solutions Engineer or our support.</p>"},{"location":"product/notifications.html","title":"Notifications","text":""},{"location":"product/notifications.html#notifications","title":"Notifications","text":"<p>As nicely stated by Murphy's law: \"Anything that can go wrong will go wrong.\". Some issues will blow in your face and be obvious, and others will be sneakier.</p> <p>In the background, Spacelift interacts with different systems (VCS providers, cloud providers, Slack, etc.), which can fail in various ways. The Notification Inbox section gives you visibility into issues arising from those interactions.</p>"},{"location":"product/notifications.html#visibility","title":"Visibility","text":"<p>Notifications are only available to admins.</p> <p>They can be checked either at the account level, which includes all the stacks.</p> <p></p> <p>Or, they can be checked for a specific stack.</p> <p></p>"},{"location":"product/notifications.html#available-categories","title":"Available Categories","text":"<p>Currently, the Notifications only include VCS provider issues, but more categories will be added soon.</p>"},{"location":"product/notifications.html#retention","title":"Retention","text":"<p>Notifications are kept for 14 days.</p>"},{"location":"product/onboarding-best-practices.html","title":"Onboarding Best Practices","text":""},{"location":"product/onboarding-best-practices.html#onboarding-best-practices","title":"Onboarding Best Practices","text":"<p>There are many ways to onboard new groups of users into Spacelift. Below are three different approaches you might take to help you get started.</p>"},{"location":"product/onboarding-best-practices.html#the-onboarding-and-stack-modules","title":"The Onboarding and Stack Modules","text":"<p>Spacelift is API first, which means everything in Spacelift is able to be terraformed. All the suggested approaches we list here will involve creating a module that will create the necessary resources for a group to be onboarded into Spacelift and a module that will create stacks inside the Space(s) that the group will be assigned to.</p> <p>The module should, minimally, create the following resources:</p> <ul> <li>Space(s) in Spacelift for the group to be assigned into.<ul> <li>It should also have an input to make the Space a child of another Space, if necessary.</li> </ul> </li> <li>A Login Policy that allows the group to log into the Space.<ul> <li>You may need to enable the login policy strategy in your organization settings.</li> </ul> </li> <li>A Cloud Integration that allows the group to connect to their cloud provider, in their specific cloud account.</li> <li>Global Plan Policies to enforce organization-wide policies.</li> <li>An Admin stack at the top level of the Space(s) to be created<ul> <li>It potentially could create a code path in git for the group to use, that the Admin stack is pointed at.</li> </ul> </li> </ul>"},{"location":"product/onboarding-best-practices.html#approach-1-segregated-admin-stacks","title":"Approach 1: Segregated Admin Stacks","text":"<p>This approach involves creating a separate Spacelift admin stack for each group of users. When a group is ready to be onboarded, they will add a reference to the \"Onboarding Module\" in their parents stack, commit it to git, and allow Spacelift to create all the necessary resources.</p> <p>Imagine you currently have a structure like this:</p> <pre><code>root\n\u251c\u2500\u2500 Administrative (Stack)\n\u251c\u2500\u2500 Team A (Space)\n\u2514\u2500\u2500 Team B (Space)\n    \u251c\u2500\u2500 Administrative (Stack)\n    \u2514\u2500\u2500 Cloud Operations (Space)\n        \u251c\u2500\u2500 Administrative (Stack)\n        \u251c\u2500\u2500 Dev (Space)\n        \u251c\u2500\u2500 Test (Space)\n        \u2514\u2500\u2500 Prod (Space)\n</code></pre> <p>Now, imagine, you're onboarding an application team, under <code>Team B</code>, called <code>App Team</code> that is a sibling of <code>Cloud Operations</code>. This is a two-step process, the first step is to call the \"Onboarding Module\" in the <code>root</code> Administrative stack. You do this in the root, because Spaces must be created at the root level. After that point, the spaces would look like this:</p> <pre><code>root\n\u251c\u2500\u2500 Administrative (Stack)\n\u251c\u2500\u2500 Team A (Space)\n\u2514\u2500\u2500 Team B (Space)\n    \u251c\u2500\u2500 Administrative (Stack)\n    \u251c\u2500\u2500 Cloud Operations (Space)\n    \u2502   \u251c\u2500\u2500 Administrative (Stack)\n    \u2502   \u251c\u2500\u2500 Dev (Space)\n    \u2502   \u251c\u2500\u2500 Test (Space)\n    \u2502   \u2514\u2500\u2500 Prod (Space)\n    \u2514\u2500\u2500 App Team (Space)\n        \u251c\u2500\u2500 Administrative (Stack)\n        \u251c\u2500\u2500 Dev (Space)\n        \u251c\u2500\u2500 Test (Space)\n        \u2514\u2500\u2500 Prod (Space)\n</code></pre> <p>From here step 2 of the process is the App team can use the administrative stack in their space to create their own stacks using the stack module, and manage their own resources. Note that the App teams administrative stack can also be used to create additional cloud integrations, contexts, etc. Your organization could also have dedicated modules for these resources.</p>"},{"location":"product/onboarding-best-practices.html#approach-2-universal-admin-stack","title":"Approach 2: Universal Admin Stack","text":"<p>This approach is similar to the Segregated Admin Stacks approach, but instead of creating a separate admin stack for each group, you create a single admin stack that can be used by all groups. This generally works better for smaller organizations that may not need to have a separate admin stack for each group. The benefit of this approach is that Spacelift's configuration all lives in one place. Your \"Onboarding\" and \"Stack\" modules could even be one and the same.</p> <p>Imagine you currently have a structure like this:</p> <pre><code>root\n\u251c\u2500\u2500 Administrative (Stack)\n\u251c\u2500\u2500 Team A (Space)\n\u2514\u2500\u2500 Team B (Space)\n    \u2514\u2500\u2500 Cloud Operations (Space)\n        \u251c\u2500\u2500 Dev (Space)\n        \u251c\u2500\u2500 Test (Space)\n        \u2514\u2500\u2500 Prod (Space)\n</code></pre> <p>Now, imagine, you're onboarding an application team, under <code>Team B</code>, called <code>App Team</code> that is a sibling of <code>Cloud Operations</code>. To do this, someone in your organization could call the \"Onboarding Module\" in the <code>root</code> Administrative stack to create all the resources necessary. Once the commit in git is called, your organization would have a structure like this:</p> <pre><code>root\n\u251c\u2500\u2500 Administrative (Stack)\n\u251c\u2500\u2500 Team A (Space)\n\u2514\u2500\u2500 Team B (Space)\n    \u251c\u2500\u2500 Cloud Operations (Space)\n    \u2502   \u251c\u2500\u2500 Dev (Space)\n    \u2502   \u251c\u2500\u2500 Test (Space)\n    \u2502   \u2514\u2500\u2500 Prod (Space)\n    \u2514\u2500\u2500 App Team (Space)\n        \u251c\u2500\u2500 Dev (Space)\n        \u251c\u2500\u2500 Test (Space)\n        \u2514\u2500\u2500 Prod (Space)\n</code></pre> <p>Moving forward, all teams would work out of the administrative stack in the <code>root</code> space.</p>"},{"location":"product/onboarding-best-practices.html#approach-3-blueprints","title":"Approach 3: Blueprints","text":"<p>Spacelift will always suggest keeping your organization's Spacelift configuration in code at all times. This is the most flexible and scalable way to manage your Spacelift configuration. This means, when you use the Blueprints approach, we suggest your organization chooses between Approach 1 or Approach 2, and then creates a blueprint for the onboarding process. The blueprint would then create commits in git adding terraform code to repositories controlled via Approach 1/2.</p> <p>The process of this would be as follows (for simplicity, we will choose approach 2):</p> <ol> <li>A team in your organization wants to be onboarded to use Spacelift</li> <li>And administrator in your organization would go to the blueprint, fill out details similar to: <code>team_name</code>, <code>team_email</code>, <code>team_github_repo</code>, <code>space_path</code> etc.</li> <li>The blueprint would then create a commit in the <code>root</code> administrative stacks code, creating a new <code>.tf</code> file that would call the \"Onboarding Module\" with the details filled out in the blueprint.</li> <li>The administrative stack would then trigger a run, creating all the necessary resources for the team to be onboarded.</li> <li>The team is now onboarded and can begin using Spacelift.</li> </ol> <p>You can also add a <code>delete</code> schedule to the blueprint with <code>delete_resources</code> set to <code>false</code> so the resulting blueprint stack is cleaned up after the onboarding process is complete and management of the space can happen in the administrative stack moving forward.</p>"},{"location":"product/archive/support.html","title":"Support","text":""},{"location":"product/archive/support.html#support","title":"Support","text":"<p>These terms are no longer available after November 3, 2024 unless otherwise agreed to in applicable written agreements.</p> <p>Spacelift offers a variety of support options depending on your needs. You should be able to find help using the resources linked below, regardless of how you use Spacelift.</p>"},{"location":"product/archive/support.html#have-you-tried","title":"Have you tried\u2026","text":"<p>Before reaching out for support, have you tried:</p> <ul> <li> <p>Searching our documentation. Most answers can be found there.</p> </li> <li> <p>Reviewing the Scope of Support section to understand what is within the scope of Spacelift support.</p> </li> </ul>"},{"location":"product/archive/support.html#contacting-support","title":"Contacting Support","text":"<p>For technical questions related to the Spacelift product, open a support ticket in the shared Slack channel (preferred, when available), or email &lt;support@spacelift.io&gt; if using Slack is not possible.</p> <p>Questions related to billing, purchasing, or subscriptions should be sent to ar@spacelift.io.</p>"},{"location":"product/archive/support.html#support-sla","title":"Support SLA","text":"<p>The SLA times listed below are the timeframes in which you can expect the first response. Spacelift will make the best effort to resolve any issues to your satisfaction as quickly as possible. However, the SLA times are not to be considered as an expected time-to-resolution.</p> Severity First Response Time Working Hours Critical 1 hour 24x7 Major 8 hours 4 am - 8 pm (ET), Mon - Fri Minor 48 hours 4 am - 8 pm (ET), Mon - Fri General Guidance 72 hours 4 am - 8 pm (ET), Mon - Fri <p>Spacelift has support engineers in Europe and the US. They observe local holidays, so the working hours might change on those days. We have engineers on-call 24/7 for Critical incidents, so those are not impacted by holidays.</p>"},{"location":"product/archive/support.html#definitions-of-severity-level","title":"Definitions of Severity Level","text":"<ul> <li>Severity 1 - Critical: A critical incident with very high impact (e.g., A customer-facing service is down for all customers).</li> <li>Severity 2 - Major: A major incident with significant impact. (e.g., A customer-facing service is down for a sub-set of customers).</li> <li>Severity 3 - Minor: A minor incident with low impact:<ul> <li>Spacelift use has a minor loss of operational functionality, regardless of the environment or usage (e.g., A system bug creates a minor inconvenience to customers).</li> <li>Important Spacelift features are unavailable or somewhat slowed, but a workaround is available.</li> </ul> </li> <li>Severity 4 - General Guidance: Implementation or production use of Spacelift is continuing, and work is not impeded (e.g., Information, an enhancement, or documentation clarification is requested, but there is no impact on the operation of Spacelift).</li> </ul> <p>Severity is assessed by Spacelift engineers based on the information at their disposal. Make sure to clearly and thoroughly communicate the extent and impact of an incident when reaching out to support to ensure it gets assigned the appropriate severity.</p>"},{"location":"product/archive/support.html#scope-of-support","title":"Scope of Support","text":"<p>The scope of support, in the simplest terms, is what we support and what we do not. Ideally, we would support everything. However, without reducing the quality of our support or increasing the price of our product, this would be impossible. These \"limitations\" help create a more consistent and efficient support experience.</p> <p>Please understand that any support that might be offered beyond the scope defined here is done at the discretion of the Support Engineer and is provided as a courtesy.</p>"},{"location":"product/archive/support.html#spacelift-features-and-adjacent-technologies","title":"Spacelift Features and Adjacent Technologies","text":"<p>Of course, we provide support for all Spacelift features, but also for adjacent parts of the third parties we integrate with.</p> <p>Here are some examples of what is in scope and what is not for some of the technologies we support:</p> Technology In Scope Out of Scope Cloud Provider Helping configure the permissions used with a Cloud Integration Helping architecture a cloud account IaC Tool Helping troubleshoot a failed deployment Helping architecture your source code VCS Provider Helping troubleshoot events not triggering Spacelift runs Advising how to best configure a VCS provider repository"},{"location":"product/archive/support.html#requirements","title":"Requirements","text":"<p>Spacelift cannot provide training on the use of the underlying technologies that Spacelift integrates with. Spacelift is a product aimed at technical users, and we expect our users to be versed in the basic usage of the technologies related to features that they seek support for.</p> <p>For example, a customer looking for help with a Kubernetes integration should understand Kubernetes to the extent that they can retrieve log files or perform other essential tasks without in-depth instruction.</p> <p>For Self-Hosted, we do not provide support for the underlying cloud account that hosts Spacelift. We expect the network, security, and other components to be configured and maintained in a way that is compatible with Spacelift requirements</p>"},{"location":"product/archive/support.html#feature-preview","title":"Feature Preview","text":""},{"location":"product/archive/support.html#alpha-features","title":"Alpha Features","text":"<p>Alpha features are not yet thoroughly tested for quality and stability, may contain bugs or errors, and be prone to see breaking changes in the future. You should not depend on them, and the functionality is subject to change. As such, support is provided on a best-effort basis.</p>"},{"location":"product/archive/support.html#beta-features","title":"Beta Features","text":"<p>We provide support for Beta features on a commercially-reasonable effort basis. Because they are not yet thoroughly tested for quality and stability, we may not yet have identified all the corner cases and may be prone to see breaking changes in the future. Also, troubleshooting might require more time and assistance from the Engineering team.</p>"},{"location":"product/billing/usage.html","title":"Usage","text":""},{"location":"product/billing/usage.html#usage","title":"Usage","text":""},{"location":"product/billing/usage.html#download-usage-data","title":"Download Usage Data","text":"<p>Usage data can be downloaded:</p> <ul> <li>from the Usage page: Organization Settings -&gt; Other -&gt; Usage</li> <li>using spacectl command: <code>spacectl profile usage-csv</code></li> <li>via the Spacelift API at <code>https://&lt;your-spacelift-host&gt;/usageanalytics/csv</code> (note: this is not a GraphQL endpoint)</li> </ul>"},{"location":"product/billing/usage.html#analyzing-usage-data","title":"Analyzing Usage Data","text":""},{"location":"product/billing/usage.html#load-csv-data-into-postgresql","title":"Load CSV data into PostgreSQL","text":"<p>Create a table for worker count and load the CSV file into the table:</p> <pre><code>create table worker_count (\n  id int,\n  count int,\n  timestamp_unix int,\n  worker_pool_name varchar(255)\n);\n</code></pre> <pre><code>psql -h &lt;host&gt; -U &lt;user&gt; -d &lt;database&gt; -c \"\\copy worker_count from '&lt;path-to-csv-file&gt;' delimiter ',' csv header\"\n</code></pre> <p>And the same for run minutes:</p> <pre><code>create table run_minutes (\n  timestamp_unix bigint,\n  state_duration_minutes float,\n  public boolean,\n  run_state varchar(255),\n  run_type varchar(255),\n  run_ulid varchar(26),\n  stack_name varchar(255),\n  stack_slug varchar(255),\n  stack_ulid varchar(26),\n  is_stack boolean,\n  worker_pool_name varchar(255),\n  worker_pool_ulid varchar(26)\n);\n</code></pre> <pre><code>psql -h &lt;host&gt; -U &lt;user&gt; -d &lt;database&gt; -c \"\\copy run_minutes from '&lt;path-to-csv-file&gt;' delimiter ',' csv header\"\n</code></pre>"},{"location":"product/billing/usage.html#elk-stack","title":"ELK stack","text":"<p>CSV files can be easily imported into Elastic Stack for visualization. The following example shows how to import your csv data into Kibana using Data Visualizer.</p> <p>Steps:</p> <ol> <li> <p>Download the CSV file.</p> <pre><code>spacectl profile usage-csv -aspect run-minutes &gt; usage.csv\n</code></pre> <p>Note: the exported CSV data doesn't contain zero values, so percentile calculations will be incorrect. To fix this, you can add zero values to the CSV file before processing it.</p> </li> <li> <p>Open Kibana and go to Machine Learning</p> <p></p> </li> <li> <p>Select the CSV file.</p> <p></p> <p></p> </li> <li> <p>Use the mappings that were automatically generated by Kibana.</p> <p></p> </li> <li> <p>Create a visualization using the imported data.</p> <p>Here's an example of a visualization that shows the number of run minutes on private workers, broken down by the run state: </p> </li> </ol>"},{"location":"product/security.html","title":"Security","text":""},{"location":"product/security.html#security","title":"Security","text":"<p>At Spacelift, your security is our first and foremost priority. We're aware of the utmost importance of security in our service and we're grateful for your trust. Here's what we're doing to earn and maintain this trust, and to keep Spacelift secure by design.</p>"},{"location":"product/security.html#certifications","title":"Certifications","text":"<p>SOC2 Type II Certified</p> <p>Certification performed by an independent external auditor, who confirms the effectiveness of internal controls in terms of Spacelift Security: Confidentiality, Integrity, Availability, and Privacy of customer data.</p>"},{"location":"product/security.html#security-audits","title":"Security Audits","text":"<p>Spacelift regularly engages with external security firms to perform audits and penetration testing at least once per year. Additionally, the Spacelift Security Team conducts internal security audits regularly in combination with automated security tooling.</p>"},{"location":"product/security.html#encryption","title":"Encryption","text":"<p>All of our data is encrypted at rest and in transit. With the exception of intra-VPC traffic between the web server and the load balancer protected by a restrictive AWS security group, all other traffic is handled using secure transport protocols. All the data sources (Amazon S3, database, Amazon SNS topics and Amazon SQS queues) are encrypted at rest using AWS KMS keys with restricted and audited access.</p> <p>Customer secrets are extra encrypted at rest in a way that should withstand even an internal attacker.</p>"},{"location":"product/security.html#security-features","title":"Security Features","text":""},{"location":"product/security.html#single-sign-on-sso","title":"Single Sign-On (SSO)","text":"<p>In addition to the default login providers (currently GitHub, GitLab, Microsoft and Google), Spacelift also supports the ability to configure Single Sign-On (SSO) via SAML or OIDC using your favorite identity provider. Using SSO, Spacelift can be configured in a password-less approach, helping your company follow a zero-trust approach. As long as your Identity Provider supports SAML or OIDC, and passing the <code>email</code> scope, you're good to go! You can learn more about our Single Sign-On support here.</p>"},{"location":"product/security.html#environment-variables","title":"Environment Variables","text":"<p>Spacelift allows for granular control of environment variables on your Stacks either by setting environment variables on a per-stack basis, or creating collections of variables as a Context. These environment variables can be created in two types: plain or secret.</p>"},{"location":"product/security.html#access-private-version-control-systems","title":"Access Private Version Control Systems","text":"<p>For customers that have private-hosted version control systems such as on-premise installations of GitHub Enterprise, or other VCS providers, Spacelift provides the ability to access your on-premise VCS securely using VCS Agent Pools.</p> <p>A single VCS Agent Pool is a way for Spacelift to communicate with a single VCS system on your side. You run VCS Agents inside of your infrastructure and configure them with your internal VCS system endpoint. They will then connect to a gateway on our backend, and we will be able to access your VCS system through them.</p> <p>Spacelift VCS Agent Pools utilize gRPC on HTTP2 for secure, high-performance connectivity.</p>"},{"location":"product/security.html#policies","title":"Policies","text":"<p>Spacelift policies provide a way to express rules as code to manage your infrastructure as a code environment. Users can build policies to control Spacelift login permissions, access controls, deployment workflows, and even govern the infrastructure itself to be deployed. Policies are based on the Open Policy Agent project and can be defined using its rule language Rego. You can learn more about policies here.</p>"},{"location":"product/security.html#responsible-disclosure","title":"Responsible disclosure","text":"<p>If you discover a vulnerability, we would like to know about it so we can take steps to address it as quickly as possible. We would like to ask you to help us better protect our clients and our systems.</p> <p>Please do the following:</p> <ul> <li> <p>email your findings to security@spacelift.io;</p> </li> <li> <p>do not take advantage of the vulnerability or problem you have discovered, for example by downloading more data than necessary to demonstrate the vulnerability or deleting or modifying other people's data;</p> </li> <li> <p>do not reveal the problem to others until it has been resolved;</p> </li> <li> <p>do not use attacks on physical security, social engineering, distributed denial of service, spam or applications of third parties, and;</p> </li> <li> <p>do provide sufficient information to reproduce the problem, so we will be able to resolve it as quickly as possible;</p> </li> </ul> <p>What we promise:</p> <ul> <li> <p>we will respond to your report within 3 business days with our evaluation of the report and an expected resolution date;</p> </li> <li> <p>if you have followed the instructions above, we will not take any legal action against you in regard to the report;</p> </li> <li> <p>we will handle your report with strict confidentiality, and not pass on your personal details to third parties without your permission;</p> </li> <li> <p>we will keep you informed of the progress towards resolving the problem;</p> </li> <li> <p>in the public information concerning the problem reported, we will give your name as the discoverer of the problem (unless you desire otherwise), and;</p> </li> <li> <p>as a token of our gratitude for your assistance, we offer a reward for every report of a security problem that was not yet known to us. The amount of the reward will be determined based on the severity of the leak and the quality of the report;</p> </li> </ul> <p>We strive to resolve all problems as quickly as possible, and we would like to play an active role in the ultimate publication on the problem after it is resolved.</p>"},{"location":"product/security/mfa.html","title":"Multi-Factor Authentication (MFA)","text":""},{"location":"product/security/mfa.html#multi-factor-authentication-mfa","title":"Multi-Factor Authentication (MFA)","text":"<p>With the introduction of IdP independent Multi-Factor Authentication (MFA), we extend our security capabilities to provide a robust and flexible authentication system. MFA at Spacelift is designed to protect your account and sensitive resources by requiring a second form of verification, adding a critical layer of security against unauthorized access.</p> <p>Warning</p> <p>Before enabling MFA, consider to set backup credentials. This ensures that you can still access your account in the event of a lost security key or other unforeseen issues. You can find more about this in the Backup Credentials section.</p>"},{"location":"product/security/mfa.html#setting-up-mfa-for-your-account","title":"Setting Up MFA for Your Account","text":""},{"location":"product/security/mfa.html#enable-mfa","title":"Enable MFA","text":"<p>Go to the Personal settings. Next, navigate to Multi-factor authentication. Here, you can add personal security keys that will be used for authentication.</p>"},{"location":"product/security/mfa.html#adding-security-keys","title":"Adding Security Keys","text":"<ul> <li>Click on the Enable button to activate MFA. </li> <li>Follow the prompt to register your security key; you can name it for easy identification.</li> <li>Once added, the key will appear in your list of security keys, complete with details like the key name, key ID, and creation date.</li> </ul>"},{"location":"product/security/mfa.html#deleting-security-keys","title":"Deleting Security Keys","text":"<p>You can remove a security key at any time. To delete, click the trash icon next to the key you wish to remove and confirm your action.</p>"},{"location":"product/security/mfa.html#setting-up-mfa-for-organization","title":"Setting Up MFA for Organization","text":"<p>In order to manage MFA in your organization, please go to the Organization settings. Next, navigate to Multi-factor authentication.</p> <p>Admins can view and delete security keys for any user within the organization to maintain the organization's security integrity.</p> <p></p>"},{"location":"product/security/mfa.html#enforce-mfa","title":"Enforce MFA","text":"<p>Organization admins can enforce MFA across the organization to ensure that all users comply with the security standards. Enforcing MFA means every active user must have at least one registered security key.</p> <p>Warning</p> <p>After MFA enforcement, existing sessions except for the current one will be invalidated. Users will be prompted to register their security keys during their next login session for continued access, please make sure they have FIDO2 complinant device to avoid lockout.</p>"},{"location":"product/security/mfa.html#after-enforcing-mfa","title":"After Enforcing MFA","text":"<p>Once MFA is enforced, all users must maintain at least one security key. The option to disable MFA (or delete all security keys) is disabled in Personal settings for them. However, Admin always has the right to delete the user's key in the Organization settings.</p>"},{"location":"product/support.html","title":"Support","text":""},{"location":"product/support.html#support","title":"Support","text":"<p>Spacelift offers a variety of support options depending on your needs. You should be able to find help using the resources linked below, regardless of how you use Spacelift.</p>"},{"location":"product/support.html#have-you-tried","title":"Have you tried\u2026","text":"<p>Before reaching out for support, have you tried:</p> <ul> <li> <p>Searching our documentation. Most answers can be found there.</p> </li> <li> <p>Reviewing the Scope of Support section to understand what is within the scope of Spacelift support.</p> </li> </ul>"},{"location":"product/support.html#contacting-support","title":"Contacting Support","text":"Plan How to get support Gold Open a support ticket in the shared Slack channel (preferred, when available), message us at support@spacelift.io or open a conversation in the chat widget in the bottom-right corner of the screen <p>Questions related to:</p> <ul> <li>billing, purchasing, or invoicing should be sent to ar@spacelift.io.</li> <li>your current subscriptions, add-ons, or renewals should be sent to your Customer Success Manager (if assigned) or cs@spacelift.io.</li> </ul>"},{"location":"product/support.html#support-via-slack-channels","title":"Support via Slack Channels","text":"<p>For tickets opened in Slack channels, our commitment to meeting the service level agreements (SLAs) as described below is contingent upon having the Slack support channel integrated within our workspace, equipped with our monitoring and ticketing tools.</p> <p>To ensure optimal support and SLA compliance, we recommend that all support interactions occur within the designated channels within our Slack workspace.</p>"},{"location":"product/support.html#support-sla","title":"Support SLA","text":"<p>The SLA times listed below are the timeframes in which you can expect the first response. Spacelift will make a reasonable effort to adhere to the response times provided below and resolve any issues to your satisfaction as quickly as possible. However, the SLA times are not to be considered an expected time to resolution.</p> Severity Gold Critical24 x 7 1 hour Major4 am - 8 pm ET, business days* 4 business hours Minor4 am - 8 pm ET, business days* 24 business hours General Guidance4 am - 8 pm ET, business days* 72 business hours <p>* Business day - any day in which normal business operations are conducted (Mon-Fri, except for US public holidays)</p>"},{"location":"product/support.html#definitions-of-severity-level","title":"Definitions of Severity Level","text":"<p>Below you will find the definition of severity for each issue:</p> <ul> <li>Severity 1 - Critical: A critical issue of Spacelift product with very high impact (e.g., a customer-facing service is down for all customers).</li> <li>Severity 2 - Major: A major issue of Spacelift product with significant impact (e.g., a customer-facing service is down for a subset of customers).</li> <li>Severity 3 - Minor:  A minor issue of Spacelift product with low impact:<ul> <li>Spacelift use has a minor loss of operational functionality, regardless of the environment or usage (e.g., a system bug creates a minor inconvenience to users).</li> <li>Important Spacelift features are unavailable or somewhat slowed, but a workaround is available.</li> </ul> </li> <li>Severity 4 - General Guidance: Implementation or production use of Spacelift is continuing, and work is not impeded (e.g., information, an enhancement, or documentation clarification is requested, but there is no impact on the operation of the services provided by Spacelift).</li> </ul> <p>Severity is assessed by Spacelift engineers based on the information at their disposal. Make sure to clearly and thoroughly communicate the extent and impact of an incident when reaching out to support to ensure it gets assigned the appropriate severity.</p>"},{"location":"product/support.html#scope-of-support","title":"Scope of Support","text":"<p>The scope of support, in the simplest terms, is what we support and what we do not. Ideally, we would support everything. However, without reducing the quality of our support or increasing the price of our product, this would be impossible. These \"limitations\" help create a more consistent and efficient support experience.</p> <p>Please understand that any support that might be offered beyond the scope defined here is done at the discretion of the Support Engineer and is provided as a courtesy</p>"},{"location":"product/support.html#spacelift-features-and-adjacent-technologies","title":"Spacelift Features and Adjacent Technologies","text":"<p>Of course, we provide support for all Spacelift features, but also for adjacent parts of the third parties we integrate with.</p> <p>Here are some examples of what is in scope and what is not for some of the technologies we support:</p> Technology In Scope Out of Scope Cloud Provider Helping configure the permissions used with a Cloud Integration Helping architecture a cloud account IaC Tool Helping troubleshoot a failed deployment Helping architecture your source code VCS Provider Helping troubleshoot events not triggering Spacelift runs Advising how to best configure a VCS provider repository"},{"location":"product/support.html#requirements","title":"Requirements","text":"<p>Spacelift cannot provide training on the use of the underlying technologies that Spacelift integrates with. Spacelift is a product aimed at technical users, and we expect our users to be versed in the basic usage of the technologies related to features that they seek support for.</p> <p>For example, a customer looking for help with a Kubernetes integration should understand Kubernetes to the extent that they can retrieve log files or perform other essential tasks without in-depth instruction.</p> <p>For Self-Hosted, we do not provide support for the underlying cloud account that hosts Spacelift. We expect the network, security, and other components to be configured and maintained in a way that is compatible with Spacelift requirements</p>"},{"location":"product/support.html#feature-preview","title":"Feature Preview","text":""},{"location":"product/support.html#alpha-features","title":"Alpha Features","text":"<p>Alpha features are not yet thoroughly tested for quality and stability, may contain bugs or errors, and be prone to see breaking changes in the future. You should not depend on them, and the functionality is subject to change. As such, support is provided on a best-effort basis.</p>"},{"location":"product/support.html#beta-features","title":"Beta Features","text":"<p>We provide support for Beta features on a commercially-reasonable effort basis. Because they are not yet thoroughly tested for quality and stability, we may not yet have identified all the corner cases and may be prone to see breaking changes in the future. Also, troubleshooting might require more time and assistance from the Engineering team.</p>"},{"location":"vendors/ansible.html","title":"Ansible","text":""},{"location":"vendors/ansible.html#ansible","title":"Ansible","text":"<p>You can find more details in the subpages:</p> <ul> <li>Getting Started</li> <li>Reference</li> <li>Using Policies with Ansible stacks</li> <li>Ansible Galaxy</li> <li>Configuration Management View</li> </ul>"},{"location":"vendors/ansible.html#why-use-ansible","title":"Why use Ansible?","text":"<p>Ansible is a versatile and battle-tested infrastructure configuration tool. It can do anything from software provisioning to configuration management and application deployment.</p> <p>You can tap into the wealth of roles, playbooks, and collections available online to get started in no time.</p>"},{"location":"vendors/ansible.html#why-use-spacelift-with-ansible","title":"Why use Spacelift with Ansible?","text":"<p>Spacelift helps you manage the complexities and compliance challenges of using Ansible. It brings with it a GitOps flow, so your infrastructure repository is synced with your Ansible Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies, which lets you automate compliance checks and build complex multi-stack workflows.</p> <p>You can also use Spacelift to mix and match Terraform, Pulumi, AWS CloudFormation, Kubernetes, and Ansible Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like a set of AWS EC2 instances with all their dependencies) and then connect that to an Ansible Stack which then transactionally configures these EC2 instances using trigger policies.</p>"},{"location":"vendors/ansible/ansible-galaxy.html","title":"Ansible Galaxy","text":""},{"location":"vendors/ansible/ansible-galaxy.html#ansible-galaxy","title":"Ansible Galaxy","text":"<p>If you followed previous examples in our Ansible documentation, you might have noticed that we do not do much in the Initialization phase.</p> <p></p> <p>If it comes to Ansible stacks, during that phase we try to auto-detect the requirements.yml file that will be used to install dependencies. We will look for it in the following locations:</p> <ul> <li><code>requirements.yml</code> in the root directory</li> <li><code>roles/requirements.yml</code> for roles requirements</li> <li><code>collections/requirements.yml</code> for collections requirements</li> </ul> <p>Tip</p> <p>We also check for the alternative <code>.yaml</code> extension for the paths listed above.</p> <p>We can also use a custom location for a requirements file based on the filepath provided in <code>SPACELIFT_ANSIBLE_REQUIREMENTS_FILE</code> environment variable. More details in the reference section.</p> <p>As an example, try using an example <code>requirements.yml</code> file.</p> Example requirements.yml file<pre><code>---\ncollections:\n  - name: community.grafana\n    version: 1.3.1\n</code></pre> <p>After our Initialization phase detects this file, it will use Ansible Galaxy to install those dependencies.</p> <p></p>"},{"location":"vendors/ansible/configuration-management.html","title":"Configuration Management","text":""},{"location":"vendors/ansible/configuration-management.html#configuration-management","title":"Configuration Management","text":"<p>Configuration Management View, is a view designed to enhance visibility, control, and monitoring of Ansible tasks across your stacks and runs.</p> <p>This feature is available only for Ansible stacks, offering a focused way to monitor the last status of each item in your Ansible inventory. Below is an overview of the updates and functionality:</p>"},{"location":"vendors/ansible/configuration-management.html#where-youll-find-it","title":"Where You\u2019ll Find It","text":""},{"location":"vendors/ansible/configuration-management.html#stack-view","title":"Stack View","text":"<p>The Configuration Management View is available in the Stack View, replacing the Resources View for Ansible stacks.</p> <p></p> <p>Info</p> <p>Since the new changes replace Resources View you will not be able to see Resources View in Ansible stacks by default. If you'd like to see it (e.g. because you have some historical resources you want to investigate there), you can toggle between the Configuration Management View and the previous Resources View using the \u201cEnable configuration management view\u201d toggle.</p>"},{"location":"vendors/ansible/configuration-management.html#resources-view","title":"Resources View","text":"<p>The Configuration Management View is also added as a separate tab in the Resources View.</p> <p></p>"},{"location":"vendors/ansible/configuration-management.html#run-view","title":"Run View","text":"<p>The Tasks Tab in the Run View provides detailed visibility into Ansible task execution during a run. </p>"},{"location":"vendors/ansible/configuration-management.html#key-features","title":"Key Features","text":""},{"location":"vendors/ansible/configuration-management.html#task-monitoring","title":"Task Monitoring","text":"<ul> <li>View the last status of every item in your Ansible inventory, showing the outcome of the most recent run.</li> <li>Navigate seamlessly through tasks to analyze their status, logs, and execution details.</li> </ul>"},{"location":"vendors/ansible/configuration-management.html#enhanced-run-list-view","title":"Enhanced Run List View","text":"<ul> <li>The updated run list now includes Ansible statuses, providing immediate insights without diving into individual runs.</li> </ul>"},{"location":"vendors/ansible/configuration-management.html#detailed-logs-in-task-details","title":"Detailed Logs in Task Details","text":"<p>Access detailed logs for task execution in the task details tab to diagnose and debug issues efficiently.</p>"},{"location":"vendors/ansible/configuration-management.html#why-use-the-configuration-management-view","title":"Why Use the Configuration Management View?","text":"<ul> <li>Increase Visibility: Unified workflows for all tools, including Terraform, OpenTofu, and Ansible.</li> <li>Encourage Automation: Seamlessly integrate infrastructure control and configuration management with stack dependencies.</li> <li>Improve Audit Capabilities: Collect, analyze, and filter data from execution logs.</li> <li>Understand Tasks Intuitively: Visualize and filter tasks with ease.</li> </ul>"},{"location":"vendors/ansible/getting-started.html","title":"Getting Started","text":""},{"location":"vendors/ansible/getting-started.html#getting-started","title":"Getting Started","text":""},{"location":"vendors/ansible/getting-started.html#prerequisites","title":"Prerequisites","text":"<p>To follow this tutorial, you should have an EC2 instance together with an SSH private key that can be used to access the instance ready.</p>"},{"location":"vendors/ansible/getting-started.html#initial-setup","title":"Initial Setup","text":"<p>Start by forking our Ansible example repository</p> <p>Looking at the code, you'll find that it configures a simple Apache HTTP Server on an EC2 instance. We are also using the AWS EC2 inventory plugin to find the hosts to configure. Feel free to modify <code>aws_ec2.yml</code> inventory file to fit your needs.</p> <p>Also, please take notice of the Spacelift runtime config file, that defines the runner image used on this stack and the <code>ANSIBLE_CONFIG</code> environment variable. Remember you can always define runner image in stack settings, and environment variables within environment settings.</p>"},{"location":"vendors/ansible/getting-started.html#creating-a-stack","title":"Creating a stack","text":"<p>In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift.</p> <p>In the first step of the stack creation wizard, choose a name for your stack:</p> <p></p> <p>In the next step, choose the repository that you have just cloned:</p> <p></p> <p>Next, choose the Ansible vendor, and enter the name of the playbook you want to execute. In the case of our example, it is playbook.yml.</p> <p></p> <p>Next, continue to the define behavior section, and enter a runner image to use. This should point at a container image that contains the version of Ansible you require. You may use your own image (with the Ansible version you choose and all the required dependencies) or use one of our default ones. In this example we are using an AWS-based runner image defined in the runtime configuration file: <code>public.ecr.aws/spacelift/runner-ansible-aws:latest</code>.</p> <p>If you have a private worker pool you'd like to use, you can specify it there instead of the default public one as well.</p> <p></p> <p>At this stage, you can go ahead and continue to the end of the stack creation wizard and create your stack:</p> <p></p>"},{"location":"vendors/ansible/getting-started.html#triggering-the-stack","title":"Triggering the Stack","text":""},{"location":"vendors/ansible/getting-started.html#making-sure-the-inventory-plugin-works","title":"Making sure the inventory plugin works","text":"<p>You can now click Trigger to create a new Spacelift Run.</p> <p>You should see the run finishing with no hosts matched. This is because the AWS EC2 inventory plugin did not detect valid AWS credentials.</p> <p></p> <p>Info</p> <p>You need to configure the AWS integration to give Spacelift access to your AWS account. You can find the details here.</p>"},{"location":"vendors/ansible/getting-started.html#configuring-ssh-keys","title":"Configuring SSH keys","text":"<p>After triggering a run again, you will see we could successfully find EC2 hosts (provided they could be localized with aws_ec2.yml inventory file filters), but we cannot connect to them using SSH. The reason for that is we did not configure SSH keys yet.</p> <p></p> <p>Let's configure the correct credentials using the Environment.</p> <p>Go to the Environment tab and add the private key that can be used to access the machine as a secret mounted file.</p> <p></p> <p>You should also specify the location of the SSH private key for Ansible, and you can do that using the <code>ANSIBLE_PRIVATE_KEY_FILE</code> environment variable.</p> <p></p>"},{"location":"vendors/ansible/getting-started.html#investigating-planned-changes","title":"Investigating planned changes","text":"<p>Triggering a run again, you should successfully see it get through the planning phase and end up in the unconfirmed state.</p> <p></p> <p>In the plan, you can see detailed information about each resource that is supposed to be created.</p> <p>At this point, you can investigate the changes Ansible playbook will apply and to which hosts.</p> <p>When you're happy with the planned changes, click Confirm to apply them.</p> <p></p> <p>By now, the machine should be configured with a simple Apache HTTP server with a sample website on port 8000.</p> <p>You can switch to the Resources tab to see the hosts you have configured, together with the history of when the host was last created and updated.</p> <p></p>"},{"location":"vendors/ansible/getting-started.html#conclusion","title":"Conclusion","text":"<p>That's it! You can find more details about the available configuration settings in the reference. Apart from configuration options available within Spacelift remember you can configure Ansible however you'd like using native Ansible configuration capabilities.</p>"},{"location":"vendors/ansible/policies.html","title":"Spacelift Policies with Ansible","text":""},{"location":"vendors/ansible/policies.html#spacelift-policies-with-ansible","title":"Spacelift Policies with Ansible","text":""},{"location":"vendors/ansible/policies.html#plan-policy-with-ansible","title":"Plan Policy with Ansible","text":"<p>Using our native Plan Policy you could implement advanced handling of different situations that can happen while running playbooks on multiple hosts.</p> <p>One good example is if you'd like to manually review situations when some of the hosts were unreachable, but still be able to apply the change regardless.</p> <p>You could test it using the following Plan Policy:</p> <pre><code>package spacelift\n\nwarn[\"Some hosts were unreachable\"] {\n  input.ansible.dark != {}\n}\n\nsample { true }\n</code></pre> <p>Once you attach the above Plan Policy to an Ansible stack that is configured in a way not to fail when finding unreachable hosts, you could automatically detect unreachable hosts using a Plan Policy and require approval using the Approval Policy.</p> <p>Please find an example policy evaluation below: </p>"},{"location":"vendors/ansible/policies.html#linking-terraform-and-ansible-workflows","title":"Linking Terraform and Ansible workflows","text":"<p>You can use our Trigger Policy to link multiple stacks together. This applies also to stacks from different vendors.</p> <p>One of the use cases is to link Terraform and Ansible workflows so that you could use Ansible to configure EC2 instances you've just created using Terraform.</p> <p>We provide an extensive example of one way to set something like this up in our Terraform-Ansible workflow demo repository.</p>"},{"location":"vendors/ansible/reference.html","title":"Reference","text":""},{"location":"vendors/ansible/reference.html#reference","title":"Reference","text":""},{"location":"vendors/ansible/reference.html#stack-settings","title":"Stack Settings","text":"<ul> <li>Playbook - A playbook file to run on a stack.</li> <li>Skip Plan - Runs on Spacelift stacks typically have a planning and an applying phase. In Ansible for the planning phase we are running Ansible in check mode. However, not all Ansible modules support check mode and it can result in a run failure. You could configure your playbook to ignore certain errors (e.g. using <code>ignore_errors: \"{{ ansible_check_mode }}\"</code>) or choose to skip the planning phase entirely (e.g. in situations when handling check failures at the playbook level is not an option). On Ansible stacks, this phase can be skipped without execution by setting the <code>SPACELIFT_SKIP_PLANNING</code> environment variable to true in the stack's environment variables.</li> </ul>"},{"location":"vendors/ansible/reference.html#other-settings","title":"Other settings","text":"<p>For most of the settings below, there is usually more than one way to configure it (usually either through environment variables or through <code>ansible.cfg</code> file). More on Ansible configuration can be found in official Ansible docs.</p>"},{"location":"vendors/ansible/reference.html#ssh-private-key-location","title":"SSH private key location","text":"<p>If you want to use SSH to connect to your hosts you will need to provide a path to  the SSH private key. You can do that using the <code>ANSIBLE_PRIVATE_KEY_FILE</code> environment variable.</p>"},{"location":"vendors/ansible/reference.html#forcing-color-mode-for-ansible","title":"Forcing color mode for Ansible","text":"<p>By default, Ansible will color the output. You can disable colored output using the <code>ANSIBLE_FORCE_COLOR</code> environment variable.</p>"},{"location":"vendors/ansible/reference.html#debugging-ansible-runs","title":"Debugging Ansible runs","text":"<p>When running into issues with Ansible playbooks a good way to debug the runs is to increase the Ansible verbosity level using the <code>ANSIBLE_VERBOSITY</code> environment variable.</p>"},{"location":"vendors/ansible/reference.html#controlling-ssh-controlpath-parameter","title":"Controlling SSH ControlPath parameter","text":"<p>Ansible uses <code>ControlMaster</code> and <code>ControlPath</code> SSH options to speed up playbook execution. On some occasions, you might want to modify default values to make them compatible with your execution environment. Depending on your exact setup, you might want to adjust some of the SSH settings Ansible uses.</p> <p>The default value for <code>ANSIBLE_SSH_CONTROL_PATH_DIR</code> is <code>/tmp/.ansible/cp</code>.</p>"},{"location":"vendors/ansible/reference.html#specifying-additional-cli-flags","title":"Specifying additional CLI flags","text":"<p>You can specify additional CLI flags using the following environment variables:</p> <ul> <li><code>SPACELIFT_ANSIBLE_CLI_ARGS</code> - will take effect for both planning and applying phases</li> <li><code>SPACELIFT_ANSIBLE_CLI_ARGS_plan</code> - will take effect only for the planning phase</li> <li><code>SPACELIFT_ANSIBLE_CLI_ARGS_apply</code> - will take effect only for the applying phase</li> </ul> <p>If both phase specific and generic flags are used, both will take effect.</p> <p></p>"},{"location":"vendors/ansible/reference.html#overriding-default-requirementsyml-file-location","title":"Overriding default <code>requirements.yml</code> file location","text":"<p>As specified in Ansible Galaxy section, we search for <code>requirements.yml</code> in a couple of default locations. If you wish to use <code>requirements.yml</code> file from a custom location, you can overwrite the location by setting <code>SPACELIFT_ANSIBLE_REQUIREMENTS_FILE</code> to a filepath of your <code>requirements.yml</code> file.</p>"},{"location":"vendors/ansible/reference.html#file-permissions","title":"File permissions","text":"<p>There are a few nuances with certain files' permissions when using Ansible.</p>"},{"location":"vendors/ansible/reference.html#ansiblecfg","title":"ansible.cfg","text":"<p>If you use <code>ansible.cfg</code> file within a repository (or - more generally - within the current working directory) make sure that permissions on that file (and parent directory) are set properly. You can find more details in official Ansible documentation in the section on avoiding security risks with <code>ansible.cfg</code></p>"},{"location":"vendors/ansible/reference.html#ssh-private-key-files","title":"SSH private key files","text":"<p>If you are using SSH to connect to your hosts, then you need to make sure that private keys delivered to the worker have the correct permissions.</p> <p>As the ssh main page states:</p> <p>These files contain sensitive data and should be readable by the user but not accessible by others (read/write/execute). <code>ssh</code> will simply ignore a private key file if it is accessible by others.</p> <p>Typically, you would like to deliver private keys directly at the worker level where you can fully manage your environment. If that is not an option, you can always use our read-only mounted files or any other option you find suitable.</p>"},{"location":"vendors/cloudformation.html","title":"AWS CloudFormation","text":""},{"location":"vendors/cloudformation.html#aws-cloudformation","title":"AWS CloudFormation","text":"<p>You can find more details in the subpages:</p> <ul> <li>Getting Started</li> <li>Reference</li> <li>Integrating with AWS Serverless Application Model (SAM)</li> <li>Integrating with the Serverless Framework</li> </ul>"},{"location":"vendors/cloudformation.html#why-use-cloudformation","title":"Why use CloudFormation?","text":"<p>CloudFormation is an excellent Infrastructure-as-Code tool that supports transactional deploys (automatically rolling back on failure), has a rich construct library, and does not require separate state management like Terraform or Pulumi.</p> <p>Even if you don't want to write YAML/JSON files directly, there are multiple frameworks that let you write your CloudFormation config in more ergonomic, general-purpose languages.</p>"},{"location":"vendors/cloudformation.html#why-use-spacelift-with-cloudformation","title":"Why use Spacelift with CloudFormation?","text":"<p>Spacelift helps you manage the complexities and compliance challenges of using CloudFormation. It brings with it a GitOps flow, so your infrastructure repository is synced with your CloudFormation Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies, which lets you automate compliance checks and build complex multi-stack workflows.</p> <p>You can also use Spacelift to mix and match Terraform, Pulumi, and CloudFormation Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like an ECS/EKS cluster with all its dependencies) and then connect that to a CloudFormation Stack which then transactionally deploys your services there using trigger policies and the Spacelift provider run resources for workflow orchestration and Contexts to export Terraform outputs as CloudFormation input parameters.</p>"},{"location":"vendors/cloudformation.html#does-spacelift-support-cloudformation-frameworks","title":"Does Spacelift support CloudFormation frameworks?","text":"<p>Yes! We support AWS CDK, AWS Serverless Application Model (SAM), and the Serverless Framework. You can read more about it in the relevant subpages of this document.</p>"},{"location":"vendors/cloudformation.html#template-bucket-limitations","title":"Template bucket limitations","text":"<p>Spacelift uses a user-provided S3 bucket to upload templates to as part of applying your changes. When creating this bucket, please make sure that the bucket name does not contain any periods (<code>.</code>). Using a bucket name containing periods will cause the template upload to fail.</p>"},{"location":"vendors/cloudformation.html#drift-detection-limitations","title":"Drift Detection limitations","text":"<p>While Spacelift now supports drift detection for CloudFormation stacks using AWS's native <code>DetectStackDrift</code> API, there are important limitations to be aware of:</p>"},{"location":"vendors/cloudformation.html#reconciliation-limitations","title":"Reconciliation Limitations","text":"<p>CloudFormation stacks are excluded from Spacelift's reconciliation workflows because CloudFormation only provides drift detection capabilities without built-in reconciliation support. This means:</p> <ul> <li>Drift detection can identify changes made outside of CloudFormation management</li> <li>Automatic reconciliation of detected drift is not supported</li> <li>You must manually address detected drift through your regular deployment processes</li> </ul>"},{"location":"vendors/cloudformation/getting-started.html","title":"Getting Started","text":""},{"location":"vendors/cloudformation/getting-started.html#getting-started","title":"Getting Started","text":""},{"location":"vendors/cloudformation/getting-started.html#initial-setup","title":"Initial Setup","text":"<p>Start by forking our AWS CloudFormation example repository</p> <p>Looking at the code, you'll find that it creates two simple Lambda Functions in nested Stacks and a common API Gateway REST API, which provides access to both of them.</p> <p>In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift.</p> <p>In the first screen, you should select the repository you've just forked, as can be seen in the picture.</p> <p></p> <p>In the next screen, you should choose the CloudFormation backend. There, fill in the Region field with the AWS region you want to create the CloudFormation Stack in. You should also create an Amazon S3 bucket for template storage and provide its name in the Template Bucket field. We won't automatically create this bucket.</p> <p>Warning</p> <p>Please make sure that the name of the S3 bucket you use as your template bucket does not contain any periods (<code>.</code>). Using bucket names with periods will cause template uploads to fail.</p> <p>The Entry Template File should be set to main.yaml (based on the code in our repository) and the Stack Name to a unique CloudFormation Stack name in your AWS account. We'll use cloudformation-example in the pictures.</p> <p></p> <p>You can leave the settings on the next page (Define Behavior) unchanged. If you have a private worker pool you'd like to use, specify it there instead of the default public one.</p> <p>Finally, choose a name for your Spacelift Stack on the last page. We'll use cloudformation-example again.</p> <p></p> <p></p> <p>You'll also have to configure the AWS integration to give Spacelift access to your AWS account. You can find the details here: AWS</p>"},{"location":"vendors/cloudformation/getting-started.html#deploying-the-stack","title":"Deploying the Stack","text":"<p>You can now click Trigger to create a new Spacelift Run.</p> <p>And... oh no! It failed! However, the error message is quite straightforward. We're lacking the relevant capability.</p> <p></p> <p>We can acknowledge this capability by setting the <code>CF_CAPABILITY_IAM</code> environment variable to <code>1</code>.</p> <p>There's a bunch of optional settings for CloudFormation Stacks we expose this way. You can read up on all of them in the reference.</p> <p></p> <p>Triggering a run again, you should successfully see it get through the planning phase and end up in the unconfirmed state.</p> <p>In the plan, you can see detailed information about each resource that is supposed to be created.</p> <p></p> <p>You can also click the ADD +10 tab to see a concise overview of the resources to be created.</p> <p></p> <p>When you're happy with the planned changes, click Confirm to apply them.</p> <p>This will show you a feed of the creation and update events happening in the root Stack and all nested Stacks, which will stop after the creation finishes.</p> <p></p> <p>Great! The resources have successfully been created.</p> <p>You can now switch to the Outputs tab and find the URLBase output. You can curl that URL with a hello1 or hello2 suffix to get responses from your Lambda Functions.</p> <p></p> <p></p> <p>You can also switch to the Resources tab to explore the resources you've created.</p> <p></p>"},{"location":"vendors/cloudformation/getting-started.html#conclusion","title":"Conclusion","text":"<p>That's it! You can find more details about the available configuration settings in the reference, or you can check out how to use AWS Serverless Application Model (SAM) or the Serverless Framework to generate your CloudFormation templates.</p>"},{"location":"vendors/cloudformation/integrating-with-cdk.html","title":"Integrating with AWS Cloud Development Kit (CDK)","text":""},{"location":"vendors/cloudformation/integrating-with-cdk.html#integrating-with-aws-cloud-development-kit-cdk","title":"Integrating with AWS Cloud Development Kit (CDK)","text":"<p>To use AWS Cloud Development Kit in an AWS CloudFormation stack you'll need to do two things: create a Docker image with AWS CDK and invoke it in <code>before_plan</code> hooks.</p>"},{"location":"vendors/cloudformation/integrating-with-cdk.html#preparing-the-image","title":"Preparing the image","text":"<p>Since our base image doesn't support AWS CDK, you will have to build your image that includes it as well as any tooling needed to run whatever language your infrastructure declaration is written in.</p> <p>The example below shows a Dockerfile with attached <code>cdk</code> and <code>go</code>:</p> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\nUSER root\n\n# Install packages\nRUN apk update &amp;&amp; apk add --update --no-cache npm\n# Update NPM\nRUN npm update -g\n# Install cdk\nRUN npm install -g aws-cdk\nRUN cdk --version\n\n# Add Go\nCOPY --from=golang:1.19-alpine /usr/local/go/ /usr/local/go/\n\nENV PATH=\"/usr/local/go/bin:${PATH}\"\nRUN go version\n</code></pre> <p>You should build it, push it to a repository, and set it as the Runner Image of your Stack.</p>"},{"location":"vendors/cloudformation/integrating-with-cdk.html#adding-before_plan-hooks","title":"Adding <code>before_plan</code> hooks","text":"<p>For the AWS CDK code to be properly interpreted by Spacelift, you have to customize the default stack workflow by enriching them with hooks. To create a CloudFormation template that can be interpreted by Spacelift, you will have to add these hooks to the <code>before_plan</code> stage:</p> <ul> <li><code>cdk bootstrap</code> - to bootstrap your AWS CDK project.</li> <li><code>cdk synth</code> - to create a CloudFormation template.</li> </ul>"},{"location":"vendors/cloudformation/integrating-with-cdk.html#limitations","title":"Limitations","text":""},{"location":"vendors/cloudformation/integrating-with-cdk.html#multiple-template-aws-cdk-definition","title":"Multiple template AWS CDK definition","text":"<p>The default CloudFormation integration in Spacelift uses a single CloudFormation template. That means that AWS CDK definitions that generate multiple templates will only have a single template picked up for further processing. To mitigate this, consider unifying the AWS CDK definition to generate a single template file.</p>"},{"location":"vendors/cloudformation/integrating-with-cdk.html#deploying-lambdas","title":"Deploying Lambdas","text":"<p>Our integration doesn't use <code>cdk deploy</code>, but rather uses template definitions created by <code>cdk synth</code>.</p> <p><code>cdk deploy</code> deploys Lambda assets to S3 which are used to deploy Lambdas by CloudFormation. Our process won't upload assets, so deploying Lambdas via a Spacelift stack configured to handle AWS CDK will result in errors.</p>"},{"location":"vendors/cloudformation/integrating-with-sam.html","title":"Integrating with AWS Serverless Application Model (SAM)","text":""},{"location":"vendors/cloudformation/integrating-with-sam.html#integrating-with-aws-serverless-application-model-sam","title":"Integrating with AWS Serverless Application Model (SAM)","text":"<p>In order to use AWS Serverless Application Model (SAM) in an AWS CloudFormation Stack you'll need to do two things: create a Docker image with SAM included and invoke SAM in <code>before_init</code> hooks.</p> <p>The first one can be done using a Dockerfile akin to this one:</p> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform\nUSER root\nWORKDIR /home/spacelift\nRUN apk -v --update --no-cache add curl\nRUN python3 -m venv /home/spacelift/.venv &amp;&amp; \\\n    source /home/spacelift/.venv/bin/activate &amp;&amp; \\\n    python3 -m ensurepip --upgrade &amp;&amp; \\\n    pip3 install --upgrade pip &amp;&amp; \\\n    pip3 install --upgrade awscli aws-sam-cli &amp;&amp; \\\n    pip3 uninstall --yes pip\nRUN source /home/spacelift/.venv/bin/activate &amp;&amp; sam --version\nUSER spacelift\n</code></pre> <p>You should build it, push it to a repository and set it as the Runner Image of your Stack.</p> <p>You'll also have to invoke SAM in order to generate raw CloudFormation files and upload Lambda artifacts to S3. You can do this by adding the following to your before initialization hooks:</p> <pre><code>source /home/spacelift/.venv/bin/activate &amp;&amp; sam package --region ${CF_METADATA_REGION} --s3-bucket ${CF_METADATA_TEMPLATE_BUCKET} --s3-prefix sam-artifacts --output-template-file ${CF_METADATA_ENTRY_TEMPLATE_FILE}\n</code></pre>"},{"location":"vendors/cloudformation/integrating-with-the-serverless-framework.html","title":"Integrating with the Serverless Framework","text":""},{"location":"vendors/cloudformation/integrating-with-the-serverless-framework.html#integrating-with-the-serverless-framework","title":"Integrating with the Serverless Framework","text":"<p>In order to use the Serverless Framework in an AWS CloudFormation Stack you'll need to do a few things: create a Docker image with the Serverless Framework included, invoke the serverless CLI in <code>before_init</code> hook, sync your artifacts with Amazon S3, and make sure the serverless config has your template bucket configured as the artifact location.</p> <p>The first one can be done using a Dockerfile akin to this one:</p> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform\nUSER root\nWORKDIR /home/spacelift\nRUN apk add --update --no-cache curl nodejs npm\nRUN npm install -g serverless\nRUN serverless --version\nUSER spacelift\n</code></pre> <p>You should build it, push it to a repository and set it as the Runner Image of your Stack.</p> <p>You'll also have to invoke the serverless CLI in order to generate raw CloudFormation files. You can do this by adding the following to your before initialization hooks:</p> <p><code>serverless package --region ${CF_METADATA_REGION}</code></p> <p>You can add the following script as a mounted file:</p> <pre><code>#!/bin/bash\n\nset -eu\nset -o pipefail\n\nSTATE_FILE=.serverless/serverless-state.json\nS3_PREFIX=$(jq -r '.package.artifactDirectoryName' &lt; \"$STATE_FILE\")\nARTIFACT=$(jq -r '.package.artifact' &lt; \"$STATE_FILE\")\n\naws s3 cp .serverless/$ARTIFACT s3://$CF_METADATA_TEMPLATE_BUCKET/$S3_PREFIX/$ARTIFACT\n</code></pre> <p>and invoke it in your before initialization hooks: <code>sh sync.sh</code></p> <p>Finally, specify the S3 bucket for artifacts in your serverless.yml configuration file:</p> <pre><code>provider:\n  deploymentBucket: your-s3-bucket\n</code></pre> <p>When creating the CloudFormation Stack, make sure that you set the entry template file to <code>.serverless/cloudformation-template-update-stack.json</code>, which is the file generated by the <code>serverless package</code> command.</p>"},{"location":"vendors/cloudformation/reference.html","title":"Reference","text":""},{"location":"vendors/cloudformation/reference.html#reference","title":"Reference","text":""},{"location":"vendors/cloudformation/reference.html#stack-settings","title":"Stack Settings","text":"<ul> <li>Region - AWS region in which to create and execute the AWS CloudFormation Stack.</li> <li>Stack Name - The name for the CloudFormation Stack controlled by this Spacelift Stack.</li> <li>Entry Template File - The path to the JSON or YAML file describing your root CloudFormation Stack. If you're generating CloudFormation code using a tool like AWS Serverless Application Model (SAM) or AWS Cloud Development Kit (AWS CDK), point this to the file containing the generated template.</li> <li>Template Bucket - Amazon S3 bucket to store CloudFormation templates in. Each created object will be prefixed by the current run ID like this: <code>&lt;run id&gt;/artifact_name</code></li> </ul>"},{"location":"vendors/cloudformation/reference.html#special-environment-variables","title":"Special Environment Variables","text":""},{"location":"vendors/cloudformation/reference.html#cloudformation-stack-parameters","title":"CloudFormation Stack Parameters","text":"<p>Use this if your CloudFormation template requires parameters to be specified.</p> <p>Each environment variable of the form <code>CF_PARAM_xyz</code> will be interpreted as the value for the parameter <code>xyz</code>.</p> <p>For example, with this template snippet:</p> <pre><code>Parameters:\n  InstanceTypeParameter:\n    Type: String\n    AllowedValues:\n      - t2.micro\n      - t2.small\n    Description: Enter t2.micro or t2.small.\n</code></pre> <p>In order to specify the InstanceTypeParameter add an environment variable to your Stack <code>CF_PARAM_InstanceTypeParameter</code> and set its value to i.e. <code>t2.micro</code></p>"},{"location":"vendors/cloudformation/reference.html#cloudformation-stack-capabilities","title":"CloudFormation Stack Capabilities","text":"<p>Some functionalities available to CloudFormation Stacks need to be explicitly acknowledged using capabilities. You can configure capabilities in Spacelift using environment variables of the form <code>CF_CAPABILITY_xyz</code> and set them to 1.</p> <p>As of the time of writing this page, available capabilities are <code>CF_CAPABILITY_IAM</code>, <code>CF_CAPABILITY_NAMED_IAM,</code> and <code>CF_CAPABILITY_AUTO_EXPAND</code>. Detailed descriptions can be found in the AWS API documentation.</p>"},{"location":"vendors/cloudformation/reference.html#available-computed-environment-variables","title":"Available Computed Environment Variables","text":"<p>To ease writing reusable scripts and hooks for your CloudFormation Stacks, the following environment variables are computed for each run: <code>CF_METADATA_REGION</code>, <code>CF_METADATA_STACK_NAME</code>, <code>CF_METADATA_ENTRY_TEMPLATE_FILE</code>, <code>CF_METADATA_TEMPLATE_BUCKET</code>.</p> <p>Their values are set to the respective Stack settings.</p>"},{"location":"vendors/cloudformation/reference.html#permissions","title":"Permissions","text":"<p>You need to provide Spacelift with access to your AWS account. You can either do this using the AWS Integration, provide ambient credentials on private workers, or pass environment variables directly.</p> <p>Warning</p> <p>Please note that, currently, it is not possible to utilize separate read and write roles to your stack. This limitation exists because templates are uploaded during the planning phase, which requires the use of the write role.</p>"},{"location":"vendors/kubernetes.html","title":"Kubernetes","text":""},{"location":"vendors/kubernetes.html#kubernetes","title":"Kubernetes","text":""},{"location":"vendors/kubernetes.html#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. For more information about Kubernetes, see the reference documentation.</p>"},{"location":"vendors/kubernetes.html#how-does-spacelift-work-with-kubernetes","title":"How does Spacelift work with Kubernetes?","text":"<p>Spacelift supports Kubernetes via <code>kubectl</code>.</p>"},{"location":"vendors/kubernetes.html#what-is-kubectl","title":"What is <code>kubectl</code>?","text":"<p>The Kubernetes command-line tool, <code>kubectl</code>, allows you to run commands against Kubernetes clusters. You can use <code>kubectl</code> to deploy applications, inspect and manage cluster resources, and view logs. For more information including a complete list of <code>kubectl</code> operations, see the <code>kubectl</code> reference documentation.</p>"},{"location":"vendors/kubernetes.html#why-use-spacelift-with-kubernetes","title":"Why use Spacelift with Kubernetes?","text":"<p>Spacelift helps you manage the complexities and compliance challenges of using Kubernetes. It brings with it a GitOps flow, so your Kubernetes Deployments are synced with your Kubernetes Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies, which lets you automate compliance checks and build complex multi-stack workflows.</p> <p>You can also use Spacelift to mix and match Terraform, Pulumi, AWS CloudFormation, and Kubernetes Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision the required infrastructure (like an ECS/EKS cluster with all its dependencies) and then deploy the following via a Kubernetes Stack.</p> <p>Anything that can be run via <code>kubectl</code> can be run within a Spacelift stack.</p> <p>To find out more about Kubernetes Workload Resources, read the reference documentation.</p>"},{"location":"vendors/kubernetes/authenticating.html","title":"Authenticating","text":""},{"location":"vendors/kubernetes/authenticating.html#authenticating","title":"Authenticating","text":"<p>The Kubernetes integration relies on using <code>kubectl</code>'s native authentication to connect to your cluster. You can use the <code>$KUBECONFIG</code> environment variable to find the location of the Kubernetes configuration file, and configure any credentials required.</p> <p>You should perform any custom authentication as part of a before init hook to make sure that <code>kubectl</code> is configured correctly before any commands are run in after init and subsequent hooks.</p> <p>The following sections provide examples of how to configure the integration manually, as well as using cloud provider-specific tooling.</p>"},{"location":"vendors/kubernetes/authenticating.html#manual-configuration","title":"Manual Configuration","text":"<p>Manual configuration allows you to connect to any Kubernetes cluster accessible by your Spacelift workers, regardless of whether your cluster is on-prem or hosted by a cloud provider. The Kubernetes integration automatically sets the <code>$KUBECONFIG</code> environment variable to point at <code>/mnt/workspace/.kube/config</code>, giving you a number of options:</p> <ul> <li>You can use a mounted file to mount a pre-prepared config file into your workspace at <code>/mnt/workspace/.kube/config</code>.</li> <li>You can use a before init hook to create a kubeconfig file, or to download it from a trusted location.</li> </ul> <p>Please refer to the Kubernetes documentation for more information on configuring kubectl.</p>"},{"location":"vendors/kubernetes/authenticating.html#aws","title":"AWS","text":"<p>The simplest way to connect to an AWS EKS cluster is using the AWS CLI tool. To do this, add the following before init hook to your Stack:</p> <pre><code>aws eks update-kubeconfig --region $REGION_NAME --name $CLUSTER_NAME\n</code></pre> <p>Info</p> <ul> <li>The <code>$REGION_NAME</code> and <code>$CLUSTER_NAME</code> environment variables must be defined in your Stack's environment.</li> <li>This relies on either using the Spacelift AWS Integration, or ensuring that your workers have permission to access the EKS cluster.</li> </ul>"},{"location":"vendors/kubernetes/custom-resources.html","title":"Custom Resources","text":""},{"location":"vendors/kubernetes/custom-resources.html#custom-resources","title":"Custom Resources","text":"<p>Spacelift supports the use of Kubernetes Custom Resources. To find out more about these supported extensions, review the official documentation.</p>"},{"location":"vendors/kubernetes/getting-started.html","title":"Getting Started","text":""},{"location":"vendors/kubernetes/getting-started.html#getting-started","title":"Getting Started","text":""},{"location":"vendors/kubernetes/getting-started.html#repository-creation","title":"Repository Creation","text":"<p>Start by creating a new deployment repository and name the file as <code>deployment.yaml</code> with the following code:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2 # tells deployment to run 2 pods matching the template\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre> <p>You can learn more about this example deployment by navigating to the Run a Stateless Application Using a Deployment website from the official Kubernetes documentation.</p> <p>Looking at the code, you will find that it deploys Nginx with 2 replicas.</p>"},{"location":"vendors/kubernetes/getting-started.html#create-a-new-stack","title":"Create a new Stack","text":"<p>In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift.</p> <p></p>"},{"location":"vendors/kubernetes/getting-started.html#integrate-vcs","title":"Integrate VCS","text":"<p>Select the repository you created in the initial step, as seen in the picture.</p> <p></p>"},{"location":"vendors/kubernetes/getting-started.html#configure-backend","title":"Configure Backend","text":"<p>Choose Kubernetes from the dropdown list and type out the namespace.</p> <p>Info</p> <p>Spacelift does not recommend leaving the Namespace blank due to the elevated level of access privilege required.</p> <p></p>"},{"location":"vendors/kubernetes/getting-started.html#define-behavior","title":"Define Behavior","text":"<p>Select the arrow next to Show Advanced Options to expose the advanced configuration options.</p> <p>To ensure the success of a Kubernetes deployment, the following options should be reviewed and validated.</p> <ul> <li>Runner Image: Use a custom one that has <code>kubectl</code> installed.</li> <li>Customize workflow: During the initialization phase, you must specify the necessary command to ensure kubeconfig is updated and authenticated to the Kubernetes Cluster you will be authenticating against.</li> </ul> <p>Info</p> <p>Spacelift can authenticate against any Kubernetes cluster, including local or cloud provider managed instances.</p> <p></p> <p>In the example above, I am authenticating to an AWS EKS Cluster and used the following command to update the kubeconfig for the necessary cluster.</p> <pre><code>aws eks update-kubeconfig --region $region-name --name $cluster-name\n</code></pre> <p>Info</p> <p>Update the previous variables according to your deployment.</p> <ul> <li><code>$region-name:</code> AWS region where your Kubernetes cluster resides</li> <li><code>$cluster-name:</code> Name of your Kubernetes clusters</li> </ul> <p>The above allows the worker to authenticate to the proper cluster before running the specified Kubernetes deployment in the repository that we created earlier.</p> <p>Warning</p> <p>Authentication with a Cloud Provider is required.</p> <p>After you Name the Stack, follow the Cloud Integrations section to ensure Spacelift can authenticate to your Kubernetes Cluster.</p>"},{"location":"vendors/kubernetes/getting-started.html#name-the-stack","title":"Name the Stack","text":"<p>Provide the name of your Stack. Labels and Description are not required but recommended.</p> <p></p> <p>Saving the Stack will redirect you to its Tracked Runs (Deployment) page.</p> <p></p>"},{"location":"vendors/kubernetes/getting-started.html#configure-integrations","title":"Configure Integrations","text":"<p>To authenticate against a Kubernetes cluster provided by Cloud Provider managed service, Spacelift requires integration with the associated Cloud Provider.</p> <p>Navigate to the Settings, Integrations page and select the dropdown arrow to access the following selection screen:</p> <p></p> <p>Warning</p> <p>Necessary permissions to the Kubernetes Cluster are required.</p> <p>The following links will help you set up the necessary integration with your Cloud Provider of choice.</p> <ul> <li>AWS</li> <li>OIDC</li> </ul> <p>Once you have configured the necessary integration, navigate the Stack landing page and Trigger a Run.</p>"},{"location":"vendors/kubernetes/getting-started.html#trigger-a-run","title":"Trigger a Run","text":"<p>To Trigger a Run, select Trigger on the right side of the Stacks view.</p> <p></p> <p>Spacelift Label</p> <p>To help identify resources deployed to your Kubernetes cluster, Spacelift will add the following label to all resources: <code>spacelift-stack=&lt;stack-slug&gt;</code></p>"},{"location":"vendors/kubernetes/getting-started.html#triggered-run-status","title":"Triggered Run Status","text":"<p>Please review the documentation for a detailed view of each Run Phase and Status associated with Kubernetes.</p>"},{"location":"vendors/kubernetes/getting-started.html#unconfirmed","title":"Unconfirmed","text":"<p>After you manually trigger the Run in the Stack view, Spacelift will deploy a runner image, initialize the Cloud Provider, Authenticate with the Kubernetes Cluster and run the Deployment specified in the repository.</p> <p>After a successful planning phase, you can check the log to see the planned changes.</p> <p></p> <p>Planning Phase</p> <p>Spacelift utilizes the dry run functionality of <code>kubectl apply</code> to compare your code to the current state of the cluster and output the list of changes to be made.</p> <p>A slightly different dry run mode depending on the scenario:</p> <ul> <li><code>--dry-run=server</code>: Utilized when resources are available</li> <li><code>--dry-run=client</code>: Utilized when no resources are available</li> </ul> <p>To confirm the Triggered run, click the CONFIRM button.</p>"},{"location":"vendors/kubernetes/getting-started.html#finished-deployment","title":"Finished Deployment","text":"<p>The following screen highlights the Finished Run and output from a successful deployment to your Kubernetes cluster.</p> <p></p> <p>Applying</p> <p>The default timeout is set to 10 minutes (10m). If a Kubernetes Deployment is expected to take longer, you can customize that using the <code>KUBECTL_ROLLOUT_TIMEOUT</code> environment variable.</p> <p>Review the documentation to find out more about Spacelift environment variables..</p>"},{"location":"vendors/kubernetes/getting-started.html#default-removal-of-deployments","title":"Default Removal of Deployments","text":"<p>Info</p> <p>By default if a YAML file is removed from your repository, the resources with an attached <code>spacelift-stack=&lt;stack-slug&gt;</code> label will be removed from the Kubernetes cluster. The <code>--prune</code> flag will be utilized. But you need to be aware that you need at least one YAML file to exist otherwise the run will fail.</p>"},{"location":"vendors/kubernetes/helm.html","title":"Helm","text":""},{"location":"vendors/kubernetes/helm.html#helm","title":"Helm","text":"<p>There is no native support within Spacelift for Helm, but you can use the <code>helm template</code> command in a before plan hook to generate the Kubernetes resource definitions to deploy.</p> <p>Please note, the following caveats apply:</p> <ul> <li>Using <code>helm template</code> means that you are not using the full Helm workflow, which may cause limitations or prevent certain Charts from working.</li> <li>You need to use a custom Spacelift worker image that has Helm installed, or alternatively you can install Helm using a before init hook.</li> </ul> <p>The rest of this page will go through an example of deploying the Spacelift Workerpool Helm Chart using the Kubernetes integration. See here for an example repository.</p>"},{"location":"vendors/kubernetes/helm.html#prerequisites","title":"Prerequisites","text":"<p>The following prerequisites are required to follow the rest of this guide:</p> <ul> <li>A Kubernetes cluster that you can authenticate to from a Spacelift stack.</li> <li>A namespace called <code>spacelift-worker</code> that exists within that cluster.</li> </ul>"},{"location":"vendors/kubernetes/helm.html#repository-creation","title":"Repository Creation","text":"<p>Start by creating a new repository for your Helm stack. This repository only needs to contain a single item - a kustomization.yaml file:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: spacelift-worker\n\nresources:\n  # spacelift-worker-pool.yaml will be generated in a pre-plan hook\n  - spacelift-worker-pool.yaml\n</code></pre> <p>The kustomization file is used to tell <code>kubectl</code> where to find the file containing the output of the <code>helm template</code> command, and prevents <code>kubectl</code> from attempting to apply every yaml file in your repository. This is important if you want to commit a <code>values.yaml</code> file to your repository.</p> <p>Info</p> <p>Our example repository contains a values.yaml file used to configure some of the chart values. This isn't required, and is simply there for illustrative purposes.</p>"},{"location":"vendors/kubernetes/helm.html#create-a-new-stack","title":"Create a new Stack","text":""},{"location":"vendors/kubernetes/helm.html#define-behavior","title":"Define Behavior","text":"<p>Follow the same steps to create your stack as per the Getting Started guide, but when you get to the Define Behavior step, add the following commands as before plan hooks:</p> <pre><code>helm repo add spacelift https://downloads.spacelift.io/helm\nhelm template spacelift-worker-pool spacelift/spacelift-worker --values values.yaml --set \"replicaCount=$SPACELIFT_WORKER_REPLICAS\" --set \"credentials.token=$SPACELIFT_WORKER_POOL_TOKEN\" --set \"credentials.privateKey=$SPACELIFT_WORKER_POOL_PRIVATE_KEY\" &gt; spacelift-worker-pool.yaml\n</code></pre> <p>Also, make sure to specify your custom Runner image that has Helm installed if you are not installing Helm using a before init hook.</p> <p>Once you've completed both steps, you should see something like this:</p> <p></p>"},{"location":"vendors/kubernetes/helm.html#configure-environment","title":"Configure Environment","text":"<p>Once you have successfully created your Stack, add values for the following environment variables to your Stack environment:</p> <ul> <li><code>SPACELIFT_WORKER_REPLICAS</code> - the number of worker pool replicas to create.</li> <li><code>SPACELIFT_WORKER_POOL_TOKEN</code> - the token downloaded when creating your worker pool.</li> <li><code>SPACELIFT_WORKER_POOL_PRIVATE_KEY</code> - your base64-encoded private key.</li> </ul> <p>Your Stack environment should look something like this:</p> <p></p>"},{"location":"vendors/kubernetes/helm.html#configure-integrations","title":"Configure Integrations","text":"<p>Configure any required Cloud Provider integrations as per the Getting Started guide.</p>"},{"location":"vendors/kubernetes/helm.html#trigger-a-run","title":"Trigger a Run","text":"<p>This example assumes that a Kubernetes namespace called <code>spacelift-worker</code> already exists. If it doesn't, create it using <code>kubectl create namespace spacelift-worker</code> before triggering a run.</p> <p>Info</p> <p>You can use a Spacelift Task to run the <code>kubectl create namespace</code> command.</p> <p>Triggering runs works exactly the same as when not using Helm. Once the planning stage has completed, you should see a preview of your changes, showing the Chart resources that will be created:</p> <p></p> <p>After approving the run, you should see the changes applying, along with a successful rollout of your Chart resources:</p> <p></p>"},{"location":"vendors/kubernetes/kustomize.html","title":"Kustomize","text":""},{"location":"vendors/kubernetes/kustomize.html#kustomize","title":"Kustomize","text":"<p>Kubernetes support in Spacelift is driven by Kustomize with native support in <code>kubectl</code>.</p> <p>We used Kustomize to make all resources created using the Spacelift Kubernetes support have unique labels attached to them:</p> <ul> <li><code>spacelift-stack: &lt;stack-slug&gt;</code></li> <li><code>app.kubernetes.io/managed-by: spacelift</code></li> </ul> <p>All operations Spacelift does will be done only on resources with the <code>spacelift-stack: &lt;stack-slug&gt;</code>.</p> <p>If you are not using Kustomize, Spacelift will transparently create a <code>kustomization.yaml</code> file that will reference all yaml files in the Stack's project root and subdirectories.</p> <p>If you are using Kustomize, then we will only add a label transformer to your kustomization.yaml file.</p>"},{"location":"vendors/kubernetes/workflow-tool.html","title":"Workflow Tool","text":""},{"location":"vendors/kubernetes/workflow-tool.html#workflow-tool","title":"Workflow Tool","text":"<p>The Workflow Tool stack setting allows you to choose between two options:</p> <ul> <li>Kubernetes.</li> <li>Custom.</li> </ul> <p>The Kubernetes option gives you out of the box support where all you need to do is choose the version you want to use and you're good to go.</p> <p>The rest of this page explains the Custom option. This option allows you to customize the commands that are executed as part of Spacelift's Kubernetes workflow. This can be useful if you want to run a custom binary instead of one of the kubectl versions supported by our Kubernetes integration out the box.</p> <p>Info</p> <p>Note that any custom binary is considered third-party software and you need to make sure you have the necessary rights (e.g. license) to use it.</p>"},{"location":"vendors/kubernetes/workflow-tool.html#how-does-it-work","title":"How does it work?","text":"<p>Each stage of the Kubernetes workflow uses certain commands to perform tasks such as listing API resources available at the server or applying the resources. We provide a built-in set of commands to use for Kubernetes, but you can also specify your own custom commands. You do this via a file called <code>.spacelift/workflow.yml</code>.</p> <p>The following is an example of what the workflow commands look like for Kubernetes:</p> <pre><code># Used to create a resource from standard input. We use this\n# query to make authorization SelfSubjectAccessReview request.\ncreateFromStdin: \"kubectl create -f - -o yaml\"\n\n\n# Used to list api resources supported on the server.\n#\n# Available template parameters:\n# - .NoHeaders    - controls the --no-headers argument of kubectl.\n# - .IsNamespaced - controls arguments to provide if we use a namespace.\n# - .OutputFormat - output format of a request, can be set to two values: name or wide.\napiResources: \"kubectl api-resources {{ if .NoHeaders }}--no-headers {{ end }}--verbs=list,create {{ if .IsNamespaced }}--namespaced {{ end }}-o {{ .OutputFormat }}\"\n\n# Used to both plan and apply changes.\n#\n# Available template parameters:\n# - .PruneWhiteList - provides a list of arguments to the --prune flag. It uses the --prune-allowlist flag, if you want to use the old --pune-whitelist, add the feature:k8s_keep_using_prune_white_list_flag label to your stack.\n# - .StackSlug      - the slug of a current stack.\n# - .DryRunStrategy - dry run strategy, can be set to values: none, client, server.\n# - .OutputFormat   - output format. Can be set to empty value or json.\n# - .Namespace      - the namespace used.\napply: \"kubectl apply -k ./ --prune {{ .PruneWhiteList }} -l \\\"spacelift-stack={{ .StackSlug }}\\\" --dry-run={{ .DryRunStrategy }}{{ if .OutputFormat }} -o {{ .OutputFormat }}{{ end }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ end}}\"\n\n# Rollout status command used to check the rollout status of resources.\n#\n# Available template parameters:\n# - .Namespace                - the namespace used.\n# - .RolloutTimeoutInSeconds  - a timeout in seconds that will be passed to the command.\n# - .Id                       - id of the resource to watch.\nrolloutStatus: \"kubectl rollout status{{ if .Namespace }} --namespace={{ .Namespace }}{{ end}} --timeout {{ .RolloutTimeoutInSeconds }}s -w {{ .Id }}\"\n\n# List a pods based on selector passed. We use getPods command\n# to display status of pods while they are being deployed.\n#\n# Available template parameters:\n# - .Namespace - the namespace used.\n# - .Pods      - a selector passed to get pods command.\ngetPods: \"kubectl get pods{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} -w -l '{{ .Pods }}'\"\n\n# Used to tear down any resources as part of deleting a stack.\n#\n# Available template parameters:\n# - .StackSlug          - the slug of a current stack.\n# - .Namespace          - the namespace used.\n# - .ResourcesToDelete  - list of resources to delete.\ndelete: \"kubectl delete --ignore-not-found -l spacelift-stack={{ .StackSlug }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} {{ .ResourcesToDelete }}\"\n\n# Available template parameters:\n# - .StackSlug            - the slug of a current stack.\n# - .Namespace            - the namespace used.\n# - .ResourceKindsToGet   - list of kinds of resources to get.\nget: \"kubectl get -o json --show-kind --ignore-not-found -l spacelift-stack={{ .StackSlug }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} {{ .ResourceKindsToGet }}\"\n</code></pre>"},{"location":"vendors/kubernetes/workflow-tool.html#how-to-configure-a-custom-tool","title":"How to configure a custom tool","text":"<p>To use a custom tool, three things are required:</p> <ol> <li>A way of providing the tool to your runs.</li> <li>A way of specifying the commands that should be executed.</li> <li>Indicating that you want to use a custom tool on your stack/module.</li> </ol>"},{"location":"vendors/kubernetes/workflow-tool.html#providing-a-tool","title":"Providing a tool","text":"<p>There are two main ways of providing a custom tool:</p> <ul> <li>Using a custom runner image.</li> <li>Using a before_init hook to download your custom tool.</li> </ul>"},{"location":"vendors/kubernetes/workflow-tool.html#specifying-the-commands","title":"Specifying the commands","text":"<p>Your custom workflow commands need to be provided in a <code>workflow.yml</code> file stored in the <code>.spacelift</code> folder at the root of your workspace. There are three main ways of providing this:</p> <ol> <li>Via a mounted file in the stack's environment.</li> <li>Via a mounted file stored in a context</li> <li>Directly via your Git repo.</li> </ol> <p>The option you choose will depend on your exact use-case, for example using the stack's environment allows you to quickly test out a new custom tool, using a context allows you to easily share the same configuration across multiple stacks, and storing the configuration in your Git repo allows you to track your settings along with the rest of your code.</p> <p>Here is an example configuration to use a fictional tool called <code>my-custom-tool</code>:</p> <pre><code>createFromStdin: \"my-custom-tool create -f - -o yaml\"\napiResources: \"my-custom-tool api-resources {{ if .NoHeaders }}--no-headers {{ end }}--verbs=list,create {{ if .IsNamespaced }}--namespaced {{ end }}-o {{ .OutputFormat }}\"\napply: \"my-custom-tool apply -k ./ --prune {{ .PruneWhiteList }} -l \\\"spacelift-stack={{ .StackSlug }}\\\" --dry-run={{ .DryRunStrategy }}{{ if .OutputFormat }} -o {{ .OutputFormat }}{{ end }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ end}}\"\nrolloutStatus: \"my-custom-tool rollout status{{ if .Namespace }} --namespace={{ .Namespace }}{{ end}} --timeout {{ .RolloutTimeoutInSeconds }}s -w {{ .Id }}\"\ngetPods: \"my-custom-tool get pods{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} -w -l '{{ .Pods }}'\"\ndelete: \"my-custom-tool delete --ignore-not-found -l spacelift-stack={{ .StackSlug }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} {{ .ResourcesToDelete }}\"\nget: \"my-custom-tool get -o json --show-kind --ignore-not-found -l spacelift-stack={{ .StackSlug }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} {{ .ResourceKindsToGet }}\"\n</code></pre>"},{"location":"vendors/kubernetes/workflow-tool.html#using-a-custom-tool-on-a-stack","title":"Using a custom tool on a Stack","text":"<p>To update your Stack to use a custom workflow tool, edit the Workflow tool setting in the Backend settings:</p> <p></p> <p>When you choose the Custom option, the version selector will disappear:</p> <p></p> <p>This is because you are responsible for ensuring that the tool is available to the Run, and Spacelift will not automatically download the tool for you.</p>"},{"location":"vendors/kubernetes/workflow-tool.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"vendors/kubernetes/workflow-tool.html#extra-debug-info","title":"Extra debug info","text":"<p>Set the environment variable <code>SPACELIFT_KUBERNETES_DEBUG</code> to <code>1</code> on a kube stack to get additional debug information.</p>"},{"location":"vendors/kubernetes/workflow-tool.html#workflow-file-not-found","title":"Workflow file not found","text":"<p>If no workflow.yml file has been created but your Stack has been configured to use a custom tool, you may get an error message like the following:</p> <p></p> <p>If this happens, please ensure you have added a <code>.spacelift/workflow.yml</code> file to your Git repository, or attached it to your Stack's environment via a mounted file.</p>"},{"location":"vendors/kubernetes/workflow-tool.html#commands-missing","title":"Commands missing","text":"<p>If your <code>.spacelift/workflow.yml</code> does not contain all the required command definitions, or if any commands are empty, you will get an error message like the following:</p> <p></p> <p>This check is designed as a protection mechanism in case new commands are added but your workflow hasn't been updated. In this case, please provide an implementation for the specified commands (in the example the <code>init</code> and <code>workspaceSelect</code> commands),</p>"},{"location":"vendors/kubernetes/workflow-tool.html#tool-not-found","title":"Tool not found","text":"<p>If your custom tool binary cannot be found you will get an error message like the following:</p> <p></p> <p>In this situation, please ensure that you are providing a custom workflow tool via a custom runner image or workflow hook.</p>"},{"location":"vendors/kubernetes/workflow-tool.html#error-unknown-flag-prune-allowlist","title":"Error: unknown flag: --prune-allowlist","text":"<p> If you use kubectl with a version less than v1.26, the prune command should use <code>--prune-whitelist</code> flag. In order to do this, please specify feature:k8s_keep_using_prune_white_list_flag label.</p>"},{"location":"vendors/pulumi.html","title":"Pulumi","text":""},{"location":"vendors/pulumi.html#pulumi","title":"Pulumi","text":"<p>Info</p> <p>Feature previews are subject to change, may contain bugs, and have not yet been refined based on real production usage.</p> <p>At a high level, Pulumi works similarly to Terraform. It uses a state backend, supports dry runs, and reconciles the actual infrastructure with your desired state. In this article, we\u2019ll explain how Spacelift concepts map to Pulumi workflows.</p> <p>If you prefer hands-on learning, check out our quickstart guides for each Pulumi-supported runtime:</p> <ul> <li>C#</li> <li>Go</li> <li>Javascript</li> <li>Python</li> </ul> <p>If you\u2019re new to Pulumi, we recommend starting with Javascript\u2014it\u2019s the most user-friendly experience we\u2019ve had with Pulumi. You can easily switch to other languages that compile to Javascript, like TypeScript or ClojureScript, later on.</p> <p>The core concepts of Spacelift remain the same when using Pulumi. Below, we\u2019ll cover some lower-level details that may be helpful.</p>"},{"location":"vendors/pulumi.html#run-execution","title":"Run Execution","text":""},{"location":"vendors/pulumi.html#initialization","title":"Initialization","text":"<p>As described in Run Initializing, Pulumi initialization runs the following:</p> <ul> <li><code>pulumi login</code> with your configured login URL</li> <li><code>pulumi stack select --create --select</code> with your configured Pulumi stack name (set in vendor-specific settings, not the Spacelift Stack name)</li> </ul> <p>After this, all pre-initialization hooks will run.</p>"},{"location":"vendors/pulumi.html#planning","title":"Planning","text":"<p>We use <code>pulumi preview --refresh --diff --show-replacement-steps</code> to display planned changes.</p>"},{"location":"vendors/pulumi.html#applying","title":"Applying","text":"<p>We use <code>pulumi up --refresh --diff --show-replacement-steps</code> to apply changes.</p>"},{"location":"vendors/pulumi.html#additional-cli-arguments","title":"Additional CLI Arguments","text":"<p>You can pass additional CLI arguments using the <code>SPACELIFT_PULUMI_CLI_ARGS_preview</code>, <code>SPACELIFT_PULUMI_CLI_ARGS_up</code>, and <code>SPACELIFT_PULUMI_CLI_ARGS_destroy</code> environment variables.</p>"},{"location":"vendors/pulumi.html#policies","title":"Policies","text":"<p>Most policies remain unchanged. The main difference is with the plan policy. Instead of a raw Terraform plan in the <code>terraform</code> field, you\u2019ll receive a <code>pulumi</code> field containing the raw Pulumi plan, following this schema:</p> <pre><code>{\n  \"pulumi\": {\n    \"steps\": [\n      {\n        \"new\": {\n          \"custom\": \"boolean\",\n          \"id\": \"string\",\n          \"inputs\": \"object - input properties\",\n          \"outputs\": \"object - output properties\",\n          \"parent\": \"string - parent resource of this resource\",\n          \"provider\": \"string - provider this resource comes from\",\n          \"type\": \"string - resource type\",\n          \"urn\": \"string - resource URN\"\n        },\n        \"old\": {\n          \"custom\": \"boolean\",\n          \"id\": \"string\",\n          \"inputs\": \"object - input properties\",\n          \"outputs\": \"object - output properties\",\n          \"parent\": \"string - parent resource of this resource\",\n          \"provider\": \"string - provider this resource comes from\",\n          \"type\": \"string - resource type\",\n          \"urn\": \"string - resource URN\"\n        },\n        \"op\": \"string - same, refresh, create, update, delete, create-replacement, or delete-replaced\",\n        \"provider\": \"string - provider this resource comes from\",\n        \"type\": \"string - resource type\",\n        \"urn\": \"string - resource URN\"\n      }\n    ]\n  },\n  \"spacelift\": {\"...\": \"...\"}\n}\n</code></pre> <p>Pulumi secrets are detected and encoded as <code>[secret]</code> instead of showing the actual value. For this reason, no additional string sanitization is performed on Pulumi plans.</p>"},{"location":"vendors/pulumi.html#limitations","title":"Limitations","text":"<ul> <li>Spacelift module CI/CD is not available for Pulumi.</li> <li>Import is not supported for Pulumi. Instead, you can run a task to import resources into your state.</li> </ul>"},{"location":"vendors/pulumi/c-sharp.html","title":"C#","text":""},{"location":"vendors/pulumi/c-sharp.html#c","title":"C#","text":"<p>In order to follow along with this article, you'll need an AWS account.</p> <p>Start with forking the Pulumi examples repo, we'll be setting up an example directory from there, namely aws-cs-webserver.</p> <p>In the root of the repository (not the aws-cs-webserver directory), add a new file:</p> .spacelift/config.yml<pre><code>version: \"1\"\n\nstack_defaults:\n  before_apply:\n    - dotnet clean\n    - rm -rf bin\n    - rm -rf obj\n</code></pre> <p><code>before_apply</code> is not yet exposed through the interface like <code>before_init</code>, so you have to set it through the config file. When compiling, the dotnet CLI creates global state which is lost after confirmation. This will mostly clean the workspace before applying, so everything will be cleanly recompiled. Why mostly? This you will see in a sec.</p> <p>Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values:</p> <ul> <li>Set the project root to <code>aws-cs-webserver</code>, as we want to run Pulumi in this subdirectory only.</li> <li>Set the runner image to <code>public.ecr.aws/spacelift/runner-pulumi-dotnet:latest</code></li> <li>Pinning to a specific Pulumi version is possible too, using a tag like <code>v2.15.4</code> - you can see the available versions  here.</li> </ul> <p></p> <p>In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose:</p> <ul> <li>A state backend, aka login URL. This can be a cloud storage bucket, like <code>s3://pulumi-state-bucket</code>, but it can also be a Pulumi Service endpoint.</li> <li>A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like <code>my-dotnet-pulumi-spacelift-stack</code>.</li> </ul> <p></p> <p>Info</p> <p>You can use <code>https://api.pulumi.com</code> as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the <code>PULUMI_ACCESS_TOKEN</code> environment variable.</p> <p>You'll now have to set up the AWS integration for the Stack, as is described in AWS.</p> <p>Go into the Environment tab in your screen, add an <code>AWS_REGION</code> environment variable and set it to your region of choice, i.e. <code>eu-central-1</code>.</p> <p>Previously I said <code>dotnet clean</code> mostly clears the state, this is because you'll also have to add the <code>NUGET_PACKAGES</code> environment variable, and set it to a directory persisted with the workspace, i.e. <code>/mnt/workspace/nuget_packages</code>.</p> <p></p> <p>You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes.</p> <p></p> <p>Confirm the run to let it apply the changes, after applying it should look like this:</p> <p></p> <p>We can see the PublicDns stack output. If we try to curl it, lo and behold:</p> <pre><code>~&gt; curl ec2-18-184-92-34.eu-central-1.compute.amazonaws.com\nHello, World!\n</code></pre> <p>In order to clean up, open the Tasks tab, and perform <code>pulumi destroy --non-interactive --yes</code> there.</p> <p></p> <p>Which will destroy all created resources.</p> <p></p>"},{"location":"vendors/pulumi/golang.html","title":"Go","text":""},{"location":"vendors/pulumi/golang.html#go","title":"Go","text":"<p>In order to follow along with this article, you'll need an AWS account.</p> <p>Start with forking the Pulumi examples repo, we'll be setting up an example directory from there, namely aws-go-s3-folder.</p> <p>Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values:</p> <ul> <li>Set the project root to <code>aws-go-s3-folder</code>, as we want to run Pulumi in this subdirectory only.</li> <li>Set the runner image to <code>public.ecr.aws/spacelift/runner-pulumi-golang:latest</code></li> <li>Pinning to a specific Pulumi version is possible too, using a tag like <code>v2.15.4</code> - you can see the available versions here.</li> </ul> <p></p> <p>In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose:</p> <ul> <li>A state backend, aka login URL. This can be a cloud storage bucket, like <code>s3://pulumi-state-bucket</code>, but it can also be a Pulumi Service endpoint.</li> <li>A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like <code>my-golang-pulumi-spacelift-stack</code>.</li> </ul> <p></p> <p>Info</p> <p>You can use <code>https://api.pulumi.com</code> as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the <code>PULUMI_ACCESS_TOKEN</code> environment variable.</p> <p>You'll now have to set up the AWS integration for the Stack, as is described in AWS.</p> <p>Go into the Environment tab in your screen, add an <code>AWS_REGION</code> environment variable and set it to your region of choice, i.e. <code>eu-central-1</code>.</p> <p></p> <p>You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes.</p> <p></p> <p>Confirm the run to let it apply the changes, after applying it should look like this:</p> <p></p> <p>We can see the websiteUrl stack output. If we try to curl it, lo and behold:</p> <pre><code>~&gt; curl s3-website-bucket-b47a23a.s3-website.eu-central-1.amazonaws.com\n&lt;html&gt;&lt;head&gt;\n    &lt;title&gt;Hello Amazon S3&lt;/title&gt;&lt;meta charset=\"UTF-8\"&gt;\n    &lt;link rel=\"shortcut icon\" href=\"/favicon.png\" type=\"image/png\"&gt;\n&lt;/head&gt;\n&lt;body&gt;&lt;p&gt;Hello, world!&lt;/p&gt;&lt;p&gt;Made with \u2764\ufe0f with &lt;a href=\"https://pulumi.com\"&gt;Pulumi&lt;/a&gt;&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>In order to clean up, open the Tasks tab, and perform <code>pulumi destroy --non-interactive --yes</code> there.</p> <p></p> <p>Which will destroy all created resources.</p> <p></p>"},{"location":"vendors/pulumi/javascript.html","title":"Javascript","text":""},{"location":"vendors/pulumi/javascript.html#javascript","title":"Javascript","text":"<p>In order to follow along with this article, you'll need an AWS account.</p> <p>Start with forking the Pulumi examples repo, we'll be setting up an example directory from there, namely aws-js-webserver.</p> <p>Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values:</p> <ul> <li>Set the project root to <code>aws-js-webserver</code>, as we want to run Pulumi in this subdirectory only.</li> <li>Add one before init script: <code>npm install</code>, which will install all necessary dependencies, before initializing Pulumi itself. The outputs will be persisted in the workspace and be there for the Planning and Applying phases.</li> <li>Set the runner image to <code>public.ecr.aws/spacelift/runner-pulumi-javascript:latest</code></li> <li>Pinning to a specific Pulumi version is possible too, using a tag like <code>v2.15.4</code> - you can see the available versions here.</li> </ul> <p></p> <p>In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose:</p> <ul> <li>A state backend, aka login URL. This can be a cloud storage bucket, like <code>s3://pulumi-state-bucket</code>, but it can also be a Pulumi Service endpoint.</li> <li>A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like <code>my-javascript-pulumi-spacelift-stack</code>.</li> </ul> <p></p> <p>Info</p> <p>You can use <code>https://api.pulumi.com</code> as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the <code>PULUMI_ACCESS_TOKEN</code> environment variable.</p> <p>You'll now have to set up the AWS integration for the Stack, as is described in AWS.</p> <p>Go into the Environment tab in your screen, add an <code>AWS_REGION</code> environment variable and set it to your region of choice, i.e. <code>eu-central-1</code>.</p> <p></p> <p>You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes.</p> <p></p> <p>Confirm the run to let it apply the changes, after applying it should look like this:</p> <p></p> <p>We can see the publicHostName stack output. If we try to curl it, lo and behold:</p> <pre><code>~&gt; curl ec2-18-184-240-9.eu-central-1.compute.amazonaws.com\nHello, World!\n</code></pre> <p>In order to clean up, open the Tasks tab, and perform <code>pulumi destroy --non-interactive --yes</code> there.</p> <p></p> <p>Which will destroy all created resources.</p> <p></p>"},{"location":"vendors/pulumi/python.html","title":"Python","text":""},{"location":"vendors/pulumi/python.html#python","title":"Python","text":"<p>In order to follow along with this article, you'll need an AWS account.</p> <p>Start with forking the Pulumi examples repo, we'll be setting up an example directory from there, namely aws-py-webserver.</p> <p>In the root of the repository (not the aws-py-webserver directory), add a new file:</p> .spacelift/config.yml<pre><code>version: \"1\"\n\nstack_defaults:\n  before_apply:\n    - pip install -r requirements.txt\n</code></pre> <p><code>before_apply</code> is not yet exposed through the interface like <code>before_init</code>, so you have to set it through the config file.</p> <p>Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values:</p> <ul> <li>Set the project root to <code>aws-py-webserver</code>, as we want to run Pulumi in this subdirectory only.</li> <li>Add one before init script: <code>pip install -r requirements.txt</code>, which will install all necessary dependencies, before initializing Pulumi itself. This will need to run both when initializing and before applying.</li> <li>Set the runner image to <code>public.ecr.aws/spacelift/runner-pulumi-python:latest</code></li> <li>Pinning to a specific Pulumi version is possible too, using a tag like <code>v2.15.4</code> - you can see the available versions here.</li> </ul> <p></p> <p>In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose:</p> <ul> <li>A state backend, aka login URL. This can be a cloud storage bucket, like <code>s3://pulumi-state-bucket</code>, but it can also be a Pulumi Service endpoint.</li> <li>A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like <code>my-python-pulumi-spacelift-stack</code>.</li> </ul> <p></p> <p>Info</p> <p>You can use <code>https://api.pulumi.com</code> as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the <code>PULUMI_ACCESS_TOKEN</code> environment variable.</p> <p>You'll now have to set up the AWS integration for the Stack, as is described in AWS.</p> <p>Go into the Environment tab in your screen, add an <code>AWS_REGION</code> environment variable and set it to your region of choice, i.e. <code>eu-central-1</code>.</p> <p></p> <p>You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes.</p> <p></p> <p>Confirm the run to let it apply the changes, after applying it should look like this:</p> <p></p> <p>We can see the <code>public_dns</code> stack output. If we try to curl it, lo and behold:</p> <pre><code>~&gt; curl ec2-3-125-48-55.eu-central-1.compute.amazonaws.com\nHello, World!\n</code></pre> <p>In order to clean up, open the Tasks tab, and perform <code>pulumi destroy --non-interactive --yes</code> there.</p> <p></p> <p>Which will destroy all created resources.</p> <p></p>"},{"location":"vendors/terraform.html","title":"Terraform","text":""},{"location":"vendors/terraform.html#terraform","title":"Terraform","text":""},{"location":"vendors/terraform.html#why-use-terraform","title":"Why use Terraform?","text":"<p>Terraform is a full-featured, battle-tested Infrastructure as Code tool. It has a vast ecosystem of providers to interact with many vendors from cloud providers such as AWS, Azure and GCP to monitoring such as New Relic and Datadog, and many many more.</p> <p>There are also plenty of community-managed modules and tools to get you started in no time.</p>"},{"location":"vendors/terraform.html#why-use-spacelift-with-terraform","title":"Why use Spacelift with Terraform?","text":"<p>Spacelift helps you manage the complexities and compliance challenges of using Terraform. It brings with it a GitOps flow, so your infrastructure repository is synced with your Terraform Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies, which lets you automate compliance checks and build complex multi-stack workflows.</p> <p>You can also use Spacelift to mix and match Terraform, Pulumi, and CloudFormation Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like an ECS/EKS cluster with all its dependencies) and then connect that to a CloudFormation Stack which then transactionally deploys your services there using trigger policies and the Spacelift provider run resources for workflow orchestration and Contexts to export Terraform outputs as CloudFormation input parameters.</p>"},{"location":"vendors/terraform.html#does-spacelift-support-terraform-wrappers","title":"Does Spacelift support Terraform wrappers?","text":"<p>Yes! We support Terragrunt and Cloud Development Kit for Terraform (CDKTF). You can read more about it in the relevant subpages of this document.</p>"},{"location":"vendors/terraform.html#additional-resources","title":"Additional resources","text":"<ul> <li>Module registry</li> <li>Provider registry (beta)</li> <li>External modules</li> <li>Provider</li> <li>State management</li> <li>External state access</li> <li>Terragrunt</li> <li>Version management</li> <li>Handling .tfvars</li> <li>CLI Configuration</li> <li>Cost Estimation</li> <li>Resource Sanitization</li> <li>Storing Complex Variables</li> <li>Debugging Guide</li> <li>Dependency Lock File</li> <li>Cloud Development Kit for Terraform (CDKTF)</li> </ul>"},{"location":"vendors/terraform/cdktf.html","title":"Cloud Development Kit for Terraform (CDKTF)","text":""},{"location":"vendors/terraform/cdktf.html#cloud-development-kit-for-terraform-cdktf","title":"Cloud Development Kit for Terraform (CDKTF)","text":"<p>The Cloud Development Kit for Terraform (CDKTF) generates JSON Terraform configuration from code in C#, Python, TypeScript, Java, or Go. Spacelift fully supports CDKTF.</p>"},{"location":"vendors/terraform/cdktf.html#building-a-custom-runner-image","title":"Building a custom runner image","text":"<p>CDKTF requires packages and tools that are not included in the default Terraform runner. These dependencies are different for each supported programming language.</p> <p>Luckily, extending the default runner Docker image to include these dependencies is easy. You will need to:</p> <ul> <li>Create a <code>Dockerfile</code> file that installs the required tools and packages for the specific programming language you want to use (see below).</li> <li>Build and publish the Docker image.</li> <li>Configure the runner image to use in the stack settings.</li> </ul> TypeScriptPythonGo <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\n\nUSER root\nRUN apk add --no-cache nodejs npm\nRUN npm install --global cdktf-cli@latest\nUSER spacelift\n</code></pre> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\n\nUSER root\nRUN apk add --no-cache nodejs npm python3\nRUN npm install --global cdktf-cli@latest\nRUN python3 -m ensurepip \\\n  &amp;&amp; python3 -m pip install --upgrade pip setuptools\n\nUSER spacelift\nRUN pip3 install --user pipenv\nENV PATH=\"/home/spacelift/.local/bin:$PATH\"\n</code></pre> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\n\nUSER root\nRUN apk add --no-cache go nodejs npm\nRUN npm install --global cdktf-cli@latest\nUSER spacelift\n</code></pre>"},{"location":"vendors/terraform/cdktf.html#synthesizing-terraform-code","title":"Synthesizing Terraform code","text":"<p>Before Terraform can plan and apply changes to your infrastructure, CDKTF must turn your C#, Python, TypeScript, Java, or Go code into Terraform configuration code. That process is called synthesizing.</p> <p>This step needs to happen before the Initializing phase of a run. This can be easily done by adding a few <code>before_init</code> hooks:</p> TypeScriptPythonGo <ul> <li><code>npm install</code></li> <li><code>cdktf synth</code></li> <li><code>cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json .</code></li> </ul> <ul> <li><code>pipenv install</code></li> <li><code>cdktf synth</code></li> <li><code>cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json .</code></li> </ul> <ul> <li><code>cdktf synth</code></li> <li><code>cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json .</code></li> </ul> <p>Warning</p> <p>If the Terraform state is managed by Spacelift, make sure to disable the local backend that CDKTF automatically adds if none is configured by adding the following command after the hooks mentioned above:</p> <pre><code>jq '.terraform.backend.local = null' cdk.tf.json &gt; cdk.tf.json.tmp &amp;&amp; mv cdk.tf.json.tmp cdk.tf.json\n</code></pre>"},{"location":"vendors/terraform/cli-configuration.html","title":"CLI Configuration","text":""},{"location":"vendors/terraform/cli-configuration.html#cli-configuration","title":"CLI Configuration","text":"<p>For some of our Terraform users, a convenient way to configure the Terraform CLI behavior is through customizing the <code>~/.terraformrc</code> file.</p> <p>During the preparing phase of a run, Spacelift creates a configuration file at the default location: <code>~/.terraformrc</code>. This file contains credentials that are needed to communicate with remote services.</p>"},{"location":"vendors/terraform/cli-configuration.html#extending-cli-configuration","title":"Extending CLI configuration","text":"<p>Extending the Terraform CLI behavior can be done by using mounted files:</p>"},{"location":"vendors/terraform/cli-configuration.html#using-mounted-files","title":"Using mounted files","text":"<p>Any mounted files with names ending in <code>.terraformrc</code> will be appended to <code>~/.terraformrc</code>.</p> <p>Info</p> <p>The Terraform CLI configuration file syntax supports these settings</p>"},{"location":"vendors/terraform/debugging-guide.html","title":"Debugging Guide","text":""},{"location":"vendors/terraform/debugging-guide.html#debugging-guide","title":"Debugging Guide","text":""},{"location":"vendors/terraform/debugging-guide.html#setting-environment-variables","title":"Setting Environment Variables","text":"<p>Environment variables are commonly used for enabling advanced logging levels for Terraform and Terragrunt. There are two ways environment variables can be set for runs.</p> <ol> <li> <p>Set Environment Variable(s) directly on a Stack's Environment (easiest method).</p> </li> <li> <p>Set Environment Variable(s) in a Context, and then attach that Context to your Spacelift Stack(s).</p> </li> </ol>"},{"location":"vendors/terraform/debugging-guide.html#terraform-debugging","title":"Terraform Debugging","text":"<p>Terraform providing an advanced logging mode that can be enabled using the <code>TF_LOG</code> environment variable. As of the writing of this documentation, <code>TF_LOG</code> has 5 different logging levels: <code>TRACE</code> <code>DEBUG</code> <code>INFO</code> <code>WARN</code> and <code>ERROR</code></p> <p>Please refer to the Setting Environment Variables section for more information on how to set these variables on your Spacelift Stack(s).</p> <p>For more information on Terraform logging, please refer directly to the Terraform documentation.</p>"},{"location":"vendors/terraform/debugging-guide.html#terragrunt-debugging","title":"Terragrunt Debugging","text":"<p>Please refer to the Debugging Terragrunt section of our Terragrunt documentation.</p>"},{"location":"vendors/terraform/dependency-lock-file.html","title":"Dependency Lock File","text":""},{"location":"vendors/terraform/dependency-lock-file.html#dependency-lock-file","title":"Dependency Lock File","text":"<p>Recent versions of Terraform can optionally track dependency selections using a Dependency Lock File named <code>.terraform.lock.hcl</code>, in a similar fashion to npm's <code>package-lock.json</code> file.</p> <p>If this file is present in the project root for your stack, Terraform will use it. Otherwise, it will dynamically determine the dependencies to use.</p>"},{"location":"vendors/terraform/dependency-lock-file.html#generating-updating-the-file","title":"Generating &amp; Updating the File","text":"<p>Terraform recommends including the Dependency Lock File in your version control repository, alongside your infrastructure code.</p> <p>You can generate or update this file by running <code>terraform init</code> locally and committing it into your repository.</p> <p>An alternative option would be to run the <code>terraform init</code> in a Task, print it to the Task logs, copy/paste the content from the Task logs into the <code>.terraform.lock.hcl</code> file, and commit it into your repository.</p>"},{"location":"vendors/terraform/external-modules.html","title":"External modules","text":""},{"location":"vendors/terraform/external-modules.html#external-modules","title":"External modules","text":"<p>Those of our customers who are not yet using our private module registry may want to pull modules from various external sources supported by Terraform. This article discusses a few most popular types of module sources and how to use them in Spacelift.</p>"},{"location":"vendors/terraform/external-modules.html#cloud-storage","title":"Cloud storage","text":"<p>The easiest option is accessing modules from Amazon S3 buckets. Access to S3 can be granted using our AWS integration. Alternatively if your worker pool has the correct IAM permissions, you may not require any authentication at all!</p>"},{"location":"vendors/terraform/external-modules.html#git-repositories","title":"Git repositories","text":"<p>Git is by far the most popular external module source. This example will focus on GitHub as the most popular one, but the advice applies to other VCS providers. In general, Terraform retrieves Git-based modules using one of the two supported transports - HTTPS or SSH. Assuming your repository is private, you will need to give Spacelift credentials required to access it.</p>"},{"location":"vendors/terraform/external-modules.html#using-https","title":"Using HTTPS","text":"<p>Git with HTTPS is slightly simpler than SSH - all you need is a personal access token, and you need to make sure that it ends up in the <code>~/.netrc</code> file, which Terraform will use to log in to the host that stores your source code.</p> <p>Assuming you already have a token you can use, create a file like this:</p> <pre><code>machine github.com\nlogin $yourLogin\npassword $yourToken\n</code></pre> <p>Then, upload this file to your stack's Spacelift environment as a mounted file. In this example, we called that file <code>github.netrc</code>:</p> <p></p> <p>Add the following commands as \"before init\" hooks to append the content of this file to the <code>~/.netrc</code> proper:</p> <pre><code>cat /mnt/workspace/github.netrc &gt;&gt; ~/.netrc\nchmod 600 ~/.netrc\n</code></pre>"},{"location":"vendors/terraform/external-modules.html#using-ssh","title":"Using SSH","text":"<p>Using SSH isn't much more complex, but it requires a bit more preparation. Once you have a public-private key pair (whether it's a personal SSH key or a single-repo deploy key), you will need to pass it to Spacelift and make sure it's used to access your VCS provider. Once again, we're going to use the mounted file functionality to pass the private key called <code>id_ed25519</code> to your stack's environment:</p> <p>Warning</p> <p>Please note: The file must explicitly be called <code>id_ed25519</code>, otherwise your runs will fail with a <code>Permission denied</code> error.</p> <p></p> <p>Add the following commands as \"before init\" hooks to \"teach\" our SSH agent to use this key for GitHub:</p> <pre><code>mkdir -p ~/.ssh\ncp /mnt/workspace/id_ed25519 ~/.ssh/id_ed25519\nchmod 400 ~/.ssh/id_ed25519\nssh-keyscan -t rsa github.com &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>The above example warrants a little explanation. First, we're making sure that the <code>~/.ssh</code> directory exists - otherwise, we won't be able to put anything in there. Then we copy the private key file mounted in our workspace to the SSH configuration directory and give it proper permissions. Last but not least, we're using the <code>ssh-keyscan</code> utility to retrieve the public SSH host key for <code>github.com</code> and add it to the list of known hosts - this will avoid your code checkout failing due to what would otherwise be an interactive prompt asking you whether to trust that key.</p> <p>Tip</p> <p>If you get an error referring to error in libcrypto, please try adding a newline to the end of the mounted file and trying again.</p>"},{"location":"vendors/terraform/external-modules.html#dedicated-third-party-registries","title":"Dedicated third-party registries","text":"<p>For users storing their modules in dedicated external private registries, like Terraform Cloud's one, you will need to supply credentials in the <code>.terraformrc</code> file - this approach is documented in the official documentation.</p> <p>In order to facilitate that, we've introduced a special mechanism for extending the CLI configuration that does not even require using <code>before_init</code> hooks. You can read more about it here.</p>"},{"location":"vendors/terraform/external-modules.html#to-mount-or-not-to-mount","title":"To mount or not to mount?","text":"<p>That is the question. And there isn't a single right answer. Instead, there is a list of questions to consider. By mounting a file, you're giving us access to its content. No, we're not going to read it, and yes, we have it encrypted using a fancy multi-layered mechanism, but still - we have it. So the main question is how sensitive the credentials are. Read-only deploy keys are probably the least sensitive - they only give read access to a single repository, so these are the ones where convenience may outweigh other concerns. On the other hand, personal access tokens may be pretty powerful, even if you generate them from service users. The same thing goes for personal SSH keys. Guard these well.</p> <p>So if you don't want to mount these credentials, what are your options? First, you can put these credentials directly into your private runner image. But that means that anyone in your organization who uses the private runner image gets access to your credentials - and that may or may not be what you wanted.</p> <p>The other option is to store the credentials externally in one of the secrets stores - like AWS Secrets Manager or HashiCorp Vault and retrieve them in one of your <code>before_init</code> scripts before putting them in the right place (<code>~/.netrc</code> file, <code>~/.ssh</code> directory, etc.).</p> <p>Info</p> <p>If you decide to mount, we advise that you store credentials in contexts and attach these to stacks that need them. This way you can avoid credentials sprawl and leak.</p>"},{"location":"vendors/terraform/external-state-access.html","title":"External state access","text":""},{"location":"vendors/terraform/external-state-access.html#external-state-access","title":"External state access","text":"<p>External state access allows you to read the state of the stack from outside authorized runs and tasks. In particular, this enables sharing the outputs between stacks using the Terraform mechanism of remote state or even accessing the state offline for analytical or compliance purposes.</p> <p>If enabled for a particular stack, any user or stack with Write permission to that stack's space will be able to access its state.</p> <p>This feature is off by default.</p>"},{"location":"vendors/terraform/external-state-access.html#enabling-external-access","title":"Enabling external access","text":"<p>Info</p> <p>Only administrative stacks or users with write permission to this Stack's space can access the state.</p> <p>You can enable the external access in a couple of ways.</p> <ul> <li>through the UI</li> </ul> <p></p> <ul> <li>using the Terraform provider.</li> </ul> <pre><code>resource \"spacelift_stack\" \"example\" {\n  name                            = \"example\"\n  repository                      = \"spacelift-stacks\"\n  branch                          = \"main\"\n  terraform_external_state_access = true\n}\n</code></pre>"},{"location":"vendors/terraform/external-state-access.html#sharing-outputs-between-stacks","title":"Sharing outputs between stacks","text":"<p>Sharing the outputs between stacks can be achieved using the Terraform mechanism of remote state.</p> <p>Given an account called spacecorp, and a stack named deep-thought, having the following Terraform configuration.</p> <pre><code>output \"answer\" {\n  value = 42\n}\n</code></pre> <p>You can read that output from a different stack by using the <code>terraform_remote_state</code> data source.</p> <pre><code>data \"terraform_remote_state\" \"deepthought\" {\n  backend = \"remote\"\n\n  config = {\n    hostname     = \"spacelift.io\" # (1)\n    organization = \"spacecorp\"    # (2)\n\n    workspaces = {\n      name = \"deep-thought\"       # (3)\n    }\n  }\n}\n\noutput \"ultimate_answer\" {\n  value = data.terraform_remote_state.deepthought.outputs.answer\n}\n</code></pre> <ol> <li>The hostname of the Spacelift system.</li> <li>The name of the account.</li> <li>The ID of the stack you wish to retrieve outputs from.</li> </ol>"},{"location":"vendors/terraform/external-state-access.html#offline-state-access","title":"Offline state access","text":"<p>Given an account called <code>spacecorp</code>, and a stack named <code>deep-thought</code>, having the following Terraform configuration.</p> <pre><code>output \"answer\" {\n  value = 42\n}\n</code></pre> <p>Before you will be able to access the state of the stack, you need to retrieve an authentication token for spacelift.io.</p> <pre><code>terraform login spacelift.io\n</code></pre> <p>Next, you need a remote backend configuration.</p> <pre><code>terraform {\n  backend \"remote\" {\n    hostname     = \"spacelift.io\" # (1)\n    organization = \"spacecorp\"    # (2)\n\n    workspaces {\n      name = \"deep-thought\"       # (3)\n    }\n  }\n}\n</code></pre> <ol> <li>The hostname of the Spacelift system.</li> <li>The name of the account.</li> <li>The ID of the stack you wish to retrieve outputs from.</li> </ol> <p>Finally, you can download the state of the stack.</p> <pre><code>terraform state pull &gt; terraform.tfstate\n</code></pre>"},{"location":"vendors/terraform/handling-tfvars.html","title":"Handling .tfvars","text":""},{"location":"vendors/terraform/handling-tfvars.html#handling-tfvars","title":"Handling .tfvars","text":"<p>For some of our Terraform users, the most convenient solution to configure a stack is to specify its input values in a variable definitions file that is then passed to Terraform executions in plan and apply phases.</p> <p>Spacelift supports this approach but does not provide a separate mechanism, depending instead on a combination of Terraform's built-in mechanisms and Spacelift-provided primitives like:</p> <ul> <li>environment variables;</li> <li>mounted files</li> <li><code>before_init</code>scripts;</li> </ul>"},{"location":"vendors/terraform/handling-tfvars.html#using-environment-variables","title":"Using environment variables","text":"<p>In Terraform, special environment variables can be used to pass extra flags to executed commands like plan or apply. These are the more generic <code>TF_CLI_ARGS</code> and <code>TF_CLI_ARGS_name</code> that only affect a specific command. In Spacelift, environment variables can be defined directly on stacks and modules, as well as on contexts attached to those. As an example, let's declare the following environment variable:</p> <p></p> <p>In our particular case we don't have this file checked in, so the run will fail:</p> <p></p> <p>But we can supply this file dynamically using mounted files functionality.</p>"},{"location":"vendors/terraform/handling-tfvars.html#using-mounted-files","title":"Using mounted files","text":"<p>If the variable definitions file is not part of the repo, we can inject it dynamically. The above example can be fixed by supplying the variables file at the requested path:</p> <p></p> <p>Note that there are \"magical\" names you can give to your variable definitions files that always get autoloaded, without the need to supply extra CLI arguments. According to the documentation, Terraform automatically loads a number of variable definitions files if they are present:</p> <ul> <li>Files named exactly <code>terraform.tfvars</code> or <code>terraform.tfvars.json</code>.</li> <li>Any files with names ending in <code>.auto.tfvars</code> or <code>.auto.tfvars.json</code>.</li> </ul> <p>The above can be used in conjunction with another Spacelift building block, <code>before_init</code> hooks.</p>"},{"location":"vendors/terraform/handling-tfvars.html#using-before_init-hooks","title":"Using <code>before_init</code> hooks","text":"<p>If you need to use different variable definitions files for different projects, would like to have them checked into the repo, but would also want to avoid supplying extra CLI arguments, you could just dynamically move files - whether as a move, copy or a symlink to one of the autoloaded locations. This should happen in one of the <code>before_init</code> steps. Example:</p> <p></p>"},{"location":"vendors/terraform/infracost.html","title":"Cost Estimation","text":""},{"location":"vendors/terraform/infracost.html#cost-estimation","title":"Cost Estimation","text":"<p>The Infracost integration allows you to run an Infracost breakdown during Spacelift runs, providing feedback on PRs, and allowing you to integrate cost data with plan policies.</p> <p>This allows you to understand how infrastructure changes will impact costs, and to build automatic guards to help prevent costs from spiraling out of control.</p>"},{"location":"vendors/terraform/infracost.html#setting-up-the-integration","title":"Setting up the integration","text":"<p>To enable Infracost on any stack you need to do the following:</p> <ul> <li>Add the <code>infracost</code> label to the stack.</li> <li>Add an <code>INFRACOST_API_KEY</code> environment variable containing your Infracost API key.</li> </ul> <p>Info</p> <p>Creating a context for your Infracost API key means you can attach your key to any stacks that need to have Infracost enabled.</p> <p>If Infracost has been configured successfully, you should see some messages during the initialization phase of your runs indicating that Infracost is enabled and that the environment variable has been found:</p> <p></p>"},{"location":"vendors/terraform/infracost.html#additional-cli-arguments","title":"Additional CLI Arguments","text":"<p>If you need to pass any additional CLI arguments to the Infracost breakdown command, you can add them to the <code>INFRACOST_CLI_ARGS</code> environment variable. Anything found in this variable is automatically appended to the command. This allows you to do things like specifying the path to your Infracost usage file.</p>"},{"location":"vendors/terraform/infracost.html#ignore-failures","title":"Ignore Failures","text":"<p>By default, a failure executing Infracost, or a non-zero exit code being returned from the command will cause runs to fail.</p> <p>This behavior can be changed by setting the <code>INFRACOST_WARN_ON_FAILURE</code> environment variable to <code>true</code>. When enabled, Infracost errors will produce a warning message, but will not cause run failures.</p>"},{"location":"vendors/terraform/infracost.html#using-the-integration","title":"Using the integration","text":"<p>Once the integration is configured, Spacelift will automatically run Infracost breakdowns during the planning and applying stages. The following sections explain the functionality provided by the integration.</p>"},{"location":"vendors/terraform/infracost.html#pull-requests","title":"Pull Requests","text":"<p>Spacelift automatically posts the usage summary to your pull requests once Infracost is enabled:</p> <p></p>"},{"location":"vendors/terraform/infracost.html#plan-policies","title":"Plan Policies","text":"<p>Spacelift includes the full Infracost breakdown report in JSON format as part of the input to your plan policies. This is contained in <code>third_party_metadata.infracost</code>. The following shows an example plan input:</p> <pre><code>{\n  \"cloudformation\": null,\n  \"pulumi\": null,\n  \"spacelift\": { ... },\n  \"terraform\": { ... },\n  \"third_party_metadata\": {\n    \"infracost\": {\n      \"projects\": [{\n        \"breakdown\": {\n          \"resources\": [...],\n          \"totalHourlyCost\": \"0.0321506849315068485\",\n          \"totalMonthlyCost\": \"23.47\"\n        },\n        \"diff\": {\n          \"resources\": [...],\n          \"totalHourlyCost\": \"0.0321506849315068485\",\n          \"totalMonthlyCost\": \"23.47\"\n        },\n        \"metadata\": {},\n        \"pastBreakdown\": {\n          \"resources\": [],\n          \"totalHourlyCost\": \"0\",\n          \"totalMonthlyCost\": \"0\"\n        },\n        \"path\": \"/tmp/spacelift-plan923575332\"\n      }],\n      \"resources\": [...],\n      \"summary\": {\n        \"unsupportedResourceCounts\": {}\n      },\n      \"timeGenerated\": \"2021-06-09T14:14:44.146230883Z\",\n      \"totalHourlyCost\": \"0.0321506849315068485\",\n      \"totalMonthlyCost\": \"23.47\",\n      \"version\": \"0.1\"\n    }\n  }\n}\n</code></pre> <p>This means that you can take cost information into account when deciding whether to ask for human approval or to block changes entirely. The following policy provides a simple example of this:</p> <pre><code>package spacelift\n\n# Prevent any changes that will cause the monthly cost to go above a certain threshold\ndeny[sprintf(\"monthly cost greater than $%d ($%.2f)\", [threshold, monthly_cost])] {\n  threshold := 100\n  monthly_cost := to_number(input.third_party_metadata.infracost.projects[0].breakdown.totalMonthlyCost)\n  monthly_cost &gt; threshold\n}\n\n# Warn if the monthly costs increase more than a certain percentage\nwarn[sprintf(\"monthly cost increase greater than %d%% (%.2f%%)\", [threshold, percentage_increase])] {\n  threshold := 5\n  previous_cost := to_number(input.third_party_metadata.infracost.projects[0].pastBreakdown.totalMonthlyCost)\n  previous_cost &gt; 0\n\n  monthly_cost := to_number(input.third_party_metadata.infracost.projects[0].breakdown.totalMonthlyCost)\n  percentage_increase := ((monthly_cost - previous_cost) / previous_cost) * 100\n\n  percentage_increase &gt; threshold\n}\n</code></pre>"},{"location":"vendors/terraform/infracost.html#resources-view","title":"Resources View","text":"<p>Infracost provides information about how individual resources contribute to the overall cost of the stack. Spacelift combines this information with our resources view to allow you to view the cost information for each resource:</p> <p></p>"},{"location":"vendors/terraform/module-registry.html","title":"Module registry","text":""},{"location":"vendors/terraform/module-registry.html#module-registry","title":"Module registry","text":""},{"location":"vendors/terraform/module-registry.html#intro","title":"Intro","text":"<p>In Terraform, modules help you abstract away common functionality in your infrastructure.</p> <p>The name of a module managed by Spacelift is of the following form:</p> <pre><code>spacelift.io/&lt;organization&gt;/&lt;module_name&gt;/&lt;provider&gt;\n</code></pre> <p>In this name we have:</p> <ul> <li>The source module registry - <code>spacelift.io</code> is used here;</li> <li>The organization which owns and maintains the module;</li> <li>The module name, this will usually be the best shorthand descriptor of what the module actually does, i.e. it could be starting a machine with an HTTP server running.</li> <li>The main Terraform provider this module is meant to work with, i.e. the provider for the cloud service the resources should be created on.</li> </ul> <p>You can use a module in your Terraform configuration this way:</p> <pre><code>module \"my-birthday-cake\" {\n  source  = \"spacelift.io/spacelift-io/cake/oven\"\n  version = \"4.2.0\"\n\n  # Inputs.\n  eggs  = 5\n  flour = \"200g\"\n}\n\noutput \"my-birthday-cake\" {\n  value = {\n    weight = module.my-birthday-cake.weight\n    allergens = module.my-birthday-cake.allergens\n  }\n}\n</code></pre> <p>As you can see, we've explicitly used a module which can make cakes using an oven. We can specify variables the module depends on, and finally use the outputs the cake module exports.</p> <p>Spacelift obviously lets you host modules, but it also does much more, providing you with robust CI/CD for your modules, leading us to the question...</p>"},{"location":"vendors/terraform/module-registry.html#why-host-your-modules-on-spacelift","title":"Why host your Modules on Spacelift?","text":"<p>Spacelift provides everything you need to make your module easily maintainable and usable. There is CI/CD for multiple specified versions of Terraform, which \"runs\" your module on each commit. You get an autogenerated page describing your Module and its intricacies, so your users can explore them and gather required information at a glimpse. It's also deeply integrated with all the features Stacks use which you know and love, like Environments, Policies, Contexts and Worker Pools.</p>"},{"location":"vendors/terraform/module-registry.html#setting-up-a-module","title":"Setting up a Module","text":""},{"location":"vendors/terraform/module-registry.html#git-repository-structure","title":"Git repository structure","text":"<p>You will have to set up a repository for your module, the structure of the repository should be as follows:</p> <pre><code>.\n\u251c\u2500\u2500 .spacelift\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config.yml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 output.tf\n\u2514\u2500\u2500 variables.tf\n</code></pre> <p>Each module must have a <code>config.yml</code> file in its <code>.spacelift</code> directory, containing information about the module along with any test cases. Details of the format of this file can be found in the module configuration section of this page.</p> <p>You can check out an example module here: https://github.com/spacelift-io/terraform-spacelift-example</p> <p>Info</p> <p>The source code for a module can be stored in a subdirectory of your repository because you can specify the project root when configuring your module in Spacelift. An example of a repository containing multiple modules can be found here: https://github.com/spacelift-io/multimodule</p>"},{"location":"vendors/terraform/module-registry.html#spacelift-setup","title":"Spacelift setup","text":"<p>In order to add a module to Spacelift, navigate to the Terraform registry section of the account view, and click the Create module button:</p> <p></p> <p>The setup steps are pretty similar to the ones for stacks.</p>"},{"location":"vendors/terraform/module-registry.html#connect-to-source-code","title":"Connect to source code","text":"<p>First, point Spacelift at the right repo and choose the \"tracked\" branch - note that repositories whose names don't follow the convention are filtered out:</p>"},{"location":"vendors/terraform/module-registry.html#add-details","title":"Add details","text":"<p>In the details section, you will be able to add a name, provider, labels and description.</p> <p>The name and provider will be inferred from your repository name if it follows the <code>terraform-&lt;provider&gt;-&lt;name&gt;</code> convention. However, if it can't be inferred or you want a custom name, you can specify them directly. The final module slug will then be based on the name and provider preceded by <code>terraform-</code> prefix, like this <code>terraform-&lt;provider&gt;-&lt;name&gt;</code>.</p>"},{"location":"vendors/terraform/module-registry.html#module-created","title":"Module Created","text":"<p>After finishing adding details to your module, you're brought to a new screen indicating that your module has been successfully created. This screen serves as a branching point where you can enhance the functionality of your module through various integrations and customizations.</p> <p>You have the flexibility to either take shortcuts to specific configurations or continue through the standard process of setting up your module.</p>"},{"location":"vendors/terraform/module-registry.html#share-module","title":"Share module","text":"<p>In this step, you can share your module with specific spaces in your account. Select the spaces that should have access to use this module in their stacks.</p> <p>Shared modules become discoverable and usable by stacks in the selected spaces, while module management remains with the owner. Learn more about module sharing.</p>"},{"location":"vendors/terraform/module-registry.html#define-behavior","title":"Define behavior","text":"<p>In the behavior section there are few settings that you can set to make your module work in a specific way:</p> <ul> <li>setting the worker pool to one you manage yourself makes sense if the module tests will be touching resources or accounts you don't want Spacelift to access directly. Plus, your private workers may have more bandwidth than the shared ones, so you may get feedback faster.</li> <li>which tool to be used to execute the workflow commands. This can be an open source (FOSS) version of Terraform, OpenTofu or a custom tool.</li> <li>a docker image used to process your module metadata. You can specify your own image or use Spacelift's public one: <code>public.ecr.aws/spacelift/runner-terraform:latest</code>. Setting this is the only way to enable the use of symbolic links in the module.</li> <li>whether the module is administrative (You will only need to set administrative to <code>true</code> if your module manages Spacelift resources (and most likely it does not)),</li> <li>whether or not to enable the local preview spacectl CLI feature;</li> <li>whether or not to protect the module from deletion;</li> </ul>"},{"location":"vendors/terraform/module-registry.html#attach-module-cloud-integration","title":"Attach Module Cloud Integration","text":"<p>Here you have the ability to attach any Cloud Integrations you have configured.</p> <p>Cloud integrations allow Spacelift to manage your resources without the need for long-lived static credentials.</p> <p>Spacelift integrates with identity management systems from major cloud providers to dynamically generate short-lived access tokens that can be used to configure their corresponding Terraform providers.</p> <p>Currently, AWS, Azure and GCP are natively supported.</p> <p>You can read more about Cloud integrations here.</p>"},{"location":"vendors/terraform/module-registry.html#attach-module-policies","title":"Attach Module Policies","text":"<p>Spacelift as a development platform is built around the concept of policies and allows defining policies that involve various decision points in the application.</p> <p>In this section, you can attach the following policy types:</p> <ul> <li>Approval: who can approve or reject a run and how a run can be approved;</li> <li>Plan: which changes can be applied;</li> <li>Push: how Git push events are interpreted;</li> <li>Trigger: what happens when blocking runs terminate;</li> </ul> <p>Policies can be automatically attached to module using the <code>autoattach:label</code> special label where <code>label</code> is the name of a label attached to stacks and/or modules in your Spacelift account you wish the policy to be attached to.</p> <p>You can read more about policies here.</p>"},{"location":"vendors/terraform/module-registry.html#attach-module-contexts","title":"Attach Module Contexts","text":"<p>Contexts are sets of environment variables and related configuration including hooks that can be shared across multiple modules. By attaching a context, you ensure your module has all the necessary configuration elements it needs to operate, without repeating the setup for each module.</p> <p>Contexts can be automatically attached to modules using the <code>autoattach:label</code> special label where <code>label</code> is the name of a label attached to stacks and/or modules in your Spacelift account you wish the context to be attached to.</p> <p>You can read more about contexts here.</p>"},{"location":"vendors/terraform/module-registry.html#summary","title":"Summary","text":"<p>On the summary section, you can now review your settings before finalizing the creation of your module. This modular approach ensures your module gets set up with all the necessary components.</p>"},{"location":"vendors/terraform/module-registry.html#environment-contexts-and-policies","title":"Environment, contexts and policies","text":"<p>Environment and context management in modules is identical to that for stacks. The only thing worth noting here is the fact that environment variables and mounted files set either through the module environment directly, or via one of its attached contexts will be passed to each of the test cases for the module.</p> <p>Attaching policies works in a similar way. One thing worth pointing out is that the behavior of Trigger policies are slightly different for modules. Instead of being provided with the list of all other accessible stacks, module trigger policies receive a list of the current consumers of the module. This allows you to automatically trigger dependent stacks when new module versions are published.</p>"},{"location":"vendors/terraform/module-registry.html#module-configuration","title":"Module configuration","text":"<p>Tip</p> <p>If you get the error <code>couldn't create version: could not create version: version \"0.1.0\" already exists for spacelift-modules</code> potentially it could be that your config.yml file is in the wrong location or named something else eg. config.yaml instead.  Modules will initially be created with version \"0.1.0\" if Spacelift cannot read that file version for whatever reason</p> <p>While by convention a single Git repository hosts a single module, that root module can have multiple submodules. Thus, we've created a way to create a number of test cases:</p> <pre><code># The version of the configuration file format\nversion: 1\n# Your module version - must be changed to release a new version\nmodule_version: 0.1.1\n\n# Any default settings that should be used for all test cases\ntest_defaults:\n  before_init: [\"terraform fmt -check\"]\n  runner_image: your/runner:image\n\n# The set of tests to run to verify your module works correctly\ntests:\n  - name: Test the module with 0.12.7\n    terraform_version: 0.12.7\n    environment:\n      TF_VAR_bacon: tasty\n\n  - name: Test the submodule with 0.13.0\n    # project_root can be set if your test case is not stored in the root directory\n    project_root: submodule\n    terraform_version: 0.13.0\n    environment:\n      TF_VAR_cabbage: awful\n\n  - name: Ensure that the submodule can fail\n    # You can use negative to indicate that the test case is expected to fail\n    negative: true\n    project_root: submodule\n    terraform_version: 0.13.0\n</code></pre> <p>This configuration is nearly identical to the one described in the Runtime configuration section, with both <code>test_defaults</code> and each test case accepting the same configuration block. Note that settings explicitly specified in each test case will override those in the <code>test_defaults</code> section. Also, notice that each test case has a name, which is a required field.</p> <p>Info</p> <p>While we don't check for name uniqueness, it's always good idea to give your test cases descriptive names, as these are then used to report job status on your commits and pull requests.</p>"},{"location":"vendors/terraform/module-registry.html#tests","title":"Tests","text":"<p>In order to verify that your module is working correctly, Spacelift can run a number of test cases for your module. Note how the configuration above allows you to set up different runtime environment (Docker image, Terraform version) etc. If you want to test the module with different inputs, these can be passed as Terraform variables (starting with <code>TF_VAR_</code>) through the test-level <code>environment</code> configuration option - see above for an example.</p> <p>While coverage is not yet calculated or enforced, we suggest that tests set up all resources defined by the module and submodules. It's generally a good idea to provide examples in the <code>examples/</code> directory of your repository showing users how they can use the module in practice. These examples can then become your test cases, and you can test them against multiple supported Terraform version to maximize compatibility.</p> <p>While running each test case, Spacelift will - as usual, initialize, plan and apply the resource, but also destroy everything in the end, checking for errors. In the meantime, it will also validate that some resources have actually been created by the tests - though as for now it does not care what these are.</p> <p>A test case can be marked as <code>negative</code>, which means that it is expected to fail. In an example above one of the test cases is expected to fail if one of the required Terraform variables is not set. Negative test cases are as useful as positive ones because they can prove that the module will not work under certain - unexpected or erroneous - circumstances.</p> <p>Test cases will be executed in parallel (as much as worker count permits) for each of the test cases version you have specified in the module configuration.</p> <p>Tests run both on proposed and tracked changes. When a tracked change occurs, we create a Version. Versions are described in more detail in the Versions section.</p> <p>Info</p> <p>Each test case will have its own commit status in GitHub / GitLab.</p>"},{"location":"vendors/terraform/module-registry.html#test-case-ordering","title":"Test case ordering","text":"<p>You can specify the order in which test cases should be executed by setting the <code>depends_on</code> property on a test case. This property accepts a list of test case <code>id</code>s that must be executed before the current test case. For example:</p> <pre><code>version: 1\nmodule_version: 1.0.0\n\ntests:\n  # This one is executed first.\n  - name: Test the module with 0.12.7\n    id: test-0.12.7\n    terraform_version: 0.12.7\n\n  # This is executed second, because it depends on the first test case.\n  - name: Test the submodule with 0.13.0\n    id: test-0.13.0\n    project_root: submodule\n    terraform_version: 0.13.0\n    depends_on: [\"test-0.12.7\"]\n\n  # This is executed third, because it depends on the second test case.\n  - name: Ensure that the submodule can fail\n    depends_on: [\"test-0.13.0\"]\n    negative: true\n    project_root: submodule\n</code></pre> <p>Note that in order to refer to a test case, you need to set a unique <code>id</code> to it.</p>"},{"location":"vendors/terraform/module-registry.html#versions","title":"Versions","text":"<p>Tip</p> <p>If you would like to import old module versions, you can use a bash script that lists the tags in the repo and then use spacectl: <code>spacectl module create-version --id &lt;MODULE ID&gt; --sha &lt;COMMIT SHA&gt; --version &lt;MODULE VERSION&gt;</code></p> <p>Whenever tests succeed on a tracked change, a new Version is created based on the <code>module_version</code> in the configuration. Important thing to note is that Spacelift will not let you reuse the number of a successful version, and will require you to strictly follow semantic versioning - ie. you can't go to from <code>0.2.0</code> to <code>0.4.0</code>, skipping <code>0.3.0</code> entirely.</p> <p></p> <p>Two proposed git flow are as follows:</p> <p>The first one would be to have a main branch and create feature branches for changes. Whenever you merge to the main branch you bump the version and release it.</p> <p>If you'd like to ensure that the version is bumped before merging, we recommend adding something similar to your CI/CD pipeline:</p> <pre><code>git diff --exit-code --quiet HEAD^ HEAD -- .spacelift/config.yml\n\nif [ $? -ne 0 ]; then\n  echo \"Make sure to bump the module version in .spacelift/config.yml\"\n  exit 1\nfi\n</code></pre> <p>If you want more control over release schedules, you could go with the following:</p> <ul> <li>A release branch</li> <li>A main branch</li> <li>Feature branches</li> </ul> <p>Whenever you add a new functionality, you may want to create a feature branch and open Pull Request from it to the main branch. Whenever you want to release a new version, you merge the main branch into the release branch.</p> <p>You can also use Git push policies to further customize this.</p> <p>Tip</p> <p>If you would like to manage your Terraform Module versions using git tags, and would like git tag events to push your module to the Spacelift module registry. Please review our Tag-driven Terraform Module Release Flow.</p> <p>Info</p> <p>If no test cases are present, the version is immediately marked green.</p>"},{"location":"vendors/terraform/module-registry.html#marking-versions-as-bad","title":"Marking versions as bad","text":"<p>Warning</p> <p>Marking a version as bad cannot be undone.</p> <p>If you don't want people to use a specific version of your module, you can mark it as bad. Currently, this feature doesn't have any technical implications - it is still downloadable and usable, but it's a good way to communicate to your users that a specific version is not recommended.</p> <p>You can mark a version as bad using the dropdown menu on the right side of the versions list or in the version details view. Make sure to leave a note explaining why the version is bad.</p> <p></p> <p>You'll be able to see in the version details that the version has been marked as bad and the description.</p> <p></p>"},{"location":"vendors/terraform/module-registry.html#modules-in-practice","title":"Modules in practice","text":"<p>In order to use modules, you have to source them from the Spacelift module registry. You can generate the necessary snippet, by opening the page of the specific module version, and clicking show instructions.</p> <p></p> <p>Info</p> <p>Stacks that use private modules need access to the Space the modules reside in, which can be achieved via space inheritance or module space sharing.</p>"},{"location":"vendors/terraform/module-registry.html#sharing-modules","title":"Sharing modules","text":""},{"location":"vendors/terraform/module-registry.html#space-sharing","title":"Space sharing","text":"<p>Modules can be shared with specific spaces within your account, giving you fine-grained control over which teams can discover and use your modules. When you share a module with a space, any stack in that space can reference it in their Terraform configurations.</p> <p>When you share a module with a space, all child spaces with inheritance enabled will also be able to access the module.</p> <p>To share a module with other spaces, select the target spaces in the sharing section under Module settings &gt; Availability</p> <p></p> <p>This can also be accomplished programmatically using our Terraform provider via the <code>space_shares</code> attribute:</p> <pre><code>resource \"spacelift_space\" \"frontend_team\" {\n  name = \"frontend-team\"\n}\n\nresource \"spacelift_space\" \"backend_team\" {\n  name = \"backend-team\"\n}\n\n# Platform module owned by root space, shared with application teams\nresource \"spacelift_module\" \"common_networking\" {\n  name       = \"networking\"\n  branch     = \"main\"\n  repository = \"terraform-networking\"\n\n  space_shares = [\n    spacelift_space.frontend_team.id,\n    spacelift_space.backend_team.id,\n  ]\n}\n</code></pre> <p>The <code>space_shares</code> attribute accepts a list of space IDs. Changes to this list will update sharing permissions, allowing you to manage module access as code.</p>"},{"location":"vendors/terraform/module-registry.html#using-shared-modules","title":"Using shared modules","text":"<p>When a module is shared with a space you have access to, you can:</p> <ul> <li>Discover it in the module list and search results.</li> <li>Reference it in your stack configurations.</li> </ul> <p>Module management remains with the owner. You cannot see module details, modify settings, create versions, or change sharing configuration for modules shared with you.</p>"},{"location":"vendors/terraform/module-registry.html#finding-shared-modules","title":"Finding shared modules","text":"<p>The module list includes both modules you own and modules shared with you.</p> <p></p> <p>Modules shared with you are greyed out because the only thing you can do with them is consume them from your stacks.</p> <p>You can use the availability filter to filter by specific spaces to see what's shared where.</p> <p></p> <p>This helps module owners understand their sharing configuration and helps consumers find the modules they can use.</p>"},{"location":"vendors/terraform/module-registry.html#when-to-use-space-level-sharing","title":"When to use space-level sharing","text":"<p>Space-level sharing is the recommended approach for most organizations. Use it when:</p> <ul> <li>Different teams within your account need access to different module sets.</li> <li>You want centralized governance over who can use which modules.</li> </ul> <p>For example, your platform team might maintain networking and security modules in a dedicated space, then share them selectively with specific application team spaces.</p>"},{"location":"vendors/terraform/module-registry.html#cross-account-sharing","title":"Cross-account sharing","text":"<p>Migrating from cross-account sharing</p> <p>If you're currently sharing modules between accounts, consider consolidating into a single account with space-based organization. This provides better visibility, simpler access control, and improved governance.</p> <p>Cross-account sharing is supported for backwards compatibility and specific multi-account scenarios, but space-level sharing is recommended for new implementations.</p> <p>Consider cross-account sharing only if you manage completely separate Spacelift accounts that need to share modules.</p>"},{"location":"vendors/terraform/module-registry.html#public-modules","title":"Public modules","text":"<p>Public modules are accessible to anyone, including users outside your Spacelift account. This is appropriate for open-source modules you want to share with the broader community.</p> <p>To make a module public, use the <code>public</code> attribute in the Terraform provider:</p> <pre><code>resource \"spacelift_module\" \"public_example\" {\n  name       = \"example\"\n  branch     = \"main\"\n  repository = \"terraform-aws-example\"\n  public     = true\n}\n</code></pre> <p>Warning</p> <p>Public modules are discoverable and usable by anyone. Only mark modules as public if you intend to share them with the community.</p>"},{"location":"vendors/terraform/module-registry.html#using-modules-outside-of-spacelift","title":"Using modules outside of Spacelift","text":"<p>Modules hosted in the private registry can be used outside of Spacelift.</p> <p>The easiest way is to have Terraform retrieve and store the credentials by running the following command in a terminal:</p> <pre><code>terraform login spacelift.io\n</code></pre> <p>After you confirm that you want to proceed, Terraform will open your default web browser and ask you to log in to your Spacelift account. Once this is done, Terraform will store the credentials in the <code>~/.terraform.d/credentials.tfrc.json</code> file for use by subsequent commands.</p> <p>Warning</p> <p>The method above requires a web browser which is not always practical, for example on remote server with no GUI. In that case, you can use credentials generated from API keys. The credentials file generated upon the creation of each API key contains a section explaining how a key can be used to set up credentials in the Terraform configuration file (<code>.terraformrc</code>). To learn more about this please refer directly to Terraform documentation.</p>"},{"location":"vendors/terraform/module-registry.html#dependabot","title":"Dependabot","text":"<p>If you want to use Dependabot to automatically update your module versions, you can use the following <code>dependabot.yml</code> configuration:</p> <pre><code>version: 2\nregistries:\n  spacelift-private-registry:\n    type: terraform-registry\n    url: https://app.spacelift.io\n    token: ${{ secrets.SPACELIFT_TOKEN }}\nupdates:\n  - package-ecosystem: \"terraform\"\n    directory: \"/\"\n    registries:\n      - spacelift-private-registry\n    schedule:\n      interval: \"daily\"\n</code></pre> <p>Please refer to the important notes for more information.</p>"},{"location":"vendors/terraform/module-registry.html#renovate","title":"Renovate","text":"<p>If you want to use Renovate to automatically update your module versions, you can use the following <code>renovate.json</code> configuration:</p> <pre><code>{\n  \"$schema\": \"https://docs.renovatebot.com/renovate-schema.json\",\n  \"hostRules\": [\n    {\n      \"matchHost\": \"app.spacelift.io\",\n      \"encrypted\": {\n        \"token\": \"&lt;SPACELIFT_TOKEN&gt;\"\n      },\n      \"hostType\": \"terraform-module\"\n    }\n  ],\n  \"packageRules\": [\n    {\n      \"matchDatasources\": [\n        \"terraform-module\"\n      ],\n      \"registryUrls\": [\n        \"https://app.spacelift.io\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"vendors/terraform/module-registry.html#important-notes","title":"Important notes","text":"<ul> <li> <p>It is important for the <code>url</code> to be <code>https://app.spacelift.io</code> and for the <code>token</code> to be a Spacelift API key.</p> </li> <li> <p><code>Spacelift Token</code> should be the token that is used for accessing Spacelift-hosted Terraform modules outside of Spacelift.</p> </li> </ul> <pre><code>Please use the following API secret when communicating with Spacelift\nprogrammatically:\n\n&lt;NotThisValue&gt;\n\nPlease add this snippet to your .terraformrc file if you want to use this API\nkey to access Spacelift-hosted Terraform modules outside of Spacelift:\n\ncredentials \"spacelift.io\" {\n  token = &lt;ThisValue&gt;\n}\n</code></pre> <ul> <li> <p>Your API key needs to have read permissions to the space your module lives in at a minimum.</p> </li> <li> <p>If you are using login policies, non-admin keys must be defined within this policy. An example snipprt of this is below;</p> </li> </ul> <pre><code>allow {\n    input.session.login == \"api::01234\"\n }\n\n space_read[\"spaceid\"] {\n     input.session.login == \"api::01234\"\n }\n</code></pre> <p>Info</p> <p>If you are receiving an empty list of modules, it is likely that the api token does not have the correct access to the space.</p>"},{"location":"vendors/terraform/provider-registry.html","title":"Provider registry","text":""},{"location":"vendors/terraform/provider-registry.html#provider-registry","title":"Provider registry","text":""},{"location":"vendors/terraform/provider-registry.html#intro","title":"Intro","text":"<p>While the Terraform ecosystem is vast and growing, sometimes there is no official provider for your use case, especially if you need to interface with internal or niche tooling. This is where the Spacelift provider registry comes in. It's a place where you can publish your own providers. These providers can be used both inside, and outside of Spacelift.</p>"},{"location":"vendors/terraform/provider-registry.html#publishing-a-provider","title":"Publishing a provider","text":""},{"location":"vendors/terraform/provider-registry.html#assumptions","title":"Assumptions","text":"<p>We will not be covering the process of writing a Terraform provider in this article. If you are interested in that, please refer to the official documentation. In this article, we will assume that you already have a provider that you want to publish.</p> <p>We will also focus on providing step-by-step instructions for GitHub Actions users. If you are using a different CI/CD tool, you will need to adapt the steps accordingly based on its documentation. Note that you don't need to use a CI/CD tool to publish a provider - you could do it from your laptop. However, we recommend using a CI/CD tool to automate the process and provide an audit trail of what's been done, when, and by whom.</p> <p>Last but not least, we assume you're going to use GoReleaser to build your provider. This is by far the most common way of managing Terraform providers which need to be available for different operating systems and architectures. If you're not familiar with GoReleaser, please refer to the official documentation. You can also check out Terraform's official <code>terraform-provider-scaffolding</code> template repository for an example of using GoReleaser with Terraform providers.</p>"},{"location":"vendors/terraform/provider-registry.html#creating-a-provider","title":"Creating a provider","text":"<p>To create a provider in Spacelift, you have three options:</p>"},{"location":"vendors/terraform/provider-registry.html#use-our-terraform-provider-preferable","title":"Use our Terraform provider (preferable)","text":"<p>This is the easiest way to do it, as it will let you manage your provider declaratively in the future:</p> provider.tf<pre><code>resource \"spacelift_terraform_provider\" \"provider\" {\n  # This is the type of your provider.\n  type        = \"myinternaltool\"\n  space_id    = \"root\"\n  description = \"Explain what this is for\"\n}\n</code></pre> <p>Warning</p> <p>The <code>type</code> attribute in the <code>spacelift_terraform_provider</code> resource refers to the unique name of the provider within Spacelift account. It must consist of lowercase letters only.</p> <p>It is possible to mark the provider as public, which will make it available to everyone. This is generally not recommended, as it will make it easy for others to use your provider without your knowledge. At the same time, this is the only way of sharing a provider between Spacelift accounts. If you're doing that, make sure there is nothing sensitive in your provider. In order to mark the provider as public, you need to set its <code>public</code> attribute to <code>true</code>.</p>"},{"location":"vendors/terraform/provider-registry.html#use-the-api","title":"Use the API","text":"<p>To create a Terraform Provider using the GraphQL API, you can use the <code>terraformProviderCreate</code> mutation. This mutation allows you to create a new provider with the specified inputs.</p> <p>After successfully creating the Terraform Provider, you will receive an output of type <code>TerraformProvider</code>, which contains various fields providing information about the provider. These fields include the ID, creation timestamp, description, labels, latest version number, public accessibility, associated space details, update timestamp, specific version details, and a list of all versions of the provider.</p> <p>For more detailed information about the GraphQL API and its integration, please refer to the API documentation.</p>"},{"location":"vendors/terraform/provider-registry.html#create-the-provider-manually-in-the-ui","title":"Create the provider manually in the UI","text":"<p>In order to create a provider in the UI, navigate to the Terraform registry section of the account view, switch to the Providers tab and click the Create provider button:</p> <p></p> <p>In the Create Terraform provider drawer, you will need to provide the type, space and optionally a description and/or labels:</p> <p></p> <p>Info</p> <p>Note that we've put the provider in the <code>root</code> space. This is because we want to give everyone access to it. If you want to make it available only to a specific team, you can put it in a team-specific different space. In general, unless providers are made public, they can be accessed by all users and stacks belonging to the same space and its children (assuming they're set to inherit resources).</p>"},{"location":"vendors/terraform/provider-registry.html#register-a-gpg-key","title":"Register a GPG key","text":"<p>Warning</p> <p>Only Spacelift root admins can manage account GPG keys. If you're not a root admin, you will need to ask one to do it for you.</p> <p>Terraform uses GPG keys to verify the authenticity of providers. Before you can publish a provider version, you need to register a GPG key with Spacelift. Similarly to creating a provider, you have three options to register a GPG key:</p>"},{"location":"vendors/terraform/provider-registry.html#use-our-cli-tool-called-spacectl","title":"Use our CLI tool called <code>spacectl</code>","text":"<p>One reason we do not want to do it declaratively through the Terraform provider is that it would inevitably lead to the private key being stored in some Terraform state, which is not ideal. <code>spacectl</code> will let you register a GPG key without storing the private key anywhere outside of your system.</p> <p>If you have an existing GPG key that you want to use, you can use <code>spacectl</code> to register it:</p> <pre><code>spacectl provider add-gpg-key \\\n    --import \\\n    --name=\"My first GPG key\" \\\n    --path=\"Path to the ASCII-armored private key\"\n</code></pre> <p>Alternatively, <code>spacectl</code> can generate a new key for you. Note that <code>spacectl</code> generates GPG keys without a passphrase:</p> <pre><code>spacectl provider add-gpg-key \\\n    --generate \\\n    --name=\"My first GPG key\" \\\n    --email=\"Your email address\" \\\n    --path=\"Path to save the ASCII-armored private key to\"\n</code></pre> <p>You can list all your GPG keys using <code>spacectl</code>:</p> <pre><code>spacectl provider list-gpg-keys\n</code></pre>"},{"location":"vendors/terraform/provider-registry.html#use-the-api_1","title":"Use the API","text":"<p>To register a GPG Key using the GraphQL API, you can utilize the <code>gpgKeyCreate</code> mutation. This mutation allows you to register a GPG key with the specified inputs.</p> <p>After successfully registering the GPG Key, you will receive an output of type <code>GpgKey</code>, which contains various fields providing information about the key. These fields include the creation timestamp, the user who created the key, the optional description, the ID, the name of the key, the revocation timestamp (if the key has been revoked), the user who revoked the key (if applicable), and the timestamp of the last update to the key.</p> <p>For more detailed information about the GraphQL API and its integration, please refer to the API documentation.</p>"},{"location":"vendors/terraform/provider-registry.html#register-the-gpg-key-manually-in-the-ui","title":"Register the GPG key manually in the UI","text":"<p>In order to register a GPG key in the UI, navigate to the Terraform registry section of the account view, switch to the GPG keys tab and click the Register GPG key button:</p> <p></p> <p>In the Register GPG key drawer, you will need to provide the name, the ASCII-armored private key and optionally a description:</p> <p></p>"},{"location":"vendors/terraform/provider-registry.html#cicd-setup","title":"CI/CD setup","text":"<p>Now that you have a provider and a GPG key, you can set up your CI/CD tool to publish provider versions. First, let's set up the GoReleaser config file in your provider repository:</p> .goreleaser.yml<pre><code>builds:\n  - env: ['CGO_ENABLED=0']\n    flags: ['-trimpath']\n    ldflags: ['-s -w -X main.version={{ .Version }} -X main.commit={{ .Commit }}']\n    goos: [darwin, linux]\n    goarch: [amd64, arm64]\n    binary: '{{ .ProjectName }}_v{{ .Version }}'\n\narchives:\n  - format: zip\n    name_template: '{{ .ProjectName }}_{{ .Version }}_{{ .Os }}_{{ .Arch }}'\n\nchecksum:\n  name_template: '{{ .ProjectName }}_{{ .Version }}_SHA256SUMS'\n  algorithm: sha256\n\nsigns:\n  - artifacts: checksum\n    args:\n      - \"--batch\"\n      - \"--local-user\"\n      - \"{{ .Env.GPG_FINGERPRINT }}\"\n      - \"--output\"\n      - \"${signature}\"\n      - \"--detach-sign\"\n      - \"${artifact}\"\n\nrelease:\n  disable: true\n</code></pre> <p>This setup assumes that the name of your project (repository) is <code>terraform-provider-$name</code>. If it is not (maybe you're using a monorepo?) then you will need to change the config accordingly, presumably by hardcoding the project name.</p> <p>Next, let's make sure you have an API key for the Spacelift account you want to publish the provider to. You can refer to the API key management section of the API documentation for more information on how to do that. Note that the key you're generating must have admin access to the space that the provider lives in. If the provider is in the <code>root</code> space, then the key must have root admin access.</p> <p>We can now add a GitHub Actions workflow definition to our repository:</p> .github/workflows/release.yml<pre><code>name: release\non:\n  push:\n\npermissions:\n  contents: write\n\njobs:\n  goreleaser:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v5\n\n      - name: Unshallow\n        run: git fetch --prune --unshallow\n\n      - name: Set up Go\n        uses: actions/setup-go@v6\n        with:\n          go-version-file: 'go.mod'\n          cache: true\n\n      - name: Import GPG key\n        uses: crazy-max/ghaction-import-gpg@v6\n        id: import_gpg\n        with:\n          # The private key must be stored in an environment variable registered\n          # with GitHub. The expected format is ASCII-armored.\n          #\n          # If you need to use a passphrase, you can populate it in this\n          # section, too.\n          gpg_private_key: ${{ secrets.GPG_PRIVATE_KEY }}\n\n      - name: Install spacectl\n        uses: spacelift-io/setup-spacectl@main\n\n      - name: Run GoReleaser\n        # We will only run GoReleaser when a tag is pushed. Semantic versioning\n        # is required, but build metadata is not supported.\n        if: startsWith(github.ref, 'refs/tags/')\n        uses: goreleaser/goreleaser-action@v6\n        with:\n          version: latest\n          args: release --clean\n        env:\n          GPG_FINGERPRINT: ${{ steps.import_gpg.outputs.fingerprint }}\n\n      - name: Release new version\n        if: startsWith(github.ref, 'refs/tags/')\n        env:\n          GPG_KEY_ID: ${{ steps.import_gpg.outputs.keyid }}\n\n          # This is the URL of the Spacelift account hosting the provider.\n          SPACELIFT_API_KEY_ENDPOINT: https://youraccount.app.spacelift.io\n\n          # This is the ID of the API key you generated earlier.\n          SPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\n\n          # This is the secret of the API key you generated earlier.\n          SPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\n        run: # Don't forget to change the provider type!\n          spacectl provider create-version --type=TYPE-change-me!!!\n</code></pre> <p>If everything is fine, pushing a tag like <code>v0.1.0</code> should create a new draft version of the provider in Spacelift. You can list the versions of your provider using <code>spacectl</code>:</p> <pre><code>spacectl provider list-versions --type=$YOUR_PROVIDER_TYPE\n</code></pre> <p>Warning</p> <p>The <code>type</code> parameter in the <code>spacectl provider list-versions</code> command refers to the unique name of the provider within Spacelift account. It must consist of lowercase letters only.</p> <p>Note the version status. Versions start their life as drafts, and you can publish them by grabbing their ID (first column) and using <code>spacectl</code>:</p> <pre><code>spacectl provider publish-version --version=$YOUR_VERSION_ID\n</code></pre> <p>Warning</p> <p>The <code>version</code> parameter in the <code>spacectl provider publish-versions</code> command refers to the unique ID of the provider version within Spacelift account.</p> <p>Once published, your version is ready to use. See the next section for more information.</p>"},{"location":"vendors/terraform/provider-registry.html#using-providers","title":"Using providers","text":"<p>Terraform providers hosted by Spacelift can be used the same way as providers hosted by the Terraform Registry. The only difference is that you need to specify the Spacelift registry URL in your Terraform configuration.</p> main.tf<pre><code>terraform {\n  required_providers {\n    yourprovider = {\n      source  = \"your-hostname/your-org/yourprovider\"\n    }\n  }\n}\n</code></pre> <p>The above example does not refer to a specific version, meaning that you are going to always use the latest available (published) version of your provider. That said, you can use any versioning syntax supported by the Terraform Registry - learn more about it here.</p>"},{"location":"vendors/terraform/provider-registry.html#using-providers-inside-spacelift","title":"Using providers inside Spacelift","text":"<p>All runs in any of the stacks belonging to the provider's space or one of its children will be able to read the provider from the Spacelift registry, same as with modules This is because the runs are automatically authenticated to the spacelift.io registry while the run is in progress.</p>"},{"location":"vendors/terraform/provider-registry.html#using-providers-outside-of-spacelift","title":"Using providers outside of Spacelift","text":"<p>If you need to use the provider outside of Spacelift, you will need to authenticate to the registry first, either interactively (eg. on your machine) or using an API key (eg. in automation). This process is the same as with modules, so please refer to that section for more information.</p> <p>Your access to the required provider will be determined by your access to the space that the provider lives in. If you have read access to the space, you will be able to use the provider.</p>"},{"location":"vendors/terraform/provider-registry.html#other-management-tasks","title":"Other management tasks","text":"<p>Beyond creating and publishing new versions, there are a few other tasks you can perform on your provider. In this section we will cover the most common ones.</p>"},{"location":"vendors/terraform/provider-registry.html#revoking-gpg-keys","title":"Revoking GPG keys","text":"<p>If you lose control over your GPG key, you will want to revoke it. Revoking a key has no automatic impact on provider versions already published, but it will prevent you from publishing new versions signed with that key. You can revoke a key using <code>spacectl</code>:</p> <pre><code>spacectl provider revoke-gpg-key --key=$ID_OF_YOUR_KEY\n</code></pre> <p>... or by using the UI. In order to do that, click the Revoke button inside the three dots menu next to the key you want to revoke:</p> <p></p> <p>If you want to also revoke any of the provider versions signed with this key, refer to the section on revoking provider versions.</p>"},{"location":"vendors/terraform/provider-registry.html#deleting-provider-versions","title":"Deleting provider versions","text":"<p>You can permanently delete any draft provider version using <code>spacectl</code>:</p> <pre><code>spacectl provider delete-version --version=$ID_OF_YOUR_VERSION\n</code></pre> <p>You can also do that in the UI, just navigate to the provider page, find the version you want to delete, and click the Delete button inside the three dots menu next to the version you want to delete:</p> <p></p> <p>A deleted version disappears from the list of versions, and you can reuse its number in the future.</p> <p>You cannot delete published versions. If you want to disable a published version, you will need to revoke it instead.</p>"},{"location":"vendors/terraform/provider-registry.html#revoking-provider-versions","title":"Revoking provider versions","text":"<p>You can revoke any published provider version using <code>spacectl</code>:</p> <pre><code>spacectl provider revoke-version --version=$ID_OF_YOUR_VERSION\n</code></pre> <p>Similarly to deleting a draft provider version, you can delete a published one in the UI by clicking the Revoke button inside the three dots menu next to the version you want to revoke:</p> <p></p> <p>This will prevent anyone from using the version in the future, but it will not delete it. The version will remain on the list of versions, and will never be available again. You will not be able to reuse the version number of a revoked version.</p>"},{"location":"vendors/terraform/resource-sanitization.html","title":"Resource Sanitization","text":""},{"location":"vendors/terraform/resource-sanitization.html#resource-sanitization","title":"Resource Sanitization","text":"<p>The Terraform state can contain very sensitive data. Sometimes this is unavoidable because of the design of certain Terraform providers or because the definition of what is sensitive isn't always simple and may vary between individuals and organizations.</p> <p>Spacelift provides two different approaches for sanitizing values when resources are stored or passed to Plan policies:</p> <ul> <li>Default Sanitization: All string values are sanitized.</li> <li>Smart Sanitization: Only the values marked as sensitive are sanitized.</li> </ul> <p>For example, if we take the following definition for an AWS RDS instance:</p> <pre><code>resource \"aws_db_instance\" \"example\" {\n  allocated_storage    = 10\n  db_name              = \"exampledb\"\n  engine               = \"mysql\"\n  engine_version       = \"5.7\"\n  instance_class       = var.instance_class\n  username             = var.username\n  password             = var.password\n  parameter_group_name = \"default.mysql5.7\"\n  skip_final_snapshot  = true\n}\n\nvariable \"instance_class\" {\n  default     = \"db.t3.micro\"\n  description = \"Instance type\"\n  type        = string\n}\n\nvariable \"username\" {\n  description = \"Admin username\"\n  sensitive   = true\n  type        = string\n}\n\nvariable \"password\" {\n  description = \"Admin password\"\n  sensitive   = true\n  type        = string\n}\n</code></pre> <p>Spacelift will supply something similar to the following to any plan policies:</p> Default SanitizationSmart Sanitization <pre><code>{\n  \u2026\n  \"terraform\": {\n    \"resource_changes\": [\n      {\n        \"address\": \"aws_db_instance.example\",\n        \"change\": {\n          \"actions\": [\"create\"],\n          \"after\": {\n            \"allocated_storage\": 10,\n            \"db_name\": \"59832d41\",\n            \"engine\": \"eae35047\",\n            \"engine_version\": \"fc40d152\",\n            \"instance_class\": \"4f8189cd\",\n            \"parameter_group_name\": \"bead1390\",\n            \"password\": \"c1707d9d\",\n            \"skip_final_snapshot\": true,\n            \"username\": \"6266e7ae\",\n            \u2026\n          },\n          \u2026\n        },\n        \u2026\n      }\n    ]\n  }\n  \u2026\n}\n</code></pre> <pre><code>{\n  \u2026\n  \"terraform\": {\n    \"resource_changes\": [\n      {\n        \"address\": \"aws_db_instance.example\",\n        \"change\": {\n          \"actions\": [\"create\"],\n          \"after\": {\n            \"allocated_storage\": 10,\n            \"db_name\": \"exampledb\",\n            \"engine\": \"mysql\",\n            \"engine_version\": \"5.7\",\n            \"instance_class\": \"db.t3.micro\",\n            \"parameter_group_name\": \"default.mysql5.7\",\n            \"password\": \"c1707d9d\",\n            \"skip_final_snapshot\": true,\n            \"username\": \"6266e7ae\",\n            \u2026\n          },\n          \u2026\n        },\n        \u2026\n      }\n    ]\n  }\n  \u2026\n}\n</code></pre> <p>As you can see in the example above, with the Default Sanitization all string values are hashed which makes it difficult to comprehend logs, work with outputs/created resources, and write policies against the changes in your stacks. With Smart Sanitization, only sensitive values are hashed.</p> <p>Smart Sanitization allows you to author Plan policies against non-sensitive string values without the need for a call to the <code>sanitized()</code> function.</p> Default SanitizationSmart Sanitization <pre><code>package spacelift\nimport future.keywords\n\nallowed_instance_classes := [\"db.t3.micro\", \"db.t3.small\", \"db.t3.medium\"]\n\n# The values have to be sanitized before they can be compared with the resource change value\nsanitized_allowed_instance_classes := {c | sanitized(allowed_instance_classes[_], c)}\n\ndeny[\"Instance class is not allowed\"] {\n  resource := input.terraform.resource_changes[_]\n  not resource.change.after.instance_class in sanitized_allowed_instance_classes\n}\n\ndeny[\"Username cannot be 'admin'\"] {\n  resource := input.terraform.resource_changes[_]\n\n  # The value has to be sanitized before they can be compared with the resource change value\n  resource.change.after.username = sanitized(\"admin\")\n}\n\n\n# Enable sampling to help us debug the policy\nsample { true }\n</code></pre> <pre><code>package spacelift\nimport future.keywords\n\n# No need to sanitize the values because the argument is not marked as sensitive\nallowed_instance_classes := [\"db.t3.micro\", \"db.t3.small\", \"db.t3.medium\"]\n\ndeny[\"Instance class is not allowed\"] {\n  resource := input.terraform.resource_changes[_]\n  not resource.change.after.instance_class in allowed_instance_classes\n}\n\ndeny[\"Username cannot be 'admin'\"] {\n  resource := input.terraform.resource_changes[_]\n\n  # The value has to be sanitized before it can be compared because the argument is marked as sensitive\n  resource.change.after.username = sanitized(\"admin\")\n}\n\n\n# Enable sampling to help us debug the policy\nsample { true }\n</code></pre> <p>Info</p> <p>The same sanitization is also applied to resources shown in the resources views.</p>"},{"location":"vendors/terraform/resource-sanitization.html#default-sanitization","title":"Default Sanitization","text":"<p>Unless you enable Smart Sanitization, or disable Sanitization altogether, Default Sanitization will be used.</p>"},{"location":"vendors/terraform/resource-sanitization.html#smart-sanitization","title":"Smart Sanitization","text":"<p>Info</p> <p>Due to limitations in the data output by Terraform, Smart Sanitization can only be used on stacks that use Terraform versions 1.0.1 or above. Using an unsupported version will fail.</p> <p>As we rely on the sensitive argument in Terraform to determine which of your values are sensitive we recommend that you ensure your variables, outputs, and resources have their <code>sensitive</code> arguments set properly.</p>"},{"location":"vendors/terraform/resource-sanitization.html#how-to-enable-smart-sanitization-in-your-stacks","title":"How to enable Smart Sanitization in your stacks","text":""},{"location":"vendors/terraform/resource-sanitization.html#when-using-the-spacelift-user-interface","title":"When using the Spacelift User Interface","text":"<p>If you're using the Spacelift user interface to create your stacks, you can enable Smart Sanitization in your existing stack settings page under the \"Vendor\" section. If you're creating new stacks it will be an option when you select your Terraform version in our wizard.</p> <p></p>"},{"location":"vendors/terraform/resource-sanitization.html#when-creating-stacks-with-the-spacelift-terraform-provider","title":"When creating stacks with the Spacelift Terraform Provider","text":"<p>If you\u2019re using the Spacelift Terraform provider for creating Spacelift stacks, you are able to set the property <code>terraform_smart_sanitization</code>. For example, to create a simple stack you can use the following code:</p> <pre><code>resource \"spacelift_stack\" \"user_dashboard_internal\" {\n    administrative               = true\n    autodeploy                   = true\n    branch                       = \"main\"\n    description                  = \u201cCreates and manages the user management internal dashboard\"\n    name                         = \"User Dashboard Internal\"\n    repository                   = \"management\"\n    terraform_smart_sanitization = true\n    terraform_version            = \"1.3.1\"\n}\n</code></pre>"},{"location":"vendors/terraform/resource-sanitization.html#disabling-sanitization","title":"Disabling Sanitization","text":"<p>If you have a situation where the <code>sanitized()</code> helper function doesn't provide you with enough flexibility to create a particular policy, you can disable sanitization completely for a stack. To do this, add the <code>feature:disable_resource_sanitization</code> label to your stack. This will disable sanitization for any future runs.</p>"},{"location":"vendors/terraform/state-management.html","title":"State management","text":""},{"location":"vendors/terraform/state-management.html#state-management","title":"State management","text":"<p>For those of you who don't want to manage Terraform state, Spacelift offers an optional sophisticated state backend synchronized with the rest of the application to maximize security and convenience. The ability to have Spacelift manage the state for you is only available during stack creation.</p> <p>As you can see, it's also possible to import an existing Terraform state at this point, which is useful for users who want to upgrade their previous Terraform workflow.</p> <p>Info</p> <p>If you're using Spacelift to manage your stack, do not specify any Terraform backend whatsoever. The one-off config will be dynamically injected into every run and task.</p>"},{"location":"vendors/terraform/state-management.html#do-or-do-not-there-is-no-try","title":"Do. Or do not. There is no try.","text":"<p>In this section we'd like to give you a few reasons why it could be useful to trust Spacelift to take care of your Terraform state. To keep things level, we'll also give you a reason not to.</p>"},{"location":"vendors/terraform/state-management.html#do","title":"Do","text":"<ol> <li> <p>It's super simple - just two clicks during stack setup. Otherwise there's nothing to set up on your end, so one    fewer sensitive thing to worry about. Feel free to refer    to how it works on our end, but overall we believe it to be a rather sensible and    secure setup, at least on par with anything you could set up on your end.</p> </li> <li> <p>It's protected against accidental or malicious access. Again, you can refer to the more technical section on the    inner workings of the state server, but the gist is that we're able to map state access and state changes to    legitimate Spacelift runs, thus automatically blocking all other unauthorized traffic. As far as we know, no other    backend is capable of that, which is one more reason to give us a go.</p> </li> </ol>"},{"location":"vendors/terraform/state-management.html#dont","title":"Don't","text":"<ol> <li>We'll let you in on a little secret now - behind the pixie dust it's still Amazon S3 all the way down, and at this    stage we store all our data in Ireland. If you're not OK with that, you're better off    managing the state on your end.</li> </ol>"},{"location":"vendors/terraform/state-management.html#how-it-works","title":"How it works","text":"<p>S3, like half of the Internet. The pixie dust we're adding on top of it involves generating one-off credentials for every run and task and injecting them directly into the root of your Terraform project as a <code>.tf</code> file.</p> <p>Warning</p> <p>If you have some Terraform state backend already specified in your code, the initialization phase will keep failing until you remove it.</p> <p>The state server is an HTTP endpoint implementing the Terraform standard state management protocol. Our backend always ensures that the credentials belong to one of the runs or tasks that are currently marked as active on our end, and their state indicates that they should be accessing or modifying the state. Once this is established, we just pass the request to S3 with the right parameters.</p>"},{"location":"vendors/terraform/state-management.html#state-history","title":"State history","text":"<p>If your state is managed by spacelift, you can list all the changes on your state, and eventually rollback to an old version if needed.</p> <p></p> <p>Info</p> <p>Not all runs or tasks will trigger a new state version, so you should not expect to see an exhaustive list of your runs and tasks in this list. For example runs that produce no Terraform changes do not result in a new state version being created.</p> <p>Non-current state versions are kept for 30 days.</p>"},{"location":"vendors/terraform/state-management.html#state-rollback","title":"State rollback","text":"<p>In certain unusual scenarios you can end up with a broken or corrupted state being created. This could happen for example if there was a bug during a Terraform provider upgrade.</p> <p>State rollback allows you to recover from this by rolling back your state to a previous version.</p> <p>Rolling back your state will not apply any changes to your current infrastructure. It just reverts your state to an older version. It's up to you to trigger the proper tasks or runs to fix the state and re-apply the desired Terraform configuration.</p> <p>Warning</p> <p>You should really understand what you are doing when performing a rollback.</p> <p>State rollback should be used as a break-glass operation just after a corrupted state has been created.</p> <p>If a stack is currently using a rolled-back state, a warning will be shown in the stack header.</p> <p></p> <p>To be able to roll back a state, the 3 conditions below must be satisfied:</p> <ul> <li>You must be a stack admin</li> <li>The stack must be locked</li> <li>The stack must not have any pending runs or tasks</li> </ul> <p>If those three conditions are met, you will be able to rollback your stack to a previous version of your state file.</p> <p></p> <p>After rollback completes successfully, a new version of your state will appear above the other state versions and will be marked as a rollback.</p> <p></p>"},{"location":"vendors/terraform/state-management.html#importing-resources-into-your-terraform-state","title":"Importing resources into your Terraform State","text":"<p>So you have an existing resource that was created by other means and would like that resource to be reflected in your terraform state. This is an excellent use case for the terraform import command. When you're managing your own terraform state, you would typically run this command locally to import said resource(s) to your state file, but what do I do when I'm using Spacelift-managed state you might ask? Spacelift Task to the rescue!</p> <p>To do this, use the following steps:</p> <ul> <li>Select the Spacelift Stack to which you would like to import state for.</li> <li>Within the navigation, select \"Tasks\"</li> </ul> <p></p> <ul> <li>Run the <code>terraform import</code> command needed to import your state file to the Spacelift-managed state by typing the   command into the text input and clicking the perform button. Note: If you are using Terragrunt on Spacelift, you will   need to run <code>terragrunt import</code></li> </ul> <p></p> <ul> <li>Follow the status of your task's execution to ensure it was executed successfully. When completed, you should see an   output similar to the following within the \"Performing\" step of your task.</li> </ul> <p></p>"},{"location":"vendors/terraform/state-management.html#importing-existing-state-file-into-your-terraform-stacks","title":"Importing existing state file into your Terraform Stacks","text":""},{"location":"vendors/terraform/state-management.html#on-an-existing-stack","title":"On an existing stack","text":"<p>State import allows you to import a state on top of the latest managed state.</p> <p>Warning</p> <p>You should really understand what you are doing when importing a state.</p> <p>To be able to import a state, the 3 conditions below must be satisfied:</p> <ul> <li>You must be a stack admin</li> <li>The stack must be locked</li> <li>The stack must not have any pending runs or tasks</li> </ul> <p>To import a new state you need to go to the state history tab on your stack, and then click on the button to import the state.</p> <p></p> <p>Info</p> <p>The maximum allowed file size for a state is 100MB.</p> <p>Then choose a valid terraform state file and upload it.</p> <p></p> <p>Once the file is uploaded, you can click on Import state.</p> <p>The imported state will appear in the list as manually imported.</p> <p></p>"},{"location":"vendors/terraform/state-management.html#during-stack-creation","title":"During stack creation","text":"<p>When creating a stack, you can optionally import an existing Terraform state file so that Spacelift can manage it going forward.</p> <p></p> <p>You can also import an existing Terraform state file when using Spacelift Terraform provider.</p> stack.tf<pre><code>resource \"spacelift_stack\" \"example-stack\" {\n  name = \"Example Stack in Spacelift\"\n\n  # Source code.\n  repository = \"&lt;Repository Name&gt;\"\n  branch = \"main\"\n\n  # State file information\n  import_state      = \"&lt;State File to Upload&gt;\"\n  import_state_file = \"&lt;Path to the State file&gt;\"\n}\n</code></pre>"},{"location":"vendors/terraform/state-management.html#exporting-spacelift-managed-terraform-state-file","title":"Exporting Spacelift-managed Terraform state file","text":"<p>Info</p> <p>If you enable external state access, you can export the stack's state from outside of Spacelift.</p> <p>If a Terraform stack's state is managed by Spacelift and you need to export it you can do so by running the following command in a Task:</p> <pre><code>terraform state pull &gt; terraform.tfstate\n</code></pre> <p>The local workspace is discarded after the Task has finished so you most likely want to combine this command with another one that pushes the <code>terraform.tfstate</code> file to some remote location.</p> <p>Here is an example of pushing the state file to an AWS S3 bucket (without using an intermediary file):</p> <pre><code>terraform state pull | aws s3 cp - s3://example-bucket/folder/sub-folder/terraform.tfstate\n</code></pre>"},{"location":"vendors/terraform/state-management.html#configure-terraform-plan-locking","title":"Configure terraform plan locking","text":"<p>By default <code>terraform plan</code> acquires a state lock. If you want to disable such lock during planning, you can pass <code>SPACELIFT_DISABLE_STATE_LOCK</code> to the stack Environment. </p> <p>Warning</p> <p>You should really understand what you are doing when disabling state lock for planning. Disabling the lock may lead to incorrect results in case of concurrent apply operation to the state.</p>"},{"location":"vendors/terraform/storing-complex-variables.html","title":"Storing Complex Variables","text":""},{"location":"vendors/terraform/storing-complex-variables.html#storing-complex-variables","title":"Storing Complex Variables","text":"<p>Terraform supports a variety of variable types such as <code>string</code>, <code>number</code>, <code>list</code>, <code>bool</code>, and <code>map</code>. The full list of Terraform's variable types can be/ found in the Terraform documentation.</p> <p>When using \"complex\" variable types with Spacelift such as <code>map</code> and <code>list</code> you'll need to utilize Terraform's jsonencode function when storing these variables as an environment variable in your Spacelift Stack environment or context.</p>"},{"location":"vendors/terraform/storing-complex-variables.html#usage-example","title":"Usage Example","text":"<pre><code>locals {\n  map = {\n    foo = \"bar\"\n  }\n  list = [\"this\", \"is\", \"a\", \"list\"]\n}\n\nresource \"spacelift_context\" \"example\" {\n  description = \"Example of storing complex variable types\"\n  name        = \"Terraform Complex Variable Types Example\"\n}\n\nresource \"spacelift_environment_variable\" \"map_example\" {\n  context_id = spacelift_context.example.id\n  name       = \"map_example\"\n  value      = jsonencode(local.map)\n  write_only = false\n}\n\nresource \"spacelift_environment_variable\" \"list_example\" {\n  context_id = spacelift_context.example.id\n  name       = \"list_example\"\n  value      = jsonencode(local.list)\n  write_only = false\n}\n</code></pre> <p>Notice the use of the <code>jsonencode</code> function when storing these complex variable types. This will allow you to successfully store these variable types within Spacelift.</p> <p></p>"},{"location":"vendors/terraform/storing-complex-variables.html#consuming-stored-variables","title":"Consuming Stored Variables","text":"<p>When consuming complex variable types in your environment, there is no need to use the <code>jsondecode()</code> function.</p>"},{"location":"vendors/terraform/terraform-provider.html","title":"Provider","text":""},{"location":"vendors/terraform/terraform-provider.html#provider","title":"Provider","text":"<p>What would you say if you could manage Spacelift resources - that is stacks, contexts, integrations, and configuration - using Spacelift? We hate ClickOps as much as anyone, so we designed everything from the ground up to be easily managed using a Terraform provider. We hope that advanced users will define most of their resources programmatically.</p>"},{"location":"vendors/terraform/terraform-provider.html#self-hosted-version-compatibility","title":"Self-Hosted Version Compatibility","text":"<p>The Terraform provider uses our GraphQL API to manage Spacelift, and relies on certain features being available in the API in order to work. What this can sometimes mean is that a new feature is added to the Terraform provider which hasn't yet been made available in the GraphQL API for Self-Hosted versions of Spacelift.</p> <p>Because of this, it's not always possible to use the latest version of the Terraform provider with Self-Hosted, and we recommend that you pin to a known-compatible version. You can do this using a <code>required_providers</code> block like in the following example:</p> <pre><code>terraform {\n    required_providers {\n        spacelift = {\n            source = \"spacelift-io/spacelift\"\n            version = \"1.14.0\"\n        }\n    }\n}\n</code></pre> <p>The following table shows the latest version of the Terraform provider known to work with our Self-Hosted versions:</p> Self-Hosted Version Max Provider Version 3.6.0 1.35.1 3.5.0 1.33.0 3.4.0 1.31.0 3.3.0 1.27.0 3.2.0 1.25.0 3.1.0 1.25.0 3.0.0 1.23.0 2.6.0 1.21.0 2.5.0 1.20.0 2.4.0 1.19.1 2.3.0 1.18.0 2.2.0 1.16.1 2.1.1 1.15.0 2.1.0 1.15.0 2.0.0 1.14.0 1.3.0 1.14.0 1.2.1 1.13.1 1.2.0 1.13.0 1.1.0-hotfix.1 1.10.0 1.1.0 1.10.0 1.0.0 1.8.1 0.0.12 1.8.0 0.0.11 1.8.0 0.0.10 1.4.0 0.0.9 1.3.1 0.0.8-hotfix.1 1.3.1 0.0.8 1.3.1"},{"location":"vendors/terraform/terraform-provider.html#taking-it-for-a-spin","title":"Taking it for a spin","text":"<p>Our Terraform provider is open source and its README always contains the latest available documentation. It's also distributed as part of our Docker runner image and available through our own provider registry. The purpose of this article isn't as much to document the provider itself but to show how it can be used to incorporate Spacelift resources into your infra-as-code.</p> <p>So, without further ado, let's define a stack:</p> stack.tf<pre><code>resource \"spacelift_stack\" \"managed-stack\" {\n  name = \"Stack managed by Spacelift\"\n\n  # Source code.\n  repository = \"testing-spacelift\"\n  branch     = \"master\"\n}\n</code></pre> <p>That's awesome. But can we put Terraform to good use and integrate it with resources from a completely different provider? Sure we can, and we have a good excuse, too. Stacks accessibility can be managed by GitHub teams, so why don't we define some?</p> stack-and-teams.tf<pre><code>resource \"github_team\" \"stack-readers\" {\n  name = \"managed-stack-readers\"\n}\n\nresource \"github_team\" \"stack-writers\" {\n  name = \"managed-stack-writers\"\n}\n\nresource \"spacelift_stack\" \"managed-stack\" {\n  name = \"Stack managed by Spacelift\"\n\n  # Source code.\n  repository = \"testing-spacelift\"\n  branch     = \"master\"\n\n}\n</code></pre> <p>Now that we programmatically combine Spacelift and GitHub resources, let's add AWS to the mix and give our new stack a dedicated IAM role:</p> stack-teams-and-iam.tf<pre><code>resource \"github_team\" \"stack-readers\" {\n  name = \"managed-stack-readers\"\n}\n\nresource \"github_team\" \"stack-writers\" {\n  name = \"managed-stack-writers\"\n}\n\nresource \"spacelift_stack\" \"managed-stack\" {\n  name = \"Stack managed by Spacelift\"\n\n  # Source code.\n  repository = \"testing-spacelift\"\n  branch     = \"master\"\n\n}\n\n# IAM role.\nresource \"aws_iam_role\" \"managed-stack\" {\n  name = \"spacelift-managed-stack\"\n\n  assume_role_policy = jsonencode({\n    Version   = \"2012-10-17\"\n    Statement = [\n      jsondecode(\n        spacelift_stack.managed-stack.aws_assume_role_policy_statement\n      )\n    ]\n  })\n}\n\n# Attaching a nice, powerful policy to it.\nresource \"aws_iam_role_policy_attachment\" \"managed-stack\" {\n  role       = aws_iam_role.managed-stack.name\n  policy_arn = \"arn:aws:iam::aws:policy/PowerUserAccess\"\n}\n\n# Telling Spacelift stack to assume it.\nresource \"spacelift_stack_aws_role\" \"managed-stack\" {\n  stack_id = spacelift_stack.managed-stack.id\n  role_arn = aws_iam_role.managed-stack.arn\n}\n</code></pre> <p>Success</p> <p>OK, so who wants to go back to clicking on things in the web GUI? Because you will likely need to do some clicking, too, at least with your first stack.</p>"},{"location":"vendors/terraform/terraform-provider.html#how-it-works","title":"How it works","text":"<p>Depending on whether you're using Terraform 0.12.x or higher, the Spacelift provider is distributed slightly differently. For 0.12.x users, the provider is distributed as a binary available in the runner Docker image in the same folder we put the Terraform binary. If you're using Terraform 0.13 and above, you can benefit from pulling our provider directly from our own provider registry. In order to do that, just point Terraform to the right location:</p> <pre><code>provider \"spacelift\" {}\n\nterraform {\n  required_providers {\n    spacelift = {\n      source = \"spacelift-io/spacelift\"\n    }\n  }\n}\n</code></pre>"},{"location":"vendors/terraform/terraform-provider.html#using-inside-spacelift","title":"Using inside Spacelift","text":"<p>Within Spacelift, the provider is configured by an environment variable <code>SPACELIFT_API_TOKEN</code> injected into each run and task belonging to stacks marked as administrative. This value is a bearer token that contains all the details necessary for the provider to work, including the full address of the API endpoint to talk to. It's technically valid for 3 hours but only when the run responsible for generating it is in Planning, Applying or Performing (for tasks) state and throughout that time it provides full administrative access to Spacelift entities that can be managed by Terraform within the same Spacelift account.</p>"},{"location":"vendors/terraform/terraform-provider.html#using-outside-of-spacelift","title":"Using outside of Spacelift","text":"<p>If you want to run the Spacelift provider outside of Spacelift, or you need to manage resources across multiple Spacelift accounts from the same Terraform project, the preferred method is to generate and use dedicated API keys. Note that unless you're just accessing whitelisted data resources, the Terraform use case will normally require marking the API key as administrative.</p> <p>In order to set up the provider to use an API key, you will need the key ID, secret, and the API key endpoint:</p> <pre><code>variable \"spacelift_key_id\" {}\nvariable \"spacelift_key_secret\" {}\n\nprovider \"spacelift\" {\n  api_key_endpoint = \"https://your-account.app.spacelift.io\"\n  api_key_id       = var.spacelift_key_id\n  api_key_secret   = var.spacelift_key_secret\n}\n</code></pre> <p>These values can also be passed using environment variables, though this will only work to set up the provider for a single Spacelift account:</p> <ul> <li><code>SPACELIFT_API_KEY_ENDPOINT</code> for <code>api_key_endpoint</code>;</li> <li><code>SPACELIFT_API_KEY_ID</code> for <code>api_key_id</code>;</li> <li><code>SPACELIFT_API_KEY_SECRET</code> for <code>api_key_secret</code>;</li> </ul> <p>If you want to talk to multiple Spacelift accounts, you just need to set up provider aliases like this:</p> <pre><code>variable \"spacelift_first_key_id\" {}\nvariable \"spacelift_first_key_secret\" {}\n\nvariable \"spacelift_second_key_id\" {}\nvariable \"spacelift_second_key_secret\" {}\n\nprovider \"spacelift\" {\n  alias = \"first\"\n\n  api_key_endpoint = \"https://first.app.spacelift.io\"\n  api_key_id       = var.spacelift_first_key_id\n  api_key_secret   = var.spacelift_first_key_secret\n}\n\nprovider \"spacelift\" {\n  alias = \"second\"\n\n  api_key_endpoint = \"https://second.app.spacelift.io\"\n  api_key_id       = var.spacelift_second_key_id\n  api_key_secret   = var.spacelift_second_key_secret\n}\n</code></pre> <p>If you're running from inside Spacelift, you can still use the default, zero-setup provider for the current account with providers for accounts set up through API keys:</p> <pre><code>variable \"spacelift_that_key_id\" {}\nvariable \"spacelift_that_key_secret\" {}\n\nprovider \"spacelift\" {\n  alias = \"this\"\n}\n\nprovider \"spacelift\" {\n  alias = \"that\"\n\n  api_key_endpoint = \"https://that.app.spacelift.io\"\n  api_key_id       = var.spacelift_that_key_id\n  api_key_secret   = var.spacelift_that_key_secret\n}\n</code></pre>"},{"location":"vendors/terraform/terraform-provider.html#proposed-workflow","title":"Proposed workflow","text":"<p>We suggest to first manually create a single administrative stack, and then use it to programmatically define other stacks as necessary. If you're using an integration like AWS, you should probably give the role associated with this stack full IAM access too, allowing it to create separate roles and policies for individual stacks.</p> <p>If you want to share data or outputs between stacks, please consider programmatically creating Stack Dependencies.</p> <p>Info</p> <p>Programmatically generated stacks can still be manually augmented, for example by setting extra elements of the environment. Thanks to the magic of Terraform, these will simply be invisible to (and thus not disturbed by) your resource definitions.</p>"},{"location":"vendors/terraform/terraform-provider.html#generate-terraform-code-from-an-existing-stack","title":"Generate Terraform code from an existing stack","text":"<p>We have a convenience feature for generating Terraform code from an existing stack. Open the stack, then choose the three dots in the top-right corner and click the \"As Terraform code\" button.</p> <p></p> <p>The resulting code will be displayed in a drawer. It includes all stack settings, as well as environment variables and attached contexts. You can copy it to your clipboard and paste it into your Terraform project.</p> <p></p>"},{"location":"vendors/terraform/terraform-provider.html#boundaries-of-programmatic-management","title":"Boundaries of programmatic management","text":"<p>Spacelift administrative tokens are not like user tokens. Specifically, they allow access to a much smaller subset of the API. They allow managing the lifecycles of stacks, contexts, integrations, and configuration, but they won't allow you to create or even access Terraform state, runs or tasks, or their associated logs.</p> <p>Administrative tokens have no superpowers either. They can't read write-only configuration elements any more than you can as a user. Unlike human users with user tokens, administrative tokens won't allow you to run <code>env</code> in a task and read back the logs.</p> <p>In general, we believe that things like runs or tasks do not fit the (relatively static) Terraform resource lifecycle model and that hiding those parts of the API from Terraform helps us ensure the integrity of potentially sensitive data - just see the example above.</p>"},{"location":"vendors/terraform/terragrunt.html","title":"Terragrunt","text":""},{"location":"vendors/terraform/terragrunt.html#terragrunt","title":"Terragrunt","text":"<p>Info</p> <p>We have recently released a new Terragrunt native platform in Spacelift and it is currently in beta. You can find documentation on this here.</p>"},{"location":"vendors/terraform/terragrunt.html#using-terragrunt","title":"Using Terragrunt","text":"<p>Whether a Terraform stack is using Terragrunt or not is controlled by the presence of <code>terragrunt</code> label on the stack:</p> <p></p> <p>If present, all workloads will use <code>terragrunt</code> instead of <code>terraform</code> as the main command. Since the Terragrunt API is a superset of Terraform's, this is completely transparent to the end user.</p> <p>Terragrunt is installed on our standard runner image. If you're not using our runner image, you can install Terragrunt separately.</p> <p>During the Initialization phase we're showing you the exact binary that will process your job, along with its location:</p> <p></p>"},{"location":"vendors/terraform/terragrunt.html#versioning-with-terragrunt","title":"Versioning with Terragrunt","text":"<p>When working with Terragrunt, you will still specify the Terraform version to be used to process your job. We don't do it for Terragrunt, which is way more relaxed in terms of how it interacts with Terraform versions, especially since we're only using a very stable subset of its API.</p> <p>On our runner image, we install a version of Terragrunt that will work with the latest version of Terraform that we support. If you need a specific version of Terragrunt, feel free to create a custom runner image and install the Terragrunt version of your choosing.</p>"},{"location":"vendors/terraform/terragrunt.html#scope-of-support","title":"Scope of support","text":"<p>We're currently using Terragrunt the same way we're using Terraform, running <code>init</code>, <code>plan</code>, and <code>apply</code> commands. This means we're not supporting (<code>run-all</code>). This functionality was designed to operate in a very different mode and environment, and is strictly outside our scope. However, run-all is supported in our new Terragrunt native platform in Spacelift which is currently in beta. You can find documentation on this here.</p> <p>We also support authentication with Spacelift modules by automatically filling the <code>TG_TF_REGISTRY_TOKEN</code> environment variable for each run. Terragrunt uses this variable to authenticate with private module registries.</p>"},{"location":"vendors/terraform/terragrunt.html#debugging-terragrunt","title":"Debugging Terragrunt","text":"<p>Similar to Terraform, Terragrunt provides an advanced logging mode, and as of the writing of this documentation, there are currently two ways it can be enabled:</p> <ol> <li> <p>Using the <code>--terragrunt-log-level debug</code> CLI flag (You'll need to set this flag using the <code>TF_CLI_ARGS</code> environment variable. For example, <code>TF_CLI_ARGS=\"--terragrunt-log-level debug\"</code>)</p> </li> <li> <p>Using the <code>TERRAGRUNT_LOG_LEVEL</code> environment variable. Logging levels supported: <code>info</code> (default), <code>panic</code> <code>fatal</code> <code>error</code> <code>warn</code> <code>debug</code> <code>trace</code></p> </li> </ol> <p>Please refer to the Setting Environment Variables section within our Terraform Debugging Guide for more information on how to set these variables on your Spacelift Stack(s).</p> <p>For more information on logging with Terragrunt, please refer to the Terragrunt documentation.</p>"},{"location":"vendors/terraform/version-management.html","title":"Version management","text":""},{"location":"vendors/terraform/version-management.html#version-management","title":"Version management","text":""},{"location":"vendors/terraform/version-management.html#intro-to-terraform-versioning","title":"Intro to Terraform versioning","text":"<p>Terraform is an actively developed open-source product with a mighty sponsor. This means frequent releases - in fact, over the last few months (since minor version <code>0.12</code>) we've been seeing nearly weekly releases. While that's all great news for us as Terraform users, we need to be aware of how version management works in order not to be caught off-guard.</p> <p>Historically (until 0.15.x), once the state was written to (applied) with a higher version of Terraform, there was no way back. Hence, you had to be very careful when updating your current Terraform versions. If you're still using an older version of Terraform with Spacelift, you will likely want to use the runtime configuration to preview the intended changes before you make the jump.</p> <p>Currently (since version 0.15.x) the state format is stable so this is no longer a problem and you don't need to be that extra careful with Terraform versions.</p>"},{"location":"vendors/terraform/version-management.html#terraform-versions-in-spacelift","title":"Terraform versions in Spacelift","text":"<p>Terraform binaries are neither distributed with Spacelift nor with its runner Docker image. Instead, Spacelift dynamically detects the right version for each workflow (run or task), downloads the right binary on demand, verifies it, and mounts it as read-only on the runner Docker container as <code>/bin/terraform</code> to be directly available in the runner's <code>$PATH</code>:</p> <p></p> <p>There are two ways to tell Spacelift which Terraform version to use. The main one is to set the version directly on the stack. The version can be set in the Vendor section of the stack configuration:</p> <p></p> <p>Note that you can either point to a specific version or define a version range, which is particularly useful if you don't want to update your code every time the Terraform version changes. The exact supported syntax options can be found here.</p> <p>The other way of specifying the Terraform version is to set it through runtime configuration. The runtime configuration is useful if you want to validate your Terraform code with a newer version of the binary before committing to it - which is especially important in older versions where the state format was not yet stable.</p> <p>If you're creating stacks programmatically but intend to make independent changes to the Terraform version (eg. using runtime configuration), we advise you to ignore any subsequent changes.</p> <p>In order to determine the version of the Terraform binary to mount, Spacelift will first look at the runtime configuration. If it does not contain the version setting for the current stack, the stack setting is then considered. If there is no version set on the current stack, the newest supported Terraform version is used. We always advise creating stacks with the newest available version of Terraform, though we realize it may not be the best option if the project is imported from elsewhere or incompatible providers are used.</p> <p>Warning</p> <p>The newest Terraform version supported by Spacelift may lag a bit behind the latest available Terraform release. We err on the side of caution and thus separately verify each version to ensure that it works as expected and that our code is compatible with its output and general behavior. We're trying to catch up roughly within a week but may temporarily blacklist a faulty version. If you need a compatibility check and a bump sooner than that, please get in touch with our support.</p> <p>Once we apply a run with a particular version of Terraform, we set it on the stack to make sure that we don't implicitly attempt to update it using a lower version.</p>"},{"location":"vendors/terraform/version-management.html#migrating-to-newer-versions","title":"Migrating to newer versions","text":"<p>In order to migrate a stack to a newer version of Terraform, we suggest opening a feature branch bumping the version through runtime configuration. Open a Pull Request in GitHub from the feature branch to the tracked branch to easily get a link to your proposed run and see if everything looks good. If it does, merge your Pull Request and enjoy working with the latest and greatest version of Terraform. Otherwise, try making necessary changes until your code is working or postpone the migration until you have the bandwidth to do so.</p> <p>Info</p> <p>In general, we suggest to try and keep up with the latest Terraform releases. The longer you wait, the more serious is the migration work going to be. Terraform evolves, providers evolve, external APIs evolve and so should your code.</p>"},{"location":"vendors/terraform/workflow-tool.html","title":"Workflow Tool","text":""},{"location":"vendors/terraform/workflow-tool.html#workflow-tool","title":"Workflow Tool","text":"<p>The Workflow Tool stack setting allows you to choose between three options:</p> <ul> <li>OpenTofu.</li> <li>Terraform (FOSS).</li> <li>Custom.</li> </ul> <p>The OpenTofu and Terraform (FOSS) options give you out of the box support for using OpenTofu and for open source versions of Terraform respectively. When you use either of those options, all you need to do is choose the version you want to use and you're good to go.</p> <p>The rest of this page explains the Custom option. This option allows you to customize the commands that are executed as part of Spacelift's Terraform workflow. This can be useful if you want to run a custom binary instead of one of the OpenTofu or Terraform versions supported out the box by Spacelift.</p> <p>Info</p> <p>Note that any custom binary is considered third-party software and you need to make sure you have the necessary rights (e.g. license) to use it.</p>"},{"location":"vendors/terraform/workflow-tool.html#how-does-it-work","title":"How does it work?","text":"<p>Each stage of the Terraform workflow uses certain commands to perform tasks such as generating a Terraform plan, or getting the current state. We provide a built-in set of commands to use for Terraform, but you can also specify your own custom commands. You do this via a file called <code>.spacelift/workflow.yml</code>.</p> <p>The following is an example of what the workflow commands look like for Terraform:</p> <pre><code># Used to initialize your root module.\ninit: terraform init -input=false\n\n# Used to select the correct workspace. Only used for Stacks that are using a custom state\n# backend, and which have the Workspace setting configured.\n#\n# Available template parameters:\n# - .WorkspaceName - contains the name of the workspace to select.\nworkspaceSelect: terraform workspace select \"{{ .WorkspaceName }}\"\n\n# Used to create a new workspace if none with the required name exists. Only used for Stacks\n# that are using a custom state backend, and which have the Workspace setting configured.\n#\n# Available template parameters:\n# - .WorkspaceName - contains the name of the workspace to select.\nworkspaceNew: terraform workspace new \"{{ .WorkspaceName }}\"\n\n# Used to generate a plan of the infrastructure changes that will be applied.\n#\n# Available template parameters:\n# - .Lock         - whether the state should be locked.\n# - .Refresh      - whether state resources should be refreshed.\n# - .PlanFileName - the name of the file to store the plan in.\n# - .Targets      - the list of targets to plan. Used during targeted replans.\nplan: terraform plan -input=false -lock={{ .Lock }} {{ if not .Refresh }}-refresh=false {{ end }}-out={{ .PlanFileName }} {{ range .Targets }}-target='{{ . }}' {{ end }}\n\n# Outputs the current state information as JSON.\nshowState: terraform show -json\n\n# Used to convert a plan file to its JSON representation.\n#\n# Available template parameters:\n# - .PlanFileName - the name of the file containing the plan.\nshowPlan: terraform show -json \"{{ .PlanFileName }}\"\n\n# Used to get the current outputs from the state.\ngetOutputs: terraform output -json\n\n# Used to apply any changes contained in the specified plan.\n#\n# Available template parameters:\n# - .PlanFileName - the name of the file containing the plan.\napply: terraform apply -auto-approve -input=false \"{{ .PlanFileName }}\"\n\n# Used to tear down any resources as part of deleting a stack.\ndestroy: terraform destroy -auto-approve -input=false\n</code></pre>"},{"location":"vendors/terraform/workflow-tool.html#how-to-configure-a-custom-tool","title":"How to configure a custom tool","text":"<p>To use a custom tool, three things are required:</p> <ol> <li>A way of providing the tool to your runs.</li> <li>A way of specifying the commands that should be executed.</li> <li>Indicating that you want to use a custom tool on your stack/module.</li> </ol>"},{"location":"vendors/terraform/workflow-tool.html#providing-a-tool","title":"Providing a tool","text":"<p>There are two main ways of providing a custom tool:</p> <ul> <li>Using a custom runner image.</li> <li>Using a before_init hook to download your custom tool.</li> </ul>"},{"location":"vendors/terraform/workflow-tool.html#specifying-the-commands","title":"Specifying the commands","text":"<p>Your custom workflow commands need to be provided in a <code>workflow.yml</code> file stored in the <code>.spacelift</code> folder at the root of your workspace. There are three main ways of providing this:</p> <ol> <li>Via a mounted file in the stack's environment.</li> <li>Via a mounted file stored in a context</li> <li>Directly via your Git repo.</li> </ol> <p>The option you choose will depend on your exact use-case, for example using the stack's environment allows you to quickly test out a new custom tool, using a context allows you to easily share the same configuration across multiple stacks, and storing the configuration in your Git repo allows you to track your settings along with the rest of your code.</p> <p>Here is an example configuration to use a fictional tool called <code>super-iac</code>:</p> <pre><code>init: super-iac init -input=false\nworkspaceSelect: super-iac workspace select \"{{ .WorkspaceName }}\"\nworkspaceNew: super-iac workspace new \"{{ .WorkspaceName }}\"\nplan: super-iac plan -input=false -lock={{ .Lock }} {{ if not .Refresh }}-refresh=false {{ end }}-out={{ .PlanFileName }} {{ range .Targets }}-target='{{ . }}' {{ end }}\nshowState: super-iac show -json\nshowPlan: super-iac show -json \"{{ .PlanFileName }}\"\ngetOutputs: super-iac output -json\napply: super-iac apply -auto-approve -input=false \"{{ .PlanFileName }}\"\ndestroy: super-iac destroy -auto-approve -input=false\n</code></pre>"},{"location":"vendors/terraform/workflow-tool.html#using-a-custom-tool-on-a-stackmodule","title":"Using a custom tool on a Stack/Module","text":"<p>To update your Stack/Module to use a custom workflow tool, edit the Workflow tool setting in the Vendor settings:</p> <p></p> <p>When you choose the Custom option, the version selector will disappear:</p> <p></p> <p>This is because you are responsible for ensuring that the tool is available to the Run, and Spacelift will not automatically download the tool for you.</p>"},{"location":"vendors/terraform/workflow-tool.html#runtime-configuration","title":"Runtime Configuration","text":"<p>You can also use runtime configuration to configure the workflow tool for a stack or module. This allows you to try out a different configuration as part of a PR without adjusting your stack/module settings.</p> <p>The following example shows how to configure a particular stack to use a custom tool:</p> <pre><code>version: \"1\"\n\nstacks:\n  custom-workflow-stack:\n    terraform_workflow_tool: \"CUSTOM\"\n</code></pre> <p>The following example shows how to do the same for a module test case:</p> <pre><code>version: 1\nmodule_version: 0.0.1\n\ntests:\n  - name: Create a pet\n    project_root: examples/create-a-pet\n    terraform_workflow_tool: \"CUSTOM\"\n</code></pre>"},{"location":"vendors/terraform/workflow-tool.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"vendors/terraform/workflow-tool.html#workflow-file-not-found","title":"Workflow file not found","text":"<p>If no workflow.yml file has been created but your Stack has been configured to use a custom tool, you may get an error message like the following:</p> <p></p> <p>If this happens, please ensure you have added a <code>.spacelift/workflow.yml</code> file to your Git repository, or attached it to your Stack's environment via a mounted file.</p>"},{"location":"vendors/terraform/workflow-tool.html#commands-missing","title":"Commands missing","text":"<p>If your <code>.spacelift/workflow.yml</code> does not contain all the required command definitions, or if any commands are empty, you will get an error message like the following:</p> <p></p> <p>This check is designed as a protection mechanism in case new commands are added but your workflow hasn't been updated. In this case, please provide an implementation for the specified commands (in the example the <code>init</code> and <code>workspaceSelect</code> commands),</p>"},{"location":"vendors/terraform/workflow-tool.html#tool-not-found","title":"Tool not found","text":"<p>If your custom tool binary cannot be found you will get an error message like the following:</p> <p></p> <p>In this situation, please ensure that you are providing a custom workflow tool via a custom runner image or workflow hook.</p>"},{"location":"vendors/terragrunt.html","title":"Terragrunt","text":""},{"location":"vendors/terragrunt.html#terragrunt","title":"Terragrunt","text":"<p>Warning</p> <p>Terragrunt support is currently in beta and has some important limitations to take into consideration. Please see our documentation here for more information.</p>"},{"location":"vendors/terragrunt.html#why-use-terragrunt","title":"Why use Terragrunt?","text":"<p>Terragrunt serves as a valuable companion to Terraform, functioning as a thin wrapper that offers a suite of additional tools, ultimately enhancing the management and deployment of your infrastructure configurations.</p>"},{"location":"vendors/terragrunt.html#improved-management-of-configurations","title":"Improved Management of Configurations","text":"<p>Working with complex Terraform configurations often becomes challenging. Terragrunt intervenes here by offering a structured approach to managing these configurations. It also provides you with a mechanism to manage dependencies between various infrastructure components effectively.</p>"},{"location":"vendors/terragrunt.html#efficient-handling-of-remote-state","title":"Efficient Handling of Remote State","text":"<p>In an infrastructure setup, managing remote state and its associated configurations is critical. Terragrunt brings in utilities that can handle remote state locking and configuration adeptly, effectively reducing the chances of race conditions and configuration errors.</p>"},{"location":"vendors/terragrunt.html#enhanced-reusability-and-maintainability","title":"Enhanced Reusability and Maintainability","text":"<p>If you're working with Infrastructure-as-Code (IaC) across multiple environments, Terragrunt's capability to reuse configurations can be a game-changer. This not only simplifies the management process but also makes the infrastructure deployments more maintainable.</p> <p>All these features combine to offer enhanced efficiency and reliability in your infrastructure deployments when using Terragrunt.</p>"},{"location":"vendors/terragrunt.html#why-use-spacelift-with-terragrunt","title":"Why use Spacelift with Terragrunt?","text":"<p>Integrating Spacelift with Terragrunt yields a robust and streamlined solution for Infrastructure-as-Code (IaC) management and deployment. Spacelift's platform is designed for IaC and therefore greatly complements Terragrunt's capabilities.</p> <p>Firstly, Spacelift's automation capabilities can bring significant benefits when paired with Terragrunt. You can automate and streamline the deployment of your complex Terraform configurations managed by Terragrunt, making your infrastructure management efficient and seamless.</p> <p>Spacelift provides a centralized platform for managing your IaC setup. Combined with Terragrunt's ability to effectively handle complex Terraform configurations and dependencies, you end up with a well-organized and optimized IaC environment that is easier to navigate and manage.</p> <p>Lastly, Spacelift is designed to integrate smoothly with your existing tech stack. Its compatibility with Terragrunt means that it can fit into your existing workflow without requiring substantial changes, ensuring a smooth transition and consistent operations.</p>"},{"location":"vendors/terragrunt.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Getting Started</li> <li>Run-all</li> <li>Limitations</li> <li>Terragrunt Tool</li> <li>Reference</li> </ul>"},{"location":"vendors/terragrunt/getting-started.html","title":"Getting Started","text":""},{"location":"vendors/terragrunt/getting-started.html#getting-started","title":"Getting Started","text":"<p>Warning</p> <p>Terragrunt support is currently in beta and has some important limitations to take into consideration. Please see our documentation here for more information.</p> <p>This documentation will be using an example repository Spacelift provides here. This repository contains two different examples, but in this guide we will use the single-stack example.</p>"},{"location":"vendors/terragrunt/getting-started.html#creating-a-new-stack","title":"Creating a new stack","text":"<p>In Spacelift, go ahead and click the Create stack button to create a new Stack.</p>"},{"location":"vendors/terragrunt/getting-started.html#naming-the-stack","title":"Naming the stack","text":"<p>Once in the stack creation wizard, Give your stack a name such as terragrunt-example then press Continue to configure the source code settings for the stack.</p> <p></p>"},{"location":"vendors/terragrunt/getting-started.html#linking-to-the-terragrunt-code","title":"Linking to the Terragrunt code","text":"<p>On the Connect to source code screen, Select Raw Git as your Provider, and provide the following URL: https://github.com/spacelift-io-examples/terragrunt-examples.git and a Project Root of single-project.</p> <p></p> <p>Pressing continue on this page will take you through to the vendor configuration page.</p> <p>This page has quite a few options but the ones that we will be using for this example are as follows:</p> <p></p> <p>Press Create &amp; continue to finish the backend configuration and move through to defining stack behavior. For this example nothing else needs to be configured, so you can click on Skip to summary and then click on Confirm to finish creating your stack.</p>"},{"location":"vendors/terragrunt/getting-started.html#deploying-the-stack","title":"Deploying the stack","text":"<p>Spacelift will take you to the Runs view for the Stack you've just created. Once on this page, press the Trigger button to trigger a new Run.</p> <p></p> <p>You should now be taken through the process of deploying your stack. This takes you through multiple stages of a run, moving through initialization, planning and ending up in an unconfirmed state.</p>"},{"location":"vendors/terragrunt/getting-started.html#examining-the-planned-changes","title":"Examining the planned changes","text":"<p>In this unconfirmed state we have time to review what is going to change if we were to continue and confirm and apply.  By pressing the <code>changes +2</code> button at the top of the page, we are taken to an overview of the planned changes.</p> <p></p> <p>For this stack we should see that 2 resources are being created and we can see what values these resources and outputs are expected to have. This view is very useful to see at a glance what is going to happen as a result of your deployment.</p>"},{"location":"vendors/terragrunt/getting-started.html#confirming-and-applying-the-changes","title":"Confirming and applying the changes","text":"<p>Info</p> <p>Due to security reasons and limitations of Terragrunt and its mocked outputs, Spacelift does not use the planfile in the applying as there is a possibility to deploy mock values.</p> <p>Once you're happy with the changes, press the Confirm button to allow the run to continue and begin the applying phase of the run.</p>"},{"location":"vendors/terragrunt/getting-started.html#conclusion","title":"Conclusion","text":"<p>You have now successfully deployed a Terragrunt stack using Spacelift! Congratulations.</p> <p>For further reading, we recommend looking into using the <code>Run all</code> setting to enable you to deploy multiple projects using <code>terragrunt run-all</code> by reading our documentation here. We also recommend that if you are looking into using Terragrunt with Spacelift that you read our page on the Limitations of Terragrunt.</p>"},{"location":"vendors/terragrunt/limitations.html","title":"Limitations","text":""},{"location":"vendors/terragrunt/limitations.html#limitations","title":"Limitations","text":""},{"location":"vendors/terragrunt/limitations.html#state-management","title":"State management","text":"<p>Terragrunt is a great tool for organizing your state management configuration and allows you to easily define how you manage your state across multiple projects. However, It is not currently possible when using Terragrunt's run-all functionality to relate state files to projects in a consistent manner. For this reason Spacelift does not support storing state for Terragrunt based stacks, and you will need to maintain your own remote state backend configuration.</p>"},{"location":"vendors/terragrunt/limitations.html#terragrunt-mocked-outputs","title":"Terragrunt mocked outputs","text":"<p>Mocked outputs in Terragrunt are placeholder values used during the development or planning phases of Terragrunt deployments.</p> <p>For example, you may have a module that provides the output of a connection string to a database, but during the planning phase that database does not yet exist. In this case you would use a mocked output in Terragrunt to ensure that any dependencies that rely on this output in their planning phase have access to at least some data.</p> <p>This mocked data is only used if the output does not already exist in the state. Therefore in situations such as the initial run of your stack, or the introduction of new outputs with mocked values, these mocked values will be used.</p>"},{"location":"vendors/terragrunt/limitations.html#mocked-outputs-and-plan-policies","title":"Mocked outputs and Plan policies","text":"<p>Due to the nature of the mocked outputs and the way that Spacelift uses the plan data to provide the input to plan policies, it is possible that these mocked output values could be used as input values for your plan policies and you should take precaution when writing policies that check against values that could be mocked.</p>"},{"location":"vendors/terragrunt/limitations.html#mocked-outputs-and-the-apply-phase","title":"Mocked outputs and the Apply phase","text":"<p>Terragrunt consumes the mocked outputs and places those values within the plan file that is stored on disk as part of the planning phase. Because the plan file has the possibility of containing mocked outputs Spacelift does not use the plan files in the apply phase. This does mean there is a possibility of changes happening between the planning and applying phase, but Spacelift has taken the stance that it is more important from a security standpoint to not allow any mocked outputs to be deployed here. Nobody wants to deploy something with a mocked, hardcoded password!</p>"},{"location":"vendors/terragrunt/limitations.html#usage-of-the-run_cmd-function","title":"Usage of the run_cmd function","text":"<p>The run_cmd function is currently limited to only work with the <code>--terragrunt-quiet</code> flag. Ensure this flag is included in your command to avoid run failures.</p>"},{"location":"vendors/terragrunt/limitations.html#resource-deletion","title":"Resource Deletion","text":"<p>Our Terragrunt support currently does not support resources being deleted during stack deletion, this includes through the Spacelift stack destructor or the \"Delete Stack\" option in the UI.</p>"},{"location":"vendors/terragrunt/reference.html","title":"Reference","text":""},{"location":"vendors/terragrunt/reference.html#reference","title":"Reference","text":""},{"location":"vendors/terragrunt/reference.html#stack-settings","title":"Stack Settings","text":"<ul> <li>Use Run All - If Spacelift should use Terragrunt's run-all functionality in this stack.</li> <li>Smart Sanitization - If Spacelift should use Smart Sanitization for the resources and outputs generated by this stack.</li> <li>Terragrunt Version - Which version or range of versions of Terragrunt should be used</li> <li>Tool - Whether to use OpenTofu, an open source version of Terraform, or a manually provisioned tool.</li> <li>Tool Version - Which version or range of versions of OpenTofu or Terraform should be used</li> </ul> <p>Info</p> <p>Spacelift makes use of the Version Compatibility Table to determine which version combinations of OpenTofu/Terraform and Terragrunt are compatible with each other.</p>"},{"location":"vendors/terragrunt/reference.html#environment-variables","title":"Environment Variables","text":"<p>In Terragrunt stacks you can use environment variables to add options to the command line arguments sent to Terragrunt. To use these environment variables, you can see the documentation here.</p> <p>There is an environment variable for each phase of the run:</p> <ul> <li><code>TG_CLI_ARGS_init</code> - Allows you to add options sent to the Terragrunt application during the initialization phase</li> <li><code>TG_CLI_ARGS_plan</code> - Allows you to add options sent to the Terragrunt application during the planning phase</li> <li><code>TG_CLI_ARGS_apply</code> - Allows you to add options sent to the Terragrunt application during the applying phase</li> </ul>"},{"location":"vendors/terragrunt/reference.html#terraform-provider","title":"Terraform provider","text":"<p>If you create a Terragrunt stack via the Terraform provider, and don't populate the <code>terraform_version</code> or <code>terragrunt_version</code> properties, we default to the latest version available at creation time. If you create a stack with these values populated, and then later remove the fields, the stack continues to use the version set during stack creation, and doesn't update to the latest available version. If you change the <code>tool</code> and <code>terraform_version</code> isn't populated, the <code>terraform_version</code> will be reset in state file.</p>"},{"location":"vendors/terragrunt/reference.html#module-registry","title":"Module Registry","text":"<p>To enable seamless authentication with Spacelift's module registry, we automatically configure the <code>TG_TF_REGISTRY_TOKEN</code> environment variable for stacks utilizing our native Terragrunt vendor.</p> <p>Below is an example of how you can format your Terraform block:</p> <pre><code>terraform {\n  source = \"tfr://spacelift.io/organization/module_name/provider?version=0.0.1\"\n}\n</code></pre>"},{"location":"vendors/terragrunt/run-all.html","title":"Using Run-All","text":""},{"location":"vendors/terragrunt/run-all.html#using-run-all","title":"Using Run-All","text":""},{"location":"vendors/terragrunt/run-all.html#introduction","title":"Introduction","text":"<p>Terragrunt's run-all feature allows you to deploy multiple modules at the same time. This very powerful feature allows you to develop large amounts of infrastructure as code with dependencies, mocked values, and while keeping things DRY.</p>"},{"location":"vendors/terragrunt/run-all.html#how-to-use-run-all","title":"How to use run-all","text":"<p>When creating a stack in Spacelift, you can enable the Use Run All property to start using this functionality natively in your own stacks:</p> <p></p> <p>When this option is enabled, Spacelift will run all Terragrunt commands with the <code>run-all</code> option, taking into account any dependencies between your projects!</p> <p>Tip</p> <p>When using Terragrunt run-all, we recommend setting the TERRAGRUNT_PARALLELISM variable to optimize CPU usage by adjusting the number of tasks processed concurrently, aligning with the capacity of our public workers and preventing worker crashes from overloaded resources.</p> <p>If additional parallelism is needed to meet your specific requirements, consider using our private workers.</p>"},{"location":"vendors/terragrunt/run-all.html#why-are-my-resources-named-strangely","title":"Why are my resources named strangely?","text":"<p> Due to the way Terragrunt works, you may find that modules and projects create outputs or resources with the same names, and we want to make sure that you can determine the origin of all of your resources without getting confused about \"Which project created <code>output.password</code>\"?</p> <p>For this reason, we prepend the addresses of your outputs and resources in the Spacelift user interface with the project they originated from. This allows you to keep on top of your resources, understand at a glance how things were created, and identify issues faster, Giving you more control and visibility over the resources you create!</p>"},{"location":"vendors/terragrunt/terragrunt-tool.html","title":"Terragrunt Tool","text":""},{"location":"vendors/terragrunt/terragrunt-tool.html#terragrunt-tool","title":"Terragrunt Tool","text":"<p>Spacelift provides the ability to choose the tool that should be used by Terragrunt to provision your infrastructure. Three options are supported:</p> <ul> <li>OpenTofu</li> <li>Terraform</li> <li>Manually Provisioned</li> </ul> <p>You can choose which tool you want to use when creating a stack, or via the Backend configuration for an existing stack:</p> <p></p> <p>If you choose either OpenTofu or Terraform, you can then also choose the version that you want to use:</p> <p></p> <p>When you do this, Spacelift will automatically download the specified version of your tool when preparing your runs, and will set the <code>TERRAGRUNT_TFPATH</code> to point at the correct tool.</p>"},{"location":"vendors/terragrunt/terragrunt-tool.html#manually-provisioning","title":"Manually Provisioning","text":"<p>If you want to use a different tool other than OpenTofu, or one of the versions of Terraform supported by Spacelift, you can choose the Manually provisioned option. When you choose this option, you are responsible for two things:</p> <ol> <li>Making sure that the tool is available for Terragrunt to use. You can use either a before init hook for this, or you can create a custom runner image for your stack that has your tool available.</li> <li>Specifying the path to your tool. You can do this via the terraform_binary Terragrunt option, for example by setting the <code>TERRAGRUNT_TFPATH</code> environment variable.</li> </ol>"},{"location":"vendors/terragrunt/terragrunt-tool.html#overriding-the-terragrunt_tfpath-variable","title":"Overriding the <code>TERRAGRUNT_TFPATH</code> variable","text":"<p>If you are already setting the <code>TERRAGRUNT_TFPATH</code> variable on your stack, your existing value will have precedence over the automatically configured value and will continue to be used. This means that if you had customized the path to the Terragrunt tool before the functionality was added natively to Spacelift, your existing stacks will continue to work.</p> <p>If you are currently using this approach to use OpenTofu with Terragrunt, and want to switch to the native approach, please make sure to remove your environment variable override so that it doesn't conflict with the built-in Spacelift functionality.</p>"}]}