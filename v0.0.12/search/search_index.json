{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"\ud83d\udc4b Hello, Spacelift!","text":"<p>Spacelift is a sophisticated, continuous integration and deployment (CI/CD) platform for infrastructure-as-code,  supporting Terraform, OpenTofu, Terragrunt, Pulumi, AWS CloudFormation, AWS CDK, Kubernetes, Ansible, and more. It's designed and implemented by long-time DevOps practitioners based on previous experience with large-scale installations - dozens of teams, hundreds of engineers, and tens of thousands of cloud resources.</p> <p>At the same time, Spacelift is super easy to get started with - you can go from zero to fully managing your cloud resources within less than a minute, with no pre-requisites. It integrates nicely with the large players in the field - notably GitHub and AWS.</p> <p>If you're new to Spacelift, please spend some time browsing through the articles in the same order as they appear in the menu - start with the main concepts and follow with integrations. If you're more advanced, you can navigate directly to the article you need, or use the search feature to find a specific piece of information. If you still have questions, feel free to reach out to us.</p>"},{"location":"index.html#do-i-need-another-cicd-for-my-infrastructure","title":"Do I need another CI/CD for my infrastructure?","text":"<p>Yes, we believe it's a good idea. While in an ideal world one CI system would be enough to cover all use cases, we don't live in an ideal world. Regular CI tools can get you started easily, but Terraform has a rather unusual execution model and a highly stateful nature. Also, mind the massive blast radius when things go wrong. We believe Spacelift offers a perfect blend of regular CI's versatility and methodological rigor of a specialized, security-conscious infrastructure tool - enough to give it a shot even if you're currently happy with your infra-as-code CI/CD setup.</p> <p>In the following sections, we'll try to present the main challenges of running Terraform in a general purpose CI system, as well as show how Spacelift addresses those. At the end of the day, it's mostly about two things - collaboration and security.</p>"},{"location":"index.html#collaboration","title":"Collaboration","text":"<p>Wait, aren't CIs built for collaboration?</p> <p>Yes, assuming stateless tools and processes. Running stateless builds and tests is what regular CIs are exceptionally good at. But many of us have noticed that deployments are actually trickier to get right. And that's hardly a surprise. They're more stateful, they may depend on what's already running. Terraform and your infrastructure, in general, is an extreme example of a stateful system. It's so stateful that it actually has something called state (see what we just did there?) as one of its core concepts.</p> <p>CIs generally struggle with that. They don't really understand the workflows they run, so they can't for example serialize certain types of jobs. Like <code>terraform apply</code>, which introduces actual changes to your infrastructure. As far as your CI system is concerned, running those in parallel is fair game. But what it does to Terraform is nothing short of a disaster - your state is confused and no longer represents any kind of reality. Untangling this mess can take forever.</p> <p>But you can add manual approval steps</p> <p>Yes, you can. But the whole point of your CI/CD system is to automate your work. First of all, becoming a human semaphore for a software tool isn't the best use of a highly skilled and motivated professional. Also, over-reliance on humans to oversee software processes will inevitably lead to costly mistakes because we, humans, are infinitely more fallible than well-programmed machines. It's ultimately much cheaper to use the right tool for the job than turn yourself into a part of a tool.</p> <p>But you can do state locking!</p> <p>Yup, we hear you. In theory, it's a great feature. In practice, it has its limitations. First, it's a massive pain when working as a team. Your CI won't serialize jobs that can write state, and state locking means that all but one of the parallel jobs will simply fail. It's a safe default, that's for sure, but not a great developer experience. And the more people work on your infrastructure, the more frustrating the process will become.</p> <p>And that's just applying changes. By default, running <code>terraform plan</code> locks the state, too. So you can't really run multiple CI jobs in parallel, even if they're only meant to preview changes, because each of them will attempt to lock the state. Yes, you can work around this by explicitly not locking state in CI jobs that you know won't make any state changes, but at this point, you've already put so much work into creating a pipeline that's fragile at best and requires you to manually synchronize it.</p> <p>And we haven't even discussed security yet.</p>"},{"location":"index.html#security","title":"Security","text":"<p>Terraform is used to manage infrastructure, which normally requires credentials. Usually, very powerful credentials. Administrative credentials, sometimes. And these can do a lot of damage. The thing with CIs is that you need to provide those credentials statically, and once you do, there's no way you can control how they're used.</p> <p>And that's what makes CIs powerful - after all, they let you run arbitrary code, normally based on some configuration file that you have checked in with your Terraform code. So, what's exactly stopping a prankster from adding <code>terraform destroy -auto-approve</code> as an extra CI step? Or printing out those credentials and using them to mine their crypto of choice?</p> <p>There are better ways to get fired.</p> <p>...you'll say and we hear you. Those jobs are audited after all. No, if we were disgruntled employees we'd never do something as stupid. We'd get an SSH session and leak those precious credentials this way. Since it's unlikely you rotate them every day, we'd take our sweet time before using them for our nefarious purposes. Which wouldn't be possible with Spacelift BTW, which generates one-off temporary credentials for major cloud providers.</p> <p>But nobody does that!</p> <p>Yes, you don't hear many of those stories. Most mistakes happen to well-meaning people. But in the world of infrastructure, even the tiniest of mistakes can cause major outages - like that typo we once made in our DNS config. That's why Spacelift adds an extra layer of policy that allows you to control - separately from your infrastructure project! - what code can be executed, what changes can be made, when and by whom. This isn't only useful to protect yourself from the baddies, but allows you to implement an automated code review pipeline.</p>"},{"location":"getting-started.html","title":"\ud83d\ude80 Getting Started","text":"<p>Hello and welcome to Spacelift! In this guide we will briefly introduce some key concepts that you need to know to work with Spacelift. These concepts will be followed by detailed instructions to help you create and configure your first run with Spacelift.</p>"},{"location":"getting-started.html#introduction-to-main-concepts","title":"Introduction to Main Concepts","text":""},{"location":"getting-started.html#stacks","title":"Stacks","text":"<p>A stack is a central entity in Spacelift. It connects with your source control repository and manages the state of infrastructure. It facilitates integration with cloud providers (AWS, Azure, Google Cloud) and other important Spacelift components. You can learn more about Stacks in Spacelift detailed documentation.</p>"},{"location":"getting-started.html#state-management","title":"State Management","text":"<p>State can be managed by your backend, or for Terraform projects - can be imported into Spacelift. It is not required to let Spacelift manage your infrastructure state. You can learn more about State Management here.</p>"},{"location":"getting-started.html#worker-pools","title":"Worker Pools","text":"<p>The underlying compute used by Spacelift is called a worker, and workers are managed in groups known as worker pools. In order for Spacelift to operate correctly, you will need to provision at least one worker pool. You can learn more about worker pools here.</p>"},{"location":"getting-started.html#policies","title":"Policies","text":"<p>Spacelift policies provide a way to express rules as code, rules that manage your Infrastructure as Code (IaC) environment, and help make common decisions such as login, access, and execution. Policies are based on the Open Policy Agent project and can be defined using its rule language Rego. You can learn more about policies here.</p>"},{"location":"getting-started.html#cloud-integration","title":"Cloud Integration","text":"<p>Spacelift provides native integration with AWS. Integration with other cloud providers is also possible via OIDC Federation or programmatic connection with their identity services. You can learn more about about cloud provider integration here.</p>"},{"location":"getting-started.html#change-workflow","title":"Change Workflow","text":"<p>Spacelift deeply integrates with your Version Control System (VCS). Pull requests are evaluated by Spacelift to provide a preview of the changes being made to infrastructure; these changes are deployed automatically when PRs are merged. You can learn more about VCS integration here.</p>"},{"location":"getting-started.html#step-by-step","title":"Step by Step","text":"<p>This section provides step-by-step instructions to help you set up and get the most out of Spacelift. If you want to learn about core concepts, please have a look at the main concepts section.</p>"},{"location":"getting-started.html#first-stack-run","title":"First Stack Run","text":"<p>You can get started with either forking our Terraform Starter repository and testing all Spacelift capabilities in under 15 minutes or you can explore Spacelift on your own by adding your own repository and going from zero to fully managing your cloud resources.</p>"},{"location":"getting-started.html#step-1-install-spacelift","title":"Step 1: Install Spacelift","text":"<p>Follow the install guide to get Spacelift up and running.</p>"},{"location":"getting-started.html#step-2-connect-your-version-control-system-vcs","title":"Step 2: Connect your Version Control System (VCS)","text":"<p>In this section we will be connecting GitHub as our VCS. You can find more information about other supported VCS providers here</p> <ol> <li>To connect GitHub as your VCS, follow the guide for setting up the GitHub integration.</li> <li>Please select any of your GitHub repositories that create local resources (we will not be integrating with any cloud providers to keep this guide simple and quick). If you do not have a GitHub repository of this kind, you can fork our terraform-starter repository (Make sure to allow the installed GitHub app access to the forked repository).</li> </ol> <p>Please refer to the Source Control section of the documentation to connect a different VCS.</p>"},{"location":"getting-started.html#step-3-create-your-first-spacelift-stack","title":"Step 3: Create Your First Spacelift Stack","text":"<p>In this section we will be cover creating your first Spacelift Stack. For more detailed instructions on creating your first Spacelift Stack, click here.</p> <p>Click on the Create Stack button.</p> <p></p> <p>Give your stack a name and click Continue.</p> <p></p> <p>In the Integrate VCS tab, choose your VCS provider, select the repository that you gave access to Spacelift in the first step and select a branch that you want to be attached with your Stack. You also have the optional choice of selecting a project root.</p> <p></p> <p>Click Continue to configure the backend.</p> <p>Choose Terraform as your backend with a supported version. Leave the default option to let Spacelift manage state for this stack. Please refer to the creating a stack section of the documentation for information on using a different backend.</p> <p></p> <p>Leave the default options checked for Define Behavior and click Continue.</p> <p></p>"},{"location":"getting-started.html#step-4-trigger-your-first-run","title":"Step 4: Trigger your First Run","text":"<p>Click on Trigger to kick start a Spacelift run that will check out the source code, run terraform commands on it and then present you with an option to apply (confirm) these changes.</p> <p></p> <p>After clicking Trigger, you will be taken directly into the run. Click on Confirm and your changes will be applied. Your output will look different based on your code repository and the resources it creates.</p> <p></p> <p>Congratulation! \ud83d\ude80 You've just created your first Spacelift stack and completed your first deployment!</p> <p>Now it is time to add other users to your Spacelift account.</p>"},{"location":"getting-started.html#adding-users","title":"Adding Users","text":"<p>Now comes the moment when you want to show Spacelift to your colleagues. There are a few different ways to grant users access to your Spacelift account but in the beginning, we are going to add them as single users.</p> <p>Go to the \"Policies\" page that can be found on the left sidebar and click the \"Create policy\" button in the top-right corner.</p> <p></p> <p>Name the policy and select \"Login policy\" as the type.</p> <p>Then copy/paste and edit the example below that matches the identity provider you used to sign up for the Spacelift account.</p> GitHubGitLab, Google, Microsoft <p>This example uses GitHub usernames to grant access to Spacelift.</p> <pre><code>package spacelift\n\nadmins  := { \"alice\" }\nallowed := { \"bob\", \"charlie\", \"danny\" }\nlogin   := input.session.login\n\nadmin { admins[login] }\nallow { allowed[login] }\ndeny  { not admin; not allow }\n</code></pre> <p>Tip</p> <p>GitHub organization admins are automatically Spacelift admins. There is no need to grant them permissions in the Login policy.</p> <p>This example uses email addresses to grant access to Spacelift.</p> <pre><code>package spacelift\n\nadmins  := { \"alice@example.com\" }\nallowed := { \"bob@example.com\" }\nlogin   := input.session.login\n\nadmin { admins[login] }\nallow { allowed[login] }\n# allow { endswith(input.session.login, \"@example.com\") } Alternatively, grant access to every user with an @example.com email address\ndeny  { not admin; not allow }\n</code></pre> <p>Now your colleagues should be able to access your Spacelift account as well!</p> <p>Note</p> <p>While the approach above is fine to get started and try Spacelift, granting access to individuals is less safe than granting access to teams and restricting access to account members. In the latter case, when they lose access to your organization, they automatically lose access to Spacelift while when whitelisting individuals and not restricting access to members only, you'll need to remember to explicitly remove them from your Spacelift Login policy, too.</p> <p>Before you go live in production with Spacelift, we recommend that you switch to using teams in Login policies and consider configuring the Single Sign-On (SSO) integration, if applicable.</p>"},{"location":"getting-started.html#additional-reading","title":"Additional Reading","text":"<ol> <li>Learn how to integrate with AWS as a cloud provider for your infrastructure</li> <li>Try creating and attaching policies with stacks for common use cases</li> <li>Spacelift workflow with GitHub change requests (PR)</li> <li>Setting up Spacelift workers</li> <li>Using environment variables and contexts with stacks</li> <li>Configuring stack behavior with common settings</li> </ol>"},{"location":"concepts/resources.html","title":"Resources","text":"<p>One major benefit of specialized tools like Spacelift - as opposed to general-purpose CI/CD platforms - is that they intimately understand the material they're working with. With regards to infra-as-code, the most important part of this story is understanding your managed resources in-depth. Both from the current perspective, but also being able to put each resource in its historical context.</p> <p>The Resources view is the result of multiple months of meticulous work understanding and documenting the lifecycle of each resource managed by Spacelift, regardless of the technology used - Terraform, Terragrunt, Pulumi or AWS CloudFormation.</p>"},{"location":"concepts/resources.html#stack-level-resources","title":"Stack-level resources","text":"<p>This screen shows you the stack-level resources view. By default, resources are shown grouped in a hierarchical manner, grouped by parent. This allows you to see the structure of each of your infrastructure projects:</p> <p></p> <p>Depending on the architecture as well as technology used, your tree may look slightly different - for example, Pulumi trees are generally deeper:</p> <p></p> <p>Apart from being grouped by parent, resources can be grouped provider and type. Let's group one of our small stacks by provider:</p> <p></p> <p>We can see lots of AWS resources, a few from Stripe, and a lonely one from Datadog. Let's now filter just the Datadog ones, and group them by type:</p> <p></p> <p>Let's now take a look at one of the resources, for example a single <code>stripe_price</code>:</p> <p></p> <p>The panel that is now showing on the right hand side of the Resources view shows the details of a single resource, which is now highlighted using a light blue overlay. On this panel, we see two noteworthy things.</p> <p>Starting with the lower right hand corner, we have the vendor-specific representation of the resource. Note how for security purposes all string values are sanitized. In fact, we never get to see them directly - we only see first 7 characters of their checksum. If you know the possible value, you can easily do the comparison. If you don't, then the secret is safe. As a side note, we do not need to sanitize anything with Pulumi because the team there did an exceptional job with regards to secret management:</p> <p></p> <p>More importantly, though, you can drill down to see the runs that either created, or last updated each of the managed resources. Let's now go back to our stripe_price, and click on the ID of the run shown in the Last updated by section. You will notice a little menu pop up:</p> <p></p> <p>Clicking on this menu item will take you to the run in question:</p> <p></p> <p>One extra click on the commit SHA will take you to the GitHub commit. Depending on your Git flow, the commit may be linked to a Pull Request, giving you the ultimate visibility into the infrastructure change management process:</p> <p></p>"},{"location":"concepts/resources.html#navigating-the-resource-tree","title":"Navigating the resource tree","text":"<p>When grouping by resources parent - that is, seeing them as a tree, you can easily click into each subtree, like if you were changing your working directory. You can do this by clicking on the dot representing each node in the tree:</p> <p></p> <p>Once you click into the subtree, you will be able to click out by clicking on the new virtual root, as if you were running <code>cd ..</code> in your terminal:</p> <p></p> <p>One important aspect about navigating the resource tree is that once you click into the subtree, you are effectively filtering by the ancestor. Which means that grouping and filtering options will now operate on a subset of resources within that subtree:</p> <p></p>"},{"location":"concepts/resources.html#account-level-resources","title":"Account-level resources","text":"<p>Warning</p> <p>Account-level resource view is still a work in progress, so some of the UX may change frequently.</p> <p>A view similar to stack-level resources is available for the entire account, too. For this presentation we will use a very symmetrical account - our automated testing instance repeatedly creating and updating the same types of resources in different ways to quickly detect potential regressions:</p> <p></p> <p>By default we group by parent, giving you the same hierarchical view as for stack-level resources. But you will notice that there's a new common account root serving as a \"virtual\" parent for stacks.</p> <p>In this view you can also filter and group by different properties, with one extra property being the Stack:</p> <p></p>"},{"location":"concepts/spacectl.html","title":"<code>spacectl</code>, the Spacelift CLI","text":"<p><code>spacectl</code> is a utility wrapping Spacelift's GraphQL API for easy programmatic access in command-line contexts - either in manual interactive mode (in your local shell), or in a predefined CI pipeline (GitHub actions, CircleCI, Jenkins etc).</p> <p>Its primary purpose is to help you explore and execute actions inside Spacelift. It provides limited functionality for creating or editing resources. To do that programatically, you can use the Spacelift Terraform Provider.</p>"},{"location":"concepts/spacectl.html#installation","title":"Installation","text":""},{"location":"concepts/spacectl.html#officially-supported-packages","title":"Officially supported packages","text":"<p>Officially supported packages are maintained by Spacelift and are the preferred ways to install <code>spacectl</code></p>"},{"location":"concepts/spacectl.html#homebrew","title":"Homebrew","text":"<p>You can install <code>spacectl</code> using Homebrew on MacOS or Linux:</p> <pre><code>brew install spacelift-io/spacelift/spacectl\n</code></pre>"},{"location":"concepts/spacectl.html#windows","title":"Windows","text":"<p>You can install <code>spacectl</code> using winget:</p> <pre><code>winget install spacectl\n</code></pre> <p>or</p> <pre><code>winget install --id spacelift-io.spacectl\n</code></pre>"},{"location":"concepts/spacectl.html#docker-image","title":"Docker image","text":"<p><code>spacectl</code> is distributed as a Docker image, which can be used as follows:</p> <pre><code>docker run -it --rm ghcr.io/spacelift-io/spacectl stack deploy --id my-infra-stack\n</code></pre> <p>Don't forget to add the required environment variables in order to authenticate.</p>"},{"location":"concepts/spacectl.html#asdf","title":"asdf","text":"<pre><code>asdf plugin add spacectl\nasdf install spacectl latest\nasdf global spacectl latest\n</code></pre>"},{"location":"concepts/spacectl.html#github-release","title":"GitHub Release","text":"<p>Alternatively, <code>spacectl</code> is distributed through GitHub Releases as a zip file containing a self-contained statically linked executable built from the source in this repository. Binaries can be download directly from the Releases page.</p>"},{"location":"concepts/spacectl.html#usage-on-github-actions","title":"Usage on GitHub Actions","text":"<p>We have setup-spacectl GitHub Action that can be used to install <code>spacectl</code>:</p> <pre><code>steps:\n- name: Install spacectl\nuses: spacelift-io/setup-spacectl@main\nenv:\nGITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n- name: Deploy infrastructure\nenv:\nSPACELIFT_API_KEY_ENDPOINT: https://mycorp.app.spacelift.io\nSPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\nSPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\nrun: spacectl stack deploy --id my-infra-stack\n</code></pre>"},{"location":"concepts/spacectl.html#community-supported-packages","title":"Community supported packages","text":"<p>Disclaimer: These packages are community-maintained, please verify the package integrity yourself before using them to install or update <code>spacectl</code>.</p>"},{"location":"concepts/spacectl.html#arch-linux","title":"Arch linux","text":"<p>Install <code>spacectl-bin</code>: from the Arch User Repository (AUR):</p> <pre><code>yay -S spacectl-bin\n</code></pre> <p>Please make sure to verify the <code>PKGBUILD</code> before installing/updating.</p>"},{"location":"concepts/spacectl.html#alpine-linux","title":"Alpine linux","text":"<p>Install <code>spacectl</code> from the Alpine Repository (alpine packages):</p> <pre><code>apk add spacectl --repository=https://dl-cdn.alpinelinux.org/alpine/edge/testing\n</code></pre> <p>Please make sure to verify the <code>APKBUILD</code> before installing/updating.</p>"},{"location":"concepts/spacectl.html#quick-start","title":"Quick Start","text":"<p>Authenticate using <code>spacectl profile login</code>:</p> <pre><code>&gt; spacectl profile login my-account\nEnter Spacelift endpoint (eg. https://unicorn.app.spacelift.io/): http://my-account.app.spacelift.tf\nSelect authentication flow:\n  1) for API key,\n  2) for GitHub access token,\n  3) for login with a web browser\nOption: 3\n</code></pre> <p>Use spacectl \ud83d\ude80:</p> <pre><code>&gt; spacectl stack list\nName                          | Commit   | Author        | State     | Worker Pool | Locked By\nstack-1                       | 1aa0ef62 | Adam Connelly | NONE      |             |\nstack-2                       | 1aa0ef62 | Adam Connelly | DISCARDED |             |\n</code></pre>"},{"location":"concepts/spacectl.html#getting-help","title":"Getting Help","text":"<p>To list all the commands available, use <code>spacectl help</code>:</p> <pre><code>&gt; spacectl help\nNAME:\n   spacectl - Programmatic access to Spacelift GraphQL API.\n\nUSAGE:\n   spacectl [global options] command [command options] [arguments...]\n\nVERSION:\n   0.26.0\n\nCOMMANDS:\n   module                   Manage a Spacelift module\n   profile                  Manage Spacelift profiles\n   provider                 Manage a Terraform provider\n   run-external-dependency  Manage Spacelift Run external dependencies\n   stack                    Manage a Spacelift stack\n   whoami                   Print out logged-in users information\n   version                  Print out CLI version\n   workerpool               Manages workerpools and their workers\n   help, h                  Shows a list of commands or help for one command\n\nGLOBAL OPTIONS:\n   --help, -h     show help\n--version, -v  print the version\n</code></pre> <p>To get help about a particular command or subcommand, use the <code>-h</code> flag:</p> <pre><code>&gt; spacectl profile -h\nNAME:\n   spacectl profile - Manage Spacelift profiles\n\nUSAGE:\n   spacectl profile command [command options] [arguments...]\n\nCOMMANDS:\n     current       Outputs your currently selected profile\n     export-token  Prints the current token to stdout. In order not to leak, we suggest piping it to your OS pastebin\n     list          List all your Spacelift account profiles\n     login         Create a profile for a Spacelift account\n     logout        Remove Spacelift credentials for an existing profile\n     select        Select one of your Spacelift account profiles\n     help, h       Shows a list of commands or help for one command\n\nOPTIONS:\n   --help, -h  show help (default: false)\n</code></pre>"},{"location":"concepts/spacectl.html#example","title":"Example","text":"<p>The following screencast shows an example of using spacectl to run a one-off task in Spacelift:</p> <p></p>"},{"location":"concepts/spacectl.html#authentication","title":"Authentication","text":"<p><code>spacectl</code> is designed to work in two different contexts - a non-interactive scripting mode (eg. external CI/CD pipeline) and a local interactive mode, where you type commands into your shell. Because of this, it supports two types of credentials - environment variables and user profiles.</p> <p>We refer to each method of providing credentials as \"credential providers\" (like AWS), and details of each method are documented in the following sections.</p>"},{"location":"concepts/spacectl.html#authenticating-using-environment-variables","title":"Authenticating using environment variables","text":"<p>The CLI supports the following authentication methods via the environment:</p> <ul> <li>Spacelift API tokens.</li> <li>GitHub tokens.</li> <li>Spacelift API keys.</li> </ul> <p><code>spacectl</code> looks for authentication configurations in the order specified above, and will stop as soon as it finds a valid configuration. For example, if a Spacelift API token is specified, GitHub tokens and Spacelift API keys will be ignored, even if their environment variables are specified.</p>"},{"location":"concepts/spacectl.html#spacelift-api-tokens","title":"Spacelift API tokens","text":"<p>Spacelift API tokens can be specified using the <code>SPACELIFT_API_TOKEN</code> environment variable. When this variable is found, the CLI ignores all the other authentication environment variables because the token contains all the information needed to authenticate.</p> <p>NOTE: API tokens are generally short-lived and will need to be re-created often.</p>"},{"location":"concepts/spacectl.html#github-tokens","title":"GitHub tokens","text":"<p>GitHub tokens are only available to accounts that use GitHub as their identity provider, but are very convenient for use in GitHub actions. To use a GitHub token, set the following environment variables:</p> <ul> <li><code>SPACELIFT_API_KEY_ENDPOINT</code> - the URL to your Spacelift account, for example <code>https://mycorp.app.spacelift.io</code>.</li> <li><code>SPACELIFT_API_GITHUB_TOKEN</code> - a GitHub personal access token.</li> </ul>"},{"location":"concepts/spacectl.html#spacelift-api-keys","title":"Spacelift API keys","text":"<p>To use a Spacelift API key, set the following environment variables:</p> <ul> <li><code>SPACELIFT_API_KEY_ENDPOINT</code> - the URL to your Spacelift account, for example <code>https://mycorp.app.spacelift.io</code>.</li> <li><code>SPACELIFT_API_KEY_ID</code> - the ID of your Spacelift API key. Available via the Spacelift application.</li> <li><code>SPACELIFT_API_KEY_SECRET</code> - the secret for your API key. Only available when the secret is created.</li> </ul> <p>More information about API authentication can be found at our GraphQL API documentation.</p>"},{"location":"concepts/spacectl.html#authenticating-using-account-profiles","title":"Authenticating using account profiles","text":"<p>In order to make working with multiple Spacelift accounts easy in interactive scenarios, Spacelift supports account management through the <code>profile</code> family of commands:</p> <pre><code>\u276f spacectl profile\nNAME:\n   spacectl profile - Manage Spacelift profiles\n\nUSAGE:\n   spacectl profile command [command options] [arguments...]\n\nCOMMANDS:\n     current       Outputs your currently selected profile\n     export-token  Prints the current token to stdout. In order not to leak, we suggest piping it to your OS pastebin\n     list          List all your Spacelift account profiles\n     login         Create a profile for a Spacelift account\n     logout        Remove Spacelift credentials for an existing profile\n     select        Select one of your Spacelift account profiles\n     help, h       Shows a list of commands or help for one command\n\nOPTIONS:\n   --help, -h  show help (default: false)\n</code></pre> <p>Each of the subcommands requires an account alias, which is a short, user-friendly name for each set of credentials (account profiles). Profiles don't need to be unique - you can have multiple sets of credentials for a single account too.</p> <p>Account profiles support three authentication methods:</p> <ul> <li> <p>GitHub access tokens</p> </li> <li> <p>API keys</p> </li> <li> <p>Login with a browser (API token).</p> </li> </ul> <p>In order to authenticate to your first profile, type in the following (make sure to replace <code>${MY_ALIAS}</code> with the actual profile alias):</p> <pre><code>\u276f spacectl profile login ${MY_ALIAS}\nEnter Spacelift endpoint (eg. https://unicorn.app.spacelift.io/):\n</code></pre> <p>In the next step, you will be asked to choose which authentication method you are going to use. Note that if your account is using SAML-based SSO authentication, then API keys and login with a browser are your only options. After you're done entering credentials, the CLI will validate them against the server, and assuming that they're valid, will persist them in a credentials file in <code>.spacelift/${MY_ALIAS}</code>. It will also create a symlink in <code>${HOME}/.spacelift/current</code> pointing to the current profile.</p> <p>By default the login process is interactive, however, if that does not fit your workflow, the steps can be predefined using flags, for example:</p> <pre><code>\u276f spacectl profile login --method browser --endpoint https://unicorn.app.spacelift.io local-test\n</code></pre> <p>You can switch between account profiles by using <code>spacectl profile select ${MY_ALIAS}</code>. What this does behind the scenes is point <code>${HOME}/.spacelift/current</code> to the new location. You can also delete stored credetials for a given profile by using the <code>spacectl profile logout ${MY_ALIAS}</code> command.</p>"},{"location":"concepts/worker-pools.html","title":"Worker pools","text":"<p>Info</p> <p>Note that private workers are an Enterprise plan feature.</p> <p>Tip</p> <p>A worker is a logical entity that processes a single run at a time. As a result, your number of workers is equal to your maximum concurrency.</p> <p>Typically, a virtual server (AWS EC2 or Azure/GCP VM) hosts a single worker to keep things simple and avoid coordination and resource management overhead.</p> <p>Containerized workers can share the same virtual server because the management is handled by the orchestrator.</p>"},{"location":"concepts/worker-pools.html#setting-up","title":"Setting up","text":""},{"location":"concepts/worker-pools.html#generate-worker-private-key","title":"Generate Worker Private Key","text":"<p>We use asymmetric encryption to ensure that any temporary run state can only be accessed by workers in a given worker pool. To support this, you need to generate a private key that can be used for this purpose, and use it to create a certificate signing request to give to Spacelift. We'll generate a certificate for you, so that workers can use it to authenticate with the Spacelift backend. The following command will generate the key and CSR:</p> <pre><code>openssl req -new -newkey rsa:4096 -nodes -keyout spacelift.key -out spacelift.csr\n</code></pre> <p>Warning</p> <p>Don't forget to store the <code>spacelift.key</code> file (private key) in a secure location. You\u2019ll need it later, when launching workers in your worker pool.</p> <p>You can set up your worker pool from the Spacelift UI by navigating to Worker Pools section of your account, or you can also create it programmatically using the <code>spacelift_worker_pool</code> resource type within the Spacelift Terraform provider.</p>"},{"location":"concepts/worker-pools.html#navigate-to-worker-pools","title":"Navigate to Worker Pools","text":""},{"location":"concepts/worker-pools.html#add-worker-pool-entity","title":"Add Worker Pool Entity","text":"<p>Give your worker pool a name, and submit the <code>spacelift.csr</code> file in the worker pool creation form. After creation of the worker pool, you\u2019ll receive a Spacelift token. This token contains configuration for your worker pool launchers, as well as the certificate we generated for you based on the certificate signing request.</p> <p>Warning</p> <p>After clicking create, you will receive a token for the worker pool. Don't forget to save your Spacelift token in a secure location as you'll need this later when launching the worker pool.</p> <p></p> <p></p>"},{"location":"concepts/worker-pools.html#launch-worker-pool","title":"Launch Worker Pool","text":"<p>The Self-Hosted release archive contains a copy of the Spacelift launcher binary built specifically for your version of Self-Hosted. You can find this at <code>bin/spacelift-launcher</code>. This binary is also uploaded to the downloads S3 bucket during the Spacelift installation process. For more information on how to find your bucket name see here.</p> <p>In order to work, the launcher expects to be able to write to the local Docker socket. Unless you're using a Docker-based container scheduler like Kubernetes or ECS, please make sure that Docker is installed and running.</p> <p>Finally, you can run the launcher binary by setting two environment variables:</p> <ul> <li><code>SPACELIFT_TOKEN</code> - the token you\u2019ve received from Spacelift on worker pool creation</li> <li><code>SPACELIFT_POOL_PRIVATE_KEY</code> - the contents of the private key file you generated, in base64.</li> </ul> <p>Info</p> <p>You need to encode the entire private key using base-64, making it a single line of text. The simplest approach is to just run <code>cat spacelift.key | base64 -w 0</code> in your command line. For Mac users, the command is <code>cat spacelift.key | base64 -b 0</code>.</p> <p>Congrats! Your launcher should now connect to the Spacelift backend and start handling runs.</p>"},{"location":"concepts/worker-pools.html#cloudformation-template","title":"CloudFormation Template","text":"<p>The easiest way to deploy workers for self-hosting is to deploy the CloudFormation template found in <code>cloudformation/workerpool.yaml</code>.</p>"},{"location":"concepts/worker-pools.html#pseudorandomsuffix","title":"PseudoRandomSuffix","text":"<p>The CloudFormation stack uses a parameter called <code>PseudoRandomSuffix</code> in order to ensure that certain resources are unique within an AWS account. The value of this parameter does not matter, other than that it is unique per worker pool stack you deploy. You should choose a value that is 6 characters long and made up of letters and numbers, for example <code>ab12cd</code>.</p>"},{"location":"concepts/worker-pools.html#create-a-secret","title":"Create a secret","text":"<p>First, create a new secret in SecretsManager, and add your token and the base64-encoded value of your private key. Use the key <code>SPACELIFT_TOKEN</code> for your token and <code>SPACELIFT_POOL_PRIVATE_KEY</code> for the private key. It should look something like this:</p> <p></p> <p>Give your secret a name and create it. It doesn't matter what this name is, but you'll need it when deploying the CloudFormation stack.</p>"},{"location":"concepts/worker-pools.html#get-the-downloads-bucket-name","title":"Get the downloads bucket name","text":"<p>The downloads bucket name is output at the end of the installation process. If you don't have a note of it, you can also get it from the resources of the spacelift-infra-s3 stack in CloudFormation:</p> <p></p>"},{"location":"concepts/worker-pools.html#ami","title":"AMI","text":"<p>You can use your own custom-built AMI for your workers, or you can use one of the pre-built images we provide. For a list of the correct AMI to use for the region you want to deploy your worker to, see the spacelift-worker-image releases page.</p> <p>Note: please make sure to choose the <code>x86_64</code> version of the AMI.</p>"},{"location":"concepts/worker-pools.html#subnets-and-security-group","title":"Subnets and Security Group","text":"<p>You will need to have an existing VPC to deploy your pool into, and will need to provide a list of subnet IDs and security groups to match your requirements.</p>"},{"location":"concepts/worker-pools.html#using-a-custom-iam-role","title":"Using a custom IAM role","text":"<p>By default we will create the instance role for the EC2 ASG as part of the worker pool stack, but you can also provide your own custom role via the <code>InstanceRoleName</code> parameter. This allows you to grant permissions to additional AWS resources that your workers need access to. A great example of this is allowing access to a private ECR in order to use a custom runner image.</p> <p>At a minimum, your role must fulfil the following requirements:</p> <ul> <li>It must have a trust relationship that allows role assumption by EC2.</li> <li>It needs to have the following managed policies attached:<ul> <li><code>AutoScalingReadOnlyAccess</code>.</li> <li><code>CloudWatchAgentServerPolicy</code>.</li> <li><code>AmazonSSMManagedInstanceCore</code>.</li> </ul> </li> </ul>"},{"location":"concepts/worker-pools.html#injecting-custom-commands-during-instance-startup","title":"Injecting custom commands during instance startup","text":"<p>You have the option to inject custom commands into the EC2 user data. This can be useful if you want to install additional software on your workers, or if you want to run a custom script during instance startup, or just add some additional environment variables.</p> <p>The script must be a valid shell script and should be put into Secrets Manager. Then you can provide the name of the secret as <code>CustomUserDataSecretName</code> when deploying the stack.</p> <p>Example:</p> <p></p> <p>In the example above, we used <code>spacelift/userdata</code> as a secret name so the parameter will look like this:</p> <pre><code>  [...]\n--parameter-overrides \\\nCustomUserDataSecretName=\"spacelift/userdata\" \\\n[...]\n</code></pre>"},{"location":"concepts/worker-pools.html#granting-access-to-a-private-ecr","title":"Granting access to a private ECR","text":"<p>To allow your worker role to access a private ECR, you can attach a policy similar to the following to your instance role (replacing <code>&lt;repository-arn&gt;</code> with the ARN of your ECR repository):</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ecr:GetDownloadUrlForLayer\",\n\"ecr:BatchGetImage\",\n\"ecr:BatchCheckLayerAvailability\"\n],\n\"Resource\": \"&lt;repository-arn&gt;\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ecr:GetAuthorizationToken\"\n],\n\"Resource\": \"*\"\n}\n]\n}\n</code></pre> <p>NOTE: repository ARNs are in the format <code>arn:&lt;partition&gt;:ecr:&lt;region&gt;:&lt;account-id&gt;:repository/&lt;repository-name&gt;</code>.</p>"},{"location":"concepts/worker-pools.html#proxy-configuration","title":"Proxy Configuration","text":"<p>If you need to use an HTTP proxy for internet access, you can provide the proxy configuration using the following CloudFormation parameters:</p> <ul> <li><code>HttpProxyConfig</code>.</li> <li><code>HttpsProxyConfig</code>.</li> <li><code>NoProxyConfig</code>.</li> </ul> <p>For example, you could use the following command to deploy a worker with a proxy configuration:</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n--stack-name spacelift-default-worker-pool \\\n--template-file \"cloudformation/workerpool.yaml\" \\\n--region \"eu-west-1\" \\\n--parameter-overrides \\\nPseudoRandomSuffix=\"ab12cd\" \\\nBinariesBucket=\"012345678901-spacelift-infra-spacelift-downloads\" \\\nSecretName=\"spacelift/default-worker-pool-credentials\" \\\nSecurityGroups=\"sg-0d1e157a19ba2106f\" \\\nSubnets=\"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\\nImageId=\"ami-0ead0234bef4f51b0\" \\\nHttpProxyConfig=\"http://proxy.example.com:1234\" \\\nHttpsProxyConfig=\"https://proxy.example.com:4321\" \\\nNoProxyConfig=\"some.domain,another.domain\" \\\n--capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre>"},{"location":"concepts/worker-pools.html#using-custom-ca-certificates","title":"Using custom CA certificates","text":"<p>If you use a custom certificate authority to issue TLS certs for components that Spacelift will communicate with, for example your VCS system, you need to provide your custom CA certificates to the worker. You do this by creating a secret in SecretsManager containing a base64 encoded JSON string.</p> <p>The format of the JSON object is as follows:</p> <pre><code>{\"caCertificates\": [\"&lt;base64-encoded-cert-1&gt;\", \"&lt;base64-encoded-cert-2&gt;\", \"&lt;base64-encoded-cert-N&gt;\"]}\n</code></pre> <p>For example, if you had a file called ca-certs.json containing the following content:</p> <pre><code>{\n\"caCertificates\": [\n\"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\"\n]\n}\n</code></pre> <p>You could then encode it to base64 using <code>base64 -w0 &lt; ca-certs.json</code> (or <code>base64 -b 0 &lt; ca-certs.json</code> on a Mac), resulting in the following string:</p> <pre><code>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\n</code></pre> <p>You would then create a secret in SecretsManager, and deploy the worker pool using the following command (replacing <code>&lt;ca-cert-secret-name&gt;</code> with the name of your secret):</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n--stack-name spacelift-default-worker-pool \\\n--template-file \"cloudformation/workerpool.yaml\" \\\n--region \"eu-west-1\" \\\n--parameter-overrides \\\nPseudoRandomSuffix=\"ab12cd\" \\\nBinariesBucket=\"012345678901-spacelift-infra-spacelift-downloads\" \\\nSecretName=\"spacelift/default-worker-pool-credentials\" \\\nSecurityGroups=\"sg-0d1e157a19ba2106f\" \\\nSubnets=\"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\\nImageId=\"ami-0ead0234bef4f51b0\" \\\nAdditionalRootCAsSecretName=\"&lt;ca-cert-secret-name&gt;\" \\\n--capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre>"},{"location":"concepts/worker-pools.html#running-the-launcher-as-root","title":"Running the launcher as root","text":"<p>By default, when the EC2 instance starts up, it creates a user called <code>spacelift</code> with a UID of 1983. This user is then used to run the launcher process.</p> <p>If for some reason this causes problems, you can run the launcher as <code>root</code> by setting the <code>RunLauncherAsSpaceliftUser</code> CloudFormation parameter to <code>false</code>.</p> <p>Tip</p> <p>Versions v0.0.7 or older of Self-Hosted always ran the launcher as root. In newer versions this behavior has changed to default to the <code>spacelift</code> user.</p>"},{"location":"concepts/worker-pools.html#poweroffonerror","title":"PowerOffOnError","text":"<p>By default, the startup script for the EC2 instances automatically terminates the instance if the launcher exits. This is to allow the instance to be automatically removed from the autoscale group and a new one added in the case of errors.</p> <p>Sometimes it can be useful to disable this behavior, for example if instances are repeatedly crashing on startup, preventing you from being able to connect to investigate any issues before they terminate.</p> <p>To do this, set the <code>PowerOffOnError</code> setting to false when deploying your CloudFormation stack.</p> <p>Info</p> <p>Please note, if you update this setting on an existing CloudFormation stack, you will need to restart all the workers in the pool before the updated setting takes effect.</p>"},{"location":"concepts/worker-pools.html#deploying-the-template","title":"Deploying the Template","text":"<p>To deploy your worker pool stack, you can use the following command:</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n--stack-name spacelift-default-worker-pool \\\n--template-file \"cloudformation/workerpool.yaml\" \\\n--region \"&lt;region&gt;\" \\\n--parameter-overrides \\\nPseudoRandomSuffix=\"ab12cd\" \\\nBinariesBucket=\"&lt;binaries-bucket&gt;\" \\\nSecretName=\"&lt;secret-name&gt;\" \\\nSecurityGroups=\"&lt;security-groups&gt;\" \\\nSubnets=\"&lt;subnets&gt;\" \\\nImageId=\"&lt;ami-id&gt;\" \\\n--capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre> <p>For example, to deploy to <code>eu-west-1</code> you might use something like this:</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n--stack-name spacelift-default-worker-pool \\\n--template-file \"cloudformation/workerpool.yaml\" \\\n--region \"eu-west-1\" \\\n--parameter-overrides \\\nPseudoRandomSuffix=\"ab12cd\" \\\nBinariesBucket=\"012345678901-spacelift-infra-spacelift-downloads\" \\\nSecretName=\"spacelift/default-worker-pool-credentials\" \\\nSecurityGroups=\"sg-0d1e157a19ba2106f\" \\\nSubnets=\"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\\nImageId=\"ami-0ead0234bef4f51b0\" \\\n--capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre> <p>To use a custom instance role, you might use something like this:</p> <pre><code>aws cloudformation deploy --no-cli-pager \\\n--stack-name spacelift-default-worker-pool \\\n--template-file \"cloudformation/workerpool.yaml\" \\\n--region \"eu-west-1\" \\\n--parameter-overrides \\\nPseudoRandomSuffix=\"ab12cd\" \\\nBinariesBucket=\"012345678901-spacelift-infra-spacelift-downloads\" \\\nSecretName=\"spacelift/default-worker-pool-credentials\" \\\nSecurityGroups=\"sg-0d1e157a19ba2106f\" \\\nSubnets=\"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\\nImageId=\"ami-0ead0234bef4f51b0\" \\\nInstanceRoleName=\"default-worker-role\" \\\n--capabilities \"CAPABILITY_NAMED_IAM\"\n</code></pre>"},{"location":"concepts/worker-pools.html#terraform-modules","title":"Terraform Modules","text":"<p>Our public AWS, Azure and GCP Terraform modules are not currently compatible with self-hosting.</p>"},{"location":"concepts/worker-pools.html#running-workers-in-kubernetes","title":"Running Workers in Kubernetes","text":"<p>You can run Spacelift workers for your self-hosted instance in Kubernetes, for example using our Helm chart. The main thing to be aware of is that the launcher is designed to work with a specific version of Spacelift, so it's important to use the correct container image for your Spacelift install.</p>"},{"location":"concepts/worker-pools.html#finding-the-launcher-image","title":"Finding the Launcher Image","text":"<p>During the installation process for your self-hosted image, an ECR repository is created for storing launcher images named <code>spacelift-launcher</code>. At the end of the installation the launcher image URI and tag are output. If you didn't take a note of it at the time, you can find the ECR repository URI via the AWS console, or by running the following command:</p> <pre><code>aws ecr describe-repositories --region &lt;aws-region&gt; --repository-names \"spacelift-launcher\" --output json | jq -r '.repositories[0].repositoryUri'\n</code></pre> <p>The repository URI will be in the format <code>&lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/spacelift-launcher</code>. To calculate the correct image to use, add the version of your self-hosted installation onto the end, for example:</p> <pre><code>012345678901.dkr.ecr.eu-west-2.amazonaws.com/spacelift-launcher:v0.0.6\n</code></pre> <p>Note: the cluster that you run the Launcher in must be able to pull the launcher image from your ECR repository, so you will need to ensure that it has the correct permissions to do so.</p>"},{"location":"concepts/worker-pools.html#helm-chart","title":"Helm Chart","text":"<p>By default our Helm chart is configured to use <code>public.ecr.aws/spacelift/launcher</code>. The <code>latest</code> tag of that image is guaranteed to always work with the SaaS version of Spacelift. For self-hosted instances, you should configure the chart to use the correct launcher image URI and tag. For example, for the image specified in the Finding the Launcher Image section, you would use the following Helm values:</p> <pre><code>launcher:\nimage:\nrepository: \"012345678901.dkr.ecr.eu-west-2.amazonaws.com/spacelift-launcher\"\ntag: \"v0.0.6\"\n</code></pre>"},{"location":"concepts/worker-pools.html#configuration-options","title":"Configuration options","text":"<p>A number of configuration variables is available to customize how your launcher behaves:</p> <ul> <li><code>SPACELIFT_DOCKER_CONFIG_DIR</code> - if set, the value of this variable will point to the directory containing Docker configuration, which includes credentials for private Docker registries. Private workers can populate this directory for example by executing <code>docker login</code> before the launcher process is started;</li> <li><code>SPACELIFT_MASK_ENVS</code>- comma-delimited list of whitelisted environment variables that are passed to the workers but should never appear in the logs;</li> <li><code>SPACELIFT_SENSITIVE_OUTPUT_UPLOAD_ENABLED</code> - If set to <code>true</code>, the launcher will upload sensitive run outputs to the Spacelift backend. This is a requirement if want to use sensitive outputs for stack dependencies;</li> <li><code>SPACELIFT_WORKER_NETWORK</code> - network ID/name to connect the launched worker containers, defaults to <code>bridge</code>;</li> <li><code>SPACELIFT_WORKER_EXTRA_MOUNTS</code> - additional files or directories to be mounted to the launched worker docker containers during either read or write runs, as a comma-separated list of mounts in the form of <code>/host/path:/container/path</code>;</li> <li><code>SPACELIFT_WORKER_WO_EXTRA_MOUNTS</code> - Additional directories to be mounted to the worker docker container during write only runs, as a comma separated list of mounts in the form of <code>/host/path:/container/path</code>;</li> <li><code>SPACELIFT_WORKER_RO_EXTRA_MOUNTS</code> - Additional directories to be mounted to the worker docker container during read only runs, as a comma separated list of mounts in the form of <code>/host/path:/container/path</code>;</li> <li><code>SPACELIFT_WORKER_RUNTIME</code> - runtime to use for worker container;</li> <li><code>SPACELIFT_WHITELIST_ENVS</code> - comma-delimited list of environment variables to pass from the launcher's own environment to the workers' environment. They can be prefixed with <code>ro_</code> to only be included in read only runs or <code>wo_</code> to only be included in write only runs;</li> <li><code>SPACELIFT_LAUNCHER_LOGS_TIMEOUT</code> - custom timeout (the default is 7 minutes) for killing jobs not producing any logs. This is a duration flag, expecting a duration-formatted value, eg <code>1000s</code> ;</li> <li><code>SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY</code> - file that contains the run initialization policy that will be parsed/used; If the run initialized policy can not be validated at the startup the worker pool will exit with an appropriate error;</li> <li><code>SPACELIFT_LAUNCHER_RUN_TIMEOUT</code> - custom maximum run time - the default is 70 minutes. This is a duration flag, expecting a duration-formatted value, eg. <code>120m</code> ;</li> <li><code>SPACELIFT_DEBUG</code>- if set to true, this will output the exact commands spacelift runs to the worker logs;</li> </ul> <p>Warning</p> <p>Server-side initialization policies are being deprecated. <code>SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY</code> shouldn't be confused with that. This policy is a Worker-side initialization policy and it can be set by using the launcher run initialization policy flag.</p> <p>For a limited time period we will be running both types of initialization policy checks but ultimately we're planning to move the pre-flight checks to the worker node, thus allowing customers to block suspicious looking jobs on their end.</p>"},{"location":"concepts/worker-pools.html#passing-metadata-tags","title":"Passing metadata tags","text":"<p>When the launcher from a worker pool is registering with the mothership, you can send along some tags that will allow you to uniquely identify the process/machine for the purpose of draining or debugging. Any environment variables using <code>SPACELIFT_METADATA_</code> prefix will be passed on. As an example, if you're running Spacelift workers in EC2, you can do the following just before you execute the launcher binary:</p> <pre><code>export SPACELIFT_METADATA_instance_id=$(ec2-metadata --instance-id | cut -d ' ' -f2)\n</code></pre> <p>Doing so will set your EC2 instance ID as <code>instance_id</code> tag in your worker.</p> <p>Please see injecting custom commands during instance startup for information about how to do this when using our CloudFormation template.</p>"},{"location":"concepts/worker-pools.html#network-security","title":"Network Security","text":"<p>Private workers need to be able to make outbound connections in order to communicate with Spacelift, as well as to access any resources required by your runs. If you have policies in place that require you to limit the outbound traffic allowed from your workers, you can use the following lists as a guide.</p>"},{"location":"concepts/worker-pools.html#aws-services","title":"AWS Services","text":"<p>Your worker needs access to the following AWS services in order to function correctly. You can refer to the AWS documentation for their IP address ranges.</p> <ul> <li>Access to the public Elastic Container Registry if using our default runner image.</li> <li>Access to your Self-Hosted server, for example <code>https://spacelift.myorg.com</code>.</li> <li>Access to the AWS IoT Core endpoints in your installation region for worker communication via MQTT.</li> <li>Access to Amazon S3 in your installation region for uploading run logs.</li> </ul>"},{"location":"concepts/worker-pools.html#other","title":"Other","text":"<p>In addition, you will also need to allow access to the following:</p> <ul> <li>Your VCS provider.</li> <li>Access to any custom container registries you use if using custom runner images.</li> <li>Access to any other infrastructure required as part of your runs.</li> </ul>"},{"location":"concepts/worker-pools.html#hardware-recommendations","title":"Hardware recommendations","text":"<p>The hardware requirments for the workers will vary depending on the stack size(How many resources managed, resource type, etc.), but we recommend at least 2GB of memory and 2 vCPUs of compute power.</p> <p>These are the recommended server types for the three main cloud providers:</p> <ul> <li>AWS: t3.small instance type</li> <li>Azure: Standard_A2_V2 virtual machine</li> <li>GCP: e2-medium instance type</li> </ul>"},{"location":"concepts/worker-pools.html#using-worker-pools","title":"Using worker pools","text":"<p>Worker pools must be explicitly attached to stacks and/or modules in order to start processing their workloads. This can be done in the Behavior section of stack and module settings:</p> <p></p> <p></p>"},{"location":"concepts/worker-pools.html#worker-pool-management-views","title":"Worker Pool Management Views","text":"<p>You can view the activity and status of every aspect of your worker pool in the worker pool detail view. You can navigate to the worker pool of your choosing by clicking on the appropriate entry in the worker pools list view. </p>"},{"location":"concepts/worker-pools.html#private-worker-pool","title":"Private Worker Pool","text":"<p>A private worker pool is a worker pool for which you are responsible for managing the workers.</p> <p></p>"},{"location":"concepts/worker-pools.html#workers","title":"Workers","text":"<p>The workers tab lists all workers for this worker pool and their status.</p>"},{"location":"concepts/worker-pools.html#status","title":"Status","text":"<p>A worker can have three possible statuses:</p> <ul> <li><code>DRAINED</code> which indicates that the workers is not accepting new work.</li> <li><code>BUSY</code> which indicates that the worker is currently processing or about to process a run.</li> <li><code>IDLE</code> which indicates that the worker is available to start processing new runs.</li> </ul>"},{"location":"concepts/worker-pools.html#queued","title":"Queued","text":"<p>Queued lists all the run that can be scheduled and are currently in progress. In progress runs will be the first entries in the list when using the view without any filtering.</p> <p>Info</p> <p>Reasons a run might not be shown in this list: a tracked run is waiting on a tracked run, the run has is dependent on other runs.</p>"},{"location":"concepts/worker-pools.html#used-by","title":"Used by","text":"<p>Stacks and/or Modules that are using the private worker pool.</p>"},{"location":"concepts/blueprint/index.html","title":"Blueprint","text":"<p>There are multiple ways to create stacks in Spacelift. Our recommended way is to use our Terraform provider and programmatically create stacks using an administrative stack.</p> <p>However, some users might not be comfortable using Terraform code to create stacks, this is where Blueprints come in handy.</p>"},{"location":"concepts/blueprint/index.html#what-is-a-blueprint","title":"What is a Blueprint?","text":"<p>A Blueprint is a template for a stack and its configuration. The template can contain variables that can be filled in by providing inputs when creating a stack from the Blueprint. The template can also contain a list of other resources that will be created when the stack is created.</p> <p>You can configure the following resources in a Blueprint:</p> <ul> <li>All stack settings including:<ul> <li>Name, description, labels, Space</li> <li>Behavioral settings: administrative, auto-apply, auto-destroy, hooks, runner image etc.</li> </ul> </li> <li>VCS configuration</li> <li>Vendor configuration for your IaaC provider</li> <li>Environment variables, both non-sensitive and sensitive</li> <li>Mounted files</li> <li>Attaching Contexts</li> <li> <p>Attaching Policies</p> </li> <li> <p>Attaching AWS integrations</p> </li> <li> <p>Schedules:</p> <ul> <li>Drift detection</li> <li>Task</li> <li>Delete</li> </ul> </li> </ul>"},{"location":"concepts/blueprint/index.html#blueprint-states","title":"Blueprint states","text":"<p>There are two states: draft and published. Draft is the default state, it means that the blueprint \"development\" is in progress and not meant to be used. You cannot create a stack from a draft blueprint.</p> <p>Published means that the blueprint is ready to be used. You can publish a blueprint by clicking the <code>Publish</code> button in the UI.</p> <p>A published blueprint cannot be moved back to draft state. You need to clone the blueprint, edit it and publish it.</p>"},{"location":"concepts/blueprint/index.html#permissions","title":"Permissions","text":"<p>Blueprints permissions are managed by Spaces. You can only create, update and delete a blueprint in a Space you have admin access to but can be read by anyone with read access to the Space.</p> <p>Once the blueprint is published and you want to create a stack from it, the read access will be enough as long as you have admin access to the Space where the stack will be created.</p>"},{"location":"concepts/blueprint/index.html#how-to-create-a-blueprint","title":"How to create a Blueprint","text":"<p>Choose <code>Blueprints</code> on the left menu and click on <code>Create blueprint</code>. As of now, we only support YAML format. The template engine will be familiar for those who used GitHub Actions before.</p> <p>The absolute minimum you'll need to provide is <code>name</code>, <code>space</code>, <code>vcs</code> and <code>vendor</code>; all others are optional. Here's a small working example:</p> <pre><code>inputs:\n- id: stack_name\nname: Stack name\nstack:\nname: ${{ inputs.stack_name }}\nspace: root\nvcs:\nbranch: main\nrepository: my-repository\nprovider: GITHUB\nvendor:\nterraform:\nmanage_state: true\nversion: \"1.3.0\"\n</code></pre> <p> </p> Preview of a Blueprint <p>The <code>Create a stack</code> button is inactive because the blueprint is in draft state. You can publish it by clicking the <code>Publish</code> button. After that, you can create a stack from the blueprint.</p> <p>Now, let's look at a massive example that covers all the available configuration options:</p> Click to expand <pre><code>inputs:\n- id: environment\nname: Environment to deploy to\n# type is not mandatory, defaults to short_text\n- id: app\nname: App name (used for naming convention)\ntype: short_text\n- id: description\nname: Description of the stack\ntype: long_text\n# long_text means you'll have a bigger text area in the UI\n- id: connstring\nname: Connection string to the database\ntype: secret\n# secret means the input will be masked in the UI\n- id: tf_version\nname: Terraform version of the stack\ntype: select\noptions:\n- \"1.3.0\"\n- \"1.4.6\"\n- \"1.5.0\"\n- id: manage_state\nname: Should Spacelift manage the state of Terraform\ndefault: true\ntype: boolean\n- id: destroy_task_epoch\nname: Epoch timestamp of when to destroy the resources\ntype: number\noptions:\n# If true, a tracked run will be triggered right after the stack is created\ntrigger_run: true\nstack:\nname: ${{ inputs.app }}-{{ inputs.environment }}-stack\nspace: root\n# The single-quote is needed to avoid YAML parsing errors since the question mark\n# and the colon is a reserved character in YAML.\ndescription: '${{ inputs.environment == \"prod\" ? \"Production stack\" : \"Non-production stack\" }}. Stack created at ${{ string(context.time) }}.'\nis_disabled: ${{ inputs.environment != 'prod' }}\nlabels:\n- Environment/${{ inputs.environment }}\n- Vendor/Terraform\n- Owner/${{ context.user.login }}\n- Blueprint/${{ context.blueprint.name }}\n- Space/${{ context.blueprint.space }}\nadministrative: false\nallow_promotion: false\nauto_deploy: false\nauto_retry: false\nlocal_preview_enabled: true\nprotect_from_deletion: false\nrunner_image: public.ecr.aws/mycorp/spacelift-runner:latest\nworker_pool: 01GQ29K8SYXKZVHPZ4HG00BK2E\nattachments:\ncontexts:\n- id: my-first-context-vnfq2\npriority: 1\nclouds:\naws:\nid: 01GQ29K8SYXKZVHPZ4HG00BK2E\nread: true\nwrite: true\nazure:\nid: 01GQ29K8SYXKZVHPZ4HG00BK2E\nread: true\nwrite: true\nsubscription_id: 12345678-1234-1234-1234-123456789012\npolicies:\n- my-push-policy-1\n- my-approval-policy-1\nenvironment:\nvariables:\n- name: MY_ENV_VAR\nvalue: my-env-var-value\ndescription: This is my non-encrypted env var\n- name: TF_VAR_CONNECTION_STRING\nvalue: ${{ inputs.connstring }}\ndescription: The connection string to the database\nsecret: true\nmounted_files:\n- path: a.json\ncontent: |\n{\n\"a\": \"b\"\n}\ndescription: This is the configuration of x feature\nhooks:\napply:\nbefore: [\"sh\", \"-c\", \"echo 'before apply'\"]\nafter: [\"sh\", \"-c\", \"echo 'after apply'\"]\ninit:\nbefore: [\"sh\", \"-c\", \"echo 'before init'\"]\nafter: [\"sh\", \"-c\", \"echo 'after init'\"]\nplan:\nbefore: [\"sh\", \"-c\", \"echo 'before plan'\"]\nafter: [\"sh\", \"-c\", \"echo 'after plan'\"]\nperform:\nbefore: [\"sh\", \"-c\", \"echo 'before perform'\"]\nafter: [\"sh\", \"-c\", \"echo 'after perform'\"]\ndestroy:\nbefore: [\"sh\", \"-c\", \"echo 'before destroy'\"]\nafter: [\"sh\", \"-c\", \"echo 'after destroy'\"]\nrun:\n# There is no before hook for run\nafter: [\"sh\", \"-c\", \"echo 'after run'\"]\nschedules:\ndrift:\ncron:\n- \"0 0 * * *\"\n- \"5 5 * * 0\"\nreconcile: true\nignore_state: true # If true, the schedule will run even if the stack is in a failed state\ntimezone: UTC\ntasks:\n# You need to provide either a cron or a timestamp_unix\n- command: \"terraform apply -auto-approve\"\ncron:\n- \"0 0 * * *\"\n- command: \"terraform apply -auto-approve\"\ntimestamp_unix: ${{ int(timestamp('2024-01-01T10:00:20.021-05:00')) }}\ndelete:\ndelete_resources: ${{ inputs.environment == 'prod' }}\ntimestamp_unix: ${{ inputs.destroy_task_epoch - 86400 }}\nvcs:\nbranch: master\nproject_root: modules/apps/${{ inputs.app }}\nnamespace: \"my-namespace\"\n# Note that this is just the name of the repository, not the full URL\nrepository: my-repository\nprovider: GITHUB # Possible values: GITHUB, GITLAB, BITBUCKET_DATACENTER, BITBUCKET_CLOUD, GITHUB_ENTERPRISE, AZURE_DEVOPS\nvendor:\nterraform:\nmanage_state: ${{ inputs.manage_state }}\nversion: ${{ inputs.tf_version }}\nworkspace: workspace-${{ inputs.environment }}\nuse_smart_sanitization: ${{ inputs.environment != 'prod' }}\nansible:\nplaybook: playbook.yml\ncloudformation:\nentry_template_file: cf/main.yml\ntemplate_bucket: template_bucket\nstack_name: ${{ inputs.app }}-${{ inputs.environment }}\nregion: '${{ inputs.environment.contains(\"prod\") ? \"us-east-1\" : \"us-east-2\" }}'\nkubernetes:\nnamespace: ${{ inputs.app }}\npulumi:\nstack_name: ${{ inputs.app }}-${{ inputs.environment }}\nlogin_url: https://app.pulumi.com\n</code></pre> <p>As you noticed if we attach an existing resource to the stack (such as Worker Pool, Cloud integration, Policy or Context) we use the unique identifier of the resource. Typically, there is a button for it in the UI but you can also find it in the URL of the resource.</p> <p> </p> Example of resource IDs"},{"location":"concepts/blueprint/index.html#template-engine","title":"Template engine","text":"<p>We built our own variable substitution engine based on Google CEL. The library is available on GitHub.</p>"},{"location":"concepts/blueprint/index.html#functions-objects","title":"Functions, objects","text":"<p>In the giant example above, you might have noticed something interesting: inline functions! CEL supports a couple of functions, such as: <code>contains</code>, <code>startsWith</code>, <code>endsWith</code>, <code>matches</code>, <code>size</code> and a bunch of others. You can find the full list in the language definition. It also supports some basic operators, such as: <code>*</code>, <code>/</code>, <code>-</code>, <code>+</code>, relations (<code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>), <code>&amp;&amp;</code>, <code>||</code>, <code>!</code>, <code>?:</code> (yes, it supports the ternary operator \ud83c\udf89) and <code>in</code>.</p> <p>Hint</p> <p>It could be useful to look into the unit tests of the library. Look for the invocations of <code>interpret</code> function.</p> <p>There is one caveat to keep in mind: keep the YAML syntax valid.</p>"},{"location":"concepts/blueprint/index.html#yaml-syntax-validity","title":"YAML syntax validity","text":"<p>There are reserved characters in YAML, such as <code>&gt;</code> (multiline string) <code>|</code> (multiline string), <code>:</code> (key-value pair marker), <code>?</code> (mapping key) etc. If you use these characters as part of a CEL expression, you'll need to use quotes around the expression to escape it. For example:</p> <p>Invalid template:</p> <pre><code>stack:\nname: ${{ 2 &gt; 1 ? \"yes\" : \"no\" }}-my-stack\n</code></pre> <p>See how the syntax highlighter is confused?</p> <p>Valid template:</p> <pre><code>stack:\nname: '${{ 2 &gt; 1 ? \"yes\" : \"no\" }}-my-stack'\n</code></pre> <p>Results in:</p> <pre><code>stack:\nname: 'yes-my-stack'\n</code></pre>"},{"location":"concepts/blueprint/index.html#interaction-with-terraform-templatefile","title":"Interaction with Terraform <code>templatefile</code>","text":"<p>When using the Terraform <code>templatefile</code> function to generate a Blueprint template body, you can run into issues because the Blueprint template engine and <code>templatefile</code> both use <code>$</code> as template delimiters. This can result in error messages like the following:</p> <pre><code>\u2502 Error: Error in function call\n\u2502\n\u2502   on main.tf line 2, in output \"content\":\n\u2502    2:   value = templatefile(\"${path.module}/test.tftpl\", {\n\u2502    3:     SPACE = \"root\"\n\u2502    4:   })\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 path.module is \".\"\n\u2502\n\u2502 Call to function \"templatefile\" failed: ./test.tftpl:5,31-32: Missing key/value separator; Expected an equals\n\u2502 sign (\"=\") to mark the beginning of the attribute value.\n</code></pre> <p>To solve this you can use <code>$${}</code> to indicate that <code>templatefile</code> should not attempt to replace a certain piece of text.</p> <p>In the following example, <code>$${{ inputs.stack_name }}</code> is escaped, whereas <code>${SPACE}</code> is not:</p> <pre><code>inputs:\n- id: stack_name\nname: Stack name\nstack:\nname: $${{ inputs.stack_name }}\nspace: ${SPACE}\nvcs:\nbranch: main\nrepository: my-repository\nprovider: GITHUB\nvendor:\nterraform:\nmanage_state: true\nversion: \"1.3.0\"\n</code></pre> <p>We can then use a call to <code>templatefile</code> like the following to render this template:</p> <pre><code>templatefile(\"${path.module}/test.tftpl\", {\nSPACE = \"root\"\n})\n</code></pre> <p>This results in the following output when the template is rendered:</p> <pre><code>inputs:\n- id: stack_name\nname: Stack name\nstack:\nname: ${{ inputs.stack_name }}\nspace: root\nvcs:\nbranch: main\nrepository: my-repository\nprovider: GITHUB\nvendor:\nterraform:\nmanage_state: true\nversion: \"1.3.0\"\n</code></pre>"},{"location":"concepts/blueprint/index.html#variables","title":"Variables","text":"<p>Since you probably don't want to create stacks with the exact same name and configuration, you'll use variables.</p>"},{"location":"concepts/blueprint/index.html#inputs","title":"Inputs","text":"<p>Inputs are defined in the <code>inputs</code> section of the template. You can use them in the template by prefixing them with <code>${{ inputs.</code> and suffixing them with <code>}}</code>. For example, <code>${{ inputs.environment }}</code> will be replaced with the value of the <code>environment</code> input. You can use these variables in CEL functions as well. For example, <code>trigger_run: ${{ inputs.environment == 'prod' }}</code> will be replaced with <code>trigger_run: true</code> or <code>trigger_run: false</code> depending on the value of the <code>environment</code> input.</p> <p>The input object has <code>id</code>, <code>name</code>, <code>description</code>, <code>type</code>, <code>default</code> and <code>options</code> fields. The mandatory fields are <code>id</code> and <code>name</code>.</p> <p>The <code>id</code> is used to refer to the input in the template. The <code>name</code> and the <code>description</code> are just helper fields for the user in the Stack creation tab. The <code>type</code> is the type of the input. The <code>default</code> is an optional default value of the input. The <code>options</code> is a list of options for the <code>select</code> input type.</p> <p>Example:</p> <pre><code>inputs:\n- id: app_name\nname: The name of the app\nstack:\nname: ${{ inputs.app_name }}-my-stack\n</code></pre>"},{"location":"concepts/blueprint/index.html#input-types","title":"Input types","text":"<p>If the input <code>type</code> is not provided, it defaults to <code>short_text</code>. Other options are:</p> Type Description <code>short_text</code> A short text input. <code>long_text</code> A long text input. Typically used for multiline strings. <code>secret</code> A secret input. The value of the input will be masked in the UI. <code>number</code> An integer input. <code>boolean</code> A boolean input. <code>select</code> A multi option input. In case of <code>select</code>, it is mandatory to provide <code>options</code>. <code>float</code> A float input. <p>An example including all the types:</p> <pre><code>inputs:\n- id: app_name\nname: The name of the stack\n# No type provided, defaults to short_text\n- id: description\nname: The description of the stack\ntype: long_text\n- id: connstring\nname: Connection string to the database\ntype: secret\n- id: number_of_instances\nname: The number of instances\ntype: number\n- id: delete_protection\nname: Is delete protection enabled?\ntype: boolean\n- id: environment\nname: The environment to deploy to\ntype: select\noptions:\n- prod\n- staging\n- dev\n- id: scale_factor\nname: The scale factor of the app\ntype: float\n# You can optionally provide a default value\ndefault: 1.5\n</code></pre>"},{"location":"concepts/blueprint/index.html#context","title":"Context","text":"<p>We also provide an input object called <code>context</code>. It contains the following properties:</p> Property Type Description <code>time</code> <code>google.protobuf.Timestamp</code> UTC time of the evaluation of the template. <code>random_string</code> <code>string</code> A random string of 6 characters (numbers and letters, no special characters). <code>random_number</code> <code>int</code> A random number between 0 and 1000000. <code>random_uuid</code> <code>string</code> A random UUID. <code>user.login</code> <code>string</code> The login of the person who triggered the blueprint creation; as provided by the SSO provider. <code>user.name</code> <code>string</code> The full name of the person who triggered the blueprint creation; as provided by the SSO provider. <code>user.account</code> <code>string</code> The account subdomain of the user who triggered the blueprint creation. <code>blueprint.name</code> <code>string</code> The name of the blueprint that was used to create the stack. <code>blueprint.space</code> <code>string</code> The space ID of the blueprint that was used to create the stack. <code>blueprint.created_at</code> <code>google.protobuf.Timestamp</code> The time when the blueprint was created. <code>blueprint.updated_at</code> <code>google.protobuf.Timestamp</code> The time when the blueprint was last updated. <code>blueprint.published_at</code> <code>google.protobuf.Timestamp</code> The time when the blueprint was published. <code>blueprint.labels</code> <code>list(string)</code> The labels of the blueprint. <p>Here is an example of using a few of them:</p> <pre><code>stack:\nname: integration-tests-${{ inputs.app }}-${{ context.random_string }}\ndescription: |\nTemporary integration test stack for ${{ inputs.app }}. Deployed in ${{ context.time.getFullYear() }}.\nThe base blueprint was created at ${{ string(context.blueprint.created_at) }}.\nlabels:\n- owner/${{ context.user.login }}\n- blueprints/${{ context.blueprint.name }}\nenvironment:\nvariables:\n- name: DEPLOYMENT_ID\nvalue: ${{ context.random_uuid }}\nschedules:\ndelete:\ndelete_resources: ${{ context.random_number % 2 == 0 }} # Russian roulette\ntimestamp_unix: ${{ int(context.time) + duration(\"30m\").getSeconds() }} # Delete the stack in 30 minutes\n</code></pre> <p>Results in:</p> <pre><code>stack:\nname: integration-tests-my-app-vG3j3a\ndescription: |\nTemporary integration test stack for my-app. Deployed in 2023.\nThe base blueprint was created at 2020-01-01T10:00:20.021-05:00.\nlabels:\n- owner/johndoe\n- blueprints/my-blueprint\nenvironment:\nvariables:\n- name: DEPLOYMENT_ID\nvalue: 6c9c4e3e-6b5d-4b3a-9c9c-4e3e6b5d4b3a\nschedules:\ndelete:\ndelete_resources: true # Russian roulette\ntimestamp_unix: 1674139424 # Delete the stack in 30 minutes\n</code></pre> <p>Note that this is not a working example as it misses a few things (<code>inputs</code> section, <code>vcs</code> etc.), but it should give you an idea of what you can do.</p> <p>Tip</p> <p>What can you do with <code>google.protobuf.Timestamp</code> and <code>google.protobuf.Duration</code>? Check out the language definition, it contains all the methods and type conversions available.</p>"},{"location":"concepts/blueprint/index.html#validation","title":"Validation","text":"<p>We do not validate drafted blueprints, you can do whatever you want with them. However, if you publish your blueprint, we'll make sure it includes the required fields and you'll get an error if it doesn't.</p> <p>One caveat: we cannot validate fields that have variables because we don't know the value of the variable. On the other hand, if you try to create a stack from the blueprint and supply the inputs to the template, we'll be able to do the full validation. Let's say:</p> <pre><code>inputs:\n- id: timestamp\nname: Delete timestamp of the stack\ntype: number\nstack:\nschedules:\ndelete:\ntimestamp_unix: ${{ inputs.timestamp }}\n</code></pre> <p>We cannot make sure that the input variable is indeed a proper 10 digit epoch timestamp, we will only find out once you supply the actual input.</p>"},{"location":"concepts/blueprint/index.html#schema","title":"Schema","text":"<p>The up-to-date schema of a Blueprint is available through a GraphQL query for authenticated users:</p> <pre><code>{\nblueprintSchema\n}\n</code></pre> <p>Tip</p> <p>Remember that there are multiple ways to interact with Spacelift. You can use the GraphQL API, the CLI, the Terraform Provider or the web UI itself if you're feeling fancy.</p> <p>For simplicity, here is the current schema, but it might change in the future:</p> Click to expand <pre><code>{\n\"$schema\": \"http://json-schema.org/draft-07/schema#\",\n\"title\": \"Blueprint\",\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"stack\"\n],\n\"properties\": {\n\"inputs\": {\n\"$ref\": \"#/definitions/inputs\"\n},\n\"stack\": {\n\"$ref\": \"#/definitions/stack\"\n},\n\"options\": {\n\"$ref\": \"#/definitions/options\"\n}\n},\n\"definitions\": {\n\"inputs\": {\n\"type\": \"array\",\n\"items\": {\n\"$ref\": \"#/definitions/input\"\n}\n},\n\"input\": {\n\"type\": \"object\",\n\"oneOf\": [\n{\n\"additionalProperties\": false,\n\"required\": [\n\"id\",\n\"name\"\n],\n\"properties\": {\n\"id\": {\n\"type\": \"string\"\n},\n\"name\": {\n\"type\": \"string\"\n},\n\"description\": {\n\"type\": \"string\"\n},\n\"default\": {\n\"oneOf\": [\n{\n\"type\": \"string\"\n},\n{\n\"type\": \"number\"\n},\n{\n\"type\": \"boolean\"\n}\n]\n},\n\"type\": {\n\"type\": \"string\",\n\"enum\": [\n\"short_text\",\n\"long_text\",\n\"secret\",\n\"boolean\",\n\"number\",\n\"float\"\n]\n}\n}\n},\n{\n\"additionalProperties\": false,\n\"required\": [\n\"id\",\n\"name\",\n\"type\",\n\"options\"\n],\n\"properties\": {\n\"id\": {\n\"type\": \"string\"\n},\n\"name\": {\n\"type\": \"string\"\n},\n\"description\": {\n\"type\": \"string\"\n},\n\"default\": {\n\"oneOf\": [\n{\n\"type\": \"string\"\n},\n{\n\"type\": \"number\"\n},\n{\n\"type\": \"boolean\"\n}\n]\n},\n\"type\": {\n\"type\": \"string\",\n\"enum\": [\n\"select\"\n]\n},\n\"options\": {\n\"type\": \"array\",\n\"minItems\": 1,\n\"items\": {\n\"type\": \"string\"\n}\n}\n}\n}\n]\n},\n\"stack\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"name\",\n\"space\",\n\"vcs\",\n\"vendor\"\n],\n\"properties\": {\n\"name\": {\n\"type\": \"string\",\n\"minLength\": 1\n},\n\"description\": {\n\"type\": \"string\"\n},\n\"labels\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"string\"\n}\n},\n\"administrative\": {\n\"type\": \"boolean\"\n},\n\"allow_promotion\": {\n\"type\": \"boolean\"\n},\n\"auto_deploy\": {\n\"type\": \"boolean\"\n},\n\"auto_retry\": {\n\"type\": \"boolean\"\n},\n\"is_disabled\": {\n\"type\": \"boolean\"\n},\n\"local_preview_enabled\": {\n\"type\": \"boolean\"\n},\n\"protect_from_deletion\": {\n\"type\": \"boolean\"\n},\n\"runner_image\": {\n\"type\": \"string\"\n},\n\"space\": {\n\"type\": \"string\",\n\"minLength\": 1\n},\n\"worker_pool\": {\n\"type\": \"string\"\n},\n\"attachments\": {\n\"$ref\": \"#/definitions/attachment\"\n},\n\"environment\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"properties\": {\n\"mounted_files\": {\n\"type\": \"array\",\n\"items\": {\n\"$ref\": \"#/definitions/mounted_file\"\n}\n},\n\"variables\": {\n\"type\": \"array\",\n\"items\": {\n\"$ref\": \"#/definitions/variable\"\n}\n}\n}\n},\n\"hooks\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"properties\": {\n\"apply\": {\n\"$ref\": \"#/definitions/before_after_hook\"\n},\n\"init\": {\n\"$ref\": \"#/definitions/before_after_hook\"\n},\n\"plan\": {\n\"$ref\": \"#/definitions/before_after_hook\"\n},\n\"perform\": {\n\"$ref\": \"#/definitions/before_after_hook\"\n},\n\"destroy\": {\n\"$ref\": \"#/definitions/before_after_hook\"\n},\n\"run\": {\n\"$ref\": \"#/definitions/after_hook\"\n}\n}\n},\n\"schedules\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"properties\": {\n\"drift\": {\n\"$ref\": \"#/definitions/drift_detection_schedule\"\n},\n\"tasks\": {\n\"type\": \"array\",\n\"items\": {\n\"$ref\": \"#/definitions/task_schedule\"\n}\n},\n\"delete\": {\n\"$ref\": \"#/definitions/delete_schedule\"\n}\n}\n},\n\"vcs\": {\n\"type\": \"object\",\n\"oneOf\": [\n{\n\"additionalProperties\": false,\n\"required\": [\n\"branch\",\n\"provider\",\n\"repository\"\n],\n\"properties\": {\n\"branch\": {\n\"type\": \"string\",\n\"minLength\": 1\n},\n\"project_root\": {\n\"type\": \"string\"\n},\n\"provider\": {\n\"type\": \"string\",\n\"enum\": [\n\"GITHUB\",\n\"GITLAB\",\n\"BITBUCKET_DATACENTER\",\n\"BITBUCKET_CLOUD\",\n\"GITHUB_ENTERPRISE\",\n\"SHOWCASE\",\n\"AZURE_DEVOPS\"\n]\n},\n\"namespace\": {\n\"type\": \"string\"\n},\n\"repository\": {\n\"type\": \"string\",\n\"minLength\": 1,\n\"description\": \"The name of the repository.\"\n}\n}\n},\n{\n\"additionalProperties\": false,\n\"required\": [\n\"branch\",\n\"provider\",\n\"repository_url\"\n],\n\"properties\": {\n\"branch\": {\n\"type\": \"string\",\n\"minLength\": 1\n},\n\"project_root\": {\n\"type\": \"string\"\n},\n\"provider\": {\n\"type\": \"string\",\n\"enum\": [\n\"RAW_GIT\"\n]\n},\n\"repository\": {\n\"type\": \"string\",\n\"description\": \"The name of the repository. If not provided, it'll be extracted from the repository_url.\"\n},\n\"namespace\": {\n\"type\": \"string\",\n\"description\": \"The namespace of the repository. If not provided, it'll be extracted from the repository_url.\"\n},\n\"repository_url\": {\n\"type\": \"string\",\n\"minLength\": 1,\n\"description\": \"The URL of the repository. This is only used for the 'GIT' provider.\"\n}\n}\n}\n]\n},\n\"vendor\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"properties\": {\n\"ansible\": {\n\"$ref\": \"#/definitions/ansible_vendor\"\n},\n\"cloudformation\": {\n\"$ref\": \"#/definitions/cloudformation_vendor\"\n},\n\"kubernetes\": {\n\"$ref\": \"#/definitions/kubernetes_vendor\"\n},\n\"pulumi\": {\n\"$ref\": \"#/definitions/pulumi_vendor\"\n},\n\"terraform\": {\n\"$ref\": \"#/definitions/terraform_vendor\"\n}\n}\n}\n}\n},\n\"attachment\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"properties\": {\n\"contexts\": {\n\"type\": \"array\",\n\"items\": {\n\"$ref\": \"#/definitions/context\"\n}\n},\n\"clouds\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"properties\": {\n\"aws\": {\n\"$ref\": \"#/definitions/aws_attachment\"\n},\n\"azure\": {\n\"$ref\": \"#/definitions/azure_attachment\"\n}\n}\n},\n\"policies\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"string\"\n}\n}\n}\n},\n\"aws_attachment\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"id\",\n\"read\",\n\"write\"\n],\n\"properties\": {\n\"id\": {\n\"type\": \"string\"\n},\n\"read\": {\n\"type\": \"boolean\"\n},\n\"write\": {\n\"type\": \"boolean\"\n}\n}\n},\n\"azure_attachment\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"id\",\n\"read\",\n\"write\",\n\"subscription_id\"\n],\n\"properties\": {\n\"id\": {\n\"type\": \"string\"\n},\n\"read\": {\n\"type\": \"boolean\"\n},\n\"write\": {\n\"type\": \"boolean\"\n},\n\"subscription_id\": {\n\"type\": \"string\"\n}\n}\n},\n\"context\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"id\"\n],\n\"properties\": {\n\"id\": {\n\"type\": \"string\"\n},\n\"priority\": {\n\"type\": \"integer\",\n\"minimum\": 0\n}\n}\n},\n\"mounted_file\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"path\",\n\"content\"\n],\n\"properties\": {\n\"path\": {\n\"type\": \"string\"\n},\n\"content\": {\n\"type\": \"string\"\n},\n\"description\": {\n\"type\": \"string\"\n},\n\"secret\": {\n\"type\": \"boolean\"\n}\n}\n},\n\"variable\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"name\",\n\"value\"\n],\n\"properties\": {\n\"name\": {\n\"type\": \"string\"\n},\n\"value\": {\n\"type\": \"string\"\n},\n\"description\": {\n\"type\": \"string\"\n},\n\"secret\": {\n\"type\": \"boolean\"\n}\n}\n},\n\"after_hook\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"properties\": {\n\"after\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"string\"\n},\n\"minLength\": 1\n}\n}\n},\n\"before_after_hook\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"properties\": {\n\"before\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"string\"\n},\n\"minLength\": 1\n},\n\"after\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"string\"\n},\n\"minLength\": 1\n}\n}\n},\n\"drift_detection_schedule\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"cron\",\n\"reconcile\"\n],\n\"properties\": {\n\"cron\": {\n\"type\": \"array\",\n\"items\": {\n\"$ref\": \"#/definitions/cron_schedule\",\n\"maxLength\": 1\n}\n},\n\"reconcile\": {\n\"type\": \"boolean\"\n},\n\"ignore_state\": {\n\"type\": \"boolean\"\n},\n\"timezone\": {\n\"type\": \"string\"\n}\n}\n},\n\"task_schedule\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"command\"\n],\n\"oneOf\": [\n{\n\"required\": [\n\"command\",\n\"cron\"\n]\n},\n{\n\"required\": [\n\"command\",\n\"timestamp_unix\"\n]\n}\n],\n\"properties\": {\n\"command\": {\n\"type\": \"string\",\n\"minLength\": 1\n},\n\"cron\": {\n\"type\": \"array\",\n\"items\": {\n\"$ref\": \"#/definitions/cron_schedule\",\n\"minLength\": 1\n}\n},\n\"timestamp_unix\": {\n\"type\": \"number\",\n\"minimum\": 1600000000\n},\n\"timezone\": {\n\"type\": \"string\"\n}\n}\n},\n\"cron_schedule\": {\n\"type\": \"string\",\n\"pattern\": \"^(\\\\*|\\\\d+|\\\\d+-\\\\d+|\\\\d+\\\\/\\\\d+)(\\\\s+(\\\\*|\\\\d+|\\\\d+-\\\\d+|\\\\d+\\\\/\\\\d+)){4}$\"\n},\n\"delete_schedule\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"timestamp_unix\"\n],\n\"properties\": {\n\"delete_resources\": {\n\"type\": \"boolean\"\n},\n\"timestamp_unix\": {\n\"type\": \"number\",\n\"minimum\": 1600000000\n}\n}\n},\n\"ansible_vendor\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"playbook\"\n],\n\"properties\": {\n\"playbook\": {\n\"type\": \"string\",\n\"minLength\": 1\n}\n}\n},\n\"cloudformation_vendor\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"entry_template_file\",\n\"template_bucket\",\n\"stack_name\",\n\"region\"\n],\n\"properties\": {\n\"entry_template_file\": {\n\"type\": \"string\",\n\"minLength\": 1\n},\n\"template_bucket\": {\n\"type\": \"string\",\n\"minLength\": 1\n},\n\"stack_name\": {\n\"type\": \"string\",\n\"minLength\": 1\n},\n\"region\": {\n\"type\": \"string\",\n\"minLength\": 1\n}\n}\n},\n\"kubernetes_vendor\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"namespace\"\n],\n\"properties\": {\n\"namespace\": {\n\"type\": \"string\",\n\"minLength\": 1\n}\n}\n},\n\"pulumi_vendor\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"stack_name\",\n\"login_url\"\n],\n\"properties\": {\n\"stack_name\": {\n\"type\": \"string\",\n\"minLength\": 1\n},\n\"login_url\": {\n\"type\": \"string\",\n\"minLength\": 1\n}\n}\n},\n\"terraform_vendor\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"required\": [\n\"manage_state\"\n],\n\"properties\": {\n\"version\": {\n\"type\": \"string\"\n},\n\"workspace\": {\n\"type\": \"string\"\n},\n\"use_smart_sanitization\": {\n\"type\": \"boolean\"\n},\n\"manage_state\": {\n\"type\": \"boolean\"\n},\n\"workflow_tool\": {\n\"type\": \"string\",\n\"enum\": [\n\"TERRAFORM_FOSS\",\n\"CUSTOM\"\n]\n}\n}\n},\n\"options\": {\n\"type\": \"object\",\n\"additionalProperties\": false,\n\"properties\": {\n\"trigger_run\": {\n\"type\": \"boolean\"\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"concepts/configuration/index.html","title":"Configuration","text":"<p>While Spacelift stacks typically link source code with infrastructure resources, it is often the broadly defined configuration that serves as the glue that's keeping everything together. These can be that access credentials, backend definitions or user-defined variables affecting the behavior of resource definitions found in the \"raw\" source code.</p> <p>This section focuses on three aspects of configuration, each of which warrants its own help article:</p> <ul> <li>Direct stack environment, that is environment variables and mounted files;</li> <li>Contexts, that is environments (often partially defined) shared between stacks and/or Terraform modules;</li> <li>Runtime configuration as defined in the <code>.spacelift/config.yml</code> file;</li> </ul>"},{"location":"concepts/configuration/index.html#a-general-note-on-precedence","title":"A general note on precedence","text":"<p>Some configuration settings can be defined on multiple levels. If they're over-defined (the same setting is defined multiple times), the end result will depend on generic rules of precedence. These rules will be the same for all applicable settings:</p> <ul> <li>stack-specific runtime configuration will take the highest precedence;</li> <li>configuration defined directly (either through the environment, or settings) on the stack will go second;</li> <li>common runtime configuration will go next;</li> <li>anything defined at the context level will take the lowest precedence - furthermore, contexts can be attached with a priority level further defining the exact precedence;</li> </ul>"},{"location":"concepts/configuration/context.html","title":"Context","text":"<p>Note</p> <p>For the sake of brevity we'll be using the term projects to refer to both stacks and modules.</p>"},{"location":"concepts/configuration/context.html#introduction","title":"Introduction","text":"<p>On a high level, context is a bundle of configuration elements (environment variables and mounted files) independent of any stack that can be managed separately and attached to as many or as few stacks as necessary. Contexts are only directly accessible to administrators from the account view:</p> <p></p> <p>The list of contexts merely shows the name and description of the context. Clicking on the name allows you to edit it.</p>"},{"location":"concepts/configuration/context.html#management","title":"Management","text":"<p>Managing a context is quite straightforward - you can create, edit, attach, detach and delete it. The below paragraphs will focus on doing that through the web GUI but doing it programmatically using our Terraform provider is an attractive alternative.</p>"},{"location":"concepts/configuration/context.html#creating","title":"Creating","text":"<p>As an account administrator you can create a new context from the Contexts screen as seen on the above screenshot by pressing the Add context button:</p> <p></p> <p>This takes you to a simple form where the only inputs are name and description:</p> <p></p> <p>The required name is what you'll see in the context list and in the dropdown when attaching the context. Make sure that it's informative enough to be able to immediately communicate the purpose of the context, but short enough so that it fits nicely in the dropdown, and no important information is cut off.</p> <p>The optional description is completely free-form and it supports Markdown. This is a good place perhaps for a thorough explanation of the purpose of the stack, perhaps a link or two, and/or a funny GIF. In the web GUI this description will only show on the Contexts screen so it's not a big deal anyway.</p> <p>Warning</p> <p>Based on the original name, Spacelift generates an immutable slug that serves as a unique identifier of this context. If the name and the slug diverge significantly, things may become confusing.</p> <p>So even though you can change the context name at any point, we strongly discourage all non-trivial changes.</p>"},{"location":"concepts/configuration/context.html#editing","title":"Editing","text":"<p>Editing the context is only a little more exciting. You can edit the context from its dedicated view by pressing the Edit button:</p> <p></p> <p>This switches the context into editing mode where you can change the name and description but also manage configuration elements the same way you'd do for the stack environment, only much simpler - without overrides and computed values:</p> <p></p>"},{"location":"concepts/configuration/context.html#attaching-and-detaching","title":"Attaching and detaching","text":"<p>Attaching and detaching contexts actually happens from the stack management view. To attach a context, select the Contexts tab. This should show you a dropdown with all the contexts available for attaching, and a slider to set the priority of the attachment:</p> <p></p> <p>Info</p> <p>A context can only be attached once to a given stack, so if it's already attached, it will not be visible in the dropdown menu.</p> <p>OK, let's attach the context with priority 0 and see what gives:</p> <p></p> <p>Now this attached context will also contribute to the stack environment...</p> <p></p> <p>...and be visible on the list of attached contexts:</p> <p></p> <p>In order to detach the context, you can just press the Detach button and the context will stop contributing to the stack's environment:</p> <p></p>"},{"location":"concepts/configuration/context.html#a-note-on-priority","title":"A note on priority","text":"<p>You may be wondering what the priority slider is for. A priority is a property of context-stack relationship - in fact, the only property. All the contexts attached to a stack are sorted by priority (lowest first), though values don't need to be unique. This ordering establishes precedence rules between contexts should there be a conflict and multiple contexts define the same value.</p>"},{"location":"concepts/configuration/context.html#deleting","title":"Deleting","text":"<p>Deleting a context is straightforward - by pressing the Delete button in the context view you can get rid of an unnecessary context:</p> <p></p> <p>Warning</p> <p>Deleting a context will also automatically detach it from all the projects it was attached to. Make sure you only delete contexts that are no longer useful. For security purposes we do not store historical stuff and actually remove the deleted data from all of our data storage systems.</p>"},{"location":"concepts/configuration/context.html#use-cases","title":"Use cases","text":"<p>We can see two main use cases for contexts, depending on whether the context data is supplied externally or produced by Spacelift.</p>"},{"location":"concepts/configuration/context.html#shared-setup","title":"Shared setup","text":"<p>If the data is external to Spacelift, it's likely that this is a form of shared setup - that is, configuration elements that are common to multiple stacks, and grouped as a context for convenience. One example of this use case is cloud provider configuration, either for Terraform or Pulumi. Instead of attaching the same values - some of them probably secret and pretty sensitive - to individual stacks, contexts allow you to define those once and then have admins attach them to the stacks that need them.</p> <p>A variation of this use case is collections of Terraform input variables that may be shared by multiple stacks - for example things relating to a particular system environment (staging, production etc). In this case the collection of variables can specify things like environment name, DNS domain name or a reference to it (eg. zone ID), tags, references to provider accounts and similar settings. Again, instead of setting these on individual stacks, an admin can group them into a context, and attach to the eligible stacks.</p>"},{"location":"concepts/configuration/context.html#remote-state-alternative-terraform-specific","title":"Remote state alternative (Terraform-specific)","text":"<p>If the data in the context is produced by one or more Spacelift stacks, contexts can be an attractive alternative to the Terraform remote state. In this use case, contexts can serve as outputs for stacks that can be consumed by (attached to) other stacks. So, instead of exposing the entire state, a stack can use Spacelift Terraform provider to define values on a context - either managed by the same stack , or managed externally. Managing a context externally can be particularly useful when multiple stacks contribute to a particular context.</p> <p>Info</p> <p>In order to use the Terraform provider to define contexts or its configuration elements the stack has to be marked as administrative.</p> <p>As an example of one such use case, let's imagine an organization where shared infrastructure (VPC, DNS, compute cluster etc.) is centrally managed by a DevOps team, which exposes it as a service to be used by individual product development teams. In order to be able to use the shared infrastructure, each team needs to address multiple entities that are generated by the central infra repo. In vanilla Terraform one would likely use remote state provider, but that might expose secrets and settings the DevOps team would rather keep it to themselves. Using a context on the other hand allows the team to decide (and hopefully document) what constitutes their \"external API\".</p> <p>The proposed setup for the above use case would involve two administrative stacks - one to manage all the stacks, and the other for the DevOps team. The management stack would programmatically define the DevOps one, and possibly also its context. The DevOps team would receive the context ID as an input variable, and use it to expose outputs as <code>spacelift_environment_variable</code> and/or <code>spacelift_mounted_file</code> resources. The management stack could then simply attach the context populated by the DevOps stack to other stacks it defines and manages.</p>"},{"location":"concepts/configuration/context.html#extending-terraform-cli-configuration-terraform-specific","title":"Extending Terraform CLI Configuration (Terraform-specific)","text":"<p>For some of our Terraform users, a convenient way to configure the Terraform CLI behavior is through customizing the <code>~/.terraformrc</code> file.</p> <p>Spacelift allows you to extend terraform CLI configuration through the use of mounted files.</p>"},{"location":"concepts/configuration/environment.html","title":"Environment","text":"<p>If you take a look at the Environment screen of a stack you will notice it's pretty busy - in fact it's the second busiest view in Spacelift (run being the undisputed winner). Ultimately though, all the records here are either environment variables or mounted files. The main part of the view represents the synthetic outcome determining what your run will \"see\" when executed. If this does not make sense yet, please hang on and read the remainder of this article.</p> <p></p>"},{"location":"concepts/configuration/environment.html#environment-variables","title":"Environment variables","text":"<p>The concept of environment variables is instinctively understood by all programmers. It's represented as a key-value mapping available to all processes running in a given environment. Both with Pulumi and Terraform, environment variables are frequently used to configure providers. Additionally, when prefixed with <code>TF_VAR_</code> they are used in Terraform to use environment variables as Terraform input variables.</p> <p>Info</p> <p>Spacelift does not provide a dedicated mechanism of defining Terraform input variables because the combination of <code>TF_VAR_</code> environment variables and mounted files should cover all use cases without the need to introduce an extra entity.</p> <p>Adding an environment variable is rather straightforward - don't worry yet about the visibility (difference between plain and secret variables). This is described in a separate section:</p> <p></p> <p>...and so is editing:</p> <p></p> <p></p>"},{"location":"concepts/configuration/environment.html#environment-variable-interpolation","title":"Environment variable interpolation","text":"<p>Note that environment variables can refer to other environment variables using simple interpolation. For example, if you have an environment variable <code>FOO</code> with a value of <code>bar</code> you can use it to define another environment variable <code>BAZ</code> as <code>${FOO}-baz</code> which will result in <code>bar-baz</code> being set as the value of <code>BAZ</code>. This interpolation is lazily and dynamically evaluated on the worker, and will work between environment variables defined in different ways, including contexts.</p>"},{"location":"concepts/configuration/environment.html#computed-values","title":"Computed values","text":"<p>You will possibly notice some environment variables being marked as <code>&lt;computed&gt;</code>, which means that their value is only computed at runtime. These are not directly set on the stack but come from various integrations - for example, AWS credentials (<code>AWS_ACCESS_KEY_ID</code> and friends) are set by the AWS integration and <code>SPACELIFT_API_TOKEN</code> is injected into each run to serve a number of purposes.</p> <p>You cannot set a computed value but you can override it - that is, explicitly set an environment variable on a stack that has the same name as the variable that comes from integration. This is due to precedence rules that warrant its own dedicated section.</p> <p>Overriding a computed value is almost like editing a regular stack variable, although worth noticing is Override replacing Edit and the lack of Delete action:</p> <p></p> <p>When you click Override, you can replace the value computed at runtime with a static one:</p> <p></p> <p>Note how it becomes a regular write-only variable upon saving:</p> <p></p> <p>If you delete this variable, it will again be replaced by the computed one. If you want to get rid of the computed variable entirely, you will need to disable the integration that originally led to its inclusion in this list.</p>"},{"location":"concepts/configuration/environment.html#spacelift-environment","title":"Spacelift environment","text":"<p>The Spacelift environment section lists a special subset of computed values that are injected into each run and that provide some Spacelift-specific metadata about the context of the job being executed. These are prefixed so that they can be used directly as input variables to Terraform configuration, and their names always clearly suggest the content:</p> Name Description TF_VAR_spacelift_account_name Spacelift account name TF_VAR_spacelift_commit_branch \u00a0The commit branch in Version Control System TF_VAR_spacelift_commit_sha \u00a0The sha of the commit in Version Control System \u00a0TF_VAR_spacelift_local_preview \u00a0If the local-preview feature is enabled or not in the stack \u00a0TF_VAR_spacelift_project_root The project root defined in the stack for the Version Control System TF_VAR_spacelift_repository \u00a0The name of the Version Control System repository TF_VAR_spacelift_run_id \u00a0The Run ID for the current run \u00a0TF_VAR_spacelift_run_state \u00a0The state of the current run TF_VAR_spacelift_run_trigger \u00a0The trigger information of the run TF_VAR_spacelift_run_type \u00a0The type of the current run TF_VAR_spacelift_stack_branch \u00a0The tracked branch for the stack \u00a0TF_VAR_spacelift_stack_id \u00a0The ID of the Stack TF_VAR_spacelift_stack_labels \u00a0The labels attached to the Stack \u00a0TF_VAR_spacelift_workspace_root \u00a0The workspace root information <p>Info</p> <p>Unless you know exactly what you're doing, we generally discourage overriding these dynamic variables, to avoid confusion.</p>"},{"location":"concepts/configuration/environment.html#per-stage-environment-variables","title":"Per-stage environment variables","text":"<p>The Spacelift flow can be broken down into a number of stages - most importantly:</p> <ul> <li>Initializing, where we prepare the workspace;</li> <li>Planning, which calculates the changes;</li> <li>Applying, which makes the actual changes;</li> </ul> <p>In this model, only the Applying phase makes any actual changes to your resources and your state and needs the credentials that support it. Yet frequently, the practice is to pass the same credentials to all stages. The reason for that is either the lack of awareness or - more often - the limitations in the tooling. Depending on your flow, this may be a potential security issue because even if you manually review every job before it reaches the Applying stage, the Planning phase can do a lot of damage.</p> <p>Spacelift supports a more security-conscious approach by allowing users to define variables that are passed to read (in practice, everything except for Applying) and write stages. By default, we pass an environment variable to all stages, but prefixes can be used to change the default behavior.</p> <p>An environment variable whose name starts with the <code>ro_</code> prefix is only passed to read stages but not to the write (Applying) stage. On the other hand, an environment variable whose name starts with the <code>wo_</code> prefix is only passed to the write (Applying) stage but not to the read ones.</p> <p>Combining the two prefixes makes it easy to create flows that limit the exposure of admin credentials to the code that has been thoroughly reviewed. The example below uses a <code>GITHUB_TOKEN</code> environment variable used by the GitHub Terraform provider variable split into two separate environment variables:</p> <p></p> <p>The first token will potentially be exposed to less-trusted code, so it makes sense to create it with read-only permissions. The second token on the other hand will only be exposed to the reviewed code and can be given write or admin permissions.</p> <p>A similar approach can be used for AWS, GCP, Azure, or any other cloud provider credentials.</p>"},{"location":"concepts/configuration/environment.html#mounted-files","title":"Mounted files","text":"<p>Every now and then an environment variable is not what you need - you need a file instead. Terraform Kubernetes provider is a great example - one of the common ways of configuring it involves setting a <code>KUBECONFIG</code> variable pointing to the actual config file which needs to be present in your workspace as well.</p> <p>It's almost like creating an environment variable, though instead of typing (or pasting) the value you'll be uploading a file:</p> <p></p> <p></p> <p>Info</p> <p>Notice how you can give your file a name that's different to the name of the uploaded entity. In fact, you can use <code>/</code> characters in the file path to nest it deeper in directory tree - for example <code>a/b/c/d/e.json</code> is a perfectly valid file path.</p> <p>Similar to environment variables, mounted files can have different visibility settings - you can learn more about it here. One thing to note here is that plaintext files can be downloaded back straight from the UI or API while secret ones will only be visible to the run executed for the stack.</p> <p>Info</p> <p>Mounted files are limited to 2 MB in size. If you need to inject larger files into your workspace, we suggest that you make them part of the Docker runner image, or retrieve them dynamically using something like wget or curl.</p>"},{"location":"concepts/configuration/environment.html#project-structure","title":"Project structure","text":"<p>When discussing mounted files, it is important to understand the structure of the Spacelift workspace. Every Spacelift workload gets a dedicated directory <code>/mnt/workspace/</code>, which also serves as a root for all the mounted files.</p> <p>Your Git repository is cloned into <code>/mnt/workspace/source/</code>, which also serves as the working directory for your project, unless explicitly overridden by the project root configuration setting (either on the stack level or on in the runtime configuration).</p> <p>Warning</p> <p>Mounted files may be put into <code>/mnt/workspace/source/</code> as well and it's a legitimate use case, for example, to dynamically inject backend settings or even add extra infra definitions. Just beware of path clashes as mounted files will override your project source code in case of conflict. Sometimes this is what you want, sometimes not.</p>"},{"location":"concepts/configuration/environment.html#attached-contexts","title":"Attached contexts","text":"<p>While contexts are important enough to warrant their own dedicated article, it's also crucial to understand how they interact with environment variables and mounted files set directly on the stack, as well as with computed values. Perhaps you've noticed the blue labels on one of the earlier screenshots. If you haven't, here they are again, with a proper highlight:</p> <p></p> <p>The highlighted label is the name of the attached context that supplies those values. The sorted list of attached contexts is located below the calculated environment view, and each entry can be unfurled to see its exact content.</p> <p>Similar to computed values, those coming from contexts can also be overridden. Here's an example:</p> <p></p> <p></p> <p></p> <p>Note how we can now Delete the variable - this would revert it to the value defined by the context. Contexts can both provide environment variables as well as mounted files, and both can be overridden directly on the stack.</p> <p>Info</p> <p>If you want to get rid of the context-provided variable or file entirely, you will need to detach the context itself.</p>"},{"location":"concepts/configuration/environment.html#a-note-on-visibility","title":"A note on visibility","text":"<p>Perhaps you may have noticed how environment variables and mounted files come in two flavors - plain and secret. Here they are in the form for the new environment variable:</p> <p></p> <p>...and here they are in the form for the new mounted file:</p> <p></p> <p>Functionally, the difference between the two is pretty simple - plain values are accessible in the web GUI and through the API, and secret ones aren't - they're only made available to Runs and Tasks. Here's an example of two environment variables in the GUI - one plain, and one secret (also referred to as write-only):</p> <p></p> <p>Mounted files are similar - plain can be downloaded from the web GUI or through the API, and secret can't. Here's the difference in the GUI:</p> <p></p> <p>While the content of secret (write-only) environment variables and mounted files is not accessible through the GUI or API, the checksums are always available so if you have the value handy and just want to check if that's the same value as the one set in Spacelift, you can compare its checksum with the one reported by us - check out the most recent GraphQL API schema for more details.</p> <p>Info</p> <p>Though all of our data is encrypted both at rest and in transit, secret (write-only) values enjoy two extra layers of protection.</p>"},{"location":"concepts/configuration/runtime-configuration/index.html","title":"Runtime configuration","text":"<p>The runtime configuration is an optional setup applied to individual runs instead of being global to the stack. It's defined in <code>.spacelift/config.yml</code> YAML file at the root of your repository. A single file is used to define settings for all stacks associated with its host Git repository, so the file structure looks like this:</p> .spacelift/config.yml<pre><code>version: \"1\"\n\nstack_defaults:\nrunner_image: your/first:runner\n# Note that tflint is not installed by\n# default - this example assumes that your\n# runner image has this available.\nbefore_init:\n- terraform fmt -check\n- tflint\n\n# Note that every field in the configuration is\n# optional, and has a reasonable default. This file\n# allows you to override those defaults, and you can\n# merely override individual fields.\nstacks:\n# The key of is the immutable slug of your stack\n# which you will find in the URL.\nbabys-first-stack: &amp;shared\nbefore_apply:\n- hostname\nproject_root: infra\nterraform_version: 0.12.4\nbabys-second-stack:\n&lt;&lt;: *shared\nterraform_version: 0.13.0\nenvironment:\nAWS_REGION: eu-west-1\n</code></pre> <p>The top level of the file contains three keys - <code>version</code> which in practice is currently ignored but may be useful in the future, <code>stacks</code> containing a mapping of immutable stack id to the stack configuration block and <code>stack_defaults</code>, containing the defaults common to all stacks using this source code repository. Note that corresponding stack-specific settings will override any stack defaults.</p> <p>Considering the precedence of settings, below is the order that will be followed, starting from the most important to the least important:</p> <ol> <li>The configuration for a specified stack defined in config.yml</li> <li>The stack configuration set in the Spacelift UI.</li> <li>The stack defaults defined config.yml</li> </ol> <p>In cases where there is no stack slug defined in the config, only the first two sources are considered:</p> <ol> <li>The stack configuration set in the Spacelift UI</li> <li>The stack defaults defined in config.yml</li> </ol> <p>Info</p> <p>Since we adopted everyone's favorite data serialization format, you can use all the YAML shenanigans you can think of - things like anchors and inline JSON can keep your config DRY and neat.</p>"},{"location":"concepts/configuration/runtime-configuration/index.html#purpose-of-runtime-configuration","title":"Purpose of runtime configuration","text":"<p>The whole concept of runtime configuration may initially sound unnecessary, but it ultimately allows flexibility that would otherwise be hard to achieve. In general, its purpose is to preview effects of changes not related to the source code (eg. Terraform or Pulumi version upgrades, variable changes etc.), before they become an established part of your infra.</p> <p>While stack environment applies both to tracked and non-tracked branches, a runtime configuration change can be pushed to a feature branch, which triggers proposed runs allowing you to preview the changes before they have a chance to affect your state.</p> <p>Info</p> <p>If the runtime configuration file is not present or does not contain your stack, default values are used - refer to each setting for its respective default.</p>"},{"location":"concepts/configuration/runtime-configuration/index.html#stacks-configuration-block","title":"<code>Stacks</code> configuration block","text":""},{"location":"concepts/configuration/runtime-configuration/index.html#before_-and-after_-hooks","title":"<code>before_</code> and <code>after_</code> hooks","text":"<p>Info</p> <p>Each collection defaults to an empty array.</p> <p>These scripts allow customizing the Spacelift workflow - see the relevant documentation here. The following are available:</p> <ul> <li><code>before_init</code></li> <li><code>after_init</code></li> <li><code>before_plan</code></li> <li><code>after_plan</code></li> <li><code>before_apply</code></li> <li><code>after_apply</code></li> <li><code>before_perform</code></li> <li><code>after_perform</code></li> <li><code>before_destroy</code></li> <li><code>after_destroy</code></li> <li><code>after_run</code></li> </ul>"},{"location":"concepts/configuration/runtime-configuration/index.html#environment-map","title":"<code>environment</code> map","text":"<p>Info</p> <p>Defaults to an empty map.</p> <p>The environment allows you to declaratively pass some environment variables to the runtime configuration of the Stack. In case of a conflict, these variables will override both the ones passed via attached Contexts and those directly set in Stack's environment.</p>"},{"location":"concepts/configuration/runtime-configuration/index.html#project_root-setting","title":"<code>project_root</code> setting","text":"<p>Info</p> <p>Defaults to an empty string, pointing to the working directory for the run.</p> <p>Project root is the path of your project directory inside the Hub repository. You can use this setting to point Spacelift to the right place if the repo contains source code for multiple stacks in various folders or serves multiple purposes like those increasingly popular monorepos combining infrastructure definitions with source code, potentially even for multiple applications.</p>"},{"location":"concepts/configuration/runtime-configuration/index.html#runner_image-setting","title":"<code>runner_image</code> setting","text":"<p>Info</p> <p>Defaults to <code>public.ecr.aws/spacelift/runner-terraform:latest</code>. See this section for more details.</p> <p>The runner image is the Docker image used to run your workloads. By making it a runtime setting, Spacelift allows testing the image before it modifies your infrastructure.</p>"},{"location":"concepts/configuration/runtime-configuration/index.html#terraform_version-setting","title":"<code>terraform_version</code> setting","text":"<p>Info</p> <p>Defaults to the latest known supported Terraform version.</p> <p>This setting is only valid on Terraform stacks and specifies the Terraform version that the run will use. The main use case is testing a newer version of Terraform before you use it to change the state since the way back is very hard. This version can only be equal to or higher than the one already used to apply state changes. For more details on Terraform version management, please refer to its dedicated help section.</p> <p>Pulumi version management is based on Docker images.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html","title":"YAML reference","text":"<p>This document is a reference for the Spacelift configuration keys that are used in the <code>.spacelift/config.yml</code> file to configure one or more Stacks.</p> <p>Warning</p> <p>The <code>.spacelift/config.yml</code> file must be located at the root of your repository, not at the project root.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#stack-settings","title":"Stack settings","text":"<ul> <li><code>version</code></li> <li><code>stack_defaults</code></li> <li><code>stacks</code></li> </ul>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#module-settings","title":"Module settings","text":"<ul> <li><code>version</code></li> <li><code>module_version</code></li> <li><code>test_defaults</code></li> <li><code>tests</code></li> </ul>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#version","title":"<code>version</code>","text":"<p>The version property is optional and currently ignored but for the sake of completeness you may want to set the value to \"1\".</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#stack_defaults","title":"<code>stack_defaults</code>","text":"<p><code>stack_defaults</code> represent default settings that will apply to every stack defined in the id-settings map. Any default setting is overridden by an explicitly set stack-specific value.</p> Key Required Type Description <code>after_apply</code> N list&lt;string&gt; List of commands executed after applying changes. <code>after_destroy</code> N list&lt;string&gt; List of commands executed after destroying managed resources. <code>after_init</code> N list&lt;string&gt; List of commands executed after first interacting with the backend (eg. terraform init). <code>after_perform</code> N list&lt;string&gt; List of commands executed after performing a custom task <code>after_plan</code> N list&lt;string&gt; List of commands executed after planning changes <code>after_run</code> N list&lt;string&gt; List of commands executed after every run, regardless of its outcome <code>before_apply</code> N list&lt;string&gt; List of commands executed before applying changes. <code>before_destroy</code> N list&lt;string&gt; List of commands executed before destroying managed resources. <code>before_init</code> N list&lt;string&gt; List of commands executed before first interacting with the backend (eg. <code>terraform init</code>). <code>before_perform</code> N list&lt;string&gt; List of commands executed before performing a custom task <code>before_plan</code> N list&lt;string&gt; List of commands executed before planning changes <code>environment</code> N map&lt;string, string&gt; Map of extra environment variables and their values passed to the job <code>project_root</code> N string Optional folder inside the repository serving as the root of your stack <code>runner_image</code> N string Name of the custom runner image, if any <code>terraform_version</code> N string For Terraform stacks, Terraform version number to be used"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#stacks","title":"<code>stacks</code>","text":"<p>The stacks section is a map using stack public ID (slug) as keys and stack settings - described in this section - as values. If you're using this mapping together with stack defaults, note that any default setting is overridden by an explicitly set stack-specific value. This is particularly important for list and map fields where one may assume that these are merged. In practice, they're not merged - they're replaced. If you want merging semantics, YAML provides native methods to merge arrays and maps.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#module_version","title":"<code>module_version</code>","text":"<p>Module version is a required string value that must conform to the semantic versioning scheme. Note that pre-releases and builds/nightlies are not supported - only the standard <code>$major.$minor.$patch</code> format will work.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#test_defaults","title":"<code>test_defaults</code>","text":"<p>Test defaults are runtime settings following this scheme that will apply to all test cases for a module.</p>"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#tests","title":"<code>tests</code>","text":"<p>The <code>tests</code> section represents a list of test cases for a module, each containing the standard runtime settings in addition to the test-specific settings:</p> Key Required Type Description name Y string Unique name of the test case negative N bool Indicates whether the test is negative (expected to fail) id N string Unique identifier of the test case which can be used to refer to the test case depends_on N list&lt;string&gt; List of test case <code>id</code>s this test depends on"},{"location":"concepts/policy/index.html","title":"Policy","text":""},{"location":"concepts/policy/index.html#introduction","title":"Introduction","text":"<p>Policy-as-code is the idea of expressing rules using a high-level programming language and treating them as you normally treat code, which includes version control as well as continuous integration and deployment. This approach extends the infrastructure-as-code approach to also cover the rules governing this infrastructure, and the platform that manages it.</p> <p>Spacelift as a development platform is built around this concept and allows defining policies that involve various decision points in the application. User-defined policies can decide:</p> <ul> <li>Login: who gets to log in to your Spacelift account and with what level of access;</li> <li>Access: who gets to access individual Stacks and with what level of access;</li> <li>Approval: who can approve or reject a run and how a run can be approved;</li> <li>Initialization: which Runs and Tasks can be started;</li> <li>Notification: routing and filtering notifications;</li> <li>Plan: which changes can be applied;</li> <li>Push: how Git push events are interpreted;</li> <li>Task: which one-off commands can be executed;</li> <li>Trigger: what happens when blocking runs terminate;</li> </ul> <p>Please refer to the following table for information on what each policy types returns, and the rules available within each policy.</p> Type Purpose Types Returns Rules Login Allow or deny login, grant admin access Positive and negative <code>boolean</code> <code>allow</code>, <code>admin</code>, <code>deny</code>, <code>deny_admin</code> Access Grant or deny appropriate level of stack access Positive and negative <code>boolean</code> <code>read</code>, <code>write</code>, <code>deny</code>, <code>deny_write</code> Approval Who can approve or reject a run and how a run can be approved Positive and negative <code>boolean</code> <code>approve, reject</code> Initialization Blocks suspicious runs before they start Negative <code>set&lt;string&gt;</code> <code>deny</code> Notification Routes and filters notifications Positive <code>map&lt;string, any&gt;</code> <code>inbox</code>, <code>slack</code>, <code>webhook</code> Plan Gives feedback on runs after planning phase Negative <code>set&lt;string&gt;</code> <code>deny</code>, <code>warn</code> Push Determines how a Git push event is interpreted Positive and negative <code>boolean</code> <code>track</code>, <code>propose</code>, <code>ignore</code>, <code>ignore_track</code>, <code>notrigger</code>, <code>notify</code> Task Blocks suspicious tasks from running Negative <code>set&lt;string&gt;</code> <code>deny</code> Trigger Selects stacks for which to trigger a tracked run Positive <code>set&lt;string&gt;</code> <code>trigger</code> <p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs.</p> <p>If you cannot find what you are looking for, please reach out to our support and we will craft a policy to do exactly what you need.</p>"},{"location":"concepts/policy/index.html#how-it-works","title":"How it works","text":"<p>Spacelift uses an open-source project called Open Policy Agent and its rule language, Rego, to execute user-defined pieces of code we call Policies at various decision points. Policies come in different flavors that we call types, with each type being executed at a different decision point.</p> <p>You can think of policies as snippets of code that receive some JSON-formatted input and are allowed to produce some output in a predefined form. This input normally represents the data that should be enough to make some decision in its context. Each policy type exposes slightly different data, so please refer to their respective schemas for more information.</p> <p>Except for login policies that are global, all other policy types operate on the stack level, and they can be attached to multiple stacks, just as contexts are, which both facilitates code reuse and allows flexibility. Policies only affect stacks they're attached to. Please refer to the relevant section of this article for more information about attaching policies.</p> <p>Multiple policies of the same type can be attached to a single stack, in which case they are evaluated separately to avoid having their code (like local variables and helper rules) affect one another. However, once these policies are evaluated against the same input, their results are combined. So if you allow user login from one policy but deny it from another, the result will still be a denial.</p>"},{"location":"concepts/policy/index.html#version","title":"Version","text":"<p>We update the version of OPA that we are using regularly, to find out the version we are currently running, you can use the following query:</p> <pre><code>query getOPAVersion{\npolicyRuntime {\nopenPolicyAgentVersion\n}\n}\n</code></pre> <p>For more detailed information about the GraphQL API and its integration, please refer to the API documentation.</p>"},{"location":"concepts/policy/index.html#policy-language","title":"Policy language","text":"<p>Rego - the language that we're using to execute policies - is a very elegant, Turing incomplete data query language. It takes a few hours (tops) to get your head around all of its quirks but if you can handle SQL and the likes of <code>jq</code>, you'll find Rego pretty familiar. For each policy, we also give you plenty of examples that you can tweak to achieve your goals, and each of those examples comes with a link allowing you to execute it in the Rego playground.</p>"},{"location":"concepts/policy/index.html#constraints","title":"Constraints","text":"<p>To keep policies functionally pure and relatively snappy, we disabled some Rego built-ins that can query external or runtime data. These are:</p> <ul> <li><code>http.send</code></li> <li><code>opa.runtime</code></li> <li><code>rego.parse_module</code></li> <li><code>time.now_ns</code></li> <li><code>trace</code></li> </ul> <p>Disabling <code>time.now_ns</code> may seem surprising at first - after all, what's wrong with getting the current timestamp? Alas, depending on the current timestamp will make your policies impure and thus tricky to test - and we encourage you to test your policies thoroughly! You will notice though that the current timestamp in Rego-compatible form (Unix nanoseconds) is available as <code>request.timestamp_ns</code> in every policy payload, so please use it instead.</p> <p>Policies must be self-contained and cannot refer to external resources (e.g., files in a VCS repository).</p>"},{"location":"concepts/policy/index.html#return-types","title":"Return Types","text":"<p>There are currently nine types of supported policies and while each of them is different, they have a lot in common. In particular, they can fall into a few groups based on what rules are expected to return.</p>"},{"location":"concepts/policy/index.html#boolean","title":"Boolean","text":"<p>Login and access policies expect rules to return a boolean value (true or false). Each type of policy defines its own set of rules corresponding to different access levels. In these cases, various types of rules can be positive or negative - that is, they can explicitly allow or deny access.</p>"},{"location":"concepts/policy/index.html#set-of-strings","title":"Set of Strings","text":"<p>The second group of policies (initialization, plan, and task) is expected to generate a set of strings that serve as direct feedback to the user. Those rules are generally negative in that they can only block certain actions - it's only their lack that counts as an implicit success.</p> <p>Here's a practical difference between the two types:</p> boolean.rego<pre><code>package spacelift\n\n# This is a simple deny rule.\n# When it matches, no feedback is provided.\ndeny {\n  true\n}\n</code></pre> string.rego<pre><code>package spacelift\n\n# This is a deny rule with string value.\n# When it matches, that value is reported to the user.\ndeny[\"the user will see this\"] {\n  true\n}\n</code></pre> <p>For the policies that generate a set of strings, you want these strings to be both informative and relevant, so you'll see this pattern a lot in the examples:</p> <pre><code>package spacelift\n\nwe_dont_create := { \"scary\", \"resource\", \"types\" }\n\n# This is an example of a plan policy.\ndeny[sprintf(\"some rule violated (%s)\", [resource.address])] {\n  some resource\n  created_resources[resource]\n\n  we_dont_create[resource.type]\n}\n</code></pre>"},{"location":"concepts/policy/index.html#complex-objects","title":"Complex objects","text":"<p>Final group of policies (notification) will generate and return more complex objects. These are typically JSON objects. In terms of syntax they are still very similar to other policies which return sets of strings, but they provide additional information inside the returned decision. For example here is a rule which will return a JSON object to be used when creating a custom notification:</p> <pre><code>package spacelift\n\ninbox[{\n  \"title\": \"Tracked run finished!\",\n  \"body\": sprintf(\"Run ID: %s\", [run.id]),\n  \"severity\": \"INFO\",\n}] {\n  run := input.run_updated.run\n  run.type == \"TRACKED\"\n  run.state == \"FINISHED\"\n}\n</code></pre>"},{"location":"concepts/policy/index.html#helper-functions","title":"Helper Functions","text":"<p>The following helper functions can be used in Spacelift policies:</p> Name Description <code>output := sanitized(x)</code> <code>output</code> is the string <code>x</code> sanitized using the same algorithm we use to sanitize secrets. <code>result := exec(x)</code> Executes the command <code>x</code>. <code>result</code> is an object containing <code>status</code>, <code>stdout</code> and <code>stderr</code>. Only applicable for run initialization policies for private workers."},{"location":"concepts/policy/index.html#creating-policies","title":"Creating policies","text":"<p>There are two ways of creating policies - through the web UI and through the Terraform provider. We generally suggest the latter as it's much easier to manage down the line and allows proper unit testing. Here's how you'd define a plan policy in Terraform and attach it to a stack (also created here with minimal configuration for completeness):</p> <pre><code>resource \"spacelift_stack\" \"example-stack\" {\n  name       = \"Example stack\"\n  repository = \"example-stack\"\n  branch     = \"master\"\n}\n\n# This example assumes that you have Rego policies in a separate\n# folder called \"policies\".\nresource \"spacelift_policy\" \"example-policy\" {\n  name = \"Example policy\"\n  body = file(\"${path.module}/policies/example-policy.rego\")\n  type = \"TERRAFORM_PLAN\"\n}\n\nresource \"spacelift_policy_attachment\" \"example-attachment\" {\n  stack_id  = spacelift_stack.example-stack.id\n  policy_id = spacelift_policy.example-policy.id\n}\n</code></pre> <p>On the other hand, if you want to create a policy in the UI, here's how you could go about that. Note that you must be a Spacelift admin to manage policies. First, go to the Policies screen in your account view, and click the Add policy button:</p> <p></p> <p>This takes you to the policy creation screen where you can choose the type of policy you want to create, and edit its body. For each type of policy you're also given an explanation and a few examples. We'll be creating an access policy that gives members of the Engineering GitHub team read access to a stack:</p> <p></p> <p>Once you're done, click on the Create policy button to save it. Don't worry, policy body is mutable so you'll always be able to edit it if need be.</p>"},{"location":"concepts/policy/index.html#attaching-policies","title":"Attaching policies","text":""},{"location":"concepts/policy/index.html#automatically","title":"Automatically","text":"<p>Policies, with the exception of Login policies, can be automatically attached to stacks using the <code>autoattach:label</code> special label where <code>label</code> is the name of a label attached to stacks and/or modules in your Spacelift account you wish the policy to be attached to.</p>"},{"location":"concepts/policy/index.html#policy-attachment-example","title":"Policy Attachment Example","text":"<p>In the example below, the policy will be automatically attached to all stacks/modules with the label <code>production</code>.</p> <p></p>"},{"location":"concepts/policy/index.html#wildcard-policy-attachments","title":"Wildcard Policy Attachments","text":"<p>In addition to being able to automatically attach policies using a specific label, you can also choose to attach a policy to stacks/modules in the account using a wildcard, for example using <code>autoattach:*</code> as a label on a policy, will attach the policy to all stacks/modules.</p>"},{"location":"concepts/policy/index.html#manually","title":"Manually","text":"<p>In the web UI attaching policies is done in the stack management view, in the Policies tab:</p> <p></p>"},{"location":"concepts/policy/index.html#policy-workbench","title":"Policy workbench","text":"<p>One thing we've noticed while working with policies in practice is that it takes a while to get them right. This is not only because the concept or the underlying language introduce a learning curve, but also because the feedback cycle can be slow: write a plan policy, make a code change, trigger a run, verify policy behavior... rinse and repeat. This can easily take hours.</p> <p>Enter policy workbench. Policy workbench allows you to capture policy evaluation events so that you can adjust the policy independently and therefore shorten the entire cycle. In order to make use of the workbench, you will first need to sample policy inputs.</p>"},{"location":"concepts/policy/index.html#sampling-policy-inputs","title":"Sampling policy inputs","text":"<p>Each of Spacelift's policies supports an additional boolean rule called <code>sample</code>. Returning <code>true</code> from this rule means that the input to the policy evaluation is captured, along with the policy body at the time and the exact result of the policy evaluation. You can for example just capture every evaluation with a simple:</p> <pre><code>sample { true }\n</code></pre> <p>If that feels a bit simplistic and spammy, you can adjust this rule to capture only certain types of inputs. For example, in this case we will only want to capture evaluations that returned in an empty least for <code>deny</code> reasons (eg. with a plan or task policy):</p> <pre><code>sample { count(deny) == 0 }\n</code></pre> <p>You can also sample a certain percentage of policy evaluations. Given that we don't generally allow nondeterministic evaluations, you'd need to depend on a source of randomness internal to the input. In this example we will use the timestamp - note that since it's originally expressed in nanoseconds, we will turn it into milliseconds to get a better spread. We'll also want to sample every 10th evaluation:</p> <pre><code>sample {\n  millis := round(input.request.timestamp_ns / 1e6)\n  millis % 100 &lt;= 10\n}\n</code></pre>"},{"location":"concepts/policy/index.html#why-sample","title":"Why sample?","text":"<p>Capturing all evaluations sounds tempting but it will also be extremely messy. We're only showing 100 most recent evaluations from the past 7 days, so if you capture everything then the most valuable samples can be drowned by irrelevant or uninteresting ones. Also, sampling adds a small performance penalty to your operations.</p>"},{"location":"concepts/policy/index.html#policy-workbench-in-practice","title":"Policy workbench in practice","text":"<p>In order to show you how to work with the policy workbench, we are going to use a task policy that whitelists just two tasks - an innocent <code>ls</code>, and tainting a particular resource. It also only samples successful evaluations, where the list of <code>deny</code> reasons is empty:</p> <p>Info</p> <p>This example comes from our test repo, which gives you hands-in experience with most Spacelift functionalities within 10-15 minutes, depending on whether you like to RTFM or not. We strongly recommend you give it a go.</p> <p></p> <p>In order to get to the policy workbench, first click on the Edit button in the upper right hand corner of the policy screen:</p> <p></p> <p>Then, click on the Show simulation panel link on the right hand side of the screen:</p> <p></p> <p>If your policy has been used evaluated and sampled, your screen should look something like this:</p> <p></p> <p>On the left hand side you have the policy body. On the right hand side there's a dropdown with timestamped evaluations (inputs) of this policy, color-coded for their ultimate outcome. Selecting one of the inputs allows you to simulate the evaluation:</p> <p></p> <p>While running simulations, you can edit both the input and the policy body. If you edit the policy body, or choose an input that has been evaluated with a different policy body, you will get a warning like this:</p> <p></p> <p>Clicking on the Show changes link within that warning shows you the exact difference between the policy body in the editor panel, and the one used for evaluating the selected input:</p> <p></p> <p>Once you're happy with your new policy body, you can click on the Save changes button to make sure that the new body is used for future evaluations.</p>"},{"location":"concepts/policy/index.html#is-it-safe","title":"Is it safe?","text":"<p>Yes, policy sampling is perfectly safe. Session data may contain some personal information like username, name and IP, but that data is only persisted for 7 days. Most importantly, in plan policies the inputs hash all the string attributes of resources, ensuring that no sensitive data leaks through this means.</p> <p></p> <p>Last but not least, the policy workbench - including access to previous inputs - is only available to Spacelift account administrators.</p>"},{"location":"concepts/policy/index.html#testing-policies","title":"Testing policies","text":"<p>Info</p> <p>In the examples for each type of policy we invite you to play around with the policy and its input in the Rego playground. While certainly useful, we won't consider it proper unit testing.</p> <p>The whole point of policy-as-code is being able to handle it as code, which involves everyone's favorite bit - testing. Testing policies is crucial because you don't want them accidentally allow the wrong crowd to do the wrong things.</p> <p>Luckily, Spacelift uses a well-documented and well-supported open source language called Rego, which has built-in support for testing. Testing Rego is extensively covered in their documentation so in this section we'll only look at things specific to Spacelift.</p> <p>Let's define a simple login policy that denies access to non-members, and write a test for it:</p> deny-non-members.rego<pre><code>package spacelift\n\ndeny { not input.session.member }\n</code></pre> <p>You'll see that we simply mock out the <code>input</code> received by the policy:</p> deny-non-members_test.rego<pre><code>package spacelift\n\ntest_non_member {\n    deny with input as { \"session\": { \"member\": false } }\n}\n\ntest_member_not_denied {\n    not deny with input as { \"session\": { \"member\": true } }\n}\n</code></pre> <p>We can then test it in the console using <code>opa test</code> command (note the glob, which captures both the source and its associated test):</p> <pre><code>\u276f opa test deny-non-members*\nPASS: 2/2\n</code></pre> <p>Testing policies that provide feedback to the users is only slightly more complex. Instead of checking for boolean values, you'll be testing for set equality. Let's define a simple run initialization policy that denies commits to a particular branch (because why not):</p> deny-sandbox.rego<pre><code>package spacelift\n\ndeny[sprintf(\"don't push to %s\", [branch])] {\n  branch := input.commit.branch\n  branch == \"sandbox\"\n}\n</code></pre> <p>In the respective test, we will check that the set return by the deny rule either has the expected element for the matching input, or is empty for non-matching one:</p> deny-sandbox_test.rego<pre><code>package spacelift\n\ntest_sandbox_denied {\n  expected := { \"don't push to sandbox\" }\n\n  deny == expected with input as { \"commit\": { \"branch\": \"sandbox\" } }\n}\n\ntest_master_not_denied {\n  expected := set()\n\n  deny == expected with input as { \"commit\": { \"branch\": \"master\" } }\n}\n</code></pre> <p>Again, we can then test it in the console using <code>opa test</code> command (note the glob, which captures both the source and its associated test):</p> <pre><code>\u276f opa test deny-sandbox*\nPASS: 2/2\n</code></pre> <p>Success</p> <p>We suggest you always unit test your policies and apply the same continuous integration principles as with your application code. You can set up a CI project using the vendor of your choice for the same repository that's linked to the Spacelift project that's defining those policies, to get an external validation.</p>"},{"location":"concepts/policy/index.html#policy-flags","title":"Policy flags","text":"<p>By default, each policy is completely self-contained and does not depend on the result of previous policies. There are at times situations where you want to introduce a chain of policies passing some data to one another. Different types of policies have access to different types of data required to make a decision, and you can use policy flags to pass that data (or more likely, a useful digest of that data) between them.</p> <p>Let's take a look at a simple example. Let's say you have a push policy with access to the list of files affected by a push or a PR event. You want to introduce a form of ownership control where changes to different files need approval from different users. For example, a change in the <code>network</code> directory may require approval from the network team, while a change in the <code>database</code> directory needs an approval from the DBAs.</p> <p>Approvals are handled by an approval policy but the problem is that it no longer retains access to the list of affected files. This is a great use case to use flags. Let's have the push policy set arbitrary review flags on the run. This can be a separate push policy as in this example, or part of one of your pre-existing push policies. For the sake of simplicity, the example below will only focus on the <code>network</code> bit.</p> flag_for_review.rego<pre><code>package spacelift\n\nnetwork_review_flag = \"review:network\"\n\nflag[network_review_flag] {\n  startswith(input.push.affected_files[_], \"network/\")\n}\n\nflag[network_review_flag] {\n  startswith(input.pull_request.diff[_], \"network/*\")\n}\n</code></pre> <p>Now, we can introduce a network approval policies using this flag.</p> network-review.rego<pre><code>package spacelift\n\nnetwork_review_required {\n  input.run.flags[_] == \"review:network\"\n}\n\napprove { not network_review_required }\napprove {\n  input.reviews.current.approvals[_].session.teams[_] == \"DBA\"\n}\n</code></pre> <p>There are a few things worth knowing about flags:</p> <ul> <li>They are arbitrary strings and Spacelift makes no assumptions about their format or content.</li> <li>They are immutable. Once set, they cannot be changed or unset;</li> <li>They are passed between policy types. If you have multiple policies of the same type, they will not be able to see each other's flags;</li> <li>They can be set by any policies that explicitly touch a run: push, approval, plan and trigger;</li> <li>They are always accessible through <code>run</code>'s <code>flags</code> property whenever the <code>run</code> resource is present in the input document;</li> </ul> <p>Also worth noting is the fact that flags are shown in the GUI, so even if you're not using them to explicitly pass the data between different types of policies, they can still be useful for debugging purposes. Below is an example of an approval policy exposing decision-making details:</p> <p></p>"},{"location":"concepts/policy/index.html#backwards-compatibility","title":"Backwards-compatibility","text":"<p>Policies, like the rest of Spacelift functionality, are generally kept fully backwards-compatible. Input fields of policies aren't removed and existing policy \"invocation sites\" are kept in place.</p> <p>Occasionally policies might be deprecated, and once unused, disabled, but this is a process in which we work very closely with any affected users to make sure they have ample time to migrate and aren't negatively affected.</p> <p>However, we do reserve the right to add new fields to policy inputs and introduce additional invocation sites. E.g. we could introduce a new input event type to the Push Policy, and existing Push Policies will start getting those events. Thus, users are expected to write their policies in a way that new input types are handled gracefully, by checking for the event type in their rules.</p> <p>For example, in a Push Policy, you might write a rule as follows:</p> backwards-compatibility.rego<pre><code>track {\n  not is_null(input.pull_request)\n  input.pull_request.labels[_] == \"deploy\"\n}\n</code></pre> <p>As you can see, the first line in the <code>track</code> rule makes sure that we only respond to events that contain the pull_request field.</p>"},{"location":"concepts/policy/approval-policy.html","title":"Approval policy","text":"<p>The approval policy allows organizations to create sophisticated run review and approval flows that reflect their preferred workflow, security goals, and business objectives. Without an explicit approval policy, anyone with write access to a stack can create a run (or a task). An approval policy can make this way more granular and contextual.</p> <p>Runs can be reviewed when they enter one of the three states - queued, unconfirmed, or pending review. When a queued run needs approval, it will not be scheduled before that approval is received, and if it is of a blocking type, it will block newer runs from scheduling, too. A queued run that's pending approval can be canceled at any point.</p> <p>Here's an example of a queued run waiting for a human review - note how the last approval policy evaluation returned an Undecided decision. There's also a Review button next to the Cancel button:</p> <p></p> <p>Review can be positive (approve) or negative (reject):</p> <p></p> <p>With a positive review, the approval policy could evaluate to Approve thus unblocking the run:</p> <p></p> <p>When an unconfirmed run needs approval, you will not be able to confirm it until that approval is received. The run can however be discarded at any point:</p> <p></p> <p>In principle, the run review and approval process are very similar to GitHub's Pull Request review, the only exception being that it's the Rego policy (rather than a set of checkboxes and dropdowns) that defines the exact conditions to approve the run.</p> <p>Tip</p> <p>If separate run approval and confirmation steps sound confusing, don't worry. Just think about how GitHub's Pull Requests work - you can approve a PR before merging it in a separate step. A PR approval means \"I'm OK with this being merged\". A run approval means \"I'm OK with that action being executed\".</p>"},{"location":"concepts/policy/approval-policy.html#rules","title":"Rules","text":"<p>Your approval policy can define the following boolean rules:</p> <ul> <li>approve: the run is approved and no longer requires (or allows) review;</li> <li>reject: the run fails immediately;</li> </ul> <p>While the 'approve' rule must be defined in order for the run to be able to progress, it's perfectly valid to not define the 'reject' rule. In that case, runs that look invalid can be cleaned up (canceled or discarded) manually.</p> <p>It's also perfectly acceptable for any given policy evaluation to return 'false' on both 'approve' and 'reject' rules. This only means that the result is yet 'undecided' and more reviews will be necessary to reach the conclusion. A perfect example would be a policy that requires 2 approvals for a given job - the first review is not yet supposed to set the 'approve' value to 'true'.</p> <p>Info</p> <p>Users must have <code>write</code> or <code>admin</code> access to the stack to be able to approve changes.</p>"},{"location":"concepts/policy/approval-policy.html#how-it-works","title":"How it works","text":"<p>When a user reviews the run, Spacelift persists their review and passes it to the approval policy, along with other reviews, plus some information about the run and its stack. The same user can review the same run as many times as they want, but only their newest review will be presented to the approval policy. This mechanism allows you to change your mind, very similar to Pull Request reviews.</p>"},{"location":"concepts/policy/approval-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request will receive:</p> <pre><code>{\n\"reviews\": { // run reviews\n\"current\": { // reviews for the current state\n\"approvals\": [{ // positive reviews\n\"author\": \"string - reviewer username\",\n\"request\": { // request data of the review\n\"remote_ip\": \"string - user IP\",\n\"timestamp_ns\": \"number - review creation Unix timestamp in nanoseconds\",\n},\n\"session\": { // session data of the review\n\"login\": \"string - username of the reviewer\",\n\"name\": \"string - full name of the reviewer\",\n\"teams\": [\"string - names of teams the reviewer was a member of\"]\n},\n\"state\": \"string - the state of the run at the time of the approval\",\n}],\n\"rejections\": [/* negative reviews, see \"approvals\" for schema */]\n},\n\"older\": [/* reviews for previous state(s), see \"current\" for schema */]\n},\n\"run\": { // the run metadata\n\"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n\"branch\": \"string - the branch the run was triggered from\",\n\"changes\": [\n{\n\"action\": \"string enum - added | changed | deleted\",\n\"entity\": {\n\"address\": \"string - full address of the entity\",\n\"name\": \"string - name of the entity\",\n\"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n\"entity_vendor\": \"string - the name of the vendor\",\n\"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n\"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n},\n\"phase\": \"string enum - plan | apply\"\n}\n],\n\"command\": \"string or null, set when the run type is TASK\",\n\"commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n\"creator_session\": {\n\"admin\": \"boolean - is the current user a Spacelift admin\",\n\"creator_ip\": \"string - IP address of the user who created the session\",\n\"login\": \"string - username of the creator\",\n\"name\": \"string - full name of the creator\",\n\"teams\": [\"string - names of teams the creator was a member of\"],\n\"machine\": \"boolean - whether the run was initiated by a human or a machine\"\n},\n\"drift_detection\": \"boolean - is this a drift detection run\",\n\"flags\" : [\"string - list of flags set on the run by other policies\" ],\n\"id\": \"string - the run ID\",\n\"runtime_config\": {\n\"before_init\": [\"string - command to run before run initialization\"],\n\"project_root\": \"string - root of the Terraform project\",\n\"runner_image\": \"string - Docker image used to execute the run\",\n\"terraform_version\": \"string - Terraform version used to for the run\"\n},\n\"state\": \"string - the current run state\",\n\"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n\"type\": \"string - type of the run\",\n\"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n\"user_provided_metadata\": [\n\"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n]\n},\n\"stack\": { // the stack metadata\n\"administrative\": \"boolean - is the stack administrative\",\n\"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n\"branch\": \"string - tracked branch of the stack\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n\"name\": \"string - name of the stack\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Stack, if any\",\n\"repository\": \"string - name of the source GitHub repository\",\n\"state\": \"string - current state of the stack\",\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n\"worker_pool\": {\n\"id\": \"string - the worker pool ID, if it is private\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n\"name\": \"string - name of the worker pool, if it is private\",\n\"public\": \"boolean - is the worker pool public\"\n}\n}\n}\n</code></pre>"},{"location":"concepts/policy/approval-policy.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p>"},{"location":"concepts/policy/approval-policy.html#two-approvals-and-no-rejections-to-approve-an-unconfirmed-run","title":"Two approvals and no rejections to approve an Unconfirmed run","text":"<p>In this example, each Unconfirmed run will require two approvals - including proposed runs triggered by Git events. Additionally, the run should have no rejections. Anyone who rejects the run will need to change their mind in order for the run to go through.</p> <p>Info</p> <p>We suggest requiring more than one review because one approval should come from the run/commit author to indicate that they're aware of what they're doing, especially if their VCS handle is different than their IdP handle. This is something we practice internally at Spacelift.</p> <pre><code>package spacelift\n\napprove { input.run.state != \"UNCONFIRMED\" }\n\napprove {\n  count(input.reviews.current.approvals) &gt; 1\n  count(input.reviews.current.rejections) == 0\n}\n</code></pre> <p>Here's a minimal example to play with.</p>"},{"location":"concepts/policy/approval-policy.html#two-to-approve-two-to-reject","title":"Two to approve, two to reject","text":"<p>This is a variation of the above policy, but one that will automatically fail any run that receives more than one rejection.</p> <pre><code>package spacelift\n\napprove { input.run.state != \"UNCONFIRMED\" }\napprove { count(input.reviews.current.approvals) &gt; 1 }\nreject  { count(input.reviews.current.rejections) &gt; 1 }\n</code></pre> <p>Here's a minimal example to play with.</p>"},{"location":"concepts/policy/approval-policy.html#require-approval-for-a-task-command-not-on-the-allowlist","title":"Require approval for a task command not on the allowlist","text":"<pre><code>package spacelift\n\nallowlist := [\"ps\", \"ls\", \"rm -rf /\"]\n\n# Approve when not a task.\napprove { input.run.type != \"TASK\" }\n\n# Approve when allowlisted.\napprove { input.run.command == allowlist[_] }\n\n# Approve with two or more approvals.\napprove { count(input.reviews.current.approvals) &gt; 1 }\n</code></pre> <p>Here's a minimal example to play with.</p>"},{"location":"concepts/policy/approval-policy.html#combining-multiple-rules","title":"Combining multiple rules","text":"<p>Usually, you will want to apply different rules to different types of jobs. Since approval policies are attached to stacks, you will want to be smart about how you combine different rules. Here's how you can do that in a readable way, combining two of the above approval flows as an example:</p> <pre><code>package spacelift\n\n# First, let's define all conditions that require explicit\n# user approval.\nrequires_approval { input.run.state == \"UNCONFIRMED\" }\nrequires_approval { input.run.type == \"TASK\" }\n\n# Then, let's automatically approve all other jobs.\napprove { not requires_approval }\n\n# Autoapprove some task commands. Note how we don't check for run type\n# because only tasks will the have \"command\" field set.\ntask_allowlist := [\"ps\", \"ls\", \"rm -rf /\"]\napprove { input.run.command == task_allowlist[_] }\n\n# Two approvals and no rejections to approve.\napprove {\n  count(input.reviews.current.approvals) &gt; 1\n  count(input.reviews.current.rejections) == 0\n}\n</code></pre> <p>Here's a minimal example to play with.</p>"},{"location":"concepts/policy/approval-policy.html#role-based-approval","title":"Role-based approval","text":"<p>Sometimes you want to give certain roles but not others the power to approve certain workloads. The policy below approves an unconfirmed run or a task when either a Director approves it, or both DevOps and Security roles approve it:</p> <pre><code>package spacelift\n\n# First, let's define all conditions that require explicit\n# user approval.\nrequires_approval { input.run.state == \"UNCONFIRMED\" }\nrequires_approval { input.run.type == \"TASK\" }\napprove           { not requires_approval }\n\napprovals := input.reviews.current.approvals\n\n# Let's define what it means to be approved by a director, DevOps and Security.\ndirector_approval { approvals[_].session.teams[_] == \"Director\" }\ndevops_approval   { approvals[_].session.teams[_] == \"DevOps\" }\nsecurity_approval { approvals[_].session.teams[_] == \"Security\" }\n\n# Approve when a single director approves:\napprove { director_approval }\n\n# Approve when both DevOps and Security approve:\napprove { devops_approval; security_approval }\n</code></pre> <p>Here's a minimal example to play with.</p>"},{"location":"concepts/policy/approval-policy.html#require-private-worker-pool","title":"Require private worker pool","text":"<p>You might want to ensure that your runs always get scheduled on a private worker pool, and do not fall back to the public worker pool.</p> <p>You could use an Approval policy similar to this one to achieve this:</p> <pre><code>package spacelift\n\n# Approve any runs on private workers\napprove { not input.stack.worker_pool.public }\n\n# Reject any runs on public workers\nreject { input.stack.worker_pool.public }\n</code></pre> <p>Here's a minimal example to play with.</p> <p>You probably want to auto-attach this policy to some, if not all, of your stacks.</p>"},{"location":"concepts/policy/login-policy.html","title":"Login policy","text":""},{"location":"concepts/policy/login-policy.html#purpose","title":"Purpose","text":"<p>Login policies can allow users to log in to the account, and optionally give them admin privileges, too. Unlike all other policy types, login policies are global and can't be attached to individual stacks. They take effect immediately once they're created and affect all future login attempts.</p> <p>Info</p> <p>Login policies are only evaluated for the Cloud or Enterprise plan.</p> <p>Warning</p> <p>Login policies don't affect GitHub organization or SSO admins and private account owners who always get admin access to their respective Spacelift accounts. This is to avoid a situation where a bad login policy locks out everyone from the account.</p> <p>Danger</p> <p>Any change made (create, update or delete) to a login policy will invalidate all active sessions, except the session making the change.</p> <p>A login policy can define the following types of boolean rules:</p> <ul> <li>allow - allows the user to log in as a non-admin;</li> <li>admin - allows the user to log in as an account-wide admin - note that you don't need to explicitly allow admin users;</li> <li>deny - denies login attempt, no matter the result of other (allow and admin) rules;</li> <li>deny_admin - denies the current user admin access to the stack, no matter the outcome of other rules;</li> <li>space_admin/space_write/space_read - manages access levels to spaces. More on that in Spaces Access Control;</li> </ul> <p>If no rules match, the default action will be to deny a login attempt.</p> <p>Note that giving folks admin access is a big thing. Admins can do pretty much everything in Spacelift - create and delete stacks, trigger runs or tasks, create, delete and attach contexts and policies, etc. Instead, you can give users limited admin access using the space_admin rule.</p> <p>Danger</p> <p>In practice, any time you define an allow or admin rule, you should probably think of restricting access using a deny rule, too. Please see the examples below to get a better feeling for it.</p>"},{"location":"concepts/policy/login-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request will receive:</p> <pre><code>{\n\"request\": {\n\"remote_ip\": \"string - IP of the user trying to log in\",\n\"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n},\n\"session\": {\n\"creator_ip\": \"string - IP address of the user who created the session\",\n\"login\": \"string - username of the user trying to log in\",\n\"member\": \"boolean - is the user a member of the account\",\n\"name\": \"string - full name of the user trying to log in - may be empty\",\n\"teams\": [\"string - names of teams the user is a member of\"]\n},\n\"spaces\": [\n{\n\"id\": \"string - ID of the space\",\n\"name\": \"string - name of the space\",\n\"labels\": [\"string - label of the space\"]\n}\n]\n}\n</code></pre> <p>Tip</p> <p>OPA string comparisons are case-sensitive so make sure to use the proper case, as defined in your Identity Provider when comparing values.</p> <p>It might be helpful to enable sampling on the policy to see the exact values passed by the Identity Provider.</p> <p>Two fields in the session object may require further explanation: member and teams.</p>"},{"location":"concepts/policy/login-policy.html#account-membership","title":"Account membership","text":"<p>When you first log in to Spacelift, we use GitHub as the identity provider and thus we're able to get some of your details from there with username (login) being the most important one. However, each Spacelift account is linked to one and only one GitHub account. Thus, when you log in to a Spacelift account, we're checking if you're a member of that GitHub account.</p> <p>When that GitHub account is an organization, we can explicitly query for your organization membership. If you're a member, you get the member field set to true. If you're not - it's false. For private accounts it's different - they can only have one member, so the check is even simpler - if your login is the same as the name of the linked GitHub account, you get the member field set to true. If it isn't - it's false.</p> <p>When using Single Sign-On with SAML, every successful login attempt will necessarily require that the member field is set to true - if the linked IdP could verify you, you must be a member.</p> <p>Warning</p> <p>Watch this field very closely - it may be very useful for your deny rules.</p>"},{"location":"concepts/policy/login-policy.html#teams","title":"Teams","text":"<p>When using the default identity provider (GitHub), Teams are only queried for organization accounts - if you're a member of the GitHub organization linked to a Spacelift account, Spacelift will query GitHub API for the full list of teams you're a member of. This list will be available in the <code>session.teams</code> field. For private accounts and non-members, this list will be empty.</p> <p>Note that Spacelift treats GitHub team membership as transitive - for example let's assume Charlie is a member of the Badass team, which is a child of team Awesome. Charlie's list of teams includes both Awesome and Badass, even though he's not a direct member of the team Awesome.</p> <p>For Single Sign-On, the list of teams is pretty much arbitrary and depends on how the SAML assertion attribute is mapped to your user record on the IdP end. Please see the relevant article for more details.</p> <p>Warning</p> <p>Watch this field very closely - it may be very useful for your allow and admin rules.</p>"},{"location":"concepts/policy/login-policy.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p> <p>We recommend having only one login policy as what sounds reasonable within a policy may not yield the expected results when the decisions are merged.</p> <p>There are three possible use cases for login policies - granting access to folks in your org who would otherwise not have it, managing access for external contributors or restricting access to specific circumstances. Let's look into these use cases one by one.</p>"},{"location":"concepts/policy/login-policy.html#managing-access-levels-within-an-organization","title":"Managing access levels within an organization","text":"<p>In high-security environments where the principle of least access is applied, it's quite possible that nobody on the infra team gets admin access to GitHub. Still, it would be pretty useful for those people to be in charge of your Spacelift account. Let's create a login policy that will allow every member of the DevOps team to get admin access, and everyone in Engineering to get regular access - we'll give them more granular access to individual stacks later using stack access policies. While at it, let's also explicitly deny access to all non-members just to be on the safe side.</p> <pre><code>package spacelift\n\nteams := input.session.teams\n\n# Make sure to use the GitHub team names, not IDs (e.g., \"Example Team\" not \"example-team\")\n# and to omit the GitHub organization name\nadmin { teams[_] == \"DevOps\" }\nallow { teams[_] == \"Engineering\" }\ndeny  { not input.session.member }\n</code></pre> <p>Here's a minimal example to play with.</p> <p>This is also important for Single Sign-On integrations: only the integration creator gets administrative permissions by default, so all other administrators must be granted their access using a login policy.</p>"},{"location":"concepts/policy/login-policy.html#granting-access-to-external-contributors","title":"Granting access to external contributors","text":"<p>Danger</p> <p>This feature is not available when using Single Sign-On - your identity provider must be able to successfully validate each user trying to log in to Spacelift.</p> <p>Sometimes you have folks (short-term consultants, most likely) who are not members of your organization but need access to your Spacelift account - either as regular members or perhaps even as admins. There's also the situation where a bunch of friends is working on a hobby project in a personal GitHub account and they could use access to Spacelift. Here are examples of a policy that allows a bunch of whitelisted folks to get regular access and one to get admin privileges:</p> GitHubGoogle <p>This example uses GitHub usernames to grant access to Spacelift.</p> <pre><code>package spacelift\n\nadmins  := { \"alice\" }\nallowed := { \"bob\", \"charlie\", \"danny\" }\nlogin   := input.session.login\n\nadmin { admins[login] }\nallow { allowed[login] }\ndeny  { not admins[login]; not allowed[login] }\n</code></pre> <p>Here's a minimal example to play with.</p> <p>This example uses email addresses managed by Google to grant access to Spacelift.</p> <pre><code>package spacelift\n\nadmins  := { \"alice@example.com\" }\nlogin   := input.session.login\n\nadmin { admins[login] }\nallow { endswith(input.session.login, \"@example.com\") }\ndeny  { not admins[login]; not allow }\n</code></pre> <p>Warning</p> <p>Note that granting access to individuals is less safe than granting access to teams and restricting access to account members. In the latter case, when they lose access to your GitHub org, they automatically lose access to Spacelift. But when whitelisting individuals and not restricting access to members only, you'll need to remember to explicitly remove them from your Spacelift login policy, too.</p>"},{"location":"concepts/policy/login-policy.html#restricting-access-to-specific-circumstances","title":"Restricting access to specific circumstances","text":"<p>Stable and secure infrastructure is crucial to your business continuity. And all changes to your infrastructure carry some risk, so you may want to somehow restrict access to it. The example below is pretty extreme but it shows a very comprehensive policy where you restrict Spacelift access to users logging in from the office IP during business hours. You may want to use elements of this policy to create your own - less draconian - version, or keep it this way to support everyone's work-life balance.</p> <p>Note that this example only defines deny rules so you'll likely want to add some allow and admin rules, too - either in this policy or in a separate one.</p> <pre><code>package spacelift\n\nnow     := input.request.timestamp_ns\nclock   := time.clock([now, \"America/Los_Angeles\"])\nweekend := { \"Saturday\", \"Sunday\" }\nweekday := time.weekday(now)\nip      := input.request.remote_ip\n\ndeny { weekend[weekday] }\ndeny { clock[0] &lt; 9 }\ndeny { clock[0] &gt; 17 }\ndeny { not net.cidr_contains(\"12.34.56.0/24\", ip) }\n</code></pre> <p>There's a lot to digest here, so a playground example may be helpful.</p>"},{"location":"concepts/policy/login-policy.html#granting-limited-admin-access","title":"Granting limited admin access","text":"<p>Very often you'd like to give a user admin access limited to a certain set of resources, so that they can manage them without having access to all other resources in that account. You can find more on that use case in Spaces.</p>"},{"location":"concepts/policy/login-policy.html#rewriting-teams","title":"Rewriting teams","text":"<p>In addition to boolean rules regulating access to your Spacelift account, the login policy exposes the team rule, which allows one to dynamically rewrite the list of teams received from the identity provider. This operation allows one to define Spacelift roles independent of the identity provider. To illustrate this use case, let's imagine you want to define a <code>Superwriter</code> role for someone who's:</p> <ul> <li>logging in from an office VPN;</li> <li>is a member of the DevOps team, as defined by your IdP;</li> <li>is not a member of the Contractors team, as defined by your IdP;</li> </ul> <pre><code>package spacelift\n\nteam[\"Superwriter\"] {\n  office_vpn\n  devops\n  not contractor\n}\n\ncontractor { input.session.teams[_] == \"Contractors\" }\ndevops     { input.session.teams[_] == \"DevOps\" }\noffice_vpn { net.cidr_contains(\"12.34.56.0/24\", input.request.remote_ip)  }\n</code></pre> <p>What's important here is that the team rule overwrites the original list of teams, meaning that if it evaluates to a non-empty collection, it will replace the original list of teams in the session. In the above example, the <code>Superwriter</code> role will become the only team for the evaluated user session.</p> <p>If the above is not what you want, and you still would like to retain the original list of teams, you can modify the above example the following way:</p> <pre><code>package spacelift\n\n# This rule will copy each of the existing teams to the\n# new modified list.\nteam[name] { name := input.session.teams[_] }\n\nteam[\"Superwriter\"] {\n  office_vpn\n  devops\n  not contractor\n}\n\ncontractor { input.session.teams[_] == \"Contractors\" }\ndevops     { input.session.teams[_] == \"DevOps\" }\noffice_vpn { net.cidr_contains(\"12.34.56.0/24\", input.request.remote_ip)  }\n</code></pre> <p>A playground example of the above is available here.</p> <p>Hint</p> <p>Because the user session is updated, the rewritten teams are available in the data input provided to the policy types that receive user information. For example, the rewritten teams can be used in Access policies.</p>"},{"location":"concepts/policy/login-policy.html#default-login-policy","title":"Default login policy","text":"<p>If no login policies are defined on the account, Spacelift behaves as if it had this policy:</p> <pre><code>package spacelift\n\nallow { input.session.member }\n</code></pre>"},{"location":"concepts/policy/notification-policy.html","title":"Notification policy","text":""},{"location":"concepts/policy/notification-policy.html#purpose","title":"Purpose","text":"<p>Notification policies can be used to filter, route and adjust the body of notification messages sent by Spacelift. The policy works at the Space level meaning that it does not need to be attached to a specific stack, but rather is always evaluated if the Space it's in can be accessed by whatever action is being evaluated. It's also important to note that all notifications go through the policy evaluation. This means any of them can be redirected to the routes defined in the policy.</p> <p>A notification policy can define the following rules:</p> <ul> <li>inbox - allows messages to be routed to the Spacelift notification inbox;</li> <li>slack - allows messages to be routed to a given slack channel;</li> <li>webhook - allows messages to be routed to a given webhook;</li> <li>pull_request - allows messages to be routed to one or more pull requests;</li> </ul> <p>If no rules match no action is taken.</p>"},{"location":"concepts/policy/notification-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request can receive:</p> <pre><code>{\n\"account\": {\n\"name\": \"string\"\n},\n\"module_version\": {\n\"module\": {\n\"id\": \"string - unique ID of the module\",\n\"administrative\": \"boolean - is the module administrative\",\n\"branch\": \"string - tracked branch of the module\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"name\": \"string - name of the module\",\n\"project_root\": \"optional string - project root as set on the Module, if any\",\n\"repository\": \"string - name of the source repository\",\n\"terraform_provider\": \"string - name of the main Terraform provider used by the module\",\n\"space\": {\n\"id\": \"string - id of a space\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"name\": \"string - name of a the space\"\n},\n\"worker_pool\": {\n\"public\": \"boolean - worker pool information\",\n\"id\": \"string - unique ID of the worker pool\",\n\"name\": \"string - name of the worker pool\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"]\n}\n},\n\"version\": {\n\"commit\": {\n\"author\": \"string\",\n\"branch\": \"string\",\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"hash\": \"string\",\n\"message\": \"string\",\n\"url\": \"string\"\n},\n\"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n\"id\": \"string - id of the version being created\",\n\"latest\": \"boolean - is the module version latest\",\n\"number\": \"string - semver version number\",\n\"state\": \"string - current module state: ACTIVE, FAILED\",\n\"test_runs\": [{\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"id\": \"string - id of the test\",\n\"state\": \"string - state of the test\",\n\"title\": \"string - title of the test\",\n\"updated_at\": \"number (timestamp in nanoseconds)\",\n}]\n}\n},\n\"run_updated\": {\n\"state\": \"string\",\n\"username\": \"string\",\n\"note\": \"string\",\n\"run\":{\n\"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n\"branch\": \"string - the branch the run was triggered from\",\n\"changes\": [\n{\n\"action\": \"string enum - added | changed | deleted\",\n\"entity\": {\n\"address\": \"string - full address of the entity\",\n\"data\": \"object - detailed information about the entity, shape depends on the vendor and type\",\n\"name\": \"string - name of the entity\",\n\"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n\"entity_vendor\": \"string - the name of the vendor\",\n\"entity_type\": \"string - the type of entity, possible values depend on the vendor\"\n},\n\"phase\": \"string enum - plan | apply\"\n}\n],\n\"command\": \"string\",\n\"commit\": {\n\"author\": \"string\",\n\"branch\": \"string\",\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"hash\": \"string\",\n\"message\": \"string\",\n\"url\": \"string\"\n},\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"creator_session\": {\n\"admin\": \"boolean\",\n\"creator_ip\": \"string\",\n\"login\": \"string\",\n\"name\": \"string\",\n\"teams\": [\"string\"],\n\"machine\": \"boolean - whether the run was kicked off by a human or a machine\"\n},\n\"drift_detection\": \"boolean\",\n\"flags\": [\"string\"],\n\"id\": \"string\",\n\"runtime_config\": {\n\"after_apply\": [\"string\"],\n\"after_destroy\": [\"string\"],\n\"after_init\": [\"string\"],\n\"after_perform\": [\"string\"],\n\"after_plan\": [\"string\"],\n\"after_run\": [\"string\"],\n\"before_apply\": [\"string\"],\n\"before_destroy\": [\"string\"],\n\"before_init\": [\"string\"],\n\"before_perform\": [\"string\"],\n\"before_plan\": [\"string\"],\n\"environment\": \"map[string]string\",\n\"project_root\": \"string\",\n\"runner_image\": \"string\",\n\"terraform_version\": \"string\"\n},\n\"policy_receipts\": [{\n\"flags\": [\"string - flag assigned to the policy\"],\n\"name\": \"string - name of the policy\",\n\"outcome\": \"string - outcode of the policy\",\n\"type\": \"string - type of the policy\"\n}],\n\"state\": \"string\",\n\"triggered_by\": \"string or null\",\n\"type\": \"string - PROPOSED or TRACKED\",\n\"updated_at\": \"number (timestamp in nanoseconds)\",\n\"user_provided_metadata\": [\"string\"]\n},\n\"stack\": {\n\"administrative\": \"boolean\",\n\"autodeploy\": \"boolean\",\n\"autoretry\": \"boolean\",\n\"branch\": \"string\",\n\"id\": \"string\",\n\"labels\": [\"string\"],\n\"locked_by\": \"string or null\",\n\"name\": \"string\",\n\"namespace\": \"string or null\",\n\"project_root\": \"string or null\",\n\"repository\": \"string\",\n\"space\": {\n\"id\": \"string\",\n\"labels\": [\"string\"],\n\"name\": \"string\"\n},\n\"state\": \"string\",\n\"terraform_version\": \"string or null\",\n\"tracked_commit\": {\n\"author\": \"string\",\n\"branch\": \"string\",\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"hash\": \"string\",\n\"message\": \"string\",\n\"url\": \"string\"\n}\n}\n},\n\"state\": \"string\",\n\"timing\": [\n{\n\"duration\": \"number (in nanoseconds)\",\n\"state\": \"string\"\n}\n],\n\"username\": \"string\",\n\"webhook_endpoints\": [\n{\n\"id\": \"custom-hook2\",\n\"labels\": [\n\"example-label1\",\n\"example-label2\"\n]\n}\n],\n\"internal_error\": {\n\"error\": \"string\",\n\"message\": \"string\",\n\"severity\": \"string - INFO, WARNING, ERROR\"\n}\n}\n</code></pre> <p>The final JSON object received as input will depend on the type of notification being sent. Event-dependent objects will only be present when those events happen. The best way to see what input your Notification policy received is to enable sampling and check the Policy Workbench, but you can also use the table below as a reference:</p> Object Received Event <code>account</code> Any event <code>webhook_endpoints</code> Any event <code>run_updated</code> Run Updated <code>internal_error</code> Internal error occurred <code>module_version</code> Module version updated"},{"location":"concepts/policy/notification-policy.html#policy-in-practice","title":"Policy in practice","text":"<p>Using the notification policy, you can completely re-write notifications or control where and when they are sent. Let's look into how the policy works for each of the defined routes.</p>"},{"location":"concepts/policy/notification-policy.html#choosing-a-space-for-your-policy","title":"Choosing a Space for your policy","text":"<p>When creating notification policies you should take into account the Space in which you're creating them. Generally the policy follows the same conventions as any other Spacelift component, with a few small caveats.</p>"},{"location":"concepts/policy/notification-policy.html#determining-space-for-run-update-notifications","title":"Determining Space for run update notifications","text":"<p>Run update messages will rely on the Space that the run is happening in. It will check any policies in that Space including policies inherited from other Spaces.</p>"},{"location":"concepts/policy/notification-policy.html#determining-space-for-internal-errors","title":"Determining Space for internal errors","text":"<p>Most internal errors will check for notification policies inside of the root Space. However if the policy is reporting about a component that belongs to a certain Space and it can determine to which one it is, then it will check for policies in that or any inherited Space. Here is a list of components it will check in order:</p> <ul> <li>Stack</li> <li>Worker pool</li> <li>AWS integration</li> <li>Policy</li> </ul> <p>Info</p> <p>If you are new to spaces, consider further exploring our documentation about them here</p>"},{"location":"concepts/policy/notification-policy.html#inbox-notifications","title":"Inbox notifications","text":"<p>Inbox notifications are what you receive in your Spacelift notification inbox. By default, these are errors that happened during some kind of action execution inside Spacelift and are always sent even if you do not have a policy created. However using the policy allows you to alter the body of those errors to add additional context, or even more importantly it allows you to create your own unique notifications.</p> <p>The inbox rule accepts multiple configurable parameters:</p> <ul> <li><code>title</code> - a custom title for the message (Optional)</li> <li><code>body</code> - a custom message body (Optional)</li> <li><code>severity</code> - the severity level for the message (Optional)</li> </ul>"},{"location":"concepts/policy/notification-policy.html#creating-new-inbox-notifications","title":"Creating new inbox notifications","text":"<p>For example here is a inbox rule which will send <code>INFO</code> level notification messages to your inbox when a tracked run has finished:</p> <pre><code>package spacelift\n\n inbox[{\n  \"title\": \"Tracked run finished!\",\n  \"body\": sprintf(\"http://example.app.spacelift.io/stack/%s/run/%s has finished\", [stack.id, run.id]),\n  \"severity\": \"INFO\",\n }] {\n   stack := input.run_updated.stack\n   run := input.run_updated.run\n   run.type == \"TRACKED\"\n   run.state == \"FINISHED\"\n }\n</code></pre> <p>View the example in the rego playground.</p>"},{"location":"concepts/policy/notification-policy.html#slack-messages","title":"Slack messages","text":"<p>Slack messages can also be controlled using the notification policy, but before creating any policies that interact with Slack you will need to add the slack integration to your Spacelift account.</p> <p>Info</p> <p>The documentation section about Slack includes additional information like: available actions, slack access policies and more. Consider exploring that part of documentation first.</p> <p>Another important point to mention is that the rules for Slack require a <code>channel_id</code> to be defined. This can be found at the bottom of a channel's About section in Slack:</p> <p></p> <p>Now you should be ready to define rules for routing Slack messages. Slack rules allow you to make the same filtering decisions as any other rule in the policy. They also allow you to edit the message bodies themselves in order to create custom messages.</p> <p>The Slack rules accept multiple configurable parameters:</p> <ul> <li><code>channel_id</code> - the Slack channel to which the message will be delivered (Required)</li> <li><code>message</code> - a custom message to be sent (Optional)</li> </ul>"},{"location":"concepts/policy/notification-policy.html#filtering-and-routing-messages","title":"Filtering and routing messages","text":"<p>For example if you wanted to receive only finished runs on a specific Slack channel you would define a rule like this:</p> <pre><code>package spacelift\n\nslack[{\"channel_id\": \"C0000000000\"}] {\n  input.run_updated != null\n\n  run := input.run_updated.run\n  run.state == \"FINISHED\"\n}\n</code></pre> <p>View the example in the rego playground.</p>"},{"location":"concepts/policy/notification-policy.html#changing-the-message-body","title":"Changing the message body","text":"<p>Together with filtering and routing messages you can also alter the message body itself, here is an example for sending a custom message where a run which tries to attach a policy requires confirmation:</p> <pre><code>package spacelift\n\nslack[{\n  \"channel_id\": \"C0000000000\",\n  \"message\": sprintf(\"http://example.app.spacelift.io/stack/%s/run/%s is trying to attach a policy!\", [stack.id, run.id]),\n}] {\n  stack := input.run_updated.stack\n  run := input.run_updated.run\n  run.type == \"TRACKED\"\n  run.state == \"UNCONFIRMED\"\n  change := run.changes[_]\n  change.phase == \"plan\"\n  change.entity.type == \"spacelift_policy_attachment\"\n}\n</code></pre> <p>View the example in the rego playground.</p>"},{"location":"concepts/policy/notification-policy.html#webhook-requests","title":"Webhook requests","text":"<p>Info</p> <p>This section of documentation requires you have configured at least one Named Webhook. Consider exploring that part of documentation first.</p> <p>Webhook notifications are a very powerful part of the notification policy. Using them, one is able to not only receive webhooks on specific events that happen in Spacelift, but also craft unique requests to be consumed by some third-party.</p> <p>The notification policy relies on named webhooks which can be created and managed in the Webhooks section of Spacelift. Any policy evaluation will always receive a list of possible webhooks together with their labels as input. The data received in the policy input should be used to determine which webhook will be used when sending the request.</p> <p>The webhook policy accepts multiple configurable parameters:</p> <ul> <li><code>endpoint_id</code> - endpoint id (slug) to which the webhook will be delivered  (Required)</li> <li><code>headers</code> - a key value map which will be appended to request headers (Optional)</li> <li><code>payload</code> - a custom valid JSON object to be sent as request body (Optional)</li> <li><code>method</code> - a HTTP method to use when sending the request (Optional)</li> </ul>"},{"location":"concepts/policy/notification-policy.html#filtering-webhook-requests","title":"Filtering webhook requests","text":"<p>Filtering and selecting webhooks can be done by using the received input data. Rules can be created where only specific actions should trigger a webhook being sent. For example we could define a rule which would allow a webhook to be sent about any drift detection run:</p> <pre><code>package spacelift\n\nwebhook[{\"endpoint_id\": endpoint.id}] {\n  endpoint := input.webhook_endpoints[_]\n  endpoint.id == \"drift-hook\"\n  input.run_updated.run.drift_detection == true\n  input.run_updated.run.type == \"PROPOSED\"\n}\n</code></pre> <p>View the example in the rego playground.</p>"},{"location":"concepts/policy/notification-policy.html#creating-a-custom-webhook-request","title":"Creating a custom webhook request","text":"<p>All requests sent will always include the default headers for verification, a payload which is appropriate for the message type and the <code>method</code> set as <code>POST</code>. However, by using the webhook rule we can modify the body of the request, change the method or add additional headers. For example, if we wanted to define a completely custom request for a tracked run we would define a rule like this:</p> <pre><code>package spacelift\n\nwebhook[wbdata] {\n  endpoint := input.webhook_endpoints[_]\n  endpoint.id == \"testing-notifications\"\n  wbdata := {\n    \"endpoint_id\": endpoint.id,\n    \"payload\": {\n      \"custom_field\": \"This is a custom message\",\n      \"run_type\": input.run_updated.run.type,\n      \"run_state\": input.run_updated.run.state,\n      \"updated_at\": input.run_updated.run.updated_at,\n    },\n    \"method\": \"PUT\",\n    \"headers\": {\n      \"custom-header\": \"custom\",\n    },\n  }\n\n  input.run_updated.run.type == \"TRACKED\"\n}\n</code></pre> <p>View the example in the rego playground.</p> <p>Using custom webhook requests also makes it quite easy to integrate Spacelift with any third-party webhook consumer.</p>"},{"location":"concepts/policy/notification-policy.html#custom-webhook-requests-in-action","title":"Custom webhook requests in action","text":""},{"location":"concepts/policy/notification-policy.html#discord-integration","title":"Discord integration","text":"<p>Discord can be integrated to receive updates about Spacelift by simply creating a new webhook endpoint in your Discord server's integrations section and providing that as the endpoint when creating a new named webhook.</p> <p>Info</p> <p> For more information about making Discord webhooks follow their official webhook guide.</p> <p>After creating the webhook on both Discord and Spacelift you will need to define a new webhook rule like this:</p> <pre><code># Send updates about tracked runs to discord.\nwebhook[wbdata] {\n  endpoint := input.webhook_endpoints[_]\n  endpoint.id == \"YOUR_WEBHOOK_ID_HERE\"\n  stack := input.run_updated.stack\n  run := input.run_updated.run\n  wbdata := {\n    \"endpoint_id\": endpoint.id,\n    \"payload\": {\n      \"embeds\": [{\n        \"title\": \"Tracked run triggered!\",\n        \"description\": sprintf(\"Stack: [%s](http://example.app.spacelift.io/stack/%s)\\nRun ID: [%s](http://example.app.spacelift.io/stack/%s/run/%s)\\nRun state: %s\", [stack.name,stack.id,run.id,stack.id, run.id,run.state]),\n        }]\n     }\n  }\n  input.run_updated.run.type == \"TRACKED\"\n}\n</code></pre> <p>And that's it! You should now be receiving updates about tracked runs to your Discord server:</p> <p></p>"},{"location":"concepts/policy/notification-policy.html#pull-request-notifications","title":"Pull request notifications","text":"<p>Pull request notifications are a very powerful part of the notification policy. Using them, one is able to not only target a single pull request but also pull requests targeting a specific branch or commit.</p> <p>The pull request rule accepts multiple configurable parameters:</p> <ul> <li><code>id</code> - a pull request ID (Optional)</li> <li><code>commit</code> - a target commit SHA (Optional)</li> <li><code>branch</code> - a target branch (Optional)</li> <li><code>body</code> - a custom comment body (Optional)</li> </ul>"},{"location":"concepts/policy/notification-policy.html#creating-a-pull-request-comment","title":"Creating a pull request comment","text":"<p>For example here is a rule which will add a comment (containing a default body) to the pull request that triggered the run:</p> <pre><code>package spacelift\n\nimport future.keywords.contains\nimport future.keywords.if\n\npull_request contains {\"id\": run.commit.pull_request_id} if {\n run := input.run_updated.run\n run.state == \"FINISHED\"\n}\n</code></pre> <p>Hint</p> <p>It works best in combination with a push policy to create proposed runs on pull requests.</p> <p>View the example in the rego playground.</p>"},{"location":"concepts/policy/notification-policy.html#adding-a-comment-to-pull-requests-targeting-a-specific-commit","title":"Adding a comment to pull requests targeting a specific commit","text":"<p>You specify a target commit SHA using the <code>commit</code> parameter:</p> <pre><code>package spacelift\n\nimport future.keywords.contains\nimport future.keywords.if\n\npull_request contains {\n \"commit\": run.commit.hash,\n \"body\": sprintf(\"https://%s.app.spacelift.io/stack/%s/run/%s has finished\", [input.account.name, stack.id, run.id]),\n} if {\n stack := input.run_updated.stack\n run := input.run_updated.run\n run.state == \"FINISHED\"\n}\n</code></pre> <p>View the example in the rego playground.</p>"},{"location":"concepts/policy/notification-policy.html#adding-a-comment-to-pull-requests-targeting-a-specific-branch","title":"Adding a comment to pull requests targeting a specific branch","text":"<p>Provide the branch name using the <code>branch</code> parameter:</p> <pre><code>package spacelift\n\nimport future.keywords.contains\nimport future.keywords.if\n\npull_request contains {\n \"branch\": \"main\",\n \"body\": sprintf(\"https://%s.app.spacelift.io/stack/%s/run/%s has finished\", [input.account.name, stack.id, run.id]),\n} if {\n stack := input.run_updated.stack\n run := input.run_updated.run\n run.state == \"FINISHED\"\n}\n</code></pre> <p>Hint</p> <p>Please note that <code>branch</code> is the base branch of the pull request. For example, if it's <code>\"branch\": input.run_updated.stack.branch</code>, that'd mean that the policy would comment into every pull request that targets the tracked branch of the stack.</p> <p>View the example in the rego playground.</p>"},{"location":"concepts/policy/notification-policy.html#changing-the-comment-body","title":"Changing the comment body","text":"<p>You can customize the comment body, even include logs from the planning or applying phase.</p> <p>A <code>spacelift::logs::planning</code> placeholder in the comment body will be replaced with logs from the planning phase. The same applies to <code>spacelift::logs::applying</code> and the applying phase.</p> <pre><code>package spacelift\n\nimport future.keywords.contains\nimport future.keywords.if\n\npull_request contains {\n \"id\": run.commit.pull_request_id,\n \"body\": body,\n} if {\n stack := input.run_updated.stack\n run := input.run_updated.run\n run.state == \"FINISHED\"\n\n body := sprintf(\n  `https://%s.app.spacelift.io/stack/%s/run/%s has finished\n\n## Planning logs\n\n%s\nspacelift::logs::planning\n%s\n`,\n  [input.account.name, stack.id, run.id, \"```\", \"```\"],\n )\n}\n</code></pre> <p>View the example in the rego playground.</p>"},{"location":"concepts/policy/notification-policy.html#complex-example-adding-a-comment-to-a-pull-request-about-changed-resources","title":"Complex example: adding a comment to a pull request about changed resources","text":"<p>The following example will add a comment to a pull request where it will list all the resources that were added, changed or deleted.</p> <p>Note</p> <p>If you'd like to customize it, feel free to add <code>sample := true</code> to the the policy and then use the Policy Workbench to see what data is available.</p> <pre><code>package spacelift\n\nimport future.keywords.contains\nimport future.keywords.if\nimport future.keywords.in\n\nheader := sprintf(\"### Resource changes ([link](https://%s.app.spacelift.io/stack/%s/run/%s))\\n\\n![add](https://img.shields.io/badge/add-%d-brightgreen) ![change](https://img.shields.io/badge/change-%d-yellow) ![destroy](https://img.shields.io/badge/destroy-%d-red)\\n\\n| Action | Resource | Changes |\\n| --- | --- | --- |\", [input.account.name, input.run_updated.stack.id, input.run_updated.run.id, count(added), count(changed), count(deleted)])\n\naddedresources := concat(\"\\n\", added)\nchangedresources := concat(\"\\n\", changed)\ndeletedresources := concat(\"\\n\", deleted)\n\nadded contains row if {\n  some x in input.run_updated.run.changes\n\n  row := sprintf(\"| Added | `%s` | &lt;details&gt;&lt;summary&gt;Value&lt;/summary&gt;`%s`&lt;/details&gt; |\", [x.entity.address, x.entity.data.values])\n  x.action == \"added\"\n  x.entity.entity_type == \"resource\"\n}\n\nchanged contains row if {\n  some x in input.run_updated.run.changes\n\n  row := sprintf(\"| Changed | `%s` | &lt;details&gt;&lt;summary&gt;New value&lt;/summary&gt;`%s`&lt;/details&gt; |\", [x.entity.address, x.entity.data.values])\n  x.entity.entity_type == \"resource\"\n\n  any([x.action == \"changed\", x.action == \"destroy-Before-create-replaced\", x.action == \"create-Before-destroy-replaced\"])\n}\n\ndeleted contains row if {\n  some x in input.run_updated.run.changes\n  row := sprintf(\"| Deleted | `%s` | :x: |\", [x.entity.address])\n  x.entity.entity_type == \"resource\"\n  x.action == \"deleted\"\n}\n\npull_request contains {\"commit\": input.run_updated.run.commit.hash, \"body\": replace(replace(concat(\"\\n\", [header, addedresources, changedresources, deletedresources]), \"\\n\\n\\n\", \"\\n\"), \"\\n\\n\", \"\\n\")} if {\n  input.run_updated.run.state == \"FINISHED\"\n  input.run_updated.run.type == \"PROPOSED\"\n}\n</code></pre> <p> </p>"},{"location":"concepts/policy/run-initialization-policy.html","title":"Initialization policy","text":"<p>Warning</p> <p>This feature is deprecated. New users should not use this feature and existing users are encouraged to migrate to the approval policy, which offers a much more flexible and powerful way to control which runs are allowed to proceed. A migration guide is available here.</p>"},{"location":"concepts/policy/run-initialization-policy.html#purpose","title":"Purpose","text":"<p>Initialization policy can prevent a Run or a Task from being initialized, thus blocking any custom code or commands from being executed. It superficially looks like a plan policy in that it affects an existing Run and prints feedback to logs, but it does not get access to the plan. Instead, it can be used to protect your stack from unwanted changes or enforce organizational rules concerning how and when runs are supposed to be triggered.</p> <p>Warning</p> <p>Server-side initialization policies are being deprecated. We will be replacing them with worker-side policies that can be set by using the launcher run initialization policy flag (<code>SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY</code>).</p> <p>For a limited time period we will be running both types of initialization policy checks but ultimately we're planning to move the pre-flight checks to the worker node, thus allowing customers to block suspicious looking jobs on their end.</p> <p>Let's create a simple initialization policy, attach it to the stack, and see what gives:</p> <pre><code>package spacelift\n\ndeny[\"you shall not pass\"] {\n  true\n}\n</code></pre> <p>...and boom:</p> <p></p>"},{"location":"concepts/policy/run-initialization-policy.html#rules","title":"Rules","text":"<p>Initialization policies are simple in that they only use a single rule - deny - with a string message. A single result for that rule will fail the run before it has a chance to start - as we've just witnessed above.</p>"},{"location":"concepts/policy/run-initialization-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request will receive:</p> <pre><code>{\n\"commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"message\": \"string - commit message\"\n},\n\"request\": {\n\"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n},\n\"run\": {\n\"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n\"changes\": [\n{\n\"action\": \"string enum - added | changed | deleted\",\n\"entity\": {\n\"address\": \"string - full address of the entity\",\n\"name\": \"string - name of the entity\",\n\"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n\"entity_vendor\": \"string - the name of the vendor\",\n\"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n\"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n},\n\"phase\": \"string enum - plan | apply\"\n}\n],\n\"commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n\"flags\" : [\"string - list of flags set on the run by other policies\" ],\n\"id\": \"string - the run ID\",\n\"runtime_config\": {\n\"before_init\": [\"string - command to run before run initialization\"],\n\"project_root\": \"string - root of the Terraform project\",\n\"runner_image\": \"string - Docker image used to execute the run\",\n\"terraform_version\": \"string - Terraform version used to for the run\"\n},\n\"state\": \"string - the current run state\",\n\"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n\"type\": \"string - PROPOSED or TRACKED\",\n\"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n\"user_provided_metadata\": [\n\"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n]\n},\n\"stack\": {\n\"administrative\": \"boolean - is the stack administrative\",\n\"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n\"branch\": \"string - tracked branch of the stack\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n\"name\": \"string - name of the stack\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Stack, if any\",\n\"repository\": \"string - name of the source GitHub repository\",\n\"state\": \"string - current state of the stack\",\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\"\n}\n}\n</code></pre>"},{"location":"concepts/policy/run-initialization-policy.html#aliases","title":"Aliases","text":"<p>In addition to our helper functions, we provide aliases for commonly used parts of the input data:</p> Alias Source <code>commit</code> <code>input.commit</code> <code>run</code> <code>input.run</code> <code>runtime_config</code> <code>input.run.runtime_config</code> <code>stack</code> <code>input.stack</code>"},{"location":"concepts/policy/run-initialization-policy.html#use-cases","title":"Use cases","text":"<p>There are two main use cases for run initialization policies - protecting your stack from unwanted changes and enforcing organizational rules. Let's look at these one by one.</p>"},{"location":"concepts/policy/run-initialization-policy.html#protect-your-stack-from-unwanted-changes","title":"Protect your stack from unwanted changes","text":"<p>While specialized, Spacelift is still a CI/CD platform and thus allows running custom code before Terraform initialization phase using <code>before_init</code>scripts. This is a very powerful feature, but as always, with great power comes great responsibility. Since those scripts get full access to your Terraform environment, how hard is it to create a commit on a feature branch that would run <code>terraform destroy -auto-approve</code>? Sure, all Spacelift runs are tracked and this prank will sooner or later be tracked down to the individual who ran it, but at that point do you still have a business?</p> <p>That's where initialization policies can help. Let's explicitly blacklist all Terraform commands if they're running as <code>before_init</code> scripts. OK, let's maybe add a single exception for a formatting check.</p> <pre><code>package spacelift\n\ndeny[sprintf(\"don't use Terraform please (%s)\", [command])] {\n  command := input.run.runtime_config.before_init[_]\n\n  contains(command, \"terraform\")\n  command != \"terraform fmt -check\"\n}\n</code></pre> <p>Feel free to play with this example in the Rego playground.</p> <p>OK, but what if someone gets clever and creates a Docker image that symlinks something very innocent-looking to <code>terraform</code>? Well, you have two choices - you could replace a blacklist with a whitelist, but a clever attacker can be really clever. So the other choice is to make sure that a known good Docker is used to execute the run. Here's an example:</p> <pre><code>package spacelift\n\ndeny[sprintf(\"unexpected runner image (%s)\", [image])] {\n  image := input.run.runtime_config.runner_image\n\n  image != \"spacelift/runner:latest\"\n}\n</code></pre> <p>Here's the above example in the Rego playground.</p> <p>Danger</p> <p>Obviously, if you're using an image other than what we control, you still have to ensure that the attacker can't push bad code to your Docker repo. Alas, this is beyond our control.</p>"},{"location":"concepts/policy/run-initialization-policy.html#enforce-organizational-rules","title":"Enforce organizational rules","text":"<p>While the previous section was all about making sure that bad stuff does not get executed, this use case presents run initialization policies as a way to ensure best practices - ensuring that the right things get executed the right way and at the right time.</p> <p>One of the above examples explicitly whitelisted Terraform formatting check. Keeping your code formatted in a standard way is generally a good idea, so let's make sure that this command always gets executed first. Note that as per Anna Karenina principle this check is most elegantly defined as a negation of another rule matching the required state of affairs:</p> <pre><code>package spacelift\n\ndeny[\"please always run formatting check first\"] {\n  not formatting_first\n}\n\nformatting_first {\n  input.run.runtime_config.before_init[i] == \"terraform fmt -check\"\n  i == 0\n}\n</code></pre> <p>Here's this example in the Rego playground.</p> <p>This time we'll skip the mandatory \"don't deploy on weekends\" check because while it could also be implemented here, there are probably better places to do it. Instead, let's enforce a feature branch naming convention. We'll keep this example simple, requiring that feature branches start with either <code>feature/</code> or <code>fix/</code>, but you can go fancy and require references to Jira tickets or even look at commit messages:</p> <pre><code>package spacelift\n\ndeny[sprintf(\"invalid feature branch name (%s)\", [branch])] {\n  branch := input.commit.branch\n\n  input.run.type == \"PROPOSED\"\n  not re_match(\"^(fix|feature)\\/.*\", branch)\n}\n</code></pre> <p>Here's this example in the Rego playground.</p>"},{"location":"concepts/policy/run-initialization-policy.html#migration-guide","title":"Migration guide","text":"<p>A run initialization policy can be expressed as an approval policy if it defines a single <code>reject</code> rule, and an <code>approve</code> rule that is its negation. Below you will find equivalents of the examples above expressed as approval policies.</p>"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-enforcing-terraform-check","title":"Migration example: enforcing Terraform check","text":"<pre><code>package spacelift\n\nreject { not formatting_first}\n\napprove { not reject }\n\nformatting_first {\n  input.run.runtime_config.before_init[i] == \"terraform fmt -check\"\n  i == 0\n}\n</code></pre>"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-disallowing-before-init-terraform-commands-other-than-formatting","title":"Migration example: disallowing before-init Terraform commands other than formatting","text":"<pre><code>package spacelift\n\nreject {\n  command := input.run.runtime_config.before_init[_]\n  contains(command, \"terraform\"); command != \"terraform fmt -check\"\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-enforcing-runner-image","title":"Migration example: enforcing runner image","text":"<pre><code>package spacelift\n\nreject {\n  input.run.runtime_config.runner_image != \"spacelift/runner:latest\"\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-enforcing-feature-branch-naming-convention","title":"Migration example: enforcing feature branch naming convention","text":"<pre><code>package spacelift\n\nreject {\n  branch := input.run.commit.branch\n  input.run.type == \"PROPOSED\"\n  not re_match(\"^(fix|feature)\\/.*\", branch)\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/stack-access-policy.html","title":"Access policy","text":"<p>Danger</p> <p>Access policies are deprecated in favour of Space access rules in the login policy. See Spaces for more details.</p>"},{"location":"concepts/policy/stack-access-policy.html#purpose","title":"Purpose","text":"<p>By default, non-admin users have no access to any Stacks or Modules and must be granted that explicitly. There are two levels of non-admin access - reader and writer, and the exact meaning of these roles is covered in a separate section. For now all we need to care about is that access policies are what we use to give appropriate level of access to individual stacks to non-admin users in your account.</p> <p>This type of access control is typically done either by building a separate user management system on your end or piggy-backing on one created by your identity provider. Both solutions have their limitations - a separate user management system makes it more difficult for organizations to onboard and offboard users, and the last thing we want is for a guy that was just fired to log in to Spacelift and have their revenge. User management systems are also pretty difficult to get right, too, especially if granular and sophisticated access controls are required.</p> <p>Piggy-backing on the identity provider is probably a safer bet and is used by many DevTools vendors. With this approach, having some access level to a GitHub repo would give you the same access level to all Spacelift stacks and/or modules associated with it. That's somewhat reasonable, but not as flexible as having your own fancy user management system. Imagine having two stacks linked to one repo, representing two environments - staging and production. It's quite possible that you'd appreciate separate access controls for these two.</p> <p>Access policies offer the best of both worlds - they give you a tool to build your own access management system using data obtained either from our identity provider (GitHub), or from your identity provider if using Single Sign-On integration. In subsequent sections we'll dive deeper into what data is exposed to your policies, how you can define access policies with different levels of access, and what those levels actually mean.</p>"},{"location":"concepts/policy/stack-access-policy.html#rules","title":"Rules","text":"<p>Your access policy can define the following boolean rules:</p> <ul> <li>write: gives the current user write access to the stack or module;</li> <li>read: gives the current user read access to the stack or module;</li> <li>deny: denies the current user all access to the stack or module, no matter the outcome of other rules;</li> <li>deny_write:  denies the current user write access to the stack or module, no matter the outcome of other rules;</li> </ul> <p>Note that write access automatically assumes read permissions, too, so there's no need to define separate read policies for writers.</p> <p>Another thing to keep in mind when defining access policies is that they are executed quickly. Internally, we expect that running all access policies on all the stacks in one request (<code>stacks</code> in the GraphQL API) will take less than 500 milliseconds - otherwise the request fails. That's actually plenty for modern computers, but think twice before creating fancy regex rules in your access policies.</p>"},{"location":"concepts/policy/stack-access-policy.html#readers-and-writers","title":"Readers and writers","text":"<p>There are two levels of non-admin access to a Spacelift stack or module - reader and writer. These are pretty intuitive for most developers, but this section will cover them in more detail to avoid any possible confusion. But first, let's try to understand the use case for different levels of access.</p> <p>In every non-trivial organization there will be different roles - folks who build and manage shared infrastructure, folks who build and manage their team or project-level infrastructure, and folks who use this infrastructure to build great things. The first group is probably the people who manage your Spacelift accounts - the admins. They need to be able to set up everything - create stacks, contexts and policies, and attach them accordingly. You'd normally use login policies to manage their access.</p> <p>The second group - folks who manage their team or project-level infrastructure - should have a reasonable level of access to their project. They should be able to define the environment, set up various integrations, trigger and confirm runs, execute tasks. This level of access is granted by the writer permission. However, writers should still operate within the boundaries defined by admins, who do that mainly by attaching contexts and policies to the stacks.</p> <p>Last but not least the third group - folks who build things on top of existing infra - don't necessarily need to define the infra, but they need to understand what's available and when things are changing. You'll probably want to allow them to contribute to infra definitions, too, and allow them to see feedback from proposed runs. They can't do anything, but they can see everything. These are the readers. Most modern organizations tend to provide this level of access to as many stakeholders as possible to maintain transparency and facilitate collaboration.</p>"},{"location":"concepts/policy/stack-access-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request will receive:</p> <pre><code>{\n\"request\": {\n\"remote_ip\": \"string - IP of the user making a request\",\n\"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n},\n\"session\": {\n\"admin\": \"boolean - is the current user a Spacelift admin\",\n\"creator_ip\": \"string - IP address of the user who created the session\",\n\"login\": \"string - GitHub username of the logged in user\",\n\"name\":  \"string - full name of the logged in GitHub user - may be empty\",\n\"teams\": [\"string - names of org teams the user is a member of\"],\n\"machine\": \"boolean - whether the creator is a machine or a user\"\n},\n\"stack\": { // when access to a stack is being evaluated\n\"id\": \"string - unique ID of the stack\",\n\"administrative\": \"boolean - is the stack administrative\",\n\"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n\"branch\": \"string - tracked branch of the stack\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n\"name\": \"string - name of the stack\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Stack, if any\",\n\"repository\": \"string - name of the source repository\",\n\"state\": \"string - current state of the stack\",\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\"\n},\n\"module\": { // when access to a module is being evaluated\n\"id\": \"string - unique ID of the module\",\n\"administrative\": \"boolean - is the stack administrative\",\n\"branch\": \"string - tracked branch of the module\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"repository\": \"string - name of the source repository\",\n\"terraform_provider\": \"string - name of the main Terraform provider used by the module\"\n}\n}\n</code></pre>"},{"location":"concepts/policy/stack-access-policy.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p> <p>With all the above theory in mind, let's jump straight to the code and define some access policies. This section will cover some common examples that can be copied more or less directly, and some contrived ones to serve as an inspiration.</p> <p>Info</p> <p>Remember that access policies must be attached to a stack or a module to take effect.</p>"},{"location":"concepts/policy/stack-access-policy.html#read-access-to-everyone-in-engineering","title":"Read access to everyone (in Engineering)","text":"<p>I get read access, you get read access, everyone gets read access. As long as they're members of the Engineering team:</p> <pre><code>package spacelift\n\nread { input.session.teams[_] == \"Engineering\" }\n</code></pre> <p>OK, that was simple. But let's also see it in the Rego playground.</p>"},{"location":"concepts/policy/stack-access-policy.html#in-case-things-go-wrong-we-want-you-to-be-there","title":"In case things go wrong, we want you to be there","text":"<p>You know when things go wrong it's usually because someone did something. Like an infra deployment. Let's try to make sure they're in the office when doing so and restrict write access to business hours and office IP range. This policy is best combined with one that gives read access.</p> <pre><code>package spacelift\n\nnow     := input.request.timestamp_ns\nclock   := time.clock([now, \"America/Los_Angeles\"])\nweekend := { \"Saturday\", \"Sunday\" }\nweekday := time.weekday(now)\nip      := input.request.remote_ip\n\nwrite      { input.session.teams[_] == \"Product team\" }\ndeny_write { weekend[weekday] }\ndeny_write { clock[0] &lt; 9 }\ndeny_write { clock[0] &gt; 17 }\ndeny_write { not net.cidr_contains(\"12.34.56.0/24\", ip) }\n</code></pre> <p>Here is this example in Rego playground.</p>"},{"location":"concepts/policy/stack-access-policy.html#protect-administrative-stacks","title":"Protect administrative stacks","text":"<p>Administrative stacks are powerful - getting write access to one is almost as good as being an admin - you can define and attach contexts and policies. So let's deny write access to them entirely. This works since access policies are not evaluated for admin users.</p> <pre><code>package spacelift\n\ndeny_write { input.stack.administrative }\n</code></pre> <p>And here's the necessary Rego playground example.</p>"},{"location":"concepts/policy/task-run-policy.html","title":"Task policy","text":"<p>Warning</p> <p>This feature is deprecated. New users should not use this feature and existing users are encouraged to migrate to the approval policy, which offers a much more flexible and powerful way to control which tasks are allowed to proceed. A migration guide is available here.</p>"},{"location":"concepts/policy/task-run-policy.html#purpose","title":"Purpose","text":"<p>Spacelift tasks are a handy feature that allows an arbitrary command to be executed within the context of your fully initialized stack. This feature is designed to make running one-off administrative tasks (eg. resource tainting) safer and more convenient. It can also be an attack vector allowing evil people to do bad things, or simply a footgun allowing well-meaning people to err in a spectacular way.</p> <p>Enter task policies. The sole purpose of task policies is to prevent certain commands from being executed, to prevent certain groups or individuals from executing any commands, or to prevent certain commands from being executed by certain groups or individuals.</p> <p>Info</p> <p>Preventing admins from running tasks using policies can only play an advisory role and should not be considered a safety measure. A bad actor with admin privileges can detach a policy from the stack and run whatever they want. Choose your admins wisely.</p> <p>Task policies are simple in that they only use a single rule - deny - with a string message. A single match for that rule will prevent a run from being created, with an appropriate API error. Let's see how that works in practice by defining a simple rule and attaching it to a stack:</p> <pre><code>package spacelift\n\ndeny[\"not in my town, you don't\"] { true }\n</code></pre> <p>And here's the outcome when trying to run a task:</p> <p></p>"},{"location":"concepts/policy/task-run-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request will receive:</p> <pre><code>{\n\"request\": {\n\"command\": \"string - command that the user is trying to execute as task\",\n\"remote_ip\": \"string - IP of the user trying to log in\",\n\"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n},\n\"session\": {\n\"admin\": \"boolean - is the current user a Spacelift admin\",\n\"creator_ip\": \"string - IP address of the user who created the session\",\n\"login\": \"string - GitHub username of the current user\",\n\"name\": \"string - full name of the current user\",\n\"teams\": [\"string - names of org teams the current user is a member of\"],\n\"machine\": \"boolean - whether the creator is a machine or a user\"\n},\n\"stack\": {\n\"administrative\": \"boolean - is the stack administrative\",\n\"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n\"branch\": \"string - tracked branch of the stack\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n\"name\": \"string - name of the stack\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Stack, if any\",\n\"repository\": \"string - name of the source GitHub repository\",\n\"state\": \"string - current state of the stack\",\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\"\n}\n}\n</code></pre>"},{"location":"concepts/policy/task-run-policy.html#aliases","title":"Aliases","text":"<p>In addition to our helper functions, we provide aliases for commonly used parts of the input data:</p> Alias Source <code>request</code> <code>input.request</code> <code>session</code> <code>input.session</code> <code>stack</code> <code>input.stack</code>"},{"location":"concepts/policy/task-run-policy.html#examples","title":"Examples","text":"<p>Let's have a look at a few examples to see what you can accomplish with task policies. You've seen one example already - disabling tasks entirely. That's perhaps both heavy-handed and naive given that admins can detach the policy if needed. So let's only block non-admins from running tasks:</p> <pre><code>package spacelift\n\ndeny[\"only admins can run tasks\"] { not input.session.admin }\n</code></pre> <p>Let's look at an example of this simple policy in the Rego playground.</p> <p>That's still pretty harsh. We could possibly allow writers to run some commands we consider safe - like resource tainting and untainting. Let's try then, and please excuse the regex:</p> <pre><code>package spacelift\n\ndeny[sprintf(\"command not allowed (%s)\", [command])] {\n  command := input.request.command\n\n  not input.session.admin\n  not re_match(\"^terraform\\\\s(un)?taint\\\\s[\\\\w\\\\-\\\\.]*$\", command)\n}\n</code></pre> <p>Feel free to play with the above example in the Rego playground.</p> <p>If you want to keep whitelisting different commands, it may be more elegant to flip the rule logic, create a series of allowed rules, and define one deny rule as <code>not allowed</code>. Let's have a look at this approach, and while we're at it let's remind everyone not to run anything during the weekend:</p> <pre><code>package spacelift\n\ncommand := input.request.command\n\ndeny[sprintf(\"command not allowed (%s)\", [command])] { not allowed }\n\ndeny[\"no tasks on weekends\"] {\n  today   := time.weekday(input.request.timestamp_ns)\n  weekend := { \"Saturday\", \"Sunday\" }\n\n  weekend[today]\n}\n\nallowed { input.session.admin }\nallowed { re_match(\"^terraform\\\\s(un)?taint\\\\s[\\\\w\\\\-\\\\.]*$\", command) }\nallowed { re_match(\"^terraform\\\\simport\\\\s[\\\\w\\\\-\\\\.]*$\", command) }\n</code></pre> <p>As usual, this example is available to play around with.</p>"},{"location":"concepts/policy/task-run-policy.html#migration-guide","title":"Migration guide","text":"<p>A task policy can be expressed as an approval policy if it defines a single <code>reject</code> rule, and an <code>approve</code> rule that is its negation. Below you will find equivalents of the examples above expressed as approval policies.</p>"},{"location":"concepts/policy/task-run-policy.html#migration-example-only-allow-terraform-taint-and-untaint","title":"Migration example: only allow terraform taint and untaint","text":"<pre><code>package spacelift\n\nreject {\n  command := input.run.command\n  not re_match(\"^terraform\\\\s(un)?taint\\\\s[\\\\w\\\\-\\\\.]*$\", command)\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/task-run-policy.html#migration-example-no-tasks-on-weekends","title":"Migration example: no tasks on weekends","text":"<pre><code>package spacelift\n\nreject {\n  today   := time.weekday(input.run.created_at)\n  weekend := { \"Saturday\", \"Sunday\" }\n\n  weekend[today]\n}\n\napprove { not reject }\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html","title":"Plan policy","text":""},{"location":"concepts/policy/terraform-plan-policy.html#purpose","title":"Purpose","text":"<p>Plan policies are evaluated during a planning phase after vendor-specific change preview command (eg. <code>terraform plan</code>) executes successfully. The body of the change is exported to JSON and parts of it are combined with Spacelift metadata to form the data input to the policy.</p> <p>Plan policies are the only ones that have access to the actual changes to the managed resources, so this is probably the best place to enforce organizational rules and best practices as well as do automated code review. There are two types of rules here that Spacelift will care about: deny and warn. Each of them must come with an appropriate message that will be shown in the logs. Any deny rules will print in red and will automatically fail the run, while warn rules will print in yellow and will at most mark the run for human review if the change affects the tracked branch and the Stack is set to autodeploy.</p> <p>Here is a super simple policy that will show both types of rules in action:</p> <pre><code>package spacelift\n\ndeny[\"you shall not pass\"] {\n  true # true means \"match everything\"\n}\n\nwarn[\"hey, you look suspicious\"] {\n  true\n}\n</code></pre> <p>Let's create this policy, attach it to a Stack and take it for a spin by triggering a run:</p> <p></p> <p>Yay, that works (and it fails our plan, too), but it's not terribly useful - unless of course you want to block all changes to your Stack in a really clumsy way. Let's look a bit deeper into the document that each plan policy receives, two possible use cases - rule enforcement and automated code review - and some cookbook examples.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#data-input","title":"Data input","text":"<p>This is the schema of the data input that each policy request will receive. If the policy is executed for the first time, the <code>previous_run</code> field will be missing.</p> <pre><code>{\n\"spacelift:\": {\n\"commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"request\": {\n\"timestamp_ns\": \"number - current Unix timestamp in nanoseconds\"\n},\n\"previous_run\": {\n\"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n\"branch\": \"string - the branch the run was triggered from\",\n\"changes\": [\n{\n\"action\": \"string enum - added | changed | deleted\",\n\"entity\": {\n\"address\": \"string - full address of the entity\",\n\"name\": \"string - name of the entity\",\n\"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n\"entity_vendor\": \"string - the name of the vendor\",\n\"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n\"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n},\n\"phase\": \"string enum - plan | apply\"\n}\n],\n\"commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n\"drift_detection\": \"boolean - is this a drift detection run\",\n\"flags\" : [\"string - list of flags set on the run by other policies\" ],\n\"id\": \"string - the run ID\",\n\"runtime_config\": {\n\"before_init\": [\"string - command to run before run initialization\"],\n\"project_root\": \"string - root of the Terraform project\",\n\"runner_image\": \"string - Docker image used to execute the run\",\n\"terraform_version\": \"string - Terraform version used to for the run\"\n},\n\"state\": \"string - the current run state\",\n\"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n\"type\": \"string - type of the run\",\n\"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n\"user_provided_metadata\": [\n\"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n]\n},\n\"run\": {\n\"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n\"branch\": \"string - the branch the run was triggered from\",\n\"changes\": [\n{\n\"action\": \"string enum - added | changed | deleted\",\n\"entity\": {\n\"address\": \"string - full address of the entity\",\n\"name\": \"string - name of the entity\",\n\"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n\"entity_vendor\": \"string - the name of the vendor\",\n\"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n\"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n},\n\"phase\": \"string enum - plan | apply\"\n}\n],\n\"commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n\"drift_detection\": \"boolean - is this a drift detection run\",\n\"flags\" : [\"string - list of flags set on the run by other policies\" ],\n\"id\": \"string - the run ID\",\n\"runtime_config\": {\n\"before_init\": [\"string - command to run before run initialization\"],\n\"project_root\": \"string - root of the Terraform project\",\n\"runner_image\": \"string - Docker image used to execute the run\",\n\"terraform_version\": \"string - Terraform version used to for the run\"\n},\n\"state\": \"string - the current run state\",\n\"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n\"type\": \"string - type of the run\",\n\"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n\"user_provided_metadata\": [\n\"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n]\n},\n\"stack\": {\n\"administrative\": \"boolean - is the stack administrative\",\n\"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n\"branch\": \"string - tracked branch of the stack\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"name\": \"string - name of the stack\",\n\"repository\": \"string - name of the source GitHub repository\",\n\"state\": \"string - current state of the stack\",\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n\"tracked_commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"worker_pool\": {\n\"id\": \"string - the worker pool ID, if it is private\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n\"name\": \"string - name of the worker pool, if it is private\",\n\"public\": \"boolean - is the worker pool public\"\n}\n}\n},\n\"terraform\": {\n\"resource_changes\": [\n{\n\"address\": \"string - full address of the resource, including modules\",\n\"type\": \"string - type of the resource, eg. aws_iam_user\",\n\"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n\"name\": \"string - name of the resource, without type\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Stack, if any\",\n\"provider_name\": \"string - provider managing the resource, eg. aws\",\n\"change\": {\n\"actions\": [\"string - create, update, delete or no-op\"],\n\"before\": \"optional object - content of the resource\",\n\"after\": \"optional object - content of the resource\"\n}\n}\n],\n\"terraform_version\": \"string\"\n}\n}\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html#aliases","title":"Aliases","text":"<p>In addition to our helper functions, we provide aliases for commonly used parts of the input data:</p> Alias Description <code>affected_resources</code> List of the resources that will be created, deleted, and updated by Terraform <code>created_resources</code> List of the resources that will be created by Terraform <code>deleted_resources</code> List of the resources that will be deleted by Terraform <code>recreated_resources</code> List of the resources that will be deleted and then created by Terraform <code>updated_resources</code> List of the resources that will be updated by Terraform"},{"location":"concepts/policy/terraform-plan-policy.html#string-sanitization","title":"String Sanitization","text":"<p>Sensitive properties in <code>\"before\"</code> and <code>\"after\"</code> objects will be sanitized to protect secret values. Sanitization hashes the value and takes the last 8 bytes of the hash.</p> <p>If you need to compare a string property to a constant, you can use the <code>sanitized(string)</code> helper function.</p> <pre><code>package spacelift\n\ndeny[\"must not target the forbidden endpoint: forbidden.endpoint/webhook\"] {\n  resource := input.terraform.resource_changes[_]\n\n  actions := {\"create\", \"delete\", \"update\"}\n  actions[resource.change.actions[_]]\n\n  resource.change.after.endpoint == sanitized(\"forbidden.endpoint/webhook\")\n}\n</code></pre>"},{"location":"concepts/policy/terraform-plan-policy.html#custom-inputs","title":"Custom inputs","text":"<p>Sometimes you might want to pass some additional data to your policy input. For example, you may want to pass the <code>configuration</code> data from the Terraform plan, the result of a third-party API or tool call. You can do that by generating a JSON file with the data you need at the root of your project. The file name must follow the pattern <code>$key.custom.spacelift.json</code> and must represent a valid JSON object. The object will be merged with the rest of the input data, as <code>input.third_party_metadata.custom.$key</code>. Be aware that the file name is case-sensitive. Below are two examples, one exposing Terraform configuration and the other exposing the result of a third-party security tool.</p> <p>Tip</p> <p>To learn more about integrating security tools with Spacelift using custom inputs, please refer to our blog post.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#example-exposing-terraform-configuration-to-the-plan-policy","title":"Example: exposing Terraform configuration to the plan policy","text":"<p>Let's say you want to expose the Terraform configuration to the plan policy to ensure that only the \"blessed\" modules are used to provision resources. You would then add the following command to the list of <code>after_plan</code> hooks:</p> <pre><code>terraform show -json spacelift.plan | jq -c '.configuration' &gt; configuration.custom.spacelift.json\n</code></pre> <p>The data will be available in the policy input as <code>input.third_party_metadata.custom.configuration</code>. Note that this depends on the <code>jq</code> tool being available in the runner image (it is installed by default on our standard image).</p>"},{"location":"concepts/policy/terraform-plan-policy.html#example-passing-custom-tool-output-to-the-plan-policy","title":"Example: passing custom tool output to the plan policy","text":"<p>For this example, let's use the awesome open-source Terraform security scanner called tfsec. What you want to accomplish is to generate <code>tfsec</code> warnings as JSON and have them reported and processed using the plan policy. In this case, you can run <code>tfsec</code> as a <code>before_init</code> hook and save the output to a file:</p> <pre><code>tfsec -s --format=json . &gt; tfsec.custom.spacelift.json\n</code></pre> <p>The data will be available in the policy input as <code>input.third_party_metadata.custom.tfsec</code>. Note that this depends on the <code>tfsec</code> tool being available in the runner image - you will need to install it yourself, either directly on the image, or as part of your <code>before_init</code> hook.</p> <p>Some vulnerability scanning tools, like tfsec, will return a non-zero exit code when they encounter vulnerabilities, which will result in a stack failure. As the majority of these tools provide a soft scanning option that will show all the vulnerabilities without considering the command as failed, we can leverage those.</p> <p>However, if there is a tool that doesn't offer that possibility, you can easily overcome this by appending <code>|| true</code> at the end of the command as this will always return a zero exit code.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#use-cases","title":"Use cases","text":"<p>Since plan policies get access to the changes to your infrastructure that are about to be introduced, they are the right place to run all sorts of checks against those changes. We believe that there are two main use cases for those checks - hard rule enforcement preventing shooting yourself in the foot and automated code review that augments human decision-making.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#organizational-rule-enforcement","title":"Organizational rule enforcement","text":"<p>In every organization, there are things you just don't do. Hard-won knowledge embodied by lengthy comments explaining that if you touch this particular line the site will go hard down and the on-call folks will be after you. Potential security vulnerabilities that can expose all your infra to the wrong crowd. Spacelift allows turning them into policies that simply can't be broken. In this case, you will most likely want to exclusively use deny rules.</p> <p>Let's start by introducing a very simple and reasonable rule - never create static AWS credentials:</p> <pre><code>package spacelift\n\n# Note that the message here is dynamic and captures resource address to provide\n# appropriate context to anyone affected by this policy. For the sake of your\n# sanity and that of your colleagues, please always add a message when denying a change.\ndeny[sprintf(message, [resource.address])] {\n  message := \"static AWS credentials are evil (%s)\"\n\n  resource := input.terraform.resource_changes[_]\n  resource.change.actions[_] == \"create\"\n\n  # This is what decides whether the rule captures a resource.\n  # There may be an arbitrary number of conditions, and they all must\n  # succeed for the rule to take effect.\n  resource.type == \"aws_iam_access_key\"\n}\n</code></pre> <p>Here's a minimal example of this rule in the Rego playground.</p> <p>If that makes sense, let's try defining a policy that implements a slightly more sophisticated piece of knowledge - that when some resources are recreated, they should be created before they're destroyed or an outage will follow. We found that to be the case with the <code>aws_batch_compute_environment</code>, among others. So here it is:</p> <pre><code>package spacelift\n\n# This is what Rego calls a set. You can add further elements to it as necessary.\nalways_create_first := { \"aws_batch_compute_environment\" }\n\ndeny[sprintf(message, [resource.address])] {\n  message  := \"always create before deleting (%s)\"\n  resource := input.terraform.resource_changes[_]\n\n  # Make sure the type is on the list.\n  always_create_first[resource.type]\n\n  some i_create, i_delete\n  resource.change.actions[i_create] == \"create\"\n  resource.change.actions[i_delete] == \"delete\"\n\n\n  i_delete &lt; i_create\n}\n</code></pre> <p>Here's the obligatory Rego playground example.</p> <p>While in most cases you'll want your rules to only look at resources affected by the change, you're not limited to doing so. You can also look at all resources and force teams to remove certain resources. Here's an example - until all AWS resources are removed all in one go, no further changes can take place:</p> <pre><code>package spacelift\n\ndeny[sprintf(message, [resource.address])] {\n  message  := \"we've moved to GCP, find an equivalent there (%s)\"\n  resource := input.terraform.resource_changes[_]\n\n  resource.provider_name == \"aws\"\n\n  # If you're just deleting, all good.\n  resource.change.actions != [\"delete\"]\n}\n</code></pre> <p>Feel free to play with a minimal example of this policy in the Rego playground.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#automated-code-review","title":"Automated code review","text":"<p>OK, so rule enforcement is very powerful, sometimes you want something more sophisticated. Instead of (or in addition to) enforcing hard rules, you can use plan policy rules to help humans understand changes better, and make informed decisions what looks good and what does not. This time we'll be adding more color to our policies and start using warn rules in addition to deny ones we've already seen in action.</p> <p>You've already seen warn rules in the first section of this article but here it is in action again:</p> <p></p> <p>It won't fail your plan and it looks relatively benign, but this little yellow note can provide great help to a human reviewer, especially when multiple changes are introduced. Also, if a stack is set to autodeploy, the presence of a single warning is enough to flag the run for a human review.</p> <p>The best way to use warn and deny rules together depends on your preferred Git workflow. We've found short-lived feature branches with Pull Requests to the tracked branch to work relatively well. In this scenario, the <code>type</code> of the <code>run</code> is important - it's PROPOSED for commits to feature branches, and TRACKED on commits to the tracked branch. You will probably want at least some of your rules to take that into account and use this mechanism to balance comprehensive feedback on Pull Requests and flexibility of being able to deploy things that humans deem appropriate.</p> <p>As a general rule when using plan policies for code review, deny when run type is PROPOSED and warn when it is TRACKED. Denying tracked runs unconditionally may be a good idea for most egregious violations for which you will not consider an exception, but when this approach is taken to an extreme it can make your life difficult.</p> <p>We thus suggest that you at most deny when the run is PROPOSED, which will send a failure status to the GitHub commit, but will give the reviewer a chance to approve the change nevertheless. If you want a human to take another look before those changes go live, either set stack autodeploy to false, or explicitly warn about potential violations. Here's an example of how to reuse the same rule to deny or warn depending on the run type:</p> <pre><code>package spacelift\n\nproposed := input.spacelift.run.type == \"PROPOSED\"\n\ndeny[reason] { proposed; reason := iam_user_created[_] }\nwarn[reason] { not proposed; reason := iam_user_created[_] }\n\niam_user_created[sprintf(\"do not create IAM users: (%s)\", [resource.address])] {\n  resource := input.terraform.resource_changes[_]\n  resource.change.actions[_] == \"create\"\n\n  resource.type == \"aws_iam_user\"\n}\n</code></pre> <p>Predictably, this fails when committed to a non-tracked (feature) branch:</p> <p></p> <p>...but as a GitHub repo admin you can still merge it if you've set your branch protection rules accordingly:</p> <p></p> <p>Cool, let's merge it and see what happens:</p> <p></p> <p>Cool, so the run stopped in its tracks and awaits human decision. At this point we still have a choice to either confirm or discard the run. In the latter case, you will likely want to revert the commit that caused the problem - otherwise all subsequent runs will be affected.</p> <p>The minimal example for the above rule is available in the Rego playground.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#require-human-review-when-resources-are-deleted-or-updated","title":"Require human review when resources are deleted or updated","text":"<p>Adding resources may ultimately cost a lot of money but it's generally pretty safe from an operational perspective. Let's use a <code>warn</code> rule to allow changes with only added resources to get automatically applied, and require all others to get a human review:</p> <pre><code>package spacelift\n\nwarn[sprintf(message, [action, resource.address])] {\n  message := \"action '%s' requires human review (%s)\"\n  review  := {\"update\", \"delete\"}\n\n  resource := input.terraform.resource_changes[_]\n  action   := resource.change.actions[_]\n\n  review[action]\n}\n</code></pre> <p>Here's a minimal example to play with.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#automatically-deploy-changes-from-selected-individuals","title":"Automatically deploy changes from selected individuals","text":"<p>Sometimes there are folks who really know what they're doing and changes they introduce can get deployed automatically, especially if they already went through code review. Below is an example that allows commits from whitelisted individuals to be deployed automatically (assumes Stack is set to autodeploy):</p> <pre><code>package spacelift\n\nwarn[sprintf(message, [author])] {\n  message     := \"%s is not on the whitelist - human review required\"\n  author      := input.spacelift.commit.author\n  whitelisted := { \"alice\", \"bob\", \"charlie\" }\n\n  not whitelisted[author]\n}\n</code></pre> <p>Here's the playground example for your enjoyment.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#require-commits-to-be-reasonably-sized","title":"Require commits to be reasonably sized","text":"<p>Massive changes make reviewers miserable. Let's automatically fail all changes that affect more than 50 resources. Let's also allow them to be deployed with mandatory human review nevertheless:</p> <pre><code>package spacelift\n\nproposed := input.spacelift.run.type == \"PROPOSED\"\n\ndeny[msg] { proposed; msg := too_many_changes[_] }\nwarn[msg] { not proposed; msg := too_many_changes[_] }\n\ntoo_many_changes[msg] {\n  threshold := 50\n\n  res := input.terraform.resource_changes\n  ret := count([r | r := res[_]; r.change.actions != [\"no-op\"]])\n  msg := sprintf(\"more than %d changes (%d)\", [threshold, ret])\n\n  ret &gt; threshold\n}\n</code></pre> <p>Here's the above example in the Rego playground, with the threshold set to 1 for simplicity.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#back-of-the-envelope-blast-radius","title":"Back-of-the-envelope blast radius","text":"<p>This is a fancy contrived example building on top of the previous one. However, rather than just looking at the total number of affected resources, it attempts to create a metric called a \"blast radius\" - that is how much the change will affect the whole stack. It assigns special multipliers to some types of resources changed and treats different types of changes differently: deletes and updates are more \"expensive\" because they affect live resources, while new resources are generally safer and thus \"cheaper\". As per our automated code review pattern, we will fail Pull Requests with changes violating this policy, but require human action through warnings when these changes hit the tracked branch.</p> <pre><code>package spacelift\n\nproposed := input.spacelift.run.type == \"PROPOSED\"\n\ndeny[msg] { proposed; msg := blast_radius_too_high[_] }\nwarn[msg] { not proposed; msg := blast_radius_too_high[_] }\n\nblast_radius_too_high[sprintf(\"change blast radius too high (%d/100)\", [blast_radius])] {\n  blast_radius := sum([blast |\n                        resource := input.terraform.resource_changes[_];\n                        blast := blast_radius_for_resource(resource)])\n\n  blast_radius &gt; 100\n}\n\nblast_radius_for_resource(resource) = ret {\n  blasts_radii_by_action := { \"delete\": 10, \"update\": 5, \"create\": 1, \"no-op\": 0 }\n\n    ret := sum([value | action := resource.change.actions[_]\n                    action_impact := blasts_radii_by_action[action]\n                    type_impact := blast_radius_for_type(resource.type)\n                    value := action_impact * type_impact])\n}\n\n# Let's give some types of resources special blast multipliers.\nblasts_radii_by_type := { \"aws_ecs_cluster\": 20, \"aws_ecs_user\": 10, \"aws_ecs_role\": 5 }\n\n# By default, blast radius has a value of 1.\nblast_radius_for_type(type) = 1 {\n    not blasts_radii_by_type[type]\n}\n\nblast_radius_for_type(type) = ret {\n    blasts_radii_by_type[type] = ret\n}\n</code></pre> <p>You can play with a minimal example of this policy in The Rego Playground.</p>"},{"location":"concepts/policy/terraform-plan-policy.html#cost-management","title":"Cost management","text":"<p>Thanks to our Infracost integration, you can take cost information into account when deciding whether to ask for human approval or to block changes entirely.</p>"},{"location":"concepts/policy/trigger-policy.html","title":"Trigger policy","text":"<p>Tip</p> <p>We now have the stack dependencies feature available which should mostly cover the use cases described below. It's a simpler and more intuitive way to define dependencies between stacks.</p>"},{"location":"concepts/policy/trigger-policy.html#purpose","title":"Purpose","text":"<p>Frequently, your infrastructure consists of a number of projects (stacks in Spacelift parlance) that are connected in some way - either depend logically on one another, or must be deployed in a particular order for some other reason - for example, a rolling deploy in multiple regions.</p> <p>Enter trigger policies. Trigger policies are evaluated at the end of each stack-blocking run (which includes tracked runs and tasks) as well as on module version releases and allow you to decide if some tracked Runs should be triggered. This is a very powerful feature, effectively turning Spacelift into a Turing machine.</p> <p>Warning</p> <p>Note that in order to support various use cases this policy type is currently evaluated every time a blocking Run reaches a terminal state, which includes states like Canceled, Discarded, Stopped or Failed in addition to the more obvious Finished. This allows for very interesting and complex workflows (eg. automated retry logic) but please be aware of that when writing your own policies.</p> <p>All runs triggered - directly or indirectly - by trigger policies as a result of the same initial run are grouped into a so-called workflow. In the trigger policy you can access all other runs in the same workflow as the currently finished run, regardless of their Stack. This lets you coordinate executions of multiple Stacks and build workflows which require multiple runs to finish in order to commence to the next stage (and trigger another Stack).</p>"},{"location":"concepts/policy/trigger-policy.html#data-input","title":"Data input","text":"<p>When triggered by a run, this is the schema of the data input that each policy request will receive:</p> <pre><code>{\n\"run\": { // the run metadata\n\"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n\"branch\": \"string - the branch the run was triggered from\",\n\"changes\": [\n{\n\"action\": \"string enum - added | changed | deleted\",\n\"entity\": {\n\"address\": \"string - full address of the entity\",\n\"name\": \"string - name of the entity\",\n\"type\": \"string - full resource type or \\\"output\\\" for outputs\",\n\"entity_vendor\": \"string - the name of the vendor\",\n\"entity_type\": \"string - the type of entity, possible values depend on the vendor\",\n\"data\": \"object - detailed information about the entity, shape depends on the vendor and type\"\n},\n\"phase\": \"string enum - plan | apply\"\n}\n],\n\"commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n\"creator_session\": {\n\"admin\": \"boolean - is the current user a Spacelift admin\",\n\"creator_ip\": \"string - IP address of the user who created the session\",\n\"login\": \"string - username of the creator\",\n\"name\": \"string - full name of the creator\",\n\"teams\": [\"string - names of teams the creator was a member of\"],\n\"machine\": \"boolean - whether the run was initiated by a human or a machine\"\n},\n\"drift_detection\": \"boolean - is this a drift detection run\",\n\"flags\" : [\"string - list of flags set on the run by other policies\" ],\n\"id\": \"string - the run ID\",\n\"runtime_config\": {\n\"before_init\": [\"string - command to run before run initialization\"],\n\"project_root\": \"string - root of the Terraform project\",\n\"runner_image\": \"string - Docker image used to execute the run\",\n\"terraform_version\": \"string - Terraform version used to for the run\"\n},\n\"state\": \"string - the current run state\",\n\"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n\"type\": \"string - type of the run\",\n\"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n\"user_provided_metadata\": [\n\"string - blobs of metadata provided using spacectl or the API when interacting with this run\"\n]\n},\n\"stack\": {\n\"administrative\": \"boolean - is the stack administrative\",\n\"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n\"branch\": \"string - tracked branch of the stack\",\n\"id\": \"string - unique stack identifier\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n\"name\": \"string - name of the stack\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Stack, if any\",\n\"repository\": \"string - name of the source GitHub repository\",\n\"state\": \"string - current state of the stack\",\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n\"tracked_commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"worker_pool\": {\n\"id\": \"string - the worker pool ID, if it is private\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n\"name\": \"string - name of the worker pool, if it is private\",\n\"public\": \"boolean - is the worker pool public\"\n}\n},\n\"stacks\": [\n{\n\"administrative\": \"boolean - is the stack administrative\",\n\"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n\"branch\": \"string - tracked branch of the stack\",\n\"id\": \"string - unique stack identifier\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n\"name\": \"string - name of the stack\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Stack, if any\",\n\"repository\": \"string - name of the source GitHub repository\",\n\"state\": \"string - current state of the stack\",\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n\"tracked_commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"worker_pool\": {\n\"id\": \"string - the worker pool ID, if it is private\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n\"name\": \"string - name of the worker pool, if it is private\",\n\"public\": \"boolean - is the worker pool public\"\n}\n}\n],\n\"workflow\": [\n{\n\"id\": \"string - Unique ID of the Run\",\n\"stack_id\": \"string - unique stack identifier\",\n\"state\": \"state - one of the states of the Run\",\n\"type\": \"string - TRACKED or TASK\"\n}\n]\n}\n</code></pre> <p>Info</p> <p>Note the presence of two similar keys: <code>stack</code> and <code>stacks</code>. The former is the Stack that the newly finished Run belongs to. The other is a list of all Stacks in the account. The schema for both is the same.</p> <p>When triggered by a new module version, this is the schema of the data input that each policy request will receive:</p> <pre><code>{\n\"module\": { // Module for which the new version was released\n\"administrative\": \"boolean - is the stack administrative\",\n\"branch\": \"string - tracked branch of the module\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"current_version\": \"Newly released module version\",\n\"id\": \"string - unique ID of the module\",\n\"name\": \"string - name of the stack\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Module, if any\",\n\"repository\": \"string - name of the source GitHub repository\",\n\"space\": {\n\"id\": \"string\",\n\"labels\": [\"string\"],\n\"name\": \"string\"\n},\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n\"worker_pool\": {\n\"id\": \"string - the worker pool ID, if it is private\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n\"name\": \"string - name of the worker pool, if it is private\",\n\"public\": \"boolean - is the worker pool public\"\n}\n}\n\n\"stacks\": [ // List of consumers of the newest available module version\n{\n\"administrative\": \"boolean - is the stack administrative\",\n\"autodeploy\": \"boolean - is the stack currently set to autodeploy\",\n\"branch\": \"string - tracked branch of the stack\",\n\"id\": \"string - unique stack identifier\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n\"name\": \"string - name of the stack\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Stack, if any\",\n\"repository\": \"string - name of the source GitHub repository\",\n\"state\": \"string - current state of the stack\",\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n\"tracked_commit\": {\n\"author\": \"string - GitHub login if available, name otherwise\",\n\"branch\": \"string - branch to which the commit was pushed\",\n\"created_at\": \"number  - creation Unix timestamp in nanoseconds\",\n\"hash\": \"string - the commit hash\",\n\"message\": \"string - commit message\"\n},\n\"worker_pool\": {\n\"id\": \"string - the worker pool ID, if it is private\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n\"name\": \"string - name of the worker pool, if it is private\",\n\"public\": \"boolean - is the worker pool public\"\n}\n}\n]\n}\n</code></pre>"},{"location":"concepts/policy/trigger-policy.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p> <p>Since trigger policies turn Spacelift into a Turing machine, you could probably use them to implement Conway's Game of Life, but there are a few more obvious use cases. Let's have a look at two of them - interdependent Stacks and automated retries.</p>"},{"location":"concepts/policy/trigger-policy.html#interdependent-stacks","title":"Interdependent stacks","text":"<p>The purpose here is to create a complex workflow that spans multiple Stacks. We will want to trigger a predefined list of Stacks when a Run finishes successfully. Here's our first take:</p> <pre><code>package spacelift\n\ntrigger[\"stack-one\"]   { finished }\ntrigger[\"stack-two\"]   { finished }\ntrigger[\"stack-three\"] { finished }\n\nfinished {\n  input.run.state == \"FINISHED\"\n  input.run.type == \"TRACKED\"\n}\n</code></pre> <p>Here's a minimal example of this rule in the Rego playground. But it's far from ideal. We can't be guaranteed that stacks with these IDs still exist in this account. Spacelift will handle that just fine, but you'll likely find if confusing. Also, for any new Stack that appears you will need to explicitly add it to the list. That's annoying.</p> <p>We can do better, and to do that, we'll use Stack labels. Labels are completely arbitrary strings that you can attach to individual Stacks, and we can use them to do something magical - have \"client\" Stacks \"subscribe\" to \"parent\" ones.</p> <p>So how's that:</p> <pre><code>package spacelift\n\ntrigger[stack.id] {\n  stack := input.stacks[_]\n  input.run.state == \"FINISHED\"\n  input.run.type == \"TRACKED\"\n  stack.labels[_] == concat(\"\", [\"depends-on:\", input.stack.id])\n}\n</code></pre> <p>Here's a minimal example of this rule in the Rego playground. The benefit of this policy is that you can attach it to all your stacks, and it will just work for your entire organization.</p> <p>Can we do better? Sure, we can even have stacks use labels to decide which types of runs or state changes they care about. Here's a mind-bending example:</p> <pre><code>package spacelift\n\ntrigger[stack.id] {\n  stack := input.stacks[_]\n  input.run.type == \"TRACKED\"\n  stack.labels[_] == concat(\"\", [\n    \"depends-on:\", input.stack.id,\n    \"|state:\", input.run.state],\n  )\n}\n</code></pre> <p>Another Rego example to play with. Now, how cool is that?</p>"},{"location":"concepts/policy/trigger-policy.html#automated-retries","title":"Automated retries","text":"<p>Here's another use case - sometimes Terraform or Pulumi deployments fail for a reason that has nothing to do with the code - think eventual consistency between various cloud subsystems, transient API errors etc. It would be great if you could restart the failed run. Oh, and let's make sure new runs are not created in a crazy loop - since policy-triggered runs trigger another policy evaluation:</p> <pre><code>package spacelift\n\ntrigger[stack.id] {\n  stack := input.stack\n  input.run.state == \"FAILED\"\n  input.run.type == \"TRACKED\"\n  is_null(input.run.triggered_by)\n}\n</code></pre> <p>Info</p> <p>Note that this will also prevent user-triggered runs from being retried. Which is usually what you want in the first place, because a triggering human is probably already babysitting the Stack anyway.</p>"},{"location":"concepts/policy/trigger-policy.html#diamond-problem","title":"Diamond Problem","text":"<p>The diamond problem happens when your stacks and their dependencies form a shape like in the following diagram:</p> <pre><code>graph LR\n  1  --&gt; 2a;\n  1  --&gt; 2b;\n  2a --&gt; 3;\n  2b --&gt; 3;</code></pre> <p>Which means that Stack 1 triggers both Stack 2a and 2b, and we only want to trigger Stack 3 when both predecessors finish. This can be elegantly solved using workflows.</p> <p>First we'll have to create a trigger policy for Stack 1:</p> <pre><code>package spacelift\n\ntrigger[\"stack-2a\"] {\n  tracked_and_finished\n}\n\ntrigger[\"stack-2b\"] {\n  tracked_and_finished\n}\n\ntracked_and_finished {\n  input.run.state == \"FINISHED\"\n  input.run.type == \"TRACKED\"\n}\n</code></pre> <p>This will trigger both Stack 2a and 2b whenever a run finishes on Stack 1.</p> <p>Now onto a trigger policy for Stack 2a and 2b:</p> <pre><code>package spacelift\n\ntrigger[\"stack-3\"] {\n  run_a := input.workflow[_]\n  run_b := input.workflow[_]\n  run_a.stack_id == \"stack-2a\"\n  run_b.stack_id == \"stack-2b\"\n  run_a.state == \"FINISHED\"\n  run_b.state == \"FINISHED\"\n}\n</code></pre> <p>Here we trigger Stack 3, whenever the runs in Stack 2a and 2b are both finished.</p> <p>You can also easily extend this to work with a label-based approach, so that you could define Stack 3's dependencies by attaching a <code>depends-on:stack-2a,stack-2b</code>label to it:</p> <pre><code>package spacelift\n\n# Helper with stack_id's of workflow runs which have already finished.\nalready_finished[run.stack_id] {\n  run := input.workflow[_]\n  run.state == \"FINISHED\"\n}\n\ntrigger[stack.id] {\n  input.run.state == \"FINISHED\"\n  input.run.type == \"TRACKED\"\n\n  # For each Stack which has a depends-on label,\n  # get a list of its dependencies.\n  stack := input.stacks[_]\n  label := stack.labels[_]\n  startswith(label, \"depends-on:\")\n  dependencies := split(trim_prefix(label, \"depends-on:\"), \",\")\n\n  # The current Stack is one of the dependencies.\n  input.stack.id == dependencies[_]\n\n  finished_dependencies := [dependency |\n                                       dependency := dependencies[_]\n                                       already_finished[dependency]]\n\n  # Check if all dependencies have finished.\n  count(finished_dependencies) == count(dependencies)\n}\n</code></pre>"},{"location":"concepts/policy/trigger-policy.html#module-updates","title":"Module updates","text":"<p>Trigger policies can be attached to modules as well. Modules track the consumers of each of their versions. When a new module version is released, the consumers of the previously newest version are assumed to be potential consumers of the newly released one. Hence, the trigger policy for a module can be used to trigger a run on all of these stacks. The module version will be updated as long as the version constraints allow the newest version to be used.</p> <p>Here is a simple trigger policy that will trigger a run on all stacks that use the latest version of the module when a new version is released:</p> <pre><code>package spacelift\n\ntrigger[stack.id] { stack := input.stacks[_] }\n</code></pre>"},{"location":"concepts/policy/push-policy/index.html","title":"Push policy","text":""},{"location":"concepts/policy/push-policy/index.html#purpose","title":"Purpose","text":"<p>Git push policies are triggered on a per-stack basis to determine the action that should be taken for each individual Stack or Module in response to a Git push or Pull Request notification. There are three possible outcomes:</p> <ul> <li>track: set the new head commit on the stack / module and create a tracked Run, ie. one that can be applied;</li> <li>propose: create a proposed Run against a proposed version of infrastructure;</li> <li>ignore: do not schedule a new Run;</li> </ul> <p>Using this policy it is possible to create a very sophisticated, custom-made setup. We can think of two main - and not mutually exclusive - use cases. The first one would be to ignore changes to certain paths - something you'd find useful both with classic monorepos and repositories containing multiple Terraform projects under different paths. The second one would be to only attempt to apply a subset of changes - for example, only commits tagged in a certain way.</p>"},{"location":"concepts/policy/push-policy/index.html#git-push-policy-and-tracked-branch","title":"Git push policy and tracked branch","text":"<p>Each stack and module points at a particular Git branch called a tracked branch. By default, any push to the tracked branch that changes a file in the project root triggers a tracked Run that can be applied. This logic can be changed entirely by a Git push policy, but the tracked branch is always reported as part of the Stack input to the policy evaluator and can be used as a point of reference.</p> <p></p> <p>When a push policy does not track a new push, the head commit of the stack/module will not be set to the tracked branch head commit. We can address this by navigating to that stack and pressing the sync button (this syncs the tracked branch head commit with the head commit of the stack/module).</p>"},{"location":"concepts/policy/push-policy/index.html#push-and-pull-request-events","title":"Push and Pull Request events","text":"<p>Spacelift can currently react to two types of events - push and pull request (also called merge request by GitLab). Push events are the default - even if you don't have a push policy set up, we will respond to those events. Pull request events are supported for some VCS providers and are generally received when you open, synchronize (push a new commit), label, or merge the pull request.</p> <p>There are some valid reasons to use pull request events in addition or indeed instead of push ones. One is that when making decisions based on the paths of affected files, push events are often confusing:</p> <ul> <li>they contain affected files for all commits in a push, not just the head commit;</li> <li>they are not context-aware, making it hard to work with pull requests - if a given push is ignored on an otherwise relevant PR, then the Spacelift status check is not provided;</li> </ul> <p>But there are more reasons depending on how you want to structure your workflow. Here are a few samples of PR-driven policies from real-life use cases, each reflecting a slightly different way of doing things.</p> <p>First, let's only trigger proposed runs if a PR exists, and allow any push to the tracked branch to trigger a tracked run:</p> <pre><code>package spacelift\n\ntrack   { input.push.branch == input.stack.branch }\npropose { not is_null(input.pull_request) }\nignore  { not track; not propose }\n</code></pre> <p>If you want to enforce that tracked runs are always created from PR merges (and not from direct pushes to the tracked branch), you can tweak the above policy accordingly to just ignore all non-PR events:</p> <pre><code>package spacelift\n\ntrack   { is_pr; input.push.branch == input.stack.branch }\npropose { is_pr }\nignore  { not is_pr }\nis_pr   { not is_null(input.pull_request) }\n</code></pre> <p>Here's another example where you respond to a particular PR label (\"deploy\") to automatically deploy changes:</p> <pre><code>package spacelift\n\ntrack   { is_pr; labeled }\npropose { true }\nis_pr   { not is_null(input.pull_request) }\nlabeled { input.pull_request.labels[_] == \"deploy\" }\n</code></pre> <p>Info</p> <p>When a run is triggered from a GitHub Pull Request and the Pull Request is mergeable (ie. there are no merge conflicts), we check out the code for something they call the \"potential merge commit\" - a virtual commit that represents the potential result of merging the Pull Request into its base branch. This should provide better quality, less confusing feedback.</p> <p>Let us know if you notice any irregularities.</p>"},{"location":"concepts/policy/push-policy/index.html#deduplicating-events","title":"Deduplicating events","text":"<p>If you're using pull requests in your flow, it is possible that we'll receive duplicate events. For example, if you push to a feature branch and then open a pull request, we first receive a push event, then a separate pull request (opened) event. When you push another commit to that feature branch, we again receive two events - push and pull request (synchronized). When you merge the pull request, we get two more - push and pull request (closed).</p> <p>It is possible that push policies resolve to the same actionable (not ignore) outcome (eg. track or propose). In those cases instead of creating two separate runs, we debounce the events by deduplicating runs created by them on a per-stack basis.</p> <p>The deduplication key consists of the commit SHA and run type. If your policy returns two different actionable outcomes for two different events associated with a given SHA, both runs will be created. In practice, this would be an unusual corner case and a good occasion to revisit your workflow.</p> <p>When events are deduplicated and you're sampling policy evaluations, you may notice that there are two samples for the same SHA, each with different input. You can generally assume that it's the first one that creates a run.</p>"},{"location":"concepts/policy/push-policy/index.html#canceling-in-progress-runs","title":"Canceling in-progress runs","text":"<p>The push policy can also be used to have the new run pre-empt any runs that are currently in progress. The input document includes the <code>in_progress</code> key, which contains an array of runs that are currently either still queued or are awaiting human confirmation. You can use it in conjunction with the cancel rule like this:</p> <pre><code>cancel[run.id] { run := input.in_progress[_] }\n</code></pre> <p>Of course, you can use a more sophisticated approach and only choose to cancel a certain type of run, or runs in a particular state. For example, the rule below will only cancel proposed runs that are currently queued (waiting for the worker):</p> <pre><code>cancel[run.id] {\n  run := input.in_progress[_]\n  run.type == \"PROPOSED\"\n  run.state == \"QUEUED\"\n}\n</code></pre> <p>You can also compare branches and cancel proposed runs in queued state pointing to a specific branch using this example policy:</p> <pre><code>cancel[run.id] {\n  run := input.in_progress[_]\n  run.type == \"PROPOSED\"\n  run.state == \"QUEUED\"\n  run.branch == input.pull_request.head.branch\n}\n</code></pre> <p>Please note that you cannot cancel module test runs. Only proposed and tracked stack runs can be canceled.</p> <p>Info</p> <p>Note that run preemption is best effort and not guaranteed. If the run is either picked up by the worker or approved by a human in the meantime then the cancellation itself is canceled.</p>"},{"location":"concepts/policy/push-policy/index.html#corner-case-track-dont-trigger","title":"Corner case: track, don't trigger","text":"<p>The <code>track</code> decision sets the new head commit on the affected stack or module. This head commit is what is going to be used when a tracked run is manually triggered, or a task is started on the stack. Usually what you want in this case is to have a new tracked Run, so this is what we do by default.</p> <p>Sometimes, however, you may want to trigger those tracked runs in a specific order or under specific circumstances - either manually or using a trigger policy. So what you want is an option to set the head commit, but not trigger a run. This is what the boolean <code>notrigger</code> rule can do for you. <code>notrigger</code> will only work in conjunction with <code>track</code> decision and will prevent the tracked run from being created.</p> <p>Please note that <code>notrigger</code> does not depend in any way on the <code>track</code> rule - they're entirely independent. Only when interpreting the result of the policy, we will only look at <code>notrigger</code> if <code>track</code> evaluates to true. Here's an example of using the two rules together to always set the new commit on the stack, but not trigger a run - for example, because it's either always triggered manually, through the API, or using a trigger policy:</p> <pre><code>track     { input.push.branch == input.stack.branch }\npropose   { not track }\nnotrigger { true }\n</code></pre>"},{"location":"concepts/policy/push-policy/index.html#take-action-from-comments-on-pull-requests","title":"Take Action from Comment(s) on Pull Request(s)","text":"<p>For more information on taking action from comments on Pull Requests, please view the documentation on pull request comments.</p>"},{"location":"concepts/policy/push-policy/index.html#customize-spacelift-ignore-event-behavior","title":"Customize Spacelift Ignore Event Behavior","text":""},{"location":"concepts/policy/push-policy/index.html#customize-vcs-check-messages-for-ignored-run-events","title":"Customize VCS Check Messages for Ignored Run Events","text":"<p>If you would like to customize messages sent back to your VCS when Spacelift runs are ignored, you can do so using the <code>message</code> function within your Push policy. Please see the example policy below as a reference for this functionality.</p>"},{"location":"concepts/policy/push-policy/index.html#customize-check-status-for-ignored-run-events","title":"Customize Check Status for Ignored Run Events","text":"<p>By default, ignored runs on a stack will return a \"skipped\" status check event, rather than a fail event. If you would like ignored run events to have a failed status check on your VCS, you can do so using the <code>fail</code> function within your Push policy. If a <code>fail</code> result is desired, set this value to true.</p>"},{"location":"concepts/policy/push-policy/index.html#example-policy","title":"Example Policy","text":"<p>The following Push policy does not trigger any run within Spacelift. Using this policy, we can ensure that the status check within our VCS (in this case, GitHub) fails and returns the message \"I love bacon.\"</p> <pre><code>fail { true }\nmessage[\"I love bacon\"] { true }\n</code></pre> <p>As a result of the above policy, users would then see this behavior within their GitHub status check:</p> <p></p> <p>Info</p> <p>Note that this behavior (customization of the message and failing of the check within the VCS), is only applicable when runs do not take place within Spacelift.</p>"},{"location":"concepts/policy/push-policy/index.html#tag-driven-terraform-module-release-flow","title":"Tag-driven Terraform Module Release Flow","text":"<p>Some users prefer to manage their Terraform Module versions using git tags, and would like git tag events to push their module to the Spacelift module registry. Using a fairly simple Push policy, this is supported. To do this, you'll want to make sure of the <code>module_version</code> block within a Push policy attached your module, and then set the version using the tag information from the git push event.</p> <p>For example, the following example Push policy will trigger a tracked run when a tag event is detected. The policy then parses the tag event data and uses that value for the module version, in the below example we remove a git tag prefixed with <code>v</code> as the Terraform Module Registry only supports versions in a numeric <code>X.X.X</code> format.</p> <p>It's important to note that for this policy, you will need to provide a mock, non-existent version for proposed runs. This precaution has been taken to ensure that pull requests do not encounter check failures due to the existence of versions that are already in use.</p> <pre><code>package spacelift\n\nmodule_version := version {\n    version := trim_prefix(input.push.tag, \"v\")\n    not propose\n}\n\nmodule_version := \"&lt;X.X.X&gt;\" {\n    propose\n}\n\ntrack { module_version != \"\" }\n</code></pre>"},{"location":"concepts/policy/push-policy/index.html#allow-forks","title":"Allow forks","text":"<p>By default, we don't trigger runs when a forked repository opens a pull request against your repository. This is because of a security concern: if let's say your infrastructure is open source, someone forks it, implements some unwanted junk in there, then opens a pull request for the original repository, it'd run automatically with the prankster's code included.</p> <p>Info</p> <p>The cause is very similar to GitHub Actions where they don't expose repository secrets when forked repositories open pull requests.</p> <p>If you still want to allow it, you can explicitly do it with <code>allow_fork</code> rule. For example, if you trust certain people or organizations:</p> <pre><code>propose { true }\nallow_fork {\n  validOwners := {\"johnwayne\", \"microsoft\"}\n  validOwners[input.pull_request.head_owner]\n}\n</code></pre> <p>In the above case, we'll allow a forked repository to run, only if the owner of the forked repository is either <code>johnwayne</code> or <code>microsoft</code>.</p> <p><code>head_owner</code> field means different things in different VCS providers:</p>"},{"location":"concepts/policy/push-policy/index.html#github-github-enterprise","title":"GitHub / GitHub Enterprise","text":"<p>In GitHub, <code>head_owner</code> is the organization or the person owning the forked repository. It's typically in the URL: <code>https://github.com/&lt;head_owner&gt;/&lt;forked_repository&gt;</code></p>"},{"location":"concepts/policy/push-policy/index.html#gitlab","title":"GitLab","text":"<p>In GitLab, it is the group of the repository which is typically the URL of the repository: <code>https://gitlab.com/&lt;head_owner&gt;/&lt;forked_repository&gt;</code></p>"},{"location":"concepts/policy/push-policy/index.html#azure-devops","title":"Azure DevOps","text":"<p>Azure DevOps is a special case because they don't provide us the friendly name of the <code>head_owner</code>. In this case, we need to refer to <code>head_owner</code> as the ID of the forked repository's project which is a UUID. One way to figure out this UUID is to open <code>https://dev.azure.com/&lt;organization&gt;/_apis/projects</code> website which lists all projects with their unique IDs. You don't need any special access to this API, you can just simply open it in your browser.</p> <p>Official documentation of the API.</p>"},{"location":"concepts/policy/push-policy/index.html#bitbucket-cloud","title":"Bitbucket Cloud","text":"<p>In Bitbucket Cloud, <code>head_owner</code> means workspace. It's in the URL of the repository: <code>https://www.bitbucket.org/&lt;workspace&gt;/&lt;forked_repository&gt;</code>.</p>"},{"location":"concepts/policy/push-policy/index.html#bitbucket-datacenterserver","title":"Bitbucket Datacenter/Server","text":"<p>In Bitbucket Datacenter/Server, it is the project key of the repository. The project key is not the display name of the project, but the abbreviation in all caps.</p> <p></p>"},{"location":"concepts/policy/push-policy/index.html#approval-and-mergeability","title":"Approval and Mergeability","text":"<p>The <code>pull_request</code> property on the input to a push policy contains the following fields:</p> <ul> <li><code>approved</code> - indicates whether the PR has been approved.</li> <li><code>mergeable</code> - indicates whether the PR can be merged.</li> <li><code>undiverged</code> - indicates that the PR branch is not behind the target branch.</li> </ul> <p>The following example shows a push policy that will automatically deploy a PR's changes once it has been approved, any required checks have completed, and the PR has a <code>deploy</code> label added to it:</p> <pre><code>package spacelift\n\n# Trigger a tracked run if a change is pushed to the stack branch\ntrack {\n  affected\n  input.push.branch == input.stack.branch\n}\n\n# Trigger a tracked run if a PR is approved, mergeable, undiverged and has a deploy label\ntrack {\n  is_pr\n  is_clean\n  is_approved\n  is_marked_for_deploy\n}\n\n# Trigger a proposed run if a PR is opened\npropose {\n  is_pr\n}\n\nis_pr {\n  not is_null(input.pull_request)\n}\n\nis_clean {\n  input.pull_request.mergeable\n  input.pull_request.undiverged\n}\n\nis_approved {\n  input.pull_request.approved\n}\n\nis_marked_for_deploy {\n  input.pull_request.labels[_] == \"deploy\"\n}\n</code></pre> <p>Each source control provider has slightly different features, and because of this the exact definition of <code>approved</code> and <code>mergeable</code> varies slightly between providers. The following sections explain the differences.</p>"},{"location":"concepts/policy/push-policy/index.html#azure-devops_1","title":"Azure DevOps","text":"<ul> <li><code>approved</code> means the PR has at least one approving review (including approved with suggestions).</li> <li><code>mergeable</code> means that the PR branch has no conflicts with the target branch, and any blocking policies are approved.</li> </ul> <p>Info</p> <p>Please note that we are unable to calculate divergance across forks in Azure DevOps, so the <code>undiverged</code> property will always be <code>false</code> for PRs created from forks.</p>"},{"location":"concepts/policy/push-policy/index.html#bitbucket-cloud_1","title":"Bitbucket Cloud","text":"<ul> <li><code>approved</code> means that the PR has at least one approving review from someone other than the PR author.</li> <li><code>mergeable</code> means that the PR branch has no conflicts with the target branch.</li> </ul>"},{"location":"concepts/policy/push-policy/index.html#bitbucket-datacenterserver_1","title":"Bitbucket Datacenter/Server","text":"<ul> <li><code>approved</code> means that the PR has at least one approving review from someone other than the PR author.</li> <li><code>mergeable</code> means that the PR branch has no conflicts with the target branch.</li> </ul>"},{"location":"concepts/policy/push-policy/index.html#github-github-enterprise_1","title":"GitHub / GitHub Enterprise","text":"<ul> <li><code>approved</code> means that the PR has at least one approval, and also meets any minimum approval requirements for the repo.</li> <li><code>mergeable</code> means that the PR branch has no conflicts with the target branch, and any branch protection rules have been met.</li> </ul>"},{"location":"concepts/policy/push-policy/index.html#gitlab_1","title":"GitLab","text":"<ul> <li><code>approved</code> means that the PR has at least one approval. If approvals are required, it is only <code>true</code> when all required approvals have been made.</li> <li><code>mergeable</code> means that the PR branch has no conflicts with the target branch, any blocking discussions have been resolved, and any required approvals have been made.</li> </ul>"},{"location":"concepts/policy/push-policy/index.html#data-input","title":"Data input","text":"<p>As input, Git push policy receives the following document:</p> <pre><code>{\n\"in_progress\": [{\n\"based_on_local_workspace\": \"boolean - whether the run stems from a local preview\",\n\"branch\": \"string - the branch this run is based on\",\n\"created_at\": \"number - creation Unix timestamp in nanoseconds\",\n\"triggered_by\": \"string or null - user or trigger policy who triggered the run, if applicable\",\n\"type\": \"string - run type: proposed, tracked, task, etc.\",\n\"state\": \"string - run state: queued, unconfirmed, etc.\",\n\"updated_at\": \"number - last update Unix timestamp in nanoseconds\",\n\"user_provided_metadata\": [\"string - blobs of metadata provided using spacectl or the API when interacting with this run\"]\n}],\n\"pull_request\": {\n\"action\": \"string - opened, reopened, closed, merged, edited, labeled, synchronize, unlabeled\",\n\"action_initiator\": \"string\",\n\"approved\": \"boolean - indicates whether the PR has been approved\",\n\"author\": \"string\",\n\"base\": {\n\"affected_files\": [\"string\"],\n\"author\": \"string\",\n\"branch\": \"string\",\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"message\": \"string\",\n\"tag\": \"string\"\n},\n\"closed\": \"boolean\",\n\"diff\": [\"string - list of files changed between base and head commit\"],\n\"draft\": \"boolean - indicates whether the PR is marked as draft\",\n\"head\": {\n\"affected_files\": [\"string\"],\n\"author\": \"string\",\n\"branch\": \"string\",\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"message\": \"string\",\n\"tag\": \"string\"\n},\n\"head_owner\": \"string\",\n\"id\": \"number\",\n\"labels\": [\"string\"],\n\"mergeable\": \"boolean - indicates whether the PR can be merged\",\n\"title\": \"string\",\n\"undiverged\": \"boolean - indicates whether the PR is up to date with the target branch\"\n}\n\"push\": {\n// For Git push events, this contains the pushed commit.\n// For Pull Request events,\n// this contains the head commit or merge commit if available (merge event).\n\"affected_files\": [\"string\"],\n\"author\": \"string\",\n\"branch\": \"string\",\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"message\": \"string\",\n\"tag\": \"string\"\n},\n\"stack\": {\n\"additional_project_globs\": [\"string - list of arbitrary, user-defined selectors\"],\n\"administrative\": \"boolean\",\n\"autodeploy\": \"boolean\",\n\"branch\": \"string\",\n\"id\": \"string\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"locked_by\": \"optional string - if the stack is locked, this is the name of the user who did it\",\n\"name\": \"string\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Stack, if any\",\n\"repository\": \"string\",\n\"state\": \"string\",\n\"terraform_version\": \"string or null\",\n\"tracked_commit\": {\n\"author\": \"string\",\n\"branch\": \"string\",\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"hash\": \"string\",\n\"message\": \"string\"\n},\n\"worker_pool\": {\n\"public\": \"boolean - indicates whether the worker pool is public or not\"\n}\n}\n}\n</code></pre> <p>When triggered by a new module version, this is the schema of the data input that each policy request will receive:</p> <pre><code>{\n\"module\": { // Module for which the new version was released\n\"administrative\": \"boolean - is the stack administrative\",\n\"branch\": \"string - tracked branch of the module\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors\"],\n\"current_version\": \"Newly released module version\",\n\"id\": \"string - unique ID of the module\",\n\"name\": \"string - name of the stack\",\n\"namespace\": \"string - repository namespace, only relevant to GitLab repositories\",\n\"project_root\": \"optional string - project root as set on the Module, if any\",\n\"repository\": \"string - name of the source GitHub repository\",\n\"space\": {\n\"id\": \"string\",\n\"labels\": [\"string\"],\n\"name\": \"string\"\n},\n\"terraform_version\": \"string or null - last Terraform version used to apply changes\",\n\"worker_pool\": {\n\"id\": \"string - the worker pool ID, if it is private\",\n\"labels\": [\"string - list of arbitrary, user-defined selectors, if the worker pool is private\"],\n\"name\": \"string - name of the worker pool, if it is private\",\n\"public\": \"boolean - is the worker pool public\"\n}\n}\n\"pull_request\": {\n\"action\": \"string - opened, reopened, closed, merged, edited, labeled, synchronize, unlabeled\",\n\"action_initiator\": \"string\",\n\"approved\": \"boolean - indicates whether the PR has been approved\",\n\"author\": \"string\",\n\"base\": {\n\"affected_files\": [\"string\"],\n\"author\": \"string\",\n\"branch\": \"string\",\n\"created_at\": \"number (timestamp in nanoseconds)\",\n\"message\": \"string\",\n\"tag\": \"string\"\n}\n}\n}\n</code></pre> <p>Based on this input, the policy may define boolean <code>track</code>, <code>propose</code> and <code>ignore</code> rules. The positive outcome of at least one <code>ignore</code> rule causes the push to be ignored, no matter the outcome of other rules. The positive outcome of at least one <code>track</code> rule triggers a tracked run. The positive outcome of at least one <code>propose</code> rule triggers a proposed run.</p> <p>Warning</p> <p>If no rules are matched, the default is to ignore the push. Therefore it is important to always supply an exhaustive set of policies - that is, making sure that they define what to track and what to propose in addition to defining what they ignore.</p> <p>It is also possible to define an auxiliary rule called <code>ignore_track</code>, which overrides a positive outcome of the <code>track</code> rule but does not affect other rules, most notably the <code>propose</code> one. This can be used to turn some of the pushes that would otherwise be applied into test runs.</p>"},{"location":"concepts/policy/push-policy/index.html#examples","title":"Examples","text":"<p>Tip</p> <p>We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs.</p> <p>If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.</p>"},{"location":"concepts/policy/push-policy/index.html#ignoring-certain-paths","title":"Ignoring certain paths","text":"<p>Ignoring changes to certain paths is something you'd find useful both with classic monorepos and repositories containing multiple Terraform projects under different paths. When evaluating a push, we determine the list of affected files by looking at all the files touched by any of the commits in a given push.</p> <p>Info</p> <p>This list may include false positives - eg. in a situation where you delete a given file in one commit, then bring it back in another commit, and then push multiple commits at once. This is a safer default than trying to figure out the exact scope of each push.</p> <p>Let's imagine a situation where you only want to look at changes to Terraform definitions - in HCL or JSON  - inside one the <code>production/</code> or <code>modules/</code> directory, and have track and propose use their default settings:</p> <pre><code>package spacelift\n\ntrack   { input.push.branch == input.stack.branch }\npropose { input.push.branch != \"\" }\nignore  { not affected }\n\naffected {\n  some i, j, k\n\n  tracked_directories := {\"modules/\", \"production/\"}\n  tracked_extensions := {\".tf\", \".tf.json\"}\n\n  path := input.push.affected_files[i]\n\n  startswith(path, tracked_directories[j])\n  endswith(path, tracked_extensions[k])\n}\n</code></pre> <p>As an aside, note that in order to keep the example readable we had to define <code>ignore</code> in a negative way as per the Anna Karenina principle. A minimal example of this policy is available here.</p>"},{"location":"concepts/policy/push-policy/index.html#status-checks-and-ignored-pushes","title":"Status checks and ignored pushes","text":"<p>By default when the push policy instructs Spacelift to ignore a certain change, no commit status check is sent back to the VCS. This behavior is explicitly designed to prevent noise in monorepo scenarios where a large number of stacks are linked to the same Git repo.</p> <p>However, in certain cases one may still be interested in learning that the push was ignored, or just getting a commit status check for a given stack when it's set as required as part of GitHub branch protection set of rules, or simply your internal organization rules.</p> <p>In that case, you may find the <code>notify</code> rule useful. The purpose of this rule is to override default notification settings. So if you want to notify your VCS vendor even when a commit is ignored, you can define it like this:</p> <pre><code>package spacelift\n\n# other rules (including ignore), see above\n\nnotify { ignore }\n</code></pre> <p>Info</p> <p>The notify rule (false by default) only applies to ignored pushes, so you can't set it to <code>false</code> to silence commit status checks for proposed runs.</p>"},{"location":"concepts/policy/push-policy/index.html#applying-from-a-tag","title":"Applying from a tag","text":"<p>Another possible use case of a Git push policy would be to apply from a newly created tag rather than from a branch. This in turn can be useful in multiple scenarios - for example, a staging/QA environment could be deployed every time a certain tag type is applied to a tested branch, thereby providing inline feedback on a GitHub Pull Request from the actual deployment rather than a plan/test. One could also constrain production to only apply from tags unless a Run is explicitly triggered by the user.</p> <p>Here's an example of one such policy:</p> <pre><code>package spacelift\n\ntrack   { re_match(`^\\d+\\.\\d+\\.\\d+$`, input.push.tag) }\npropose { input.push.branch != input.stack.branch }\n</code></pre>"},{"location":"concepts/policy/push-policy/index.html#default-git-push-policy","title":"Default Git push policy","text":"<p>If no Git push policies are attached to a stack or a module, the default behavior is equivalent to this policy:</p> <pre><code>package spacelift\n\ntrack {\n  affected\n  input.push.branch == input.stack.branch\n}\n\npropose { affected }\npropose { affected_pr }\n\nignore  { \n    not affected\n    not affected_pr\n}\nignore  { input.push.tag != \"\" }\n\naffected {\n    filepath := input.push.affected_files[_]\n    startswith(filepath, input.stack.project_root)\n}\n\naffected {\n    filepath := input.push.affected_files[_]\n    glob_pattern := input.stack.additional_project_globs[_]\n    glob.match(glob_pattern, [\"/\"], filepath)\n}\n\naffected_pr {\n    filepath := input.pull_request.diff[_]\n    startswith(filepath, input.stack.project_root)\n}\n\naffected_pr {\n    filepath := input.pull_request.diff[_]\n    glob_pattern := input.stack.additional_project_globs[_]\n    glob.match(glob_pattern, [\"/\"], filepath)\n}\n</code></pre>"},{"location":"concepts/policy/push-policy/index.html#waiting-for-cicd-artifacts","title":"Waiting for CI/CD artifacts","text":"<p>There are cases where you want pushes to your repo to trigger a run in Spacelift, but only after a CI/CD pipeline (or a part of it) has completed. An example would be when you want to trigger an infra deploy after some docker image has been built and pushed to a registry. This is achievable via push policies by using the External Dependencies feature.</p>"},{"location":"concepts/policy/push-policy/index.html#prioritization","title":"Prioritization","text":"<p>Although we generally recommend using our default scheduling order (tracked runs and tasks, then proposed runs, then drift detection runs), you can also use push policies to prioritize certain runs over others. For example, you may want to prioritize runs triggered by a certain user or a certain branch. To that effect, you can use the boolean <code>prioritize</code> rule to mark a run as prioritized. Here's an example:</p> <pre><code>package spacelift\n\n# other rules (including ignore), see above\n\nprioritize { input.stack.labels[_] == \"prioritize\" }\n</code></pre> <p>The above example will prioritize runs on any stack that has the <code>prioritize</code> label set. Please note that run prioritization only works for private worker pools. An attempt to prioritize a run on a public worker pool using this policy will be a no-op.</p>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html","title":"External Dependencies","text":""},{"location":"concepts/policy/push-policy/run-external-dependencies.html#purpose","title":"Purpose","text":"<p>External dependencies is a feature within push policies that allows a user to define a set of dependencies, which, while being external to Spacelift, must be completed before a Spacelift run can start. A common use case of this feature is making Spacelift wait for a CI/CD pipeline to complete before executing a run.</p>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#how-it-works","title":"How it works","text":"<p>Using this feature consists of two parts:</p> <ul> <li>defining dependencies in push policies</li> <li>marking dependencies as finished or failed using spacectl</li> </ul>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#defining-dependencies","title":"Defining dependencies","text":"<p>To define dependencies, you need to add a <code>external_dependency</code> rule to your push policy definition. This way, any run that gets created via this policy, also has the dependency defined.</p> <p>The following rule adds a dependency to all runs created by a policy.</p> <pre><code>external_dependency[sprintf(\"%s-binary-build\", [input.push.hash])] { true }\n</code></pre> <p>You can of course have more complex rules, that decide on the set of external dependencies based on e.g. the current stack's labels.</p> <p>Warning</p> <p>Make sure to include unique strings such as commit hashes in the dependencies names, as this is the only way to ensure that the dependency is unique for each source control event.</p>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#marking-dependencies-as-finished-or-failed","title":"Marking dependencies as finished or failed","text":"<p>To mark a dependency as finished or failed, you need to use the spacectl command line tool. You can do so with following commands:</p> <pre><code>spacectl run-external-dependency mark-completed --id \"&lt;commit-sha&gt;-binary-build\" --status finished\n\nspacectl run-external-dependency mark-completed --id \"&lt;commit-sha&gt;-binary-build\" --status failed\n</code></pre> <p>Info</p> <p>Run will be eligible for execution only after all of its dependencies are marked as finished. At the same time, if any of the dependencies has failed, the run will be marked as failed as well.</p> <p>Warning</p> <p>In order to mark a run dependency as finished or failed, spacectl needs to be authenticated and have write access to all the spaces that have runs with the given dependency defined.</p>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#example-with-gh-actions","title":"Example with GH Actions","text":"<p>The following example shows how to use this feature with GitHub Actions.</p> <p>The first thing we need to do is to define a push policy with dependencies. Our policy will look like this:</p> <pre><code>package spacelift\n\ntrack {\n    input.push != null\n    input.push.branch == input.stack.branch\n}\n\nexternal_dependency[sprintf(\"%s-binary-build\", [input.push.hash])] { true }\nexternal_dependency[sprintf(\"%s-docker-image-build\", [input.push.hash])] { true }\n</code></pre> <p>We are defining two dependencies. One for a binary build and one for a docker image build.</p> <p>Next, we need to create a GitHub Action pipeline that will mark the dependencies as finished or failed. This pipeline will define two jobs, one for each dependency. We will use <code>sleep</code> to mock the build process.</p> <pre><code>name: Build\n\non:\npush:\n\njobs:\nbuild-binaries:\nname: Build binaries\nruns-on: ubuntu-latest\n\nsteps:\n- name: Install spacectl\nuses: spacelift-io/setup-spacectl@main\nenv:\nGITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n- name: Check out repository code\nuses: actions/checkout@v4\n\n- name: Build binaries\nrun: |\nsleep 15\necho \"building binaries done\"\n\n- name: Notify Spacelift of build completion (success)\nif: success()\nenv:\nSPACELIFT_API_KEY_ENDPOINT: https://&lt;youraccount&gt;.app.spacelift.io\nSPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\nSPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\nrun: spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-binary-build\" --status finished\n\n- name: Notify Spacelift of build completion (failed)\nif: failure()\nenv:\nSPACELIFT_API_KEY_ENDPOINT: https://&lt;youraccount&gt;.app.spacelift.io\nSPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\nSPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\nrun: spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-binary-build\" --status failed\n\nbuild-docker-images:\nname: Build docker images\nruns-on: ubuntu-latest\n\nsteps:\n- name: Install spacectl\nuses: spacelift-io/setup-spacectl@main\nenv:\nGITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n- name: Check out repository code\nuses: actions/checkout@v4\n\n- name: Build docker images\nrun: |\nsleep 30\necho \"building images done\"\n\n- name: Notify Spacelift of build completion (success)\nif: success()\nenv:\nSPACELIFT_API_KEY_ENDPOINT: https://&lt;youraccount&gt;.app.spacelift.io\nSPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\nSPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\nrun: spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-docker-image-build\" --status finished\n\n- name: Notify Spacelift of build completion (failed)\nif: failure()\nenv:\nSPACELIFT_API_KEY_ENDPOINT: https://&lt;youraccount&gt;.app.spacelift.io\nSPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\nSPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\nrun: spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-docker-image-build\" --status failed\n</code></pre> <p>Warning</p> <p>Make sure to replace <code>&lt;youraccount&gt;</code> with your Spacelift account name and fill in necessary secrets if you decide to use this example.</p>"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#testing-example","title":"Testing example","text":"<p>Having the policy and the pipeline defined, we can now test it. Creating a new commit in the repository will trigger the pipeline. As we can see a run was created in Spacelift, but it's in queued state. The run will not start until all the dependencies are marked as finished.</p> <p></p> <p>After the binary-build dependency has been marked as completed the run is still queued, as the docker-image-build dependency is still not resolved.</p> <p></p> <p>The run starts only after all the dependencies are marked as finished.</p> <p></p> <p>We can also test what happens if a step in the pipeline fails. In order to test this, we can change the <code>build-binaries</code> job in the pipeline from</p> <pre><code>      - name: Build binaries\nrun: |\nsleep 15\necho \"building binaries done\"\n</code></pre> <p>to</p> <pre><code>      - name: Build binaries\nrun: |\nsleep 15\necho \"building binaries failed\"\nexit 1\n</code></pre> <p>Now, when we push a commit to the repo, after a while, we will see that the new run is marked as failed, with a note explaining that one of the dependencies is marked as failed.</p> <p></p>"},{"location":"concepts/run/index.html","title":"Run","text":"<p>Every job that can touch your Spacelift-managed infrastructure is called a Run. There are four main types of runs, and each of them warrants a separate section.</p> <p>Three of them are children of Stacks:</p> <ul> <li>task, which is a freeform command you can execute on your infrastructure;</li> <li>proposed run, which serves as a preview of introduced changes;</li> <li>tracked run, which is a form of deployment;</li> </ul> <p>There's also a fourth type of run - module test case. Very similar to a tracked run, it's executed on a Terraform module.</p>"},{"location":"concepts/run/index.html#execution-model","title":"Execution model","text":"<p>In Spacelift, each run is executed on a worker node, inside a Docker container. We maintain a number of these worker nodes (collectively known as the public worker pool) that are available to all customers, but also allow individual customers to run our agent on their end, for their exclusive use. You can read more about worker pools here.</p> <p>Regardless of whether you end up using a private or a public worker pool, each Spacelift run involves a handover between spacelift.io (which we like to call the mothership) and the worker node. After the handover, the worker node is fully responsible for running the job and communicating the results of the job back to the mothership.</p> <p>Info</p> <p>It's important to know that it's always the worker node executing the run and accessing your infrastructure, never the mothership.</p>"},{"location":"concepts/run/index.html#common-run-states","title":"Common run states","text":"<p>Regardless of the type of the job performed, some phases and terminal states are common. We discuss them here, so that we can refer to them when describing various types of runs in more detail.</p>"},{"location":"concepts/run/index.html#queued","title":"Queued","text":"<p>Queued means that the run is not Ready for processing, as it's either blocked, waiting for dependencies to finish, or requires additional action from a user.</p> <p>Spacelift serializes all state-changing operations to the Stack. Both tracked runs and tasks have the capacity to change the state, they're never allowed to run in parallel. Instead, each of them gets an exclusive lock on the stack, blocking others from starting.</p> <p>If your run or task is currently blocked by something else holding the lock on the stack, you'll see the link to the blocker in run state list:</p> <p></p> <p>There can also be other reasons why the run is in this state and is not being promoted to state Ready:</p> <ul> <li>It needs to be approved</li> <li>It's waiting for dependant stacks to finish</li> <li>It's waiting for external dependencies to finish</li> </ul> <p>Queued is a passive state meaning no operations are performed while a run is in this state. The user can also cancel the run while it's still queued, transitioning it to the terminal Canceled state.</p>"},{"location":"concepts/run/index.html#ready","title":"Ready","text":"<p>Ready state means that the run is eligible for processing and is waiting for a worker to become available. A run will stay in this state until it's picked up by a worker.</p> <p>When using the public worker pool, you will have to wait until a worker becomes available. For private workers, please refer to the worker pools documentation for troubleshooting advice.</p> <p>Ready is a passive state meaning no operations are performed while a run is in this state. When a worker is available, the state will automatically transition to Preparing. The user is also able to cancel the run even if it's ready for processing, transitioning it to the terminal Canceled state.</p>"},{"location":"concepts/run/index.html#canceled","title":"Canceled","text":"<p>Canceled state means that the user has manually stopped a Queued run or task even before it had the chance to be picked up by the worker.</p> <p>Canceled is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can follow it.</p>"},{"location":"concepts/run/index.html#preparing","title":"Preparing","text":"<p>The preparing state is the first one where real work is done. At this point both the run is eligible for processing and there's a worker node ready to process it. The preparing state is all about the handover and dialog between spacelift.io and the worker.</p> <p>Here's an example of one such handover:</p> <p></p> <p>Note that Ground Control refers to the bit directly controlled by us, in a nod to late David Bowie. The main purpose of this phase is for Ground Control to make sure that the worker node gets everything that's required to perform the job, and that it can take over the execution.</p> <p>Once the worker is able to pull the Docker image and use it to start the container, this phase is over and the initialization phase begins. If the process fails for whatever reason, the run is marked as failed.</p>"},{"location":"concepts/run/index.html#initializing","title":"Initializing","text":"<p>The last phase where actual work is done and which is common to all types of run is the initialization. This phase is handled exclusively by the worker and involves running pre-initialization hooks and vendor-specific initialization process. For Terraform stacks it would mean running <code>terraform init</code>, in the right directory and with the right parameters.</p> <p>Important thing to note with regards to pre-initialization hooks and the rest of the initialization process is that all these run in the same shell session, so environment variables exported by pre-initialization hooks are accessible to the vendor-specific initialization process. This is often the desired outcome when working with external secret managers like HashiCorp Vault.</p> <p>If this phase fails for whatever reason, the run is marked as failed. Otherwise, the next step is determined by the type of the run being executed.</p>"},{"location":"concepts/run/index.html#failed","title":"Failed","text":"<p>If a run transitions into the failed state means that something, at some point went wrong and this state can follow any state. In most cases this will be something related to your project like:</p> <ul> <li>errors in the source code;</li> <li>pre-initialization checks failing;</li> <li>plan policies rejecting your change;</li> <li>deployment-time errors;</li> </ul> <p>In rare cases errors in Spacelift application code or third party dependencies can also make the job fail. These cases are clearly marked by a notice corresponding to the failure mode, reported through our exception tracker and immediately investigated.</p> <p>Failed is a passive state meaning no operations are performed while the run is in this state. It's also a terminal state meaning that no further state can supersede it.</p>"},{"location":"concepts/run/index.html#finished","title":"Finished","text":"<p>Finished state means that the run was successful, though the success criteria will depend on the type of run. Please read the documentation for the relevant run type for more details.</p> <p>Finished is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it.</p>"},{"location":"concepts/run/index.html#stopping-runs","title":"Stopping runs","text":"<p>Some types of runs in some phases may safely be interrupted. We allow sending a stop signal from the GUI and API to the run, which is then passed to the worker handling the job. It's then up to the worker to handle or ignore that signal.</p> <p>Stopped state indicates that a run has been stopped while Initializing or Planning, either manually by the user or - for proposed changes - also by Spacelift. Proposed changes will automatically be stopped when a newer version of the code is pushed to their branch. This is mainly designed to limit the number of unnecessary API calls to your resource providers, though it saves us a few bucks on EC2, too.</p> <p>Here's an example of a run manually stopped while Initializing:</p> <p></p> <p>Stopped is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it.</p>"},{"location":"concepts/run/index.html#logs-retention","title":"Logs Retention","text":"<p>Run logs are kept for 60 days.</p>"},{"location":"concepts/run/index.html#run-prioritization","title":"Run prioritization","text":"<p>You can change the priority of a run while it's in a non-terminal state if a private worker pool is processing the run. We will process prioritized runs before runs from other stacks. However, a stack's standard order of run types (tasks, tracked, and proposed runs) execution will still be respected.</p> <p>The priority of a run can be changed in the worker pool queue view:</p> <p></p> <p>Runs can be prioritized manually or automatically, using a push policy. Note that we only recommend automatic prioritization in special circumstances. For most users, the default prioritization (tracked runs, then proposed runs, then drift detection runs) provides an optimal user experience.</p>"},{"location":"concepts/run/index.html#zero-trust-model","title":"Zero Trust Model","text":"<p>For your most sensitive Stacks you can use additional verification of Runs based on arbitrary metadata you can provide to Runs when creating or confirming them.</p> <p>This metadata can be passed through the API or by using the spacectl CLI to create or confirm runs. Every time such an interaction happens, you can add a new piece of metadata, which will form a list of metadata blobs inside of the Run. This will then be available in policies, including the private-worker side initialization policy.</p> <p>This way you can i.e. sign the runs when you confirm them and later verify this signature inside of the private worker, through the initialization policy. There you can use the exec function, which lets you run an arbitrary binary inside of the docker image. This binary would verify that the content of the run and signature match and that the signature is a proper signature of somebody from your company.</p> <p>This works for any kind of Run, including tasks.</p>"},{"location":"concepts/run/proposed.html","title":"Proposed run (preview)","text":"<p>Proposed runs are previews of changes that would be applied to your infrastructure if the new code was to somehow become canonical, for example by pushing it to the tracked branch.</p> <p>Proposed runs are generally triggered by Git push events. By default, whenever a push occurs to any branch other than the tracked branch, a proposed run is started - one for each of the affected stacks. This default behavior can be extensively customized using our push policies.</p> <p>The purpose of proposed runs is not to make changes to your infrastructure but to merely preview and report them during the planning phase.</p>"},{"location":"concepts/run/proposed.html#planning","title":"Planning","text":"<p>Once the workspace is prepared by the Initializing phase, planning runs a vendor-specific preview command and interprets the results. For Terraform that command is <code>terraform plan</code>, for Pulumi - <code>pulumi preview</code>. The result of the planning phase is the collection of currently managed resources and outputs as well as planned changes. This is used as an input to plan policies (optional) and to calculate the delta - always.</p> <p>Note that the Planning phase can be safely stopped by the user.</p> <p>On Ansible stacks, this phase can be skipped without execution by setting the <code>SPACELIFT_SKIP_PLANNING</code> environment variable to true in the stack's environment variables.</p>"},{"location":"concepts/run/proposed.html#plan-policies","title":"Plan policies","text":"<p>If any plan policies are attached to the current stack, each of these policies is evaluated to automatically determine whether the change is acceptable according to the rules adopted by your organization. Here is an example of an otherwise successful planning phase that still fails due to policy violations:</p> <p></p> <p>You can read more about plan policies here.</p>"},{"location":"concepts/run/proposed.html#pending-review","title":"Pending Review","text":"<p>If any plan policy results in a warning and there are approval policies attached, the run will enter a pending review state after planning, in which approval policies will be evaluated. The run will only finish once all approval policies approve.</p> <p>This is useful to e.g. block pull requests related to a proposed run when the changes made by the run should be reviewed by another team, like security.</p>"},{"location":"concepts/run/proposed.html#delta","title":"Delta","text":"<p>If the planning phase is successful (which includes policy evaluation), Spacelift analyses the diff and counts the resources and outputs that would be added, changed and deleted if the changes were to be applied. Here's one example of one such delta being reported:</p> <p></p>"},{"location":"concepts/run/proposed.html#success-criteria","title":"Success criteria","text":"<p>The planning phase will fail if:</p> <ul> <li>infrastructure definitions are incorrect - eg. malformed, invalid etc.;</li> <li>external APIs (eg. AWS, GCP, Azure etc.) fail when previewing changes;</li> <li>plan policies return one or more deny reasons;</li> <li>a worker node crashes - eg. you kill a private worker node while it's executing the job;</li> </ul> <p>If that happens, the run will transition to the failed state. Otherwise, the proposed run terminates in the finished state.</p>"},{"location":"concepts/run/proposed.html#reporting","title":"Reporting","text":"<p>The results of proposed runs are reported in multiple ways:</p> <ul> <li>always - in VCS, as commit statuses and pull request comments - please refer to GitHub and GitLab documentation for the exact details;</li> <li>through Slack notifications - if set up;</li> <li>through webhooks - if set up;</li> </ul>"},{"location":"concepts/run/pull-request-comments.html","title":"Pull Request Comments","text":""},{"location":"concepts/run/pull-request-comments.html#pull-request-plan-commenting","title":"Pull Request Plan Commenting","text":""},{"location":"concepts/run/pull-request-comments.html#via-notification-policy","title":"Via Notification policy","text":"<p>We have a nice example in our Notification policy documentation that shows how to add a comment to a pull request about changed resources. It is fully customizable, so you can change the message to your liking.</p>"},{"location":"concepts/run/pull-request-comments.html#via-stack-label-legacy","title":"Via Stack label (legacy)","text":"<p>To enable this feature, simply add the label <code>feature:add_plan_pr_comment</code> to the stacks you wish to have plan commenting enabled for on pull requests.</p> <p></p> <p>Once enabled, on any future pull request activity, the result of the plan will be commented onto the pull request.</p> <p></p>"},{"location":"concepts/run/pull-request-comments.html#pull-request-comment-driven-actions","title":"Pull Request Comment-Driven Actions","text":"<p>To enable support for pull request comment events in Spacelift, you will need to ensure the following permissions are enabled within your VCS app integration. Note that if your VCS integration was created using the Spacelift VCS setup wizard, then these permissions have already been set up automatically, and no action is needed.</p> <ul> <li>Read access to <code>issues</code> repository permissions</li> <li>Subscribe to <code>issues:comments</code> event</li> </ul> <p>Assuming the above permissions are configured on your VCS application, you can then access pull request comment event data from within your Push policy, and build customizable workflows using this data.</p> <p>Warning</p> <p>Please note that Spacelift will only evaluate comments that begin with<code>/spacelift</code> to prevent users from unintended actions against their resources managed by Spacelift. Furthermore, Spacelift only processes event data for new comments, and will not receive event data for edited or deleted comments.</p> <p>Example Push policy to trigger a tracked run from a pull request comment event:</p> <pre><code>package spacelift\n\ntrack {\n    input.pull_request.action == \"commented\"\n    input.pull_request.comment == concat(\" \", [\"/spacelift\", \"deploy\", input.stack.id])\n}\n</code></pre> <p>Using a Push policy such as the example above, a user could trigger a tracked run on their Spacelift stack by commenting something such as:</p> <pre><code>/spacelift deploy my-stack-id\n</code></pre>"},{"location":"concepts/run/run-promotion.html","title":"Run Promotion","text":""},{"location":"concepts/run/run-promotion.html#what-is-run-promotion","title":"What is Run Promotion?","text":"<p>As a quick summary of the differences between the two types of runs: proposed runs only display changes to be made, while tracked runs apply (deploy) the proposed changes.</p> <p>Promoting a proposed run is triggering a tracked run for the same Git commit.</p>"},{"location":"concepts/run/run-promotion.html#using-run-promotion","title":"Using Run Promotion","text":""},{"location":"concepts/run/run-promotion.html#pre-requisites","title":"Pre-Requisites","text":"<ol> <li> <p>For a run to be promote-able, the proposed run must point to a commit that is newer than the stack's current commit.</p> </li> <li> <p>To promote a run, you first need to ensure that you have <code>Allow run promotion</code> enabled in the stack settings of your stack(s) in which you'd like to promote runs.</p> </li> </ol> <p>Stack Settings &gt; Behavior &gt; Advanced Options &gt; Allow Run Promotion</p> <p></p>"},{"location":"concepts/run/run-promotion.html#promote-from-proposed-run-view","title":"Promote from Proposed Run View","text":"<p>Assuming you've enabled Run Promotion within the stack settings, and the commit to be promoted is newer than the stack's current commit. On a given proposed run, you will then see the \"Promote\" button as seen in the screenshot below. You simply need to click this button to promote the proposed run into a tracked run.</p> <p></p>"},{"location":"concepts/run/run-promotion.html#promote-from-a-pull-request","title":"Promote from a Pull Request","text":"<p>For Spacelift users utilizing GitHub, a similar feature is available directly from the GitHub Pull Request. Assuming the same criteria is met as mentioned previously: 1) The commit to be promoted is newer than the stack's current commit 2) Run Promotion is enabled on the stack - Then, you will see a <code>Deploy</code> button available within the Checks tab of the pull request. This button will promote your proposed run into a tracked run.</p> <p></p>"},{"location":"concepts/run/task.html","title":"Task","text":"<p>While tasks enjoy the privilege of having their own GUI screen, they're just another type of run. The core difference is that after the common initialization phase, a task will run your custom command instead of a string of preordained vendor-specific commands.</p>"},{"location":"concepts/run/task.html#the-purpose-of-tasks","title":"The purpose of tasks","text":"<p>The main purpose of task is to perform arbitrary changes to your infrastructure in a coordinated, safe and audited way. Tasks allow ultimate flexibility and can be used to check the environment (see the humble <code>ls -la</code> on the above screenshot), perform benign read-only operations like showing parts of the Terraform state, or even make changes to the state itself, like tainting a resource.</p> <p>Given that thanks to the Docker integration you have full control over the execution environment of your workloads, there's hardly a limit to what you can do.</p> <p>Danger</p> <p>Obvious abuse of shared workers will get you kicked out of the platform. But you can abuse private workers all you like.</p> <p>With the above caveat, let's go through the main benefits of using Spacelift tasks.</p>"},{"location":"concepts/run/task.html#coordinated","title":"Coordinated","text":"<p>Tasks are always treated as operations that may change the underlying state, and are thus serialized. No two tasks will ever run simultaneously, nor will a task execute while a tracked run is in progress. This prevents possible concurrent updates to the state that would be possible without a centrally managed mutex.</p> <p>What's more, some tasks will be more sensitive than others. While a simple <code>ls</code> is probably nothing to be afraid of, the two-way state migration described above could have gone wrong in great many different ways. The stack locking mechanism thus allows taking exclusive control over one or more stacks by a single individual, taking the possibility of coordination to a whole new level.</p>"},{"location":"concepts/run/task.html#safe","title":"Safe","text":"<p>Any non-trivial infrastructure project will inevitably be full of credentials and secrets which are possibly too sensitive to be stored even on a work laptop. Tasks allow any operation to be executed remotely, preventing the leak of sensitive data.</p> <p>Spacelift's integration with infra providers like AWS also allows authentication without any credentials whatsoever, which further protects you from the shame and humiliation of having the keys to the kingdom leaked by running the occasional <code>env</code> command, as you do. Actually, let's run it in Spacelift to see what gives:</p> <p></p> <p>Yes, the secrets are masked in the output and won't leak due to an honest mistake.</p> <p>Danger</p> <p>There are limits to the extent we can protect you from a determined attacker with write access to your stack. We don't want to give you a false sense of security where none is warranted. You may want to look into task policies to prevent certain (or even all) tasks from being executed.</p>"},{"location":"concepts/run/task.html#audited","title":"Audited","text":"<p>Unlike arbitrary operations performed on your local machine, tasks are recorded for eternity, so in cases where some archaeology is necessary, it's easy to see what happened and when. Tasks are attributed to individuals (or API keys) that triggered them and the access model ensures that only stack writers can trigger tasks, giving you even more control over your infrastructure.</p>"},{"location":"concepts/run/task.html#performing-a-task","title":"Performing a task","text":"<p>Apart from the common run phases described in the general run documentation, tasks have just one extra state - performing. That's when the arbitrary user-supplied command is executed, wrapped in <code>sh -c</code> to support all the shell goodies we all love to abuse. In particular, you can use as many <code>&amp;&amp;</code> and <code>||</code> as you wish.</p> <p>Performing a task will succeed and the task will transition to the finished state iff the exit code of your command is 0 (the Unix standard). Otherwise the task is marked as failed. Performing cannot be stopped since we must assume that it involves state changes.</p> <p>Tip</p> <p>Tasks are not interactive so you may need to add the <code>-force</code> argument to the command.</p>"},{"location":"concepts/run/task.html#skipping-initialization","title":"Skipping initialization","text":"<p>In rare cases it may be useful to perform tasks without initialization - like when the initialization would fail without some changes being introduced. An obvious example here are Terraform version migrations. This corner case is served by explicitly skipping the initialization. In the GUI (on by default), you will find the toggle to control this behavior:</p> <p></p> <p>Let's execute a task without initialization on a Terraform stack:</p> <p></p> <p>Notice how the operation failed because it is expected to be executed on an initialized Terraform workspace. But the same operation would easily succeed if we were to run it in the default mode, with initialization:</p> <p></p>"},{"location":"concepts/run/test-case.html","title":"Module test case","text":"<p>Module test cases are special types of runs that are executed not on Stacks but on Spacelift-managed Terraform modules. Note that this article does not cover modules specifically - for that please refer directly to their documentation. The purpose of this article is to explain how modules test cases are executed and how they're different from other types of runs.</p> <p>In a nutshell, module test cases are almost identical to autodeployed tracked runs. But unlike stacks, modules are stateless and do not manage any resources directly, so just after the changes are applied and resources are created, they are immediately destroyed during the destroying phase. Here is what a fully successful module test case looks like:</p> <p></p> <p>Note that the destroying phase will run regardless of whether the applying phase succeeds or fails. This is because the failure could have been partial, and some resources may still have been created.</p> <p>The destroying phase can be skipped without execution by setting the <code>SPACELIFT_SKIP_DESTROYING</code> environment variable to true in the stack's environment variables.</p>"},{"location":"concepts/run/test-case.html#success-criteria","title":"Success criteria","text":"<p>A module test case will only transition to the successful finished state if all the previous phases succeed. If any of the phases fails, the run will be marked as failed.</p>"},{"location":"concepts/run/tracked.html","title":"Tracked run (deployment)","text":"<p>Tracked runs represent the actual changes to your infrastructure caused by changes to your infrastructure definitions and/or configuration. In that sense, they can be also called deployments. Tracked runs are effectively an extension of proposed runs - instead of stopping at the planning phase, they also allow you to apply the previewed changes.</p> <p>They are presented on the Runs screen, which is the main screen of the Stack view:</p> <p></p> <p>Each of the tracked runs is represented by a separate element containing some information about the attempted deployment:</p> <p></p> <p>Also worth noting is the colorful strip present on some runs - as long as the planning phase was successful this visually represents the resources and outputs diff introduced by the change:</p> <p></p>"},{"location":"concepts/run/tracked.html#triggering-tracked-runs","title":"Triggering tracked runs","text":"<p>Tracked runs can be triggered in of the three ways - manually by the user, by a Git push or by a trigger policy.</p>"},{"location":"concepts/run/tracked.html#triggering-manually","title":"Triggering manually","text":"<p>Any account admin or stack writer can trigger a tracked run on a stack:</p> <p></p> <p>Runs triggered by individuals and machine users are marked accordingly:</p> <p></p> <p></p>"},{"location":"concepts/run/tracked.html#triggering-from-git-events","title":"Triggering from Git events","text":"<p>Tracked runs can also be triggered by Git push and tag events. By default, whenever a push occurs to the tracked branch, a tracked run is started - one for each of the affected stacks. This default behavior can be extensively customized using our push policies.</p> <p>Runs triggered by Git push and/or tag events can are marked accordingly:</p> <p></p>"},{"location":"concepts/run/tracked.html#triggering-from-policies","title":"Triggering from policies","text":"<p>Trigger policies can be used to create sophisticated workflows representing arbitrarily complex processes like staged rollouts or cascading updates. This is an advanced topic, which is described in more detail in its dedicated section. But if you see something like this, be aware of the fact that a trigger policy must have been involved:</p> <p></p>"},{"location":"concepts/run/tracked.html#handling-no-op-changes","title":"Handling no-op changes","text":"<p>If the planning phase detects no changes to the resources and outputs managed by the stack, the tracked run is considered a no-op. In that case it transitions directly from planning to finished state, just like a proposed run. Otherwise, it will go through the approval flow.</p>"},{"location":"concepts/run/tracked.html#approval-flow","title":"Approval flow","text":"<p>If the tracked run detects a change to its managed resources or outputs, it goes through the approval flow. This can be automated or manual.</p> <p>The automated flow involves a direct transition between the planning and applying phase, without an extra human intervention. This is a convenient but not always the safest option.</p> <p>Changes can be automatically applied if both these conditions are met:</p> <ul> <li>autodeploy is turned \"on\" for the Stack;</li> <li>if plan policies are attached, none of them returns any warnings;</li> </ul> <p>Otherwise, the change will go through the manual flow described below.</p>"},{"location":"concepts/run/tracked.html#unconfirmed","title":"Unconfirmed","text":"<p>If a change is detected and human approval is required, a tracked run will transition from the planning state to unconfirmed. At that point the worker node encrypts uploads the entire workspace to a dedicated Amazon S3 location and finishes its involvement with the run.</p> <p>The resulting changes are shown to the user for the final approval:</p> <p></p> <p>Unconfirmed is a passive state meaning no operations are performed while a run is in this state.</p> <p>If the user approves (confirms) the plan, the run transitions to the temporary Confirmed state and waits for a worker node to pick it up. If the user doesn't like the plan and discards it, the run transitions to the terminal Discarded state.</p>"},{"location":"concepts/run/tracked.html#targeted-replan","title":"Targeted replan","text":"<p>When a run is in the Unconfirmed state it's also possible to replan it. When replanning, a user is able to generate a new plan to apply by only picking specific changes from the current plan. This is working similarly to how passing the <code>-target</code> option to a terraform plan command does, without giving you the headache of writing the name of each resource you want to add to your targeted run.</p> <p>To get to the replan screen after the run reaches the unconfirmed state, click on the Changes button in the left corner, select the resources you would like to have a targeted plan for, and then, the replan option will pop out, similar to the screenshot below.</p> <p></p> <p>Warning</p> <p>Targeted replan feature is currently in open public beta and is free to use regardless of your pricing plan. Once GA, it will be available as part of our Enterprise plan.</p>"},{"location":"concepts/run/tracked.html#discarded","title":"Discarded","text":"<p>Discarded state follows Unconfirmed and indicates that the user did not like the changes detected by the Planning phase.</p> <p>Discarded is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it.</p>"},{"location":"concepts/run/tracked.html#confirmed","title":"Confirmed","text":"<p>Confirmed state follows Unconfirmed indicates that a user has accepted the plan generated in the Planning phase and wants to apply it but no worker has picked up the job yet. This state is similar to Queued in a sense that shows only temporarily until one of the workers picks up the associated job and changes the state to Applying. On the other hand, there is no way to stop a run once it's confirmed.</p> <p>Confirmed is a passive state meaning no operations are performed while a run is in this state.</p>"},{"location":"concepts/run/tracked.html#applying","title":"Applying","text":"<p>If the run required a manual approval step, this phase is preceded by another handover (preparing phase) since the run again needs to be yielded to a worker node. This preparing phase is subtly different internally but ultimately serves the same purpose from the user perspective. Here's an example:</p> <p></p> <p>This preparation phase is very unlikely to fail, but if it does (eg. the worker node becomes unavailable during the transition), the run will transition to the terminal failed state. If the handover succeeds, or the run does not go through the manual approval process, the applying phase begins and attempts to deploy the changes. Here's an example:</p> <p></p> <p>This phase can be skipped without execution by setting the <code>SPACELIFT_SKIP_APPLYING</code> environment variable to true in the stack's environment variables.</p>"},{"location":"concepts/run/tracked.html#success-criteria","title":"Success criteria","text":"<p>If the run is a no-op or the applying phase succeeds, the run transitions to the finished state. On the other hand, if anything goes wrong, the run is marked as failed.</p>"},{"location":"concepts/run/tracked.html#reporting","title":"Reporting","text":"<p>The results of tracked runs are reported in multiple ways:</p> <ul> <li>as deployments in VCS unless the change is a no-op - please refer to GitHub and GitLab documentation for the exact details;</li> <li>through Slack notifications - if set up;</li> <li>through webhooks - if set up;</li> </ul>"},{"location":"concepts/run/user-provided-metadata.html","title":"User-Provided Metadata","text":"<p>Occasionally you might want to add additional information to your Runs which isn\u2019t handled on a first-class basis by Spacelift. You can attach this kind of information using the run metadata parameter, which is available through spacectl as well as the GraphQL API.</p>"},{"location":"concepts/run/user-provided-metadata.html#usage","title":"Usage","text":"<p>Let\u2019s start with a small example. You\u2019ll need a private worker for this.</p> <p>On the machine where the worker resides, create a simple policy in a file:</p> <pre><code>package spacelift\nsample { true }\n</code></pre> <p>And then start the worker with an additional environment variable:</p> <pre><code>SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY=/path/to/your/policy.rego\n</code></pre> <p>This policy will make our launcher sample each initialization policy evaluation and print it as a log on stderr.</p> <p>We\u2019ll also need a Stack to which this worker is attached.</p> <p>We can now trigger a run and provide an arbitrary metadata string:</p> <pre><code>~&gt; spacectl stack deploy --id testing-spacelift --run-metadata \"deploy-metadata\"\nYou have successfully created a deployment\nThe live run can be visited at http://cube2222.app.spacelift.tf/stack/testing-spacelift/run/01FEKAGP4AYV0DWP4QDFTANRES\n</code></pre> <p>And in the private worker logs we should suitably see (formatted for readability):</p> <pre><code>{\n\"caller\": \"setup.go:201\",\n\"level\": \"info\",\n\"msg\": \"Sample 0/INITIALIZATION/7YGHCNF7W6VMBQ49XQ42MH4JD1/allow\",\n\"sample\": {\n\"body\": \"package spacelift\\nsample { true }\\n\",\n\"input\": {\n\"docker_image\": \"\",\n\"run\": {\n\"based_on_local_workspace\": false,\n\"changes\": [],\n\"commit\": {\n\"author\": \"cube2222\",\n\"branch\": \"master\",\n\"created_at\": 1628243895000000000,\n\"message\": \"Update main.tf\"\n},\n\"created_at\": 1630588655754344000,\n\"id\": \"01FEKAGP4AYV0DWP4QDFTANRES\",\n\"state\": \"PREPARING\",\n\"triggered_by\": \"api::01FEGXFB7TWQ2NNF95W7HPRE2E\",\n\"updated_at\": 1630588656197898500,\n\"user_provided_metadata\": [\n\"deploy-metadata\". // (1)\n]\n},\n\"static_run_environment\": {\n\"account_name\": \"cube2222\",\n\"auto_deploy\": false,\n\"before_apply\": null,\n\"before_init\": null,\n\"command\": \"\",\n\"commit_branch\": \"master\",\n\"commit_sha\": \"7d629c6c3f3b6da07e28a87727f0586e577d98c1\",\n\"endpoint_logs\": \"tcp://169.254.0.3:1983\",\n\"endpoint_registry\": \"registry.spacelift.io\",\n\"environment_variables\": {},\n\"project_root\": \"\",\n\"refresh_state\": true,\n\"repository_path\": \"cube2222/testing-spacelift\",\n\"run_type\": \"TRACKED\",\n\"run_ulid\": \"01FEKAGP4AYV0DWP4QDFTANRES\",\n\"skip_init\": false,\n\"stack_labels\": null,\n\"stack_slug\": \"testing-spacelift\",\n\"terraform_version\": \"0.14.10\",\n\"vendor_specific_config\": {\n\"vendor\": \"terraform\",\n\"typed_config\": {\n\"use_terragrunt\": false,\n\"use_infracost\": false\n}\n}\n},\n\"worker_version\": \"development\"\n},\n\"outcome\": \"allow\",\n\"results\": {\n\"deny\": [],\n\"sample\": true\n},\n\"error\": \"\"\n},\n\"ts\": \"2021-09-02T13:17:37.785219048Z\"\n}\n</code></pre> <ol> <li>The metadata string</li> </ol> <p>Great!</p> <p>We can now go ahead and confirm this run:</p> <pre><code>~&gt; spacectl stack confirm --id testing-spacelift --run-metadata \"confirm-metadata\" --run 01FEKAGP4AYV0DWP4QDFTANRES\nYou have successfully confirmed a deployment\nThe live run can be visited at http://cube2222.app.spacelift.tf/stack/testing-spacelift/run/01FEKAGP4AYV0DWP4QDFTANRES\n</code></pre> <p>In the policy sample log for the relevant metadata key we\u2019ll see an additional entry, which was added when confirming:</p> <pre><code>\"user_provided_metadata\": [\n\"deploy-metadata\",\n\"confirm-metadata\"\n]\n</code></pre> <p>And that's basically it! It's a very flexible building block which lets you build various automation and compliance helper tooling.</p>"},{"location":"concepts/run/user-provided-metadata.html#run-signatures","title":"Run signatures","text":"<p>A standard use case for this feature would be to sign your runs when you\u2019re creating them.</p> <p>You'll have to bring the infrastructure for managing keys and signatures yourself - usually you'll already have something like that internally. But in short you can create a cryptographic signature of the parameters for a run you\u2019re about to create - based on the commit SHA, run type, stack, date, etc. - and then you can pass that signature to Spacelift when creating the run.</p> <p>Later, in the initialization policy you can use the exec function to run your custom binary for verifying that signature. This way - for your most sensitive stacks - you can verify whether runs you are receiving from the Spacelift backend are legit, intentionally created by an employee of your company.</p> <p>Tip</p> <p>We created a reference implementation to demonstrate how to sign runs and verify signatures.</p>"},{"location":"concepts/spaces/index.html","title":"Spaces","text":""},{"location":"concepts/spaces/index.html#introduction","title":"Introduction","text":"<p>With increased usage comes a bigger need for access control and self-service. Having a single team of admins doesn't scale when you start having tens or hundreds of people using Spacelift daily.</p> <p>You may want to delegate partial admin rights to those teams, enable them to manage their own limited environments, but without giving them keys to the whole account and other teams' environments.</p> <p>In Spacelift, this can be achieved by splitting your account into multiple Spaces.</p> <p>Spaces are sets that can be filled with various kinds of Spacelift entities: Stacks, Policies, Contexts, Modules, Worker Pools, and Cloud Integrations.</p> <p>Initially, you start with a <code>root</code> and a <code>legacy</code> space. The <code>root</code> space is the top-level space of your account, while the <code>legacy</code> space exists for backward compatibility with pre-spaces RBAC. You can then create more spaces, ending up with a big tree of segregated environments.</p>"},{"location":"concepts/spaces/index.html#what-problems-do-spaces-solve","title":"What problems do Spaces solve?","text":"<p>First and foremost, Spaces let you give users limited admin access. This means that, in their space, they can create Stacks, Policies, etc. while not interfering with entities present in other Spaces.</p> <p>Additionally, Spaces bring the ability to share resources between all these isolated environments, which means you can have a single worker pool and a single set of policies that can be reused by the whole organization.</p>"},{"location":"concepts/spaces/access-control.html","title":"Access Control","text":"<p>With Spaces, the whole permission management process is done within Login policies where you can specify what type of role a user gets within the given spaces.</p>"},{"location":"concepts/spaces/access-control.html#roles","title":"Roles","text":"<p>There are 3 roles that you can assign to users on a space-by-space basis:</p> <ul> <li>Read - cannot create or modify neither stacks nor any attachable entities, but can view them</li> <li>Write - an extension to Read, as it can actually trigger runs in the stacks it sees</li> <li>Admin - can create and modify stacks and attachable entities, as well as trigger runs</li> </ul> <p>A special case is someone who is given Admin permissions to the <code>root</code> space - we would call that person a Root Space Admin. Any Root Space Admin is perceived to be an admin of the whole account. Only they can modify the space tree or access account-wide settings.</p> <p>Info</p> <p>Administrative stacks get the Admin role in the space they belong to.</p> <p>Warning</p> <p>Administrative stacks in the legacy space get admin access to the root space for backward compatibility reasons.</p> <p>A comparison table of what users with given roles are capable of can be found below.</p> Action\\Who Root Space Admin Admin Writer Reader Setup SSO \u2705 \u274c \u274c \u274c Setup VCS \u2705 \u274c \u274c \u274c Manage Sessions \u2705 \u274c \u274c \u274c Manage Spaces \u2705 \u274c \u274c \u274c Manage Login Policies \u2705 \u274c \u274c \u274c Manage Stack Config Settings \u2705 \u2705 \u274c \u274c Manage Worker Pools, Contexts \u2705 \u2705 \u274c \u274c Manage Stack Env Vars \u2705 \u2705 \u2705 \u274c Trigger runs \u2705 \u2705 \u2705 \u274c View Stacks \u2705 \u2705 \u2705 \u2705 View Spaces \u2705 \u2705 \u2705 \u2705 View Worker Pools, Contexts \u2705 \u2705 \u2705 \u2705"},{"location":"concepts/spaces/access-control.html#login-policies","title":"Login Policies","text":"<p>The way you can control access to Spaces in your Spacelift account is by using Login policies.</p> <p>We have introduced new rules that allow you to assign access to spaces:</p> <ul> <li>space_read</li> <li>space_write</li> <li>space_admin</li> </ul> <p>Here is a valid login policy that uses all of them:</p> <pre><code>package spacelift\n\ndevelopers := { \"bob\" }\nlogin   := input.session.login\nis_developer { developers[login] }\nallow { is_developer }\n\n# Let's give every developer read access to any space\nspace_read[space.id] {\n  space := input.spaces[_]\n  is_developer\n}\n\n# Assign write role to developers for spaces with \"developers-are-writers\" label\nspace_write[space.id] {\n  space := input.spaces[_]\n  space.labels[_] == \"developers-are-writers\"\n  is_developer\n}\n\n# Assign admin role for the root space for anyone in the admin team\nspace_admin[\"root\"] {\n  input.session.teams[_] == \"admin\"\n}\n</code></pre> <p>Warning</p> <p>Please note that Login policies are only allowed to be created in the <code>root</code> space, therefore only <code>root</code> space admins and administrative stacks, as well as <code>legacy</code> space administrative stacks can create or modify them.</p> <p>Warning</p> <p>A logged-in user's access levels only get updated when they log out and in again, so newly added spaces might not be visible to some users. An exception is that the space's creator immediately gets access to it.</p>"},{"location":"concepts/spaces/access-control.html#inheritance","title":"Inheritance","text":"<p>Inheritance is a toggle that defines whether a space inherits resources from its parent space or not. When set to true, any stack in the child space can use resources such as worker pools or contexts from the parent space. If a space inherits from a parent and its parent inherits from the grandparent, then the space inherits from the grandparent as well.</p> <p>Inheritance also modifies how roles propagate between spaces.</p> <p>In a scenario when inheritance between spaces is turned off, the roles are propagated only down the space tree. On the other hand, when inheritance is enabled, then a user with any role in the child space also gets Read role in their parent.</p> <p>Below is a diagram that demonstrates how this all works in practice. This is a view for a user that was given the following roles by Login policies:</p> <ul> <li>Read in <code>read access space</code></li> <li>Write in <code>write access space</code></li> <li>Admin in <code>admin access space</code></li> </ul> <p>Dashed lines indicate a lack of inheritance, while when it's enabled the lines are solid.</p> <p></p> <p>Let's analyze the tree starting from the left.</p> <p>As mentioned, the user was granted Write access to the <code>write access space</code> space. Because inheritance is enabled, they also received Read access to the <code>access propagates up</code> space and the <code>root</code> space. The reason for that is to allow users to see resources that their space can now use.</p> <p>Next, the user was given Admin access to the <code>admin access space</code> space. Regardless of the inheritance being off, they also received Admin access to the <code>access propagates down</code> space. This makes sense, as we want to allow admins to still manage their spaces subtree even if they want to disable resource sharing between some spaces.</p> <p>Finally, the user was given Read access to the <code>read access space</code> space. Because inheritance is off, they did not receive Read access to the <code>legacy</code> space.</p> <p>Info</p> <p>Inheritance works well with Policy Autoattachment. By creating a policy with an <code>autoattach:*</code> label you enforce it on all the stacks in all the spaces that inherit the space where the policy resides.</p>"},{"location":"concepts/spaces/creating-a-space.html","title":"Creating a Space","text":"<p>Creating and modifying spaces takes place in the Spaces tab in the UI.</p> <p></p>"},{"location":"concepts/spaces/creating-a-space.html#spaces-view","title":"Spaces View","text":"<p>The view shows a tree of all the spaces visible to you in your account. The immutable <code>root</code> space is at the top, and from there you can build any tree structure you want.</p> <p>This view behaves a bit differently for users that are admins of the <code>root</code> space.</p> <p>If you are not an admin of the <code>root</code> space, you will only see the spaces that you have access to, additionally, you will see a path from the spaces you have access to, to the <code>root</code> space. Each space card would indicate what access level to that space you have.</p> <p>If you are an admin of the <code>root</code> space you don't see individual access levels, as you are automatically an admin of all spaces.</p> <p></p>"},{"location":"concepts/spaces/creating-a-space.html#creating-a-single-space","title":"Creating a Single Space","text":"<p>If you are an admin of the <code>root</code> space you can create a space either by clicking a Create space button in the top right corner of the view, or by clicking the blue addition button at the bottom of a space card.</p> <p></p> <p>Clicking either will open a modal where you can enter the name of the space, its parent and optionally a description and labels.</p> <p>You then can click Create to create the space.</p> <p></p>"},{"location":"concepts/spaces/creating-a-space.html#editing-the-space","title":"Editing the Space","text":"<p>An admin of the <code>root</code> space has the ability to modify spaces. This can be done by clicking on a space card, which opens up a form similar to the one used for creating a space. After performing any changes you can click Save to save them.</p> <p></p>"},{"location":"concepts/spaces/deleting-a-space.html","title":"Deleting a Space","text":"<p>Click on the Delete space button to delete a space:</p> <p></p> <p>The delete operation is only permitted if the space is empty, and it is a leaf of the space tree.</p> <p>If any of the following conditions is true, the space cannot be deleted:</p> <ul> <li>The space has any child spaces</li> <li>The space contains any stacks or modules</li> <li>The space contains any attachable entities such as worker pools, contexts, cloud integrations, etc.</li> </ul> <p>Info</p> <p>You cannot delete neither the <code>root</code> nor the <code>legacy</code> space.</p>"},{"location":"concepts/spaces/migrating-out-of-the-legacy-space.html","title":"Migrating out of the Legacy Space","text":"<p>After the introduction of Spaces most of your resources will end up in the <code>legacy</code> space. This space is there to provide backward-compatibility with your existing setup, and it's the only place Access policies still work.</p> <p>To get the most out of Spacelift, you'll want to move out of the <code>legacy</code> space into other spaces. For each entity, you can do that by going into the entity's settings and choosing a new space for it. However, you'll have to keep a few things in mind.</p> <p>First, moving entities can't break any relationships between them. For example, if you have a policy attached to a stack, you can't move the policy to a space where it won't be accessible to the stack.</p> <p>Second, you have to move entities one by one, which means that moving stacks with their attachments (policies, contexts, worker pools, etc.) needs to be done in multiple steps.</p> <ol> <li>Create your new space as a child of the <code>root</code> space with inheritance enabled.</li> <li>Move all the attachable entities (policies, contexts, worker pools, etc.) from the <code>legacy</code> space to the <code>root</code> space. This way they're accessible from both the <code>legacy</code> space and your new space.</li> <li>Move the stacks to your new space.</li> <li>Move the attachable entities to your new space.</li> </ol> <p></p> <p>Additionally, when you're ready to stop using access policies in the <code>legacy space</code>, you can start providing users a level of access to the <code>legacy</code> space using the space_admin, space_write, and space_read directives of the Login policy. The moment the Login policy specifies any of these for a user for the <code>legacy</code> space, Access policies will stop being evaluated for this user.</p>"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html","title":"Moving a Space or any Entity","text":"<p>If after creating multiple Spaces you conclude that the tree structure could be better, it is possible to restructure the tree completely.</p> <p>Restructuring the tree can be achieved either by creating new spaces and then moving entities (stacks, policies, etc.) to them or by moving (re-parenting) spaces (which includes the entities they contain).</p> <p>Both approaches are valid, but some conditions must be satisfied for the move operations to succeed.</p>"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-a-stack","title":"Moving a Stack","text":"<p>Stacks can be moved to a different space using the Name tab in the Stack's settings.</p> <p></p> <p>For the move operation to succeed, the stack has to maintain access to any attachable resources it uses (worker pools, contexts, cloud integrations, etc.).</p> <p>In other words, the new space the stack will be in must either inherit (directly or indirectly via parental chain) from the spaces that the used attachable resources are in or those resources have to be defined in the new space.</p>"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-an-attachable-entity-worker-pool-context-etc","title":"Moving an Attachable Entity (Worker Pool, Context, etc.)","text":"<p>All the attachable entities can be moved to a different space either:</p> <p></p> <p>Moving entities is possible only if all the stacks that have been using them would still be able to access them after the move in compliance with the inheritance rules.</p>"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-a-space","title":"Moving a Space","text":"<p>Moving a space is essentially changing its parent space.</p> <p>This can be done by choosing a new parent in the space's settings:</p> <p></p> <p>This operation affects the whole subtree, because all the children of the space being moved remain its children, so they change their location in the tree as well.</p> <p>Changing a parent for a space is possible only if after the re-parenting process all the stacks in the whole affected subtree would still be able to access any entities that are attached outside of the subtree, in compliance with the inheritance rules.</p>"},{"location":"concepts/spaces/structuring-your-spaces-tree.html","title":"Structuring your Spaces Tree","text":"<p>Based on experience with other tools, a first intuition might be to structure your spaces team-first or service-first. Something akin to the following picture:</p> <p></p> <p>However, there are likely Spacelift resources you will want to reuse across all development environment services, not across all environments for a single service. For example, a Worker Pool. That is because resources like Worker Pools are usually shared across security domains, not logical domains.</p> <p>Due to this an architecture akin to the following is more advisable:</p> <p></p> <p>This way you can create your Worker Pools, Contexts and Policies in the <code>dev</code>, <code>preprod</code>, and <code>prod</code> spaces, and then reuse them in all spaces below those.</p>"},{"location":"concepts/stack/index.html","title":"Stack","text":"<p>Stack is one of the core concepts in Spacelift. A stack is an isolated, independent entity and the choice of the word mirrors products like AWS CloudFormation, or Pulumi (of which we support both). You can think about a stack as a combination of source code, current state of the managed infrastructure (eg. Terraform state file) and configuration in the form of environment variables and mounted files.</p> <p>Unless you're using Spacelift only to host and test private Terraform modules, your account should probably contain one or more stacks to be of any use. For example:</p> <p></p> <p>Here's a few helpful articles about stacks:</p> <ul> <li>In this article, you can learn how to create a new stack;</li> <li>Here you can see all the settings that are available for the stack;</li> <li>Here you can learn about stack locking;</li> </ul>"},{"location":"concepts/stack/index.html#stack-state","title":"Stack state","text":"<p>Similar to runs and tasks, stacks also have states. A stack's state is the last state of its most recently processed tracked run that has progressed beyond the queued state and which was not canceled. Only if the stack has no runs yet a special state \"None\" is applied:</p> <p></p> <p>Stack states allow users to see at a glance the overall health of their infrastructure, and the level of development activity associated with it.</p>"},{"location":"concepts/stack/creating-a-stack.html","title":"Creating a stack","text":"<p>Unless you're defining a stack programmatically using our Terraform provider, you will be creating one from the root of your Spacelift account:</p> <p></p> <p>Info</p> <p>You need to be an admin to create a stack. By default, GitHub account owners and admins are automatically given Spacelift admin privileges, but this can be customized using login policies and/or SSO integration.</p> <p>The stack creation process involves five simple steps:</p> <ol> <li>Naming, describing and labeling;</li> <li>Creating a link between your new stack and an existing Git repository;</li> <li>Defining backend-specific behavior (different for each supported backend, eg. Terraform, AWS CloudFormation, Pulumi, or Kubernetes</li> <li>Defining common behavior of the stack;</li> <li>Creating stack hooks;</li> </ol>"},{"location":"concepts/stack/creating-a-stack.html#name-your-stack","title":"Name your stack","text":"<p>Staring with the most difficult step - naming things. Here's where you give your new stack a nice informative name and an optional description - this one even supports Markdown:</p> <p></p> <p>You'll be able to change the name and description later, too - with one caveat. Based on the original name, Spacelift generates an immutable slug that serves as a unique identifier of this stack. If the name and the slug diverge significantly, things may become confusing.</p> <p>Here you will be able to choose which space your stack belongs to. Initially, you start with a root and a legacy space. The root space is the top-level space of your account, while the legacy space exists for backward compatibility with pre-spaces RBAC.</p> <p>Also, this is the opportunity to set a few labels. Labels are useful for searching and grouping things, but also work extremely well with policies.</p>"},{"location":"concepts/stack/creating-a-stack.html#integrate-vcs","title":"Integrate VCS","text":"<p>In this step, you will need to tell Spacelift where to look for the Terraform code for the stack - a combination of Git repository and one of its existing branches. The branch that you specify set here is what we called a tracked branch. By default, anything that you push to this branch will be considered for deployment. Anything you push to a different branch will be tested for changes against the current state.</p> <p>The project root configuration is where inside the repository Spacelift should look for the infra project source code (e.g. create a stack for a specific folder in the repository).</p> <p>A few things worth noting:</p> <ul> <li>you can point multiple Spacelift stacks to the same repository, even the same branch;</li> <li>the default behavior can be tweaked extensively to work with all sorts of Git and deployment workflows (yes, we like monorepos, too) using push and trigger policies, which are more advanced topics;</li> <li>in order to learn what exactly our Git hosting provider integration means, please refer to GitHub and GitLab integration documentation;</li> </ul> <p>Info</p> <p>If you're using our default GitHub App integration, we only list the repositories you've given us access to. If some repositories appear to be missing in the selection dropdown, it's likely that you've installed the app on a few selected repositories. That's fine, too, just whitelist the desired repositories and retry.</p>"},{"location":"concepts/stack/creating-a-stack.html#configure-backend","title":"Configure backend","text":"<p>At this point you'll probably know whether you want to create a Terraform, OpenTofu, Terragrunt, AWS CloudFormation, Pulumi, Ansible or Kubernetes stack. Each of the supported vendors has some settings that are specific to it, and the backend configuration step is where you can define them.</p>"},{"location":"concepts/stack/creating-a-stack.html#terraform","title":"Terraform","text":"<p>When selecting Terraform, you can choose which version of Terraform to start with - we support Terraform 0.12.0 and above, up to the latest version of MPL Terraform. You don't need to dwell on this decision since you can change the version later - Spacelift supports full Terraform version management allowing you to even preview the impact of upgrading to a newer version.</p> <p>The next decisions involves your Terraform state. First, whether you want us to provide a Terraform state backend for your state. We do offer that as a convenience feature, though Spacelift works just fine with any remote backend, like Amazon S3.</p> <p>Info</p> <p>If you want to bring your own backend, there's no point in doing additional state locking - Spacelift itself provides a more sophisticated state access control mechanism than Terraform.</p> <p>If you choose not to use our state backend, feel free to proceed. If you do want us to manage your state, you have an option to import an existing state file from your previous backend. This is only relevant if you're migrating an existing Terraform project to Spacelift. If you have no state yet and Spacelift will be creating resources from scratch, this step is unnecessary.</p> <p>Warning</p> <p>Remember - this is the only time you can ask Spacelift to be the state backend for a given stack, so choose wisely. You can read more about state management here.</p> <p>In addition to these options, we also offer external state access for read-only purposes, this is available for administrative stacks or users with write permission to this Stack's space.</p>"},{"location":"concepts/stack/creating-a-stack.html#opentofu","title":"OpenTofu","text":"<p>Right now, creating an OpenTofu has only one slight difference from creating a Terraform stack and that difference refers to setting the workflow tool to OpenTofu.</p>"},{"location":"concepts/stack/creating-a-stack.html#pulumi","title":"Pulumi","text":"<p>When creating a Pulumi stack, you will need to provide two things. First, the login URL to your Pulumi state backend, as currently we don't provide one like we do for Terraform, so you will need to bring your own.</p> <p>Second, you need to specify the name of the Pulumi stack. This is separate from the name of the Spacelift stack, which you will specify in the next step. That said, nothing prevents you from keeping them in sync.</p>"},{"location":"concepts/stack/creating-a-stack.html#cloudformation","title":"CloudFormation","text":"<p>If you're using CloudFormation with Spacelift, there are a few pieces of information you'll need to provide. First, you'll need to specify the region where your CloudFormation stack will be located.</p> <p>Additionally, you'll need to provide the name of the corresponding CloudFormation stack for this Spacelift stack. This will help us keep track of the different resources in your infrastructure.</p> <p>You'll also need to provide the path to the template file in your repository that describes the root CloudFormation stack and finally you'll need to specify the S3 bucket where your processed CloudFormation templates will be stored. This will enable us to manage your CloudFormation state and ensure that all changes are properly applied.</p>"},{"location":"concepts/stack/creating-a-stack.html#kubernetes","title":"Kubernetes","text":"<p>When you create a Kubernetes stack in Spacelift, you have the option to specify the namespace of the Kubernetes cluster that you want to run commands on. You can leave this empty for multi-namespace Stacks.</p> <p>You can also provide the version of kubectl that you want the worker to download. This is useful if you need to work with a specific version of kubectl for compatibility or testing purposes. The worker will download the specified version of kubectl at runtime, ensuring that the correct version is available for executing commands on the cluster.</p>"},{"location":"concepts/stack/creating-a-stack.html#terragrunt","title":"Terragrunt","text":"<p> Creating a Terragrunt stack in Spacelift, gives you the option to specify the Terraform and Terragrunt versions you want to use.</p> <p>You also have the possibility of enabling the run-all feature of Terragrunt, which is useful in scenarios where organizations rely on this in their current process and are unable to do a full migration yet.</p> <p>Support is currently in Beta.</p>"},{"location":"concepts/stack/creating-a-stack.html#ansible","title":"Ansible","text":"<p>When you create an Ansible stack in Spacelift, you have the option to select the playbook file you want to use. You can define policies for your stack as you would do for any other stack.</p> <p>Support is currently in Beta.</p>"},{"location":"concepts/stack/creating-a-stack.html#define-behavior","title":"Define behavior","text":"<p>Regardless of which of the supported backends (Terraform, Pulumi etc.) you're setting up your stack to use, there are a few common settings that apply to all of them. You'll have a chance to define them in the next step:</p> <p></p> <p>The behavior settings are:</p> <ul> <li>whether the stack is administrative;</li> <li>worker pool to use, if applicable (default uses the Spacelift public worker pool);</li> <li>whether the changes should automatically deploy;</li> <li>whether obsolete tests should be automatically retried;</li> <li>whether or not to protect the stack from deletion;</li> <li>whether or not to enable the local preview spacectl CLI feature;</li> <li>whether or not run promotion is enabled;</li> <li>optionally specify a custom Docker image to use to for your job container;</li> </ul>"},{"location":"concepts/stack/creating-a-stack.html#create-stack-hooks","title":"Create Stack Hooks","text":"<p>You also have the ability to control what happens before and after each runner phase using Stack Hooks. In this phase, you can define commands that run in between the following phases:</p> <ul> <li>Initialization</li> <li>Planning</li> <li>Applying</li> <li>Destroying</li> <li>Performing</li> <li>Finally</li> </ul>"},{"location":"concepts/stack/deleting-a-stack.html","title":"Deleting a Stack","text":"<p>When you are ready to delete your stack, you can do so by navigating to your stack settings and clicking on the delete button. You will get a warning first to let you know that deleting this stack does not delete any of the resources that this stack manages.</p> <p></p>"},{"location":"concepts/stack/deleting-a-stack.html#deleting-resources-managed-by-a-stack","title":"Deleting Resources Managed by a Stack","text":"<p>Depending on the backend of your stack, there are different commands you can run as a task before deleting the stack.</p> Backend Command Terraform <code>terraform destroy -auto-approve</code> CloudFormation <code>aws cloudformation delete-stack --stack-name &lt;cloudformation-stack-name&gt;</code> Pulumi <code>pulumi destroy --non-interactive --yes</code> Kubernetes <code>kubectl delete --ignore-not-found -l spacelift-stack=&lt;stack-slug&gt; $(kubectl api-resources --verbs=list,create -o name &amp;#124; paste -s -d, -)</code> <p>Tip</p> <p>For Terraform, you can also run a task through our CLI tool spacectl.</p>"},{"location":"concepts/stack/deleting-a-stack.html#scheduled-delete","title":"Scheduled Delete","text":"<p>If you would like to schedule to delete a stack, please see our documentation on Scheduling.</p>"},{"location":"concepts/stack/deleting-a-stack.html#using-the-api","title":"Using the API","text":"<p>If you would like to delete a stack using our API, please see our documentation on GraphQL API.</p>"},{"location":"concepts/stack/drift-detection.html","title":"Drift detection","text":"<p>Info</p> <p>Note that drift detection only works on private workers, which is an Enterprise plan feature.</p>"},{"location":"concepts/stack/drift-detection.html#drift-happens","title":"Drift happens","text":"<p>In infrastructure-as-code, the concept of drift represents the difference between the desired and the actual state of the infrastructure managed by your tool of choice - Terraform, Pulumi, AWS CloudFormation, etc. In practice, there are two sources of drift.</p> <p>The first source covers changes directly introduced by external actors - either humans or machines (scripts). If an on-call SRE changes your database parameters otherwise controlled by Terraform, you've introduced drift. If an external script updates your Kubernetes cluster in a way that conflicts with its Pulumi definition, it's drift as well.</p> <p>The other source of drift comes from the dependency of your resources on external data sources. For example, if your load balancer only expects to receive traffic from Cloudflare, you may want to restrict ingress to a predefined range of IPs. However, that range may be dynamic, and your IaC tool queries it every time it runs. If there's any change to the external data source, it's showing up as drift, too.</p> <p>In the first scenario, drift is an unwanted by-product of emergencies or broken processes. In the latter, it's both desired and inevitable - it's proof that your otherwise declarative system responds to external changes. In other words - drift happens, so deal with it. \ud83d\ude0e</p>"},{"location":"concepts/stack/drift-detection.html#video-walkthrough","title":"Video Walkthrough","text":""},{"location":"concepts/stack/drift-detection.html#how-spacelift-helps","title":"How Spacelift helps","text":"<p>Spacelift comes with a built-in mechanism to detect and - optionally - reconcile drift. We do it by periodically executing proposed runs on your stable infrastructure (in Spacelift, we generally represent it by the FINISHED stack state) and checking for any changes.</p> <p>To get started, create a drift detection configuration from the Scheduling section of your stack settings. You will be able to add multiple cron rules to define when your reconciliation jobs should be scheduled, as well as decide whether you want your jobs to trigger tracked runs (reconciliation jobs) in response to detected drift:</p> <p></p> <p>Info</p> <p>Note that, at least currently, drift detection only works on private workers.</p>"},{"location":"concepts/stack/drift-detection.html#to-reconcile-or-not-to-reconcile","title":"To reconcile, or not to reconcile","text":"<p>We generally suggest turning reconciliation \"on\" as it ensures that you get the most out of drift detection. Reconciliation jobs are equivalent to manually triggering tracked runs and obey the same rules and constraints. In particular, they respect their stacks' auto-deploy setting and trigger plan policies - see this section for more details.</p> <p>However, if you choose not to reconcile changes, you can still get value out of drift detection - in this case, drifted resources can be seen in the Resources view, both on the stack and account level. Also, drift detection jobs trigger webhooks like regular runs, where they're clearly marked as such (<code>driftDetection</code> field).</p> <p></p>"},{"location":"concepts/stack/drift-detection.html#drift-detection-in-practice","title":"Drift detection in practice","text":"<p>With drift detection enabled on the stack, proposed runs are quietly executing in the background. If they do not detect any changes, the only way you'd know about them is by viewing all runs in the Account &gt; Runs section and filtering or grouping by drift detection parameter - here is an example:</p> <p></p> <p>But once your job detects drift (and you've enabled reconciliation), it triggers a regular tracked run. This run is subject to the same rules as a regular tracked run is. For example, if you set your stack not to deploy changes automatically, the run will end up in an Unconfirmed state, waiting for your decision. The same thing will happen if a plan policy produces a warning using a matched <code>warn</code> rule.</p>"},{"location":"concepts/stack/drift-detection.html#policy-input","title":"Policy input","text":"<p>The only real difference between a drift detection job and one triggered manually is that the run section of your policy input will have the <code>drift_detection</code> field set to <code>true</code> - and this applies to both plan and trigger policies. You can use this mechanism to add extra controls to your drift detection strategy. For example, if you're automatically deploying your changes but want a human to look at drift before reconciling it, you can add the following section to your plan policy body:</p> <pre><code>package spacelift\n\nwarn[\"Drift reconciliation requires manual approval\"] {\n  input.spacelift.run.drift_detection\n}\n</code></pre>"},{"location":"concepts/stack/organizing-stacks.html","title":"Organizing stacks","text":"<p>Depending on the complexity of your infrastructure, the size of your team, your particular needs and your preferred way of working you may end up managing a lot of stacks. This obviously makes it harder to quickly find what you're looking for. As practitioners ourselves, we're providing you a few tools to make this process more manageable - from the basic query-based searching to filtering by status and the coolest of all, label-based folders.</p>"},{"location":"concepts/stack/organizing-stacks.html#video-walkthrough","title":"Video Walkthrough","text":""},{"location":"concepts/stack/organizing-stacks.html#query-based-searching-and-filtering","title":"Query-based searching and filtering","text":"<p>Historically the first tool we offered was the search bar:</p> <p></p> <p>The search bar allows you to search and filter by the following stack properties:</p> <ul> <li>name;</li> <li>ID (slug);</li> <li>any of its labels;</li> </ul> <p>Note how the search phrase is highlighted, and irrelevant stacks are filtered out:</p> <p></p>"},{"location":"concepts/stack/organizing-stacks.html#filtering-by-status","title":"Filtering by status","text":"<p>Filtering stacks by status is a very useful mechanism for identifying action items like plans pending confirmation (unconfirmed state) or failed jobs that require fixing. For that, use the Filter stacks by status section on the sidebar to the left. If you click on any of the statuses, the list of stacks will be filtered to only include stacks with a given status:</p> <p></p> <p>Note that if no stacks in the account have a particular status at the moment, that status is missing from the list.</p>"},{"location":"concepts/stack/organizing-stacks.html#label-based-folders","title":"Label-based folders","text":"<p>Probably the most useful way of grouping stacks is by attaching folder labels to them. You can read more about labels here, including how to set them, and folder labels are just regular labels, prefixed with <code>folder:</code>. In order to make it more obvious in the GUI and save some screen real estate, we replace the <code>folder:</code> prefix by the folder icon...</p> <p></p> <p>...but once you start editing labels, the magic is gone:</p> <p></p> <p>For every folder label, a sidebar section is included in the Folders menu, allowing you to search by that label. The number to the right hand side indicates that number of stacks with that label:</p> <p></p>"},{"location":"concepts/stack/organizing-stacks.html#nesting-and-multiple-folder-labels","title":"Nesting and multiple folder labels","text":"<p>Perhaps worth mentioning is the fact that folder labels can be nested, allowing you to create either hierarchies, or arbitrary classifications of your stacks.</p> <p>Also, a single stack can have any number of folder labels set, in which case it belongs to all the collections. In that, folder labels are like labels in Gmail rather than directories in your filesystem.</p>"},{"location":"concepts/stack/organizing-stacks.html#saving-filters-in-views","title":"Saving filters in views","text":"<p>It is possible to save your filters with a Filters Tab. You can select all the filters that you would like to apply, add a search query or sorting in the top right corner, click New View, enter the view name, and click Save. This view is now saved for this account. You can also mark your new view as your default view during creation. Next time you log in or navigate to stacks, your personal default view will be applied.</p> <p></p> <p>If you forgot to mark your view as default then you can easily do the same thing in the Views Tab.</p> <p></p>"},{"location":"concepts/stack/organizing-stacks.html#shared-views","title":"Shared views","text":"<p>Views can be shared or private. While first creating the view, it is available only to your user. If you have admin access, you can make it public for all the users of the account by hovering over the saved view and clicking the small eye icon \"Share within account\". This way, all the users within this application can see the saved view and who created it.</p>"},{"location":"concepts/stack/organizing-stacks.html#resetting","title":"Resetting","text":"<p>To quickly reset your default view to Spacelift default state, click the \"Reset to Spacelift default view\" button. It will result in clearing all sorting, search, and filter parameters, as well as managed filter settings.</p> <p></p>"},{"location":"concepts/stack/organizing-stacks.html#manage-view","title":"Manage view","text":"<ul> <li>If you change your filter, search and/or sorting settings, you can update the currently selected view by clicking on Update item under \"Manage view\" button. The blue icon on the manage view button indicates an update possibility.</li> <li>Edit name allows editing name of the current view</li> <li>Delete allows removing your private view (Shared and Spacelift default views can not be removed). You can delete the view from the Views tab as well.</li> </ul>"},{"location":"concepts/stack/scheduling.html","title":"Scheduling","text":""},{"location":"concepts/stack/scheduling.html#what-is-scheduling","title":"What is scheduling?","text":"<p>Info</p> <p>Note that scheduling only works on private workers, which is an Enterprise plan feature.</p> <p>Scheduling allows you to trigger a stack deletion or task at a specific time or periodically based on the cron rules defined.</p>"},{"location":"concepts/stack/scheduling.html#scheduled-delete-stack-ttl","title":"Scheduled Delete Stack (TTL)","text":"<p>A Delete Stack schedule allows you to delete the stack and (optionally) its resources at the specific timestamp (UNIX timestamp).</p> <p>Add a schedule with the Delete Stack type from the Scheduling section of your stack settings.</p> <p>Actions when the schedule defines that the resources should be deleted:</p> <ul> <li>a destruction run will be triggered at the specified time.</li> <li>after this run is successful, the stack will be deleted.</li> </ul> <p>When the resources should not be deleted, we will delete the stack at the specified time.</p> <p></p>"},{"location":"concepts/stack/scheduling.html#scheduled-task","title":"Scheduled Task","text":"<p>A scheduled task enables you to run a command at a specific timestamp or periodically based on the cron rules defined.</p> <p>Add a schedule with the Task type from the Scheduling section of your stack settings. After creating this schedule, a task will be triggered with the defined command (at a specific timestamp or periodically based on the cron rules defined).</p> <p></p>"},{"location":"concepts/stack/stack-dependencies.html","title":"Stack dependencies","text":"<p>Stacks can depend on other stacks. This is useful when you want to run a stack only after another stack have finished running. For example, you might want to deploy a database stack before a stack that uses the database.</p> <p>Info</p> <p>Stack dependencies only respect tracked runs. Proposed runs and tasks are not considered.</p>"},{"location":"concepts/stack/stack-dependencies.html#goals","title":"Goals","text":"<p>Stack dependencies aim to solve the problem of ordering the execution of related runs triggered by the same VCS event.</p> <p>Stack dependencies do not manage stack lifecycle events such as creating or deleting stacks. In fact, you cannot delete a stack if it has dependencies.</p>"},{"location":"concepts/stack/stack-dependencies.html#defining-stack-dependencies","title":"Defining stack dependencies","text":"<p>Stack dependencies can be defined in the <code>Dependencies</code> tab of the stack.</p> <p></p> <p>Info</p> <p>In order to create a dependency between two stacks you need to have at least reader permission to one stack (dependency) and admin permission to the other (dependee). See Spaces Access Control for more information.</p>"},{"location":"concepts/stack/stack-dependencies.html#defining-references-between-stacks","title":"Defining references between stacks","text":"<p>You have the option to refer to outputs of other stacks: your stack will be only triggered if the referenced output has been created or changed.</p> <p></p> <p>You can either choose an existing output value or add one that doesn't exist yet but will be created by the stack. On the receiving end, you need to choose an environment variable (<code>Input name</code>) to store the output value in.</p> <p></p> <p>Tip</p> <p>If you use Terraform, make sure to use <code>TF_VAR_</code> prefix for environment variable names.</p>"},{"location":"concepts/stack/stack-dependencies.html#enabling-sensitive-outputs-for-references","title":"Enabling sensitive outputs for references","text":"<p>A stack output can be sensitive or non-sensitive. For example, in Terraform you can mark an output <code>sensitive = true</code>. Sensitive outputs are being masked in the Spacelift UI and in the logs.</p> <p>Spacelift will upload sensitive outputs to the server - this is enabled by default on our public worker pool.</p> <p>On private worker pools however, it needs to be enabled explicitly by adding <code>SPACELIFT_SENSITIVE_OUTPUT_UPLOAD_ENABLED=true</code> environment variable to the worker. This is a requirement if you wish to utilize sensitive outputs for stack dependencies.</p> <p>In self-hosted, make sure to add <code>export SPACELIFT_SENSITIVE_OUTPUT_UPLOAD_ENABLED=true</code> to your <code>CustomUserDataSecretName</code> secret. You'll find more information about that variable at the Worker Pool page.</p>"},{"location":"concepts/stack/stack-dependencies.html#stack-dependency-reference-limitations","title":"Stack dependency reference limitations","text":"<p>When a stack has an upstream dependency with a reference, it relies on the existence of the outputs.</p> <pre><code>graph TD;\n    Storage --&gt; |TF_VAR_AWS_S3_BUCKET_ARN|storageColor(StorageService);\n\n    style storageColor fill:#51cbad</code></pre> <p>If you trigger <code>StorageService</code> in the above scenario, you need to make sure <code>Storage</code> has produced <code>TF_VAR_AWS_S3_BUCKET_ARN</code> already. Otherwise you'll get the following error:</p> <pre><code>job assignment failed: the following inputs are missing: Storage.TF_VAR_AWS_S3_BUCKET_ARN =&gt; TF_VAR_AWS_S3_BUCKET_ARN\n</code></pre> <p>Note</p> <p>We have enabled the output uploading to our backend on 2023 August 21. This means that if you have a stack that produced an output before that date, you'll need to rerun it to make the output available for references.</p> <p>We upload outputs during the Apply phase. If you stumble upon the error above, you'll need to make sure that the stack producing the output had a tracked run with an Apply phase.</p> <p>You can simply do it by adding a dummy output to the stack and removing it afterwards:</p> <pre><code>output \"dummy\" {\nvalue = \"dummy\"\n}\n</code></pre>"},{"location":"concepts/stack/stack-dependencies.html#vendor-limitations","title":"Vendor limitations","text":"<p>Ansible and Kubernetes does not have the concept of outputs, so you cannot reference the outputs of them. They can be on the receiving end though:</p> <pre><code>graph TD;\n    A[Terraform Stack] --&gt; |VARIABLE_1|B[Kubernetes Stack];\n    A --&gt; |VARIABLE_2|C[Ansible Stack];</code></pre>"},{"location":"concepts/stack/stack-dependencies.html#scenario-1","title":"Scenario 1","text":"<pre><code>graph TD;\n    Infrastructure --&gt; |TF_VAR_VPC_ID|Database;\n    Database --&gt; |TF_VAR_CONNECTION_STRING|PaymentService;</code></pre> <p>In case your <code>Infrastructure</code> stack has a <code>VPC_ID</code>, you can set that as an input to your <code>Database</code> stack (e.g. <code>TF_VAR_VPC_ID</code>). When the <code>Infrastructure</code> stack finishes running, the <code>Database</code> stack will be triggered and the <code>TF_VAR_VPC_ID</code> environment variable will be set to the value of the <code>VPC_ID</code> output of the <code>Infrastructure</code> stack.</p> <p>If there is one or more references defined, the stack will only be triggered if the referenced output has been created or changed. If they remain the same, the downstream stack will be skipped.</p>"},{"location":"concepts/stack/stack-dependencies.html#scenario-2","title":"Scenario 2","text":"<pre><code>graph TD;\n    Infrastructure --&gt; |TF_VAR_VPC_ID|Database;\n    Database --&gt; |TF_VAR_CONNECTION_STRING|PaymentService;\n    Database --&gt; CartService;</code></pre> <p>You can also mix references and referenceless dependencies. In the above case, <code>CartService</code> will be triggered whenever <code>Database</code> finishes running, regardless of the <code>TF_VAR_CONNECTION_STRING</code> output.</p>"},{"location":"concepts/stack/stack-dependencies.html#dependencies-overview","title":"Dependencies overview","text":"<p>In the <code>Dependencies</code> tab of the stack, there is a button called <code>Dependencies graph</code> to view the full dependency graph of the stack.</p> <p></p>"},{"location":"concepts/stack/stack-dependencies.html#how-it-works","title":"How it works","text":"<p>Stack dependencies are directed acyclic graphs (DAGs). This means that a stack can depend on multiple stacks, and a stack can be depended on by multiple stacks but there cannot be loops: you will receive an error if you try to add a stack to a dependency graph that will create a cycle.</p> <p>When a tracked run is created in the stack (either triggered manually or by a VCS event), and the stack is a dependency of other stack(s), those stacks will queue up tracked runs and wait until the current stack's tracked run has finished running.</p> <p>If a run fails in the dependency chain, all subsequent runs will be cancelled.</p> <p>It will be easier to understand in a second.</p>"},{"location":"concepts/stack/stack-dependencies.html#examples","title":"Examples","text":""},{"location":"concepts/stack/stack-dependencies.html#scenario-1_1","title":"Scenario 1","text":"<pre><code>graph TD;\n    BaseInfra--&gt;Database;\n    BaseInfra--&gt;networkColor(Network);\n    BaseInfra--&gt;Storage;\n    Database--&gt;paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    Database--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n\n    style networkColor fill:#51cbad\n    style paymentSvcColor fill:#51abcb\n    style cartSvcColor fill:#51abcb</code></pre> <p>In the above example, if <code>Network</code> stack receives a push event to the tracked branch, it will start a run immediately and queue up <code>PaymentService</code> and <code>CartService</code>. When <code>Network</code> finishes running, those two will start running. Since <code>PaymentService</code> and <code>CartService</code> does not depend on each other, they can run in parallel.</p> <p><code>BaseInfra</code> remains untouched, we never go up in the dependency graph.</p>"},{"location":"concepts/stack/stack-dependencies.html#scenario-2_1","title":"Scenario 2","text":"<pre><code>graph TD;\n    baseInfraColor(BaseInfra)--&gt;databaseColor(Database);\n    baseInfraColor(BaseInfra)--&gt;networkColor(Network);\n    baseInfraColor(BaseInfra)--&gt;storageColor(Storage);\n    databaseColor(Database)--&gt;|TF_VAR_CONNECTION_STRING|paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    databaseColor(Database)--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n\n    style baseInfraColor fill:#51cbad\n    style networkColor fill:#51abcb\n    style paymentSvcColor fill:#51abcb\n    style cartSvcColor fill:#51abcb\n    style storageColor fill:#51abcb\n    style databaseColor fill:#51abcb</code></pre> <p>If <code>BaseInfra</code> receives a push event, it will start running immediately and queue up all of the stacks below. The order of the runs: <code>BaseInfra</code>, then <code>Database</code> &amp; <code>Network</code> &amp; <code>Storage</code> in parallel, finally <code>PaymentService</code> &amp; <code>CartService</code> in parallel.</p> <p>Since <code>PaymentService</code> and <code>CartService</code> does not depend on <code>Storage</code>, they will not wait until it finishes running.</p> <p>Note: <code>PaymentService</code> references <code>Database</code> with <code>TF_VAR_CONNECTION_STRING</code>. But since it also depends on <code>Network</code> with no references, it'll run regardless of the <code>TF_VAR_CONNECTION_STRING</code> output. If the <code>Database</code> stack does not have the corresponding output, the <code>TF_VAR_CONNECTION_STRING</code> environment variable will not be injected into the run.</p>"},{"location":"concepts/stack/stack-dependencies.html#scenario-3","title":"Scenario 3","text":"<pre><code>graph TD;\n    baseInfraColor(BaseInfra)--&gt;databaseColor(Database);\n    baseInfraColor(BaseInfra)--&gt;networkColor(Network);\n    baseInfraColor(BaseInfra)--&gt;storageColor(Storage);\n    databaseColor(Database)--&gt;paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    databaseColor(Database)--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n\n    style baseInfraColor fill:#51cbad\n    style networkColor fill:#e21316\n    style paymentSvcColor fill:#ecd309\n    style cartSvcColor fill:#ecd309\n    style storageColor fill:#51abcb\n    style databaseColor fill:#51abcb</code></pre> <p>In this scenario, similarly to the previous one <code>BaseInfra</code> received a push, started running and queued up all of the stacks below. However, <code>Network</code> stack has failed which means that the rest of the runs (<code>PaymentService</code> and <code>CartService</code>) will be skipped.</p> <p>Same level stacks (<code>Database</code> &amp; <code>Storage</code>) are not affected by the failure.</p>"},{"location":"concepts/stack/stack-dependencies.html#scenario-4","title":"Scenario 4","text":"<pre><code>graph TD;\n    baseInfraColor(BaseInfra)--&gt;databaseColor(Database);\n    baseInfraColor(BaseInfra)--&gt;networkColor(Network);\n    baseInfraColor(BaseInfra)--&gt;storageColor(Storage);\n    databaseColor(Database)--&gt;paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    databaseColor(Database)--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n\n    style baseInfraColor fill:#51cbad\n    style networkColor fill:#51cbad\n    style paymentSvcColor fill:#51abcb\n    style cartSvcColor fill:#51abcb\n    style storageColor fill:#51cbad\n    style databaseColor fill:#51cbad</code></pre> <p>Let's assume that the infrastructure (<code>BaseInfra</code>, <code>Database</code>, <code>Network</code> and <code>Storage</code>) is a monorepo, and a push event affects all 4 stacks. The situation isn't any different than Scenario 2. The dependencies are still respected and the stacks will run in the proper order: <code>BaseInfra</code> first, then <code>Database</code> &amp; <code>Network</code> &amp; <code>Storage</code> in parallel, finally <code>PaymentService</code> &amp; <code>CartService</code> in parallel.</p>"},{"location":"concepts/stack/stack-dependencies.html#scenario-5","title":"Scenario 5","text":"<pre><code>graph TD;\n    baseInfraColor(BaseInfra)--&gt;databaseColor(Database);\n    baseInfraColor(BaseInfra)--&gt;networkColor(Network);\n    baseInfraColor(BaseInfra)--&gt;storageColor(Storage);\n    databaseColor(Database)--&gt;paymentSvcColor(PaymentService);\n    networkColor(Network)--&gt;paymentSvcColor(PaymentService);\n    databaseColor(Database)--&gt;cartSvcColor(CartService);\n    networkColor(Network)--&gt;cartSvcColor(CartService);\n    storageColor(Storage)--&gt;|TF_VAR_AWS_S3_BUCKET_ARN|storageSvcColor(StorageService);\n\n    style baseInfraColor fill:#51cbad\n    style networkColor fill:#51abcb\n    style paymentSvcColor fill:#51abcb\n    style cartSvcColor fill:#51abcb\n    style storageColor fill:#51abcb\n    style databaseColor fill:#51cbad\n    style storageSvcColor fill:#ecd309</code></pre> <p>If <code>BaseInfra</code> and <code>Database</code> are a monorepo and a push event affects both of them, this scenario isn't any different than Scenario 2 and Scenario 4. The order from top to bottom is still the same: <code>BaseInfra</code> first, then <code>Database</code> &amp; <code>Network</code> &amp; <code>Storage</code> in parallel, finally <code>PaymentService</code> &amp; <code>CartService</code> in parallel.</p> <p><code>Storage</code> and <code>StorageService</code>: let's say that the S3 bucket resource of <code>Storage</code> already exists. This means that the bucket ARN didn't change, so <code>StorageService</code> will be skipped.</p>"},{"location":"concepts/stack/stack-dependencies.html#trigger-policies","title":"Trigger policies","text":"<p>Stack dependencies are a simpler alternative to Trigger policies that cover most use cases. If your use case does not fit Stack dependencies, consider using a Trigger policy.</p> <p>There is no connection between the two features, and the two shouldn't be combined to avoid confusion or even infinite loops in the dependency graph.</p>"},{"location":"concepts/stack/stack-dependencies.html#stack-deletion","title":"Stack deletion","text":"<p>A stack cannot be deleted if it has upstream or downstream dependencies. If you want to delete a stack, you need to delete all of its dependencies first.</p>"},{"location":"concepts/stack/stack-dependencies.html#ordered-stack-creation-and-deletion","title":"Ordered Stack creation and deletion","text":"<p>As mentioned earlier, Stack Dependencies do not aim to handle the lifecycle of the stacks.</p> <p>Ordering the creation and deletion of stacks in a specific order is not impossible though. If you manage your Spacelift stacks with the Spacelift Terraform Provider, you can easily do it by setting <code>spacelift_stack_destructor</code> resources and setting the <code>depends_on</code> Terraform attribute on them.</p> <p>Here is a simple example of creating a dependency between two stacks, immediately triggering a run on the parent stack (which cascades to the child stack) and setting up a destructor for them. By setting up a destructor resource with the proper <code>depends_on</code> attribute, it ensures that the deletion of the stacks will happen in the proper order. First child, then parent. This is also an easy way to create short-lived environments.</p> <pre><code># Parent stack\nresource \"spacelift_stack\" \"infra\" {\nname       = \"Base infrastructure\"\nrepository = \"infra\"\nbranch     = \"main\"\nautodeploy = true\n}\n\n# Child stack\nresource \"spacelift_stack\" \"app\" {\nname       = \"Application\"\nrepository = \"app\"\nbranch     = \"main\"\nautodeploy = true\n}\n\n# Create the parent-child dependency for run execution ordering\nresource \"spacelift_stack_dependency\" \"this\" {\nstack_id            = spacelift_stack.app.id\ndepends_on_stack_id = spacelift_stack.infra.id\n\ndepends_on = [\nspacelift_stack_destructor.app,\nspacelift_stack_destructor.infra\n]\n}\n\n# Trigger a run on the parent stack, to create the infrastructure\n# and deploy the application.\nresource \"spacelift_run\" \"this\" {\nstack_id = spacelift_stack.infra.id\n\nkeepers = {\nbranch = spacelift_stack.infra.branch\n}\n\n  # Make sure the dependency exists before triggering the run\ndepends_on = [\nspacelift_stack_dependency.this\n]\n}\n\n# Create the destructor for the parent stack\nresource \"spacelift_stack_destructor\" \"infra\" {\nstack_id = spacelift_stack.infra.id\n}\n\n# Create the destructor for the child stack\nresource \"spacelift_stack_destructor\" \"app\" {\nstack_id = spacelift_stack.app.id\n\ndepends_on = [\nspacelift_stack_destructor.infra\n]\n}\n</code></pre> <p>What happens during <code>terraform apply</code>:</p> <ul> <li>Terraform creates the two stacks</li> <li>Sets up the dependency between them</li> <li>Triggers a run on the parent stack (<code>infra</code>)</li> <li>Which in turn automatically triggers a run on the child stack (<code>app</code>) as well</li> </ul> <p>You might notice the two destructors at the end. They don't do anything yet, but they will be used during <code>terraform destroy</code>. Destroy order:</p> <ul> <li>Terraform destroys the dependency</li> <li>Destroys the child stack (<code>app</code>) and its resources</li> <li>Finally, destroys the parent stack (<code>infra</code>) and its resources</li> </ul>"},{"location":"concepts/stack/stack-locking.html","title":"Stack locking","text":"<p>Spacelift supports locking the stack for one person's exclusive use. This can be very handy if a delicate operation is taking place that could easily be affected by someone else making changes in the meantime. Every stack writer can lock the stack unless it's already locked.</p> <p>The owner of the lock is the only one who can trigger runs and tasks for the entire duration of the lock. Locks never expire, and only its creator and Spacelift admins can release it.</p> <p></p> <p>Info</p> <p>Note that while a stack is locked, auto deploy is disabled to prevent accidental deployments.</p>"},{"location":"concepts/stack/stack-settings.html","title":"Stack settings","text":"<p>This article covers all settings that are set directly on the stack. It's important to note that these are not the only settings that affect how runs and tasks within a given stack are processed - environment, attached contexts, runtime configuration and various integrations will all play a role here, too.</p>"},{"location":"concepts/stack/stack-settings.html#video-walkthrough","title":"Video Walkthrough","text":""},{"location":"concepts/stack/stack-settings.html#common-settings","title":"Common settings","text":""},{"location":"concepts/stack/stack-settings.html#administrative","title":"Administrative","text":"<p>This setting indicates whether a stack has administrative privileges within the space it lives in. Runs executed by administrative stacks receive an API token that gives them administrative access to a subset of the Spacelift API used by our Terraform provider, which means they can create, update and destroy Spacelift resources.</p> <p>The main use case is to create one or a small number of administrative stacks that declaratively define the rest of Spacelift resources like other stacks, their environments, contexts, policies, modules, worker pools etc. in order to avoid ClickOps.</p> <p>Another pattern we've seen is stacks exporting their outputs as a context to avoid exposing their entire state through the Terraform remote state pattern or using external storage mechanisms, like AWS Parameter Store or Secrets Manager.</p> <p>If this sounds interesting and you want to give it a try, please refer to the help article exclusively dedicated to Spacelift's Terraform provider.</p>"},{"location":"concepts/stack/stack-settings.html#autodeploy","title":"Autodeploy","text":"<p>Indicates whether changes to the stack can be applied automatically. When autodeploy is set to true, any change to the tracked branch will automatically be applied if the planning phase was successful and there are no plan policy warnings.</p> <p>Consider setting it to true if you always do a code review before merging to the tracked branch, and/or want to rely on plan policies to automatically flag potential problems. If each candidate change goes through a meaningful human code review with stack writers as reviewers, having a separate step to confirm deployment may be overkill. You may also want to refer to a dedicated section on using plan policies for automated code review.</p>"},{"location":"concepts/stack/stack-settings.html#autoretry","title":"Autoretry","text":"<p>Indicates whether obsolete proposed changes will be retried automatically. When autoretry is set to true and a change gets applied, all Pull Requests to the tracked branch conflicting with that change will be reevaluated based on the changed state.</p> <p>This saves you from manually retrying runs on Pull Requests when the state changes. This way it also gives you more confidence, that the proposed changes will actually be the actual changes you get after merging the Pull Request.</p> <p>Autoretry is only supported for Stacks with a private Worker Pool attached.</p>"},{"location":"concepts/stack/stack-settings.html#customizing-workflow","title":"Customizing workflow","text":"<p>Spacelift workflow can be customized by adding extra commands to be executed before and after each of the following phases:</p> <ul> <li>Initialization (<code>before_init</code> and <code>after_init</code>, respectively)</li> <li>Planning (<code>before_plan</code> and <code>after_plan</code>, respectively)</li> <li>Applying (<code>before_apply</code> and <code>after_apply</code>, respectively)</li> <li>Destroying (<code>before_destroy</code> and <code>after_destroy</code>, respectively)<ul> <li>used during module test cases</li> <li>used by stacks during destruction that have corresponding stack_destructor_resource</li> </ul> </li> <li>Performing (<code>before_perform</code> and <code>after_perform</code>, respectively)</li> <li>Finally (<code>after_run</code>): Executed after each actively processed run, regardless of its outcome. These hooks will execute as part of the last \"active\" state of the run and will have access to an environment variable called <code>TF_VAR_spacelift_final_run_state</code> indicating the final state of the run.</li> </ul> <p>Note here that all hooks, including the <code>after_run</code> ones, execute on the worker. Hence, the <code>after_run</code> hooks will not fire if the run is not being processed by the worker - for example, if the run is terminated outside of the worker (eg. canceled, discarded), there is an issue setting up the workspace or starting the worker container, or the worker container is killed while processing the run.</p> <p>These commands may serve one of two general purposes - either to make some modifications to your workspace (eg. set up symlinks, move files around etc.) or perhaps to run validations using something like <code>tfsec</code>, <code>tflint</code> or <code>terraform fmt</code>.</p> <p>Tip</p> <p>We don\u2019t recommend using newlines (<code>\\n</code>) in hooks. The reason is that we are chaining the Spacelift commands (eg. <code>terraform plan</code>) commands with pre/post hooks with double ampersand (<code>&amp;&amp;</code>) and using commands separated by newlines can cause a non-zero exit code by a command to be hidden if the last command in the newline-separated block succeeds. If you'd like to run multiple commands in a hook, add multiple hooks instead.</p> <p>Danger</p> <p>When a run resumes after having been paused for any reason (e.g., confirmation, approval policy), the remaining phases are run in a new container. As a result, any tool installed in a phase that occurred before the pause won't be available in the subsequent phases. A better way to achieve this would be to bake the tool into a custom runner image.</p> <p>Info</p> <p>If any of the \"before\" hooks fail (non-zero exit code), the relevant phase is not executed. If the phase itself fails, none of the \"after\" hooks get executed.</p> <p>The workflow can be customized either using our Terraform provider or in the GUI. The GUI has a very nice editor that allows you to select the phase you want to customize and add commands before and after each phase. You will be able to add and remove commands, reorder them using drag and drop and edit them in-line. Note how the commands that precede the customized phase are the \"before\" hooks (<code>ps aux</code> and <code>ls</code> in the example below), and the ones that go after it are the \"after\" hooks (<code>ls -la .terraform</code>):</p> <p></p> <p>Perhaps worth noting is the fact that these commands run in the same shell session as the phase itself. So the phase will have access to any shell variables exported by the preceding scripts.</p> <p>Environment variables are preserved from one phase to the next.</p> <p>Info</p> <p>These scripts can be overridden by the runtime configuration specified in the <code>.spacelift/config.yml</code> file.</p>"},{"location":"concepts/stack/stack-settings.html#runtime-commands","title":"Runtime commands","text":"<p>Spacelift can handle special commands to change the workflow behavior. Runtime commands use the echo command in a specific format.</p> <p>You could use those commands in any lifecycle step of the workflow.</p> <p></p> <pre><code>echo \"::command arg1 arg2\"\n</code></pre> <p>Below is a list of supported commands. See the more detailed doc after this table.</p> Command Description <code>::add-mask</code> Adds a set of values that should be masked in log output"},{"location":"concepts/stack/stack-settings.html#add-mask","title":"::add-mask","text":"<p>When you mask a value, it is treated as a secret and will be redacted in the logs output. Each masked word separated by whitespace is replaced with five <code>*</code> characters.</p>"},{"location":"concepts/stack/stack-settings.html#example","title":"Example","text":"<pre><code># Multiple masks can be set with a single command\necho \"::add-mask secret-string another-secret-string\"\n\n# You can pull a secret dynamically, for example here we can mask the account ID\necho \"::add-mask $(aws sts get-caller-identity | jq -r .Account)\"\n</code></pre>"},{"location":"concepts/stack/stack-settings.html#enable-local-preview","title":"Enable local preview","text":"<p>Indicates whether creating proposed Runs based on user-uploaded local workspaces is allowed.</p> <p>If this is enabled, you can use spacectl to create a proposed run based on the directory you're in:</p> <pre><code>spacectl stack local-preview --id &lt;stack-id&gt;\n</code></pre> <p>Danger</p> <p>This in effect allows anybody with write access to the Stack to execute arbitrary code with access to all the environment variables configured in the Stack.</p> <p>Use with caution.</p>"},{"location":"concepts/stack/stack-settings.html#name-and-description","title":"Name and description","text":"<p>Stack name and description are pretty self-explanatory. The required name is what you'll see in the stack list on the home screen and menu selection dropdown. Make sure that it's informative enough to be able to immediately communicate the purpose of the stack, but short enough so that it fits nicely in the dropdown, and no important information is cut off.</p> <p>The optional description is completely free-form and it supports Markdown. This is perhaps a good place for a thorough explanation of the purpose of the stack, perhaps a link or two, and an obligatory cat GIF.</p> <p>Warning</p> <p>Based on the original name, Spacelift generates an immutable slug that serves as a unique identifier of this stack. If the name and the slug diverge significantly, things may become confusing.</p> <p>So even though you can change the stack name at any point, we strongly discourage all non-trivial changes.</p>"},{"location":"concepts/stack/stack-settings.html#labels","title":"Labels","text":"<p>Labels are arbitrary, user-defined tags that can be attached to Stacks. A single Stack can have an arbitrary number of these, but they must be unique. Labels can be used for any purpose, including UI filtering, but one area where they shine most is user-defined policies which can modify their behavior based on the presence (or lack thereof) of a particular label.</p> <p>There are some magic labels that you can add to your stacks. These labels add/remove functionalities based on their presence.</p> <p>List of the most useful labels:</p> <ul> <li>infracost -- Enables Infracost on your stack</li> <li>feature:add_plan_pr_comment -- Enables Pull Request Plan Commenting</li> <li>feature:disable_pr_comments - Disables Pull Request Comments</li> <li>feature:disable_resource_sanitization -- Disables resource sanitization</li> <li>feature:ignore_runtime_config -- Ignores .spacelift/config</li> <li>terragrunt -- Old way of using Terragrunt from the Terraform backend</li> <li>ghenv: Name -- GitHub Deployment environment (defaults to the stack name)</li> <li>ghenv: - -- Disables the creation of GitHub deployment environments</li> <li>autoattach:label -- Used for policies/contexts to autoattach the policy/contexts to all stacks containing that label</li> </ul>"},{"location":"concepts/stack/stack-settings.html#project-root","title":"Project root","text":"<p>Project root points to the directory within the repo where the project should start executing. This is especially useful for monorepos, or indeed repositories hosting multiple somewhat independent projects. This setting plays very well with Git push policies, allowing you to easily express generic rules on what it means for the stack to be affected by a code change.</p> <p>Info</p> <p>The project root can be overridden by the runtime configuration specified in the <code>.spacelift/config.yml</code> file.</p>"},{"location":"concepts/stack/stack-settings.html#project-globs","title":"Project globs","text":"<p>The project globs option allows you to specify files and directories outside of the project root that the stack cares about. In the absence of push policies, any changes made to the project root and any paths specified by project globs will trigger Spacelift runs.</p> <p></p> <p>You aren't required to add any project globs if you don't want to, but you have the option to add as many project globs as you want for a stack.</p> <p>Under the hood, the project globs option takes advantage of the filepath.Match function to do pattern matching.</p> <p>Example matches:</p> <ul> <li>A directory and all of its content: <code>dir/*</code></li> <li>Match all files with a specific extension: <code>dir/*.tf</code></li> <li>Match all files that start with a string, end with another and have a predefined number of chars in the middle -- <code>data-???-report</code> will match three chars between data and report</li> <li>Match all files that start with a string, and finish with any character from a sequence: <code>dir/instance[0-9].tf</code></li> </ul> <p>As you can see in the example matches, these are the regex rules that you are already accustomed to.</p>"},{"location":"concepts/stack/stack-settings.html#repository-and-branch","title":"Repository and branch","text":"<p>Repository and branch point to the location of the source code for a stack. The repository must either belong to the GitHub account linked to Spacelift  (its choice may further be limited by the way the Spacelift GitHub app has been installed) or to the GitLab server integrated with your Spacelift account. For more information about these integrations, please refer to our GitHub and GitLab documentation respectively.</p> <p>Thanks to the strong integration between GitHub and Spacelift, the link between a stack and a repository can survive the repository being renamed in GitHub. If you're storing your repositories in GitLab then you need to make sure to manually (or programmatically, using Terraform) point the stack to the new location of the source code.</p> <p>Info</p> <p>Spacelift does not support moving repositories between GitHub accounts, since Spacelift accounts are strongly linked to GitHub ones. In that case the best course of action is to take your Terraform state, download it and import it while recreating the stack (or multiple stacks) in a different account. After that, all the stacks pointing to the old repository can be safely deleted.</p> <p>Moving a repository between GitHub and GitLab or the other way around is simple, however. Just change the provider setting on the Spacelift project, and point the stack to the new source code location.</p> <p>Branch signifies the repository branch tracked by the stack. By default, that is unless a Git push policy explicitly determines otherwise, a commit pushed to the tracked branch triggers a deployment represented by a tracked run. A push to any other branch by default triggers a test represented by a proposed run. More information about git push policies, tracked branches, and head commits can be found here.</p> <p>Results of both tracked and proposed runs are displayed in the source control provider using their specific APIs - please refer to our GitHub and GitLab documentation respectively to understand how Spacelift feedback is provided for your infrastructure changes.</p> <p>Info</p> <p>A branch must exist before it's pointed to in Spacelift.</p>"},{"location":"concepts/stack/stack-settings.html#runner-image","title":"Runner image","text":"<p>Since every Spacelift job (which we call runs) is executed in a separate Docker container, setting a custom runner image provides a convenient way to prepare the exact runtime environment your infra-as-code flow is designed to use.</p> <p>Additionally, for our Pulumi integration overriding the default runner image is the canonical way of selecting the exact Pulumi version and its corresponding language SDK.</p> <p>You can find more information about our use of Docker in this dedicated help article.</p> <p>Info</p> <p>Runner image can be overridden by the runtime configuration specified in the <code>.spacelift/config.yml</code> file.</p> <p>Warning</p> <p>On the public worker pool, Docker images can only be pulled from allowed registries. On private workers, images can be stored in any registry, including self-hosted ones.</p>"},{"location":"concepts/stack/stack-settings.html#worker-pool","title":"Worker pool","text":""},{"location":"concepts/stack/stack-settings.html#terraform-specific-settings","title":"Terraform-specific settings","text":""},{"location":"concepts/stack/stack-settings.html#terraform-version","title":"Version","text":"<p>The Terraform version is set when a stack is created to indicate the version of Terraform that will be used with this project. However, Spacelift covers the entire Terraform version management story, and applying a change with a newer version will automatically update the version on the stack.</p>"},{"location":"concepts/stack/stack-settings.html#terraform-workspace","title":"Workspace","text":"<p>Terraform workspaces are supported by Spacelift, too, as long as your state backend supports them. If the workspace is set, Spacelift will try to first select, and then - should that fail - automatically create the required workspace on the state backend.</p> <p>If you're managing Terraform state through Spacelift, the workspace argument is ignored since Spacelift gives each stack a separate workspace by default.</p>"},{"location":"concepts/stack/stack-settings.html#pulumi-specific-settings","title":"Pulumi-specific settings","text":""},{"location":"concepts/stack/stack-settings.html#pulumi-login-url","title":"Login URL","text":"<p>Login URL is the address Pulumi should log into during Run initialization. Since we do not yet provide a full-featured Pulumi state backend, you need to bring your own (eg. Amazon S3).</p> <p>You can read more about the login process here. More general explanation of Pulumi state management and backends is available here.</p>"},{"location":"concepts/stack/stack-settings.html#pulumi-stackname","title":"Stack name","text":"<p>The name of the Pulumi stack which should be selected for backend operations. Please do not confuse it with the Spacelift stack name - they may be different, though it's probably good if you can keep them identical.</p>"},{"location":"faq/index.html","title":"FAQ","text":"<p>Spacelift has many features and hidden nuggets so it is easy to overlook some of them but we have you covered with this list of frequently asked questions.</p> <p>If you still cannot find the answer to your question below, please reach out to our support team.</p>"},{"location":"faq/index.html#providing-admin-consent-for-microsoft-login","title":"Providing Admin consent for Microsoft login","text":"<p>In order to sign up for Spacelift using an Azure AD account via our Microsoft login option, the Spacelift application needs to be installed into your Azure AD directory. To do this you either need to be an Azure AD administrator, or your Azure AD configuration needs to allow users to install applications.</p> <p>If you don't have permission, you will receive the following message when attempting to sign up:</p> <p></p> <p>If this happens, it means that you need ask an Azure AD admin to provide Admin consent, as described in the Microsoft documentation.</p> <p>To do this, your Azure AD admin can use a URL like the following to grant permission to Spacelift:</p> <pre><code>https://login.microsoftonline.com/&lt;tenant-id&gt;/adminconsent?client_id=fba648b0-4b78-4224-b510-d96ff51eeef9\n</code></pre> <p>Info</p> <p>NOTE: make sure to replace <code>&lt;tenant-id&gt;</code> with your Azure AD Tenant ID!</p> <p>After granting admin consent, your administrator will be redirected to Spacelift and receive the following error message, which can be safely ignored:</p> <p></p> <p>After admin consent has been provided, you should be able to sign-up for Spacelift using your Microsoft account.</p>"},{"location":"faq/index.html#platforms","title":"Platforms","text":""},{"location":"faq/index.html#terraform","title":"Terraform","text":""},{"location":"faq/index.html#how-do-i-import-the-terraform-state-for-my-stack","title":"How do I import the Terraform state for my stack?","text":"<p>The Terraform state file can be imported during the creation of a stack.</p>"},{"location":"faq/index.html#how-do-i-export-the-terraform-state-for-my-stack","title":"How do I export the Terraform state for my stack?","text":"<p>The Terraform state file can be pulled and then exported using a Task.</p> <p>For example, to export the state to an Amazon S3 bucket, you would run the following command as a Task:</p> <pre><code>terraform state pull &gt; state.json &amp;&amp; aws s3 cp state.json s3://&lt;PATH&gt;\n</code></pre> <p>Warning</p> <p>For that example to work, the stack needs to have write access to the AWS S3 bucket, possibly via an AWS Integration.</p>"},{"location":"faq/index.html#how-do-i-switch-from-spacelift-managing-the-terraform-state-to-me-managing-it","title":"How do I switch from Spacelift managing the Terraform state to me managing it?","text":"<p>You would first need to export the state file to a suitable location.</p> <p>The state management setting can not be changed once a stack has been created so you will need to recreate the stack and make sure that the \"Manage state\" setting is disabled.</p>"},{"location":"faq/index.html#how-do-i-manipulate-the-terraform-state-file","title":"How do I manipulate the Terraform state file?","text":"<p>You can manipulate the Terraform state by running <code>terraform state &lt;SUBCOMMAND&gt;</code> commands in a Task.</p> <p>This applies whether you or Spacelift manages the Terraform state file.</p>"},{"location":"faq/index.html#how-do-i-import-existing-resources-into-a-terraform-stack","title":"How do I import existing resources into a Terraform stack?","text":"<p>Just run the <code>terraform import \u2026</code> in a Task.</p> <p>This applies whether you or Spacelift manages the Terraform state file.</p>"},{"location":"faq/index.html#policies","title":"Policies","text":""},{"location":"faq/index.html#my-policy-works-fine-in-the-workbench-but-not-on-my-stackmodule","title":"My policy works fine in the workbench but not on my stack/module","text":"<p>Except for the Login policies, all policies must be attached to stacks or modules to be evaluated so let's first confirm this by verifying that the stack or module is listed in the \"Used by\" section on the policy page. If it does not show up there, you will need to attach the policy.</p> <p>If your policy is attached to your stack/module and you still do not see the expected behavior from that policy, you should make sure that sampling is enabled for that policy, and then review the recorded samples in the Policy Workbench. That should give you valuable insight.</p> <p>If you do not see any sampled events despite sampling being enabled and having performed events that should have triggered events, make sure that the appropriate type was selected when the policy was created.</p>"},{"location":"faq/index.html#i-do-not-see-some-samples-for-my-login-policy","title":"I do not see some samples for my Login policy","text":"<p>Login policies are not evaluated for account creators and SSO admins who always get admin access to their respective Spacelift accounts. This is to avoid a situation where a bad Login policy locks out everyone from the account.</p> <p>The side-effect is that you will not see samples for these users.</p>"},{"location":"faq/index.html#are-approval-policies-and-run-confirmation-the-same-thing","title":"Are Approval policies and run confirmation the same thing?","text":"<p>Approval policies and run confirmation are related but different concepts.</p> <p>Just think about how GitHub's Pull Requests work - you can approve a PR before merging it in a separate step. Just like a PR approval means \"I'm OK with this being merged\", a run approval means \"I'm OK with that action being executed\" but nothing will happen until someone clicks on the \"Merge\" or \"Confirm\" button, respectively.</p>"},{"location":"faq/index.html#billing","title":"Billing","text":""},{"location":"faq/index.html#what-counts-as-a-user","title":"What counts as a user?","text":"<p>Everyone who logged in to the Services in a given month is counts as a user.</p> <p>API keys are virtual users and are billed like regular users, too. Thus, each API key used during any billing cycle counts against the total number of users.</p> <p>When setting up SSO, future logins will appear as new users since Spacelift cannot map those without your assistance. New users will count against your quota, and you may run out of seats. If you run into this problem, you can contact us.</p>"},{"location":"integrations/api.html","title":"GraphQL API","text":""},{"location":"integrations/api.html#graphql","title":"GraphQL","text":"<p>GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools.</p> <p>Spacelift provides a GraphQL API for you to control your Spacelift account programmatically and/or through an API Client if you choose to do so. A smaller subset of this API is also used by the Spacelift Terraform provider, as well as the Spacelift CLI (spacectl). The API can be accessed at the <code>/graphql</code> endpoint of your account using <code>POST</code> HTTP method.</p> An example of request and response <pre><code>$ curl --request POST \\\n  --url http://&lt;account-name&gt;.app.spacelift.io/graphql \\\n  --header 'Authorization: Bearer &lt;token&gt;' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\"query\":\"{ stacks { id name, administrative, createdAt, description }}\"}'\n</code></pre> <p>The request body looks like this when formatted a bit nicer:</p> <pre><code>{\nstacks\n{\nid\nname,\nadministrative,\ncreatedAt,\ndescription\n}\n}\n</code></pre> <p>And the response looks like this:</p> <pre><code>{\n\"data\": {\n\"stacks\": [\n{\n\"id\": \"my-stack-1\",\n\"name\": \"My Stack 1\",\n\"administrative\": false,\n\"createdAt\": 1672916942,\n\"description\": \"The is my first stack\"\n},\n{\n\"id\": \"my-stack-2\",\n\"name\": \"My Stack 2\",\n\"administrative\": false,\n\"createdAt\": 1674218834,\n\"description\": \"The is my second stack\"\n}\n]\n}\n}\n</code></pre>"},{"location":"integrations/api.html#recommendation","title":"Recommendation","text":"<p>Our recommendation is to use the Spacelift API Key to authenticate with the GraphQL API.</p> <p>Our choice of tool is Insomnia. Insomnia is a free, open-source tool that allows you to easily create and manage API requests. You can also use Postman, but the walkthrough in this guide will be based on Insomnia.</p>"},{"location":"integrations/api.html#usage-demo","title":"Usage Demo","text":"<p>The below guide walks through an example of generating your Spacelift token with spacectl and using it to communicate with Spacelift.</p> <p>Prerequisites:</p> <ul> <li>Insomnia downloaded and installed</li> <li>Spacelift account with admin access (for ability to create API Keys)</li> </ul>"},{"location":"integrations/api.html#authenticating-with-the-graphql-api","title":"Authenticating with the GraphQL API","text":"<p>If your Spacelift account is called <code>example</code> you would be able to access your GraphQL by sending POST requests to: <code>https://example.app.spacelift.io/graphql</code></p> <p>All requests need to be authenticated using a JWT bearer token, which we will discuss in more detail below.</p> <p>There are currently three ways of obtaining this token:</p> <ol> <li>Spacelift API Key &gt; Token - for long-term usage (recommended)</li> <li>SpaceCTL CLI &gt; Token - for temporary usage</li> <li>Personal GitHub Token &gt; Token</li> </ol>"},{"location":"integrations/api.html#spacelift-api-key-token","title":"Spacelift API Key &gt; Token","text":"<p>Spacelift supports creating and managing machine users with programmatic access to the Spacelift GraphQL API. These \"machine users\" are called API Keys and can be created by Spacelift admins through the Settings panel.</p> <p>Note</p> <p>API keys are virtual users and are billed like regular users, too. Thus, each API key used (exchanged for a token) during any given billing cycle counts against the total number of users.</p> <p>Steps to create API key in the UI:</p> <p>Click on Settings in the bottom left corner of the UI</p> <p> </p> <p>Choose API Keys menu and click on Add new API key</p> <p> </p> <p>The API key creation form will allow you to specify an arbitrary key name, along with the Admin setting and the list of teams. If the key is given admin privileges, it has full access to the Spacelift API and won't be subject to access policies.</p> <p>For non-administrative keys, you may want to add a virtual list of teams that the key should \"belong to\" so that existing access policies based on GitHub teams or SAML assertions can work with your API keys just as they do with regular users.</p> <p>Without further ado, let's create a non-administrative API key with virtual membership in two teams: Developers and DevOps:</p> <p> </p> <p>Once you click the Add Key button, the API Key will be generated and a file will be automatically downloaded. The file contains the API token in two forms - one to be used with our API, and the other one as a <code>.terraformrc</code> snippet to access your private modules outside of Spacelift:</p> <p> </p> <p>The config file looks something like this:</p> <pre><code>Please use the following API secret when communicating with Spacelift\nprogrammatically:\n\nSECRET_VALUE40ffc46887297384892384789239\n\nPlease add this snippet to your .terraformrc file if you want to use this API\nkey to access Spacelift-hosted Terraform modules outside of Spacelift:\n\ncredentials \"spacelift.io\" {\n  token = \"TOKEN_VALUEQwZmZjNDY4ODdiMjI2ZWE4NDhjMWQwNWZiMWE5MGU4NWMwZTFlY2Q4NDAxMGI2ZjA2NzkwMmI1YmVlMWNmMGE\"\n}\n</code></pre> <p>Warning</p> <p>Make sure you persist this data somewhere on your end - we don't store the token and it cannot be retrieved or recreated afterwards.</p>"},{"location":"integrations/api.html#spacectl-cli-token","title":"SpaceCTL CLI &gt; Token","text":"<p>One approach to generating this token is using the Spacelift spacectl CLI. We consider this the easiest method, as the heavy lifting to obtain the token is done for you.</p> <p>Steps:</p> <ol> <li>Follow the instructions on the <code>spacectl</code> GitHub repository to install the CLI on your machine.</li> <li>Authenticate to your Spacelift account using <code>spacectl profile login</code></li> <li>Once authenticated, run <code>spacectl profile export-token</code> to receive the bearer token needed for future GraphQL queries/mutations.</li> </ol>"},{"location":"integrations/api.html#personal-github-token-token","title":"Personal GitHub Token &gt; Token","text":"<p>Info</p> <p>This option is only available to those using GitHub as their identity provider. If you have enabled any other Single Sign-On methods on your account, this method will not work. If this applies to you, you will need to use the Spacelift API Key &gt; Token method instead.</p> <p>Steps:</p> <ol> <li>Using a GitHub Account that has access to your Spacelift account, create a GitHub Personal Access Token. Copy the value of this token to a secure location, as you'll need it in the next step.</li> <li>Using your favorite API Client (e.g. Insomnia or GraphiQL). Make a GraphQL POST request to your account's GraphQL endpoint (example below).</li> </ol> <p>Request Details:</p> <p>POST to <code>https://example.app.spacelift.io/graphql</code></p> <p>Info</p> <p>Replace \"example\" with the name of your Spacelift account.</p> <p>Query:</p> <pre><code>mutation GetSpaceliftToken($token: String!) {\noauthUser(token: $token) {\njwt\n}\n}\n</code></pre> <p>Info</p> <p>You'll need to pass in token as a query variable for the above example query to work. When making a GraphQL query with your favorite API Client, you should see a section called GraphQL variables where you can pass in an input.</p> <p>GraphQL Variables Input:</p> <pre><code>{\n\"token\": \"PASTE-TOKEN-VALUE-HERE\"\n}\n</code></pre> <p>Assuming all went well, the result of the above query will return your JWT bearer token, which you will now be able to use to authenticate other queries. Once acquired, ensure you use this bearer token in your requests. If you want to access the API reliably in an automated way, we suggest using the Spacelift API Key &gt; Token approach as Spacelift tokens expire after 1 hour.</p>"},{"location":"integrations/api.html#insomnia-setup","title":"Insomnia setup","text":"<p>You can create request libraries in Insomnia to make it easier to work with the Spacelift API. You can also automate the JWT token generation process using the Environment Variables feature.</p> <p>Copy the following JSON to your clipboard:</p> Click here to expand <pre><code>{\n\"_type\": \"export\",\n\"__export_format\": 4,\n\"__export_date\": \"2023-01-23T19:49:05.605Z\",\n\"__export_source\": \"insomnia.desktop.app:v2022.7.0\",\n\"resources\": [\n{\n\"_id\": \"req_d7fb83c13cc945da9e21cd9b94722d3d\",\n\"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n\"modified\": 1674497188638,\n\"created\": 1656577781496,\n\"url\": \"{{ _.BASE_URL }}/graphql\",\n\"name\": \"Authentication - Get JWT\",\n\"description\": \"\",\n\"method\": \"POST\",\n\"body\": {\n\"mimeType\": \"application/graphql\",\n\"text\": \"{\\\"query\\\":\\\"mutation GetSpaceliftToken($keyId: ID!, $keySecret: String!) {\\\\n  apiKeyUser(id: $keyId, secret: $keySecret) {\\\\n    id\\\\n\\\\t\\\\tjwt\\\\n  }\\\\n}\\\",\\\"variables\\\":{\\\"keyId\\\":\\\"{{ _.API_KEY_ID }}\\\",\\\"keySecret\\\":\\\"{{ _.API_KEY_SECRET }}\\\"},\\\"operationName\\\":\\\"GetSpaceliftToken\\\"}\"\n},\n\"parameters\": [],\n\"headers\": [\n{\n\"name\": \"Content-Type\",\n\"value\": \"application/json\",\n\"id\": \"pair_85e4a9afc2e6491ca59b52f77d94e81f\"\n}\n],\n\"authentication\": {},\n\"metaSortKey\": -1656577781496,\n\"isPrivate\": false,\n\"settingStoreCookies\": true,\n\"settingSendCookies\": true,\n\"settingDisableRenderRequestBody\": false,\n\"settingEncodeUrl\": true,\n\"settingRebuildPath\": true,\n\"settingFollowRedirects\": \"global\",\n\"_type\": \"request\"\n},\n{\n\"_id\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n\"parentId\": null,\n\"modified\": 1656576979763,\n\"created\": 1656576979763,\n\"name\": \"Spacelift\",\n\"description\": \"\",\n\"scope\": \"collection\",\n\"_type\": \"workspace\"\n},\n{\n\"_id\": \"req_83de84158a16459fa4bfce6042859df6\",\n\"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n\"modified\": 1674497166036,\n\"created\": 1656577541263,\n\"url\": \"{{ _.BASE_URL }}/graphql\",\n\"name\": \"Get Stacks\",\n\"description\": \"\",\n\"method\": \"POST\",\n\"body\": {\n\"mimeType\": \"application/graphql\",\n\"text\": \"{\\\"query\\\":\\\"{ \\\\n\\\\tstacks\\\\n\\\\t{\\\\n\\\\t\\\\tid\\\\n\\\\t\\\\tname,\\\\n\\\\t\\\\tadministrative,\\\\n\\\\t\\\\tcreatedAt,\\\\n\\\\t\\\\tdescription\\\\n\\\\t}\\\\n}\\\"}\"\n},\n\"parameters\": [],\n\"headers\": [\n{\n\"name\": \"Content-Type\",\n\"value\": \"application/json\",\n\"id\": \"pair_80893dda7c0f4266b48bd09d0eaa3222\"\n}\n],\n\"authentication\": {\n\"type\": \"bearer\",\n\"token\": \"{{ _.API_TOKEN }}\"\n},\n\"metaSortKey\": -1656577721437.75,\n\"isPrivate\": false,\n\"settingStoreCookies\": true,\n\"settingSendCookies\": true,\n\"settingDisableRenderRequestBody\": false,\n\"settingEncodeUrl\": true,\n\"settingRebuildPath\": true,\n\"settingFollowRedirects\": \"global\",\n\"_type\": \"request\"\n},\n{\n\"_id\": \"env_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\",\n\"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n\"modified\": 1660646140956,\n\"created\": 1656576979773,\n\"name\": \"Base Environment\",\n\"data\": {},\n\"dataPropertyOrder\": {},\n\"color\": null,\n\"isPrivate\": false,\n\"metaSortKey\": 1656576979773,\n\"_type\": \"environment\"\n},\n{\n\"_id\": \"jar_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\",\n\"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n\"modified\": 1656576979775,\n\"created\": 1656576979775,\n\"name\": \"Default Jar\",\n\"cookies\": [],\n\"_type\": \"cookie_jar\"\n},\n{\n\"_id\": \"spc_dbcf993f70b44bb18eee1b2362bb5bdc\",\n\"parentId\": \"wrk_3b73a2a7403445a48acdc8396803c4e8\",\n\"modified\": 1656576979770,\n\"created\": 1656576979770,\n\"fileName\": \"Spacelift\",\n\"contents\": \"\",\n\"contentType\": \"yaml\",\n\"_type\": \"api_spec\"\n},\n{\n\"_id\": \"env_ea5c30c23af449f792c71d160678eff5\",\n\"parentId\": \"env_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\",\n\"modified\": 1669716444669,\n\"created\": 1669716373608,\n\"name\": \"Spacelift\",\n\"data\": {\n\"BASE_URL\": \"https://ACCOUNT_NAME.app.spacelift.io\",\n\"API_KEY_ID\": \"insert-your-real-api-key-here\",\n\"API_KEY_SECRET\": \"insert-your-real-api-secret-here\",\n\"API_TOKEN\": \"{% response 'body', 'req_d7fb83c13cc945da9e21cd9b94722d3d', 'b64::JC5kYXRhLmFwaUtleVVzZXIuand0::46b', 'never', 60 %}\"\n},\n\"dataPropertyOrder\": {\n\"&amp;\": [\n\"BASE_URL\",\n\"API_KEY_ID\",\n\"API_KEY_SECRET\",\n\"API_TOKEN\"\n]\n},\n\"color\": \"#6b84ff\",\n\"isPrivate\": false,\n\"metaSortKey\": 828288489886.5,\n\"_type\": \"environment\"\n}\n]\n}\n</code></pre> <p>In the home screen of Insomnia, click on <code>Import From</code> then click on <code>Clipboard</code>.</p> <p> </p> <p>The <code>Spacelift</code> collection will appear. Click on it.</p> <p>On the top left corner, click the <code>\ud83d\udd35 Spacelift</code> icon, then choose <code>Manage Environments</code>.</p> <p> </p> <p>Here, make sure you fill the first three variables properly:</p> <ul> <li><code>BASE_URL</code> should be the URL of your Spacelift account. For example, <code>https://my-account.app.spacelift.io</code></li> <li><code>API_KEY_ID</code> is the ID of the API key you created in the previous step. It should be a 26-character ULID.</li> <li><code>API_KEY_SECRET</code> can be found in the file that was downloaded when you created the API key.</li> </ul> <p>Don't worry about the 4th.</p> <p> </p> <p>That's it! Now just send an <code>Authentication - Get JWT</code> request which populates <code>API_TOKEN</code> environment variable, then send the other <code>Get Stacks</code> request to see the list of stacks in your account.</p> <p>If you want to create another request, just right click on <code>Get Stacks</code> and duplicate it. Then, change the query to whatever you want.</p> <p>Hint</p> <p>Don't forget that the JWT expires after 10 hours. Run the authentication request again to get a new one.</p>"},{"location":"integrations/api.html#viewing-the-graphql-schema","title":"Viewing the GraphQL Schema","text":"<p>Our GraphQL schema is self-documenting. The best way to view the latest documentation is using a dedicated GraphQL client like Insomnia or GraphiQL.</p> <p>You may also view the documentation using a static documentation website generator like GraphDoc.</p> <p>Note: As of the writing of these examples, the latest version of Postman does not currently support viewing GraphQL Schemas from a URL, but does support autocompletion.</p> <p>Warning</p> <p>Please replace the URL in the below examples with the one pointing to your Spacelift account.</p>"},{"location":"integrations/api.html#insomnia","title":"Insomnia","text":""},{"location":"integrations/api.html#graphiql","title":"GraphiQL","text":"<p>Input your GraphQL Endpoint for your Spacelift Account.</p> <p></p> <p>Use the Documentation Explorer within GraphiQL</p> <p></p>"},{"location":"integrations/audit-trail.html","title":"Audit trail","text":"<p>Info</p> <p>Note that Audit Trail is an Enterprise plan feature.</p> <p>Spacelift optionally supports auditing all operations that change Spacelift resources. This is handled by asynchronously sending webhooks to a user-supplied endpoint.</p>"},{"location":"integrations/audit-trail.html#setup","title":"Setup","text":"<p>In order to set up the audit trail, navigate to the Audit trail section of your account settings and click the Set up button:</p> <p></p> <p>You will then need to provide a webhook endpoint and an arbitrary secret that you can later use for verifying payload. Let's use ngrok for the purpose of this tutorial:</p> <p></p> <p>If you choose to automatically enable the functionality, clicking the Save button will verify that payloads can be delivered (the endpoint returns a 2xx status code). This gives us an opportunity to look at the payload:</p> <pre><code>{\n\"account\": \"example\",\n\"action\": \"audit_trail_webhook.set\",\n\"actor\": \"github::name\",\n\"context\": {\n\"mutation\": \"auditTrailSetWebhook\"\n},\n\"data\": {\n\"args\": {\n\"Enabled\": true,\n\"Endpoint\": \"https://example-audithook.com/\",\n\"SecretSHA\": \"xxxfffdddwww\"\n}\n},\n\"remoteIP\": \"0.0.0.0\",\n\"timestamp\": 1674124447947\n}\n</code></pre> <p>...and the headers - the interesting ones are highlighted:</p> <p></p>"},{"location":"integrations/audit-trail.html#usage","title":"Usage","text":"<p>Every audit trail payload conforms to the same schema:</p> <ul> <li><code>account</code>: name (subdomain) of the affected Spacelift account;</li> <li><code>action</code>: name of the performed action;</li> <li><code>actor</code>: actor performing the action - the <code>::</code> format shows both the actor identity (second element), and the source of the identity (first element)</li> <li><code>context</code>: some contextual metadata about the request;</li> <li><code>data</code>: action-specific payload showing arguments passed to the request. Any sensitive arguments (like secrets) are sanitized;</li> </ul> <p>Below is a sample:</p> <pre><code>{\n\"account\": \"example\",\n\"action\": \"stack.create\",\n\"actor\": \"github::name\",\n\"context\": {\n\"mutation\": \"stackCreate\"\n},\n\"data\": {\n\"ID\": \"audit-trail-demo\",\n\"args\": {\n\"Input\": {\n\"Administrative\": false,\n\"AfterApply\": [],\n\"AfterDestroy\": [],\n\"AfterInit\": [],\n\"AfterPerform\": [],\n\"AfterPlan\": [],\n\"AfterRun\": [],\n\"Autodeploy\": false,\n\"Autoretry\": false,\n\"BeforeApply\": [],\n\"BeforeDestroy\": [],\n\"BeforeInit\": [],\n\"BeforePerform\": [],\n\"BeforePlan\": [],\n\"Branch\": \"showcase\",\n\"Description\": \"\",\n\"GithubActionDeploy\": true,\n\"IsDisabled\": null,\n\"Labels\": [],\n\"LocalPreviewEnabled\": false,\n\"Name\": \"audit-trail-demo\",\n\"Namespace\": \"spacelift-io\",\n\"ProjectRoot\": \"\",\n\"ProtectFromDeletion\": false,\n\"Provider\": \"SHOWCASE\",\n\"Repository\": \"terraform-starter\",\n\"RunnerImage\": null,\n\"Space\": \"legacy\",\n\"TerraformVersion\": null,\n\"VendorConfig\": {\n\"Ansible\": null,\n\"CloudFormation\": null,\n\"Kubernetes\": null,\n\"Pulumi\": null,\n\"Terraform\": {\n\"use_smart_sanitization\": null,\n\"version\": \"1.3.7\",\n\"workspace\": null\n}\n},\n\"WorkerPool\": null\n},\n\"ManageState\": true,\n\"Slug\": null,\n\"StackObjectID\": null\n}\n},\n\"remoteIP\": \"0.0.0.0\",\n\"timestamp\": 1674124447947\n}\n</code></pre>"},{"location":"integrations/audit-trail.html#disabling-and-deleting-the-audit-trail","title":"Disabling and deleting the audit trail","text":"<p>The audit trail can be disabled and deleted at any point, but for both events we will send the appropriate payload. We suggest that you always treat these at least as important security signals, if not alerting conditions:</p> <pre><code>{\n\"account\": \"example\",\n\"action\": \"audit_trail_webhook.delete\",\n\"actor\": \"github::user\",\n\"context\": {\n\"mutation\": \"auditTrailDeleteWebhook\"\n},\n\"data\": {},\n\"remoteIP\": \"0.0.0.0\",\n\"timestamp\": 1674124447947\n}\n</code></pre>"},{"location":"integrations/audit-trail.html#verifying-payload","title":"Verifying payload","text":"<p>Spacelift uses the same similar verification mechanism as GitHub. With each payload we send 2 headers, <code>X-Signature</code> and <code>X-Signature-256</code>. <code>X-Signature</code> header contains the SHA1 hash of the payload, while <code>X-Signature-256</code> contains the SHA256 hash. We're using the exact same mechanism as GitHub to generate signatures, please refer to this article for details.</p>"},{"location":"integrations/audit-trail.html#sending-logs-to-aws","title":"Sending logs to AWS","text":"<p>We provide a reference implementation for sending the Audit Trail logs to an AWS S3 bucket.</p> <p>It works as-is but can also be tweaked to route the logs to other destinations with minimal effort.</p>"},{"location":"integrations/audit-trail.html#failures","title":"Failures","text":"<p>Audit trail deliveries are retried on failure.</p>"},{"location":"integrations/docker.html","title":"Docker","text":"<p>Every job in Spacelift is processed inside a fresh, isolated Docker container. This approach provides reasonable isolation and resource allocation and - let's face it - is a pretty standard approach these days.</p>"},{"location":"integrations/docker.html#standard-runner-image","title":"Standard runner image","text":"<p>By default, Spacelift uses the latest version of the<code>public.ecr.aws/spacelift/runner-terraform</code> image, a simple Alpine image with a small bunch of universally useful packages. Feel free to refer to the Dockerfile that builds this image.</p> <p>Info</p> <p>Given that we use Continuous Deployment on our backend and Terraform provider, we explicitly don't want to version the runner image. Feature previews are available under a <code>future</code> tag, but we'd advise against using these as the API might change unexpectedly.</p>"},{"location":"integrations/docker.html#standard-runner-image-flavors","title":"Standard runner image flavors","text":"<ul> <li><code>runner-terraform:latest</code> (default) - includes <code>aws</code> CLI</li> <li><code>runner-terraform:gcp-latest</code> - includes <code>gcloud</code> CLI</li> <li><code>runner-terraform:azure-latest</code> - includes <code>az</code> CLI</li> </ul> <p>Note</p> <p>The reason we have separate images for cloud providers is that the <code>gcloud</code> and <code>az</code> CLIs are enormous and we don't want to bloat the default image with them.</p>"},{"location":"integrations/docker.html#allowed-registries-on-public-worker-pools","title":"Allowed registries on public worker pools","text":"<p>On public worker pools, only Docker images from the following registries are allowed to be used for runner images:</p> <ul> <li>azurecr.io (Azure Container Registry)</li> <li>dkr.ecr.&lt;region&gt;.amazonaws.com (All regions are supported)</li> <li>docker.io</li> <li>docker.pkg.dev</li> <li>gcr.io (Google Cloud Container Registry)</li> <li>ghcr.io (GitHub Container Registry)</li> <li>public.ecr.aws</li> <li>quay.io</li> </ul> <ul> <li>registry.gitlab.com</li> <li>registry.hub.docker.com</li> </ul>"},{"location":"integrations/docker.html#customizing-the-runner-image","title":"Customizing the runner image","text":"<p>The best way to customizing your Terraform execution environment is to build a custom runner image and use runtime configuration to tell Spacelift to use it instead of the standard runner. If you're not using Spacelift provider with Terraform 0.12, you can use any image supporting (by far the most popular) AMD64 architecture and add your dependencies to it.</p> <p>If you want our tooling in your image, there are two possible approaches. The first approach is to build on top of our image. We'd suggest doing that only if your customizations are relatively simple. For example, let's add a custom CircleCI provider to your image. They have a releases page allowing you to just <code>curl</code> the right version of the binary and put it in the <code>/bin</code> directory:</p> Dockerfile<pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\n\nWORKDIR /tmp\nRUN curl -O -L https://github.com/mrolla/terraform-provider-circleci/releases/download/v0.3.0/terraform-provider-circleci-linux-amd64 \\\n&amp;&amp; mv terraform-provider-circleci-linux-amd64 /bin/terraform-provider-circleci \\\n&amp;&amp; chmod +x /bin/terraform-provider-circleci\n</code></pre> <p>For more sophisticated use cases it may be cleaner to use Docker's multistage build feature to build your image and add our tooling on top of it. As an example, here's the case of us building a Terraform sops provider from source using a particular version. We want to keep our image small so we'll use a separate builder stage.</p> <p>The following approach works for Terraform version 0.12 and below, where custom Terraform providers colocated with the Terraform binary are automatically used.</p> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest as spacelift\nFROM golang:1.13-alpine as builder\n\nWORKDIR /tmp\n\n# Note how we don't bother building the provider statically because\n# we're using Alpine for our final runner image, too.\nRUN git clone https://github.com/carlpett/terraform-provider-sops.git \\\n&amp;&amp; cd terraform-provider-sops \\\n&amp;&amp; git checkout c5ffe6ebfac0a56fd60d5e7d77e0f2a73c34c3b7 \\\n&amp;&amp; go build -o /terraform-provider-sops\n\nFROM alpine:3.10\nCOPY --from=spacelift /bin/terraform-provider-spacelift /bin/terraform-provider-spacelift\nCOPY --from=builder /terraform-provider-sops /bin/terraform-provider-sops\n\nRUN adduser --disabled-password --no-create-home --uid=1983 spacelift\n</code></pre> <p>An additional requirement is the presence of <code>ps</code> command in the image. The Spacelift worker periodically checks the status of the container image and it uses <code>ps</code> to do so.</p> <p>Info</p> <p>Note the <code>adduser</code> bit. Spacelift runs its Docker workflows as user <code>spacelift</code> with UID 1983, so make sure that:</p> <ul> <li>this user exists and has the right UID, otherwise you won't have access to your files;</li> <li>whatever you need accessed and executed in your custom image has the right ownership and/or permissions;</li> </ul> <p>Depending on your image flavor, the exact command to add the user may be different.</p> <p>Tip</p> <p>Any <code>ENTRYPOINT</code> and <code>CMD</code> customization will be ignored because the Spacelift worker binary must be the root process in the container.</p> <p>If you need to customize the shell (e.g., dynamically set environment variables or export functions), you can do so in a <code>before_init</code> hook.</p>"},{"location":"integrations/docker.html#custom-providers-from-terraform-013-onwards","title":"Custom providers from Terraform 0.13 onwards","text":"<p>Since Terraform 0.13, custom providers require a slightly different approach. You will build them the same way as described above, but the path now will be different. In order to work with the new API, we require that you put the provider binaries in the <code>/plugins</code> directory and maintain a particular naming scheme. The above <code>sops</code> provider example will work with Terraform 0.13 if the following stanza is added to the <code>Dockerfile</code>.</p> <pre><code>COPY --from=builder /terraform-provider-sops /plugins/registry.myorg.io/myorg/sops/1.0.0/linux_amd64/terraform-provider-sops\n</code></pre> <p>In addition, the custom provider must be explicitly required in the Terraform code, like this:</p> <pre><code>terraform {\nrequired_providers {\nspacelift = {\nsource = \"registry.myorg.io/myorg/sops\"\n}\n}\n}\n</code></pre> <p>Note that the source as defined above and the plugin path as defined in the Dockerfile are entirely arbitrary but must match. You can read more in the official Terraform 0.13 upgrade documentation.</p>"},{"location":"integrations/docker.html#using-private-docker-images","title":"Using private Docker images","text":"<p>If you're using Spacelift's default public worker pool, you're required to use public images. This is by design - if we allowed using private images, they would be cached by the Docker daemon and accessible to all customers using the same shared worker.</p> <p>Hence, only private workers support private Docker images. To enable private image support, first, execute <code>docker login</code> command with the proper registry credentials. Spacelift agent will read the credentials from Docker's configuration directory, but you will need to point it to the correct location by setting the <code>SPACELIFT_DOCKER_CONFIG_DIR</code> environment variable.</p>"},{"location":"integrations/docker.html#special-case-ecr","title":"Special case: ECR","text":"<p>ECR is a special case because those credentials tend to expire pretty quickly, and you'd need to add a mechanism to refresh them periodically if you wanted to maintain live access to the registry (cached images would not be affected by expired credentials). Given that many of our customers use EC2 to host their worker pools, we implemented a special mechanism to support private images hosted in ECR.</p> <p>This access is seamless - if the launcher detects that a runner image is hosted in ECR, it tries to use the existing credentials (e. g. EC2 instance role credentials) to generate the registry access token automatically on each job execution. With ECR images you don't even need to execute <code>docker login</code>.</p>"},{"location":"integrations/docker.html#best-practices","title":"Best practices","text":"<p>Here's a bunch of things we consider essential to keep your Docker usage relatively safe.</p>"},{"location":"integrations/docker.html#if-unsure-build-from-source","title":"If unsure, build from source","text":"<p>Building from the source is generally safer than using a pre-built binary, especially if you can review the code beforehand and make sure you're always building the code you've reviewed. You can use a Git commit hash, like we did above.</p>"},{"location":"integrations/docker.html#use-well-known-bases","title":"Use well-known bases","text":"<p>If you're building an image from a source other than <code>public.ecr.aws/spacelift/runner-terraform</code>, please prefer well-known and well-supported base images. Official images are generally safe, so choose something like <code>golang:1.13-alpine</code> over things like <code>imtotallylegit/notascamipromise:best-golang-image</code>. There's a bunch of services out there offering Docker image vulnerability scanning, so that's an option as well.</p>"},{"location":"integrations/docker.html#limit-push-access","title":"Limit push access","text":"<p>Your stack is only as safe as the runner image you're using for it. A malicious actor is able to doctor your runner image in a way that will allow them to take over your stack and all its associated cloud provider accounts in a snap. Please always review the code, and only allow <code>docker push</code> access to your most trusted associates.</p> <p>Info</p> <p>In our default public worker pool, we only support publicly available Docker images. If you need private Docker images, you can log in to any Docker registry from a worker in a private worker pool.</p>"},{"location":"integrations/webhooks.html","title":"Webhooks","text":"<p>Spacelift can be configured to send webhook notifications for various events to an HTTP endpoint of your choice.</p>"},{"location":"integrations/webhooks.html#setting-up-webhooks","title":"Setting up webhooks","text":"<p>Webhooks can be set up by Spacelift administrators. They can be easily created or modified in the <code>Webhooks</code> section.</p>"},{"location":"integrations/webhooks.html#navigate-to-the-webhooks-section","title":"Navigate to the webhooks section","text":""},{"location":"integrations/webhooks.html#fill-required-fields","title":"Fill required fields","text":"<p>Info</p> <p>The <code>Secret</code> parameter is optional and is used to validate the received payload. You can learn more about it in the validating payload section.</p>"},{"location":"integrations/webhooks.html#reference-webhooks-in-policy-rules","title":"Reference webhooks in policy rules","text":"<p>Webhook messages are delivered using the notification policy. When defining rules, the policy expects you to reference the webhook by its <code>ID</code> which you can copy from the webhook list view:</p> <p></p>"},{"location":"integrations/webhooks.html#exploring-deliveries","title":"Exploring deliveries","text":"<p>Webhook deliveries and their response statuses are stored and can be explored by selecting a specific webhook and viewing its details. You'll be presented with a list of deliveries, their status codes and when they happened. You can also click on each delivery to view more details about it:</p> <p></p>"},{"location":"integrations/webhooks.html#default-webhook-payloads","title":"Default webhook payloads","text":"<p>The following section documents the default webhook payloads sent for each event type. However, if required, webhook payloads can be customized via a notification policy.</p>"},{"location":"integrations/webhooks.html#run-events","title":"Run events","text":"<p>Here's an example of the default webhook payload for a notification about a finished tracked run:</p> <pre><code>{\n\"account\": \"spacelift-io\",\n\"state\": \"FINISHED\",\n\"stateVersion\": 4,\n\"timestamp\": 1596979684,\n\"run\": {\n\"id\": \"01EF9PFPNFFM2MQXTJKHK1B869\",\n\"branch\": \"master\",\n\"commit\": {\n\"authorLogin\": \"marcinwyszynski\",\n\"authorName\": \"Marcin Wyszynski\",\n\"hash\": \"0ee3a3b7266daf5a1d44a193a0f48ce995fa75eb\",\n\"message\": \"Update demo.tf\",\n\"timestamp\": 1596705932,\n\"url\": \"https://github.com/spacelift-io/demo/commit/0ee3a3b7266daf5a1d44a193a0f48ce995fa75eb\"\n},\n\"createdAt\": 1596979665,\n\"delta\": {\n\"added\": 0,\n\"changed\": 0,\n\"deleted\": 0,\n\"resources\": 1\n},\n\"triggeredBy\": \"marcinw@spacelift.io\",\n\"type\": \"TRACKED\"\n},\n\"stack\": {\n\"id\": \"spacelift-demo\",\n\"name\": \"Spacelift demo\",\n\"description\": \"\",\n\"labels\": []\n}\n}\n</code></pre> <p>The payload consists of a few fields:</p> <ul> <li><code>account</code> is the name (subdomain) of the account generating the webhook - useful in case you're pointing webhooks from various accounts at the same endpoint;</li> <li><code>state</code> is a string representation of the run state at the time of the notification being triggered;</li> <li><code>stateVersion</code> is the ordinal number of the state, which can be used to ensure that notifications that may be sent or received out-of-order are correctly processed;</li> <li><code>timestamp</code> is the unix timestamp of the state transition;</li> <li><code>run</code> contains information about the run, its associated commit and delta (if any);</li> <li><code>stack</code> contains some basic information about the parent Stack of the <code>run</code>;</li> </ul>"},{"location":"integrations/webhooks.html#internal-error-events","title":"Internal error events","text":"<pre><code>{\n\"title\": \"Invalid Stack Slug Triggered\",\n\"body\": \"policy tried to trigger Stack 'this-is-not-a-stack' which either doesn't exist or this policy doesn't have access to\",\n\"error\": \"policy triggered for Stack that doesn't exist\",\n\"severity\": \"ERROR\",\n\"account\": \"spacelift-io\"\n}\n</code></pre> <p>Internal errors will always have the same fields set and some of them will be static for an event:</p> <ul> <li><code>title</code> is the title (summary) of the error.</li> <li><code>body</code> is the is the full explanation of what went wrong.</li> <li><code>error</code> is a description of the error that happened.</li> <li><code>severity</code> can be one of three different constants: <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>.</li> <li><code>account</code> is the account for which the error happened.</li> </ul>"},{"location":"integrations/webhooks.html#validating-payload","title":"Validating payload","text":"<p>In order to validate the incoming payload, you will need to have the secret handy - the one you've generated yourself when creating or updating the webhook.</p> <p>Every webhook payload comes with two signature headers generated from the combination of the secret and payload. <code>X-Signature</code> header contains the SHA1 hash of the payload, while <code>X-Signature-256</code> contains the SHA256 hash. We're using the exact same mechanism as GitHub to generate signatures, please refer to GitHub docs for details.</p>"},{"location":"integrations/webhooks.html#attaching-webhooks-to-stacks","title":"Attaching webhooks to stacks","text":"<p>Warning</p> <p>We recommend that you use notification policies to route stack events to your webhooks. Stack webhook integrations are provided for backwards compatibility.</p> <p>Webhooks can be set up by Spacelift administrators on per-stack basis. In order to do that, navigate to the Integrations section of the stack settings view. From the list of available integrations, select the Add webhook option:</p> <p></p> <p>Info</p> <p>You can set up as many webhooks for a Stack as you need, though each one must have a unique endpoint.</p> <p>You will be presented with a simple setup form that requires you to provide and endpoint to which the payload is sent, and an optional secret that you can use to validate that the incoming requests are indeed coming from Spacelift:</p> <p></p> <p>Please note that it's up to you to come up with a reasonably non-obvious secret.</p> <p>Once saved, the webhook will appear on the list of integrations:</p> <p></p> <p>Info</p> <p>Unlike some other secrets in Spacelift, the webhook secret can be viewed by anyone with read access to the stack. If you suspect foul play, consider regenerating your secret.</p> <p>By default webhooks are enabled which means that they are triggered every time there's a run state change event on the Stack they are attached to. If you want to temporarily disable some of the endpoints, you can do that without having to delete the whole integration.</p> <p>To do that, just click on the Edit button on the desired webhook integration section:</p> <p></p> <p>...and click on the Enabled toggle to see it going gray:</p> <p></p> <p>Reversing this action is equally simple - just follow the same steps making sure that the toggle goes green:</p> <p></p>"},{"location":"integrations/chatops/msteams.html","title":"Microsoft Teams","text":"<p>Microsoft Teams is a Slack alternative and a part of the Microsoft Office 365 suite. It's a chat-based workspace where teams can organize and discuss their work. Many DevOps teams use it to communicate and collaborate on infrastructure and application deployments. Hence, Spacelift has a first-class integration with Microsoft Teams.</p> <p>The integration creates a webhook in Spacelift that will send notifications to a Microsoft Teams channel when:</p> <ul> <li>a tracked run needs confirmation;</li> <li>a tracked run or a task finishes;</li> <li>a module version succeeds or fails;</li> </ul> <p>Based on this configuration, the module will send notifications that look like these:</p> <p></p> <p></p>"},{"location":"integrations/chatops/msteams.html#prerequisites","title":"Prerequisites","text":"<p>In order to set up the integration, you'll to perform some manual steps in Microsoft Teams. The Spacelift end of the integration is handled programmatically, by a Terraform module.</p>"},{"location":"integrations/chatops/msteams.html#in-microsoft-teams","title":"In Microsoft Teams","text":"<p>In order to set up the integration, you'll need to create a Microsoft Teams webhook and copy its URL. You can do this by following these steps:</p> <ol> <li>Open the channel in which you want to receive notifications from Spacelift.</li> <li>Click the ellipsis (...) next to the channel name and select Connectors</li> <li>Search for Incoming Webhook and click Configure.</li> <li>Click Add to create the webhook.</li> <li>Copy the webhook URL, you'll need it in the next step.</li> </ol>"},{"location":"integrations/chatops/msteams.html#in-spacelift","title":"In Spacelift","text":"<p>The Teams integration is based on our notification policy feature, which requires at least an active Cloud tier subscription. While building a notification-based Teams integration from scratch is possible, we've created a Terraform module that will set up all the necessary integration elements for you.</p> <p>This module will only create Spacelift assets:</p> <ul> <li>a notification policy that will send data to Microsoft Teams;</li> <li>a webhook endpoint that serve as a notification target for the policy;</li> </ul>"},{"location":"integrations/chatops/msteams.html#monitoring-and-troubleshooting","title":"Monitoring and troubleshooting","text":"<p>Once the integration is set up, you can monitor the notifications in the <code>My channel</code> channel in Microsoft Teams. You can also monitor the notifications in the corresponding notification policy and its webhook endpoint in Spacelift.</p>"},{"location":"integrations/chatops/slack.html","title":"Slack","text":"<p>At Spacelift, we're using Slack for internal communication. And we know that other tech companies do the same, so we've created a first-class integration that we ourselves enjoy using.</p> <p>Here are examples of messages the Spacelift application sends to Slack;</p> <p></p> <p></p>"},{"location":"integrations/chatops/slack.html#linking-your-spacelift-account-to-the-slack-workspace","title":"Linking your Spacelift account to the Slack workspace","text":"<p>As a Spacelift and Slack admin, you can link your Spacelift account to the Slack workspace by going to the Slack section of the Settings screen.</p> <p>The integration is an OAuth2 exchange which installs Slack Spacelift app in your workspace.</p> <p>Once you install the Spacelift app, the account-level integration is finished and the Slack section of the Settings screen informs you that the two are talking to one another:</p> <p></p> <p>Installing the Slack app doesn't automatically cause Spacelift to flood your Slack channels with torrents of notifications. These are set up on a per-stack basis using Slack commands and the management uses the Slack interface.</p> <p>Though before that happens, you need to allow requests coming from Slack to access Spacelift stacks.</p>"},{"location":"integrations/chatops/slack.html#managing-access-to-stacks-with-policies","title":"Managing access to Stacks with policies","text":"<p>Our Slack integration allows users in the Slack workspace to interact with stacks by adding the ability to change their run state or view changes that are planned or were applied.</p> <p>Similar to regular requests to our HTTP APIs, requests and actions coming from Slack are subject to the policy-based access validation. If you haven't had a chance to review the policy and Spaces documentation yet, please do it now before proceeding any further - you're risking a chance of getting lost.</p>"},{"location":"integrations/chatops/slack.html#available-actions","title":"Available actions","text":"<p>Currently, we allow:</p> <ul> <li>Confirming and discarding tracked runs.</li> <li>Viewing planned and actual changes.</li> </ul> <p>Both of these actions require specific permissions to be configured using the login policy. Confirming or discarding runs requires Write level permissions while viewing changes requires Read level permissions. The documentation sections about policies below describe how to setup and manage these permissions.</p> <p>Info</p> <p>The default login policy decision for Slack requests is to deny all access.</p>"},{"location":"integrations/chatops/slack.html#login-policy","title":"Login policy","text":"<p>Using login policies is the preferred way to control access for the Slack integration. Using them you can control who can access stacks which are in a specific Space.</p> <p>They allow for granular space access control using the provided policy data such as slack workspace details, Slack team information and user which interacted with the message data. Using the Login policy you can define rules which would allow to have Read or Write level permissions for certain actions.</p> <p>Login policies also don't need to be attached to a specific stack in order to work but are instead evaluated during every stack mutation or read attempt from the integration.</p> <p>Warning</p> <p>It's important to know that if you have multiple login policies, failing to evaluate one of them or having at least one of them result in a deny decision after the evaluation is done, will result in the overall decision being a <code>deny all</code>.</p> <p>Here is an example of data which the login policy receives when evaluating stack access for the integration:</p> <pre><code>{\n\"request\": {\n\"timestamp_ns\": \"&lt;int&gt; - a unix timestamp for when this request happened\"\n},\n\"slack\": {\n\"channel\": {\n\"id\": \"&lt;string&gt; - a channel ID, example: C042YPN0000\",\n\"name\": \"&lt;string&gt; - a channel name, example: spc-finished\"\n},\n\"command\": \"&lt;string&gt;\",\n\"team\": {\n\"id\": \"&lt;string&gt; - a team ID for which this user belongs, example: T0431750000\",\n\"name\": \"&lt;string&gt; - a team name represented as string, example: slack-workspace-name\"\n},\n\"user\": {\n\"deleted\": \"&lt;boolean&gt;\",\n\"display_name\": \"&lt;string&gt;\",\n\"enterprise\": {\n\"enterprise_id\": \"&lt;string&gt;\",\n\"enterprise_name\": \"&lt;string&gt;\",\n\"id\": \"&lt;string&gt;\",\n\"is_admin\": \"&lt;boolean&gt;\",\n\"is_owner\": \"&lt;boolean&gt;\",\n},\n\"teams\": {\n\"id\": \"&lt;string&gt;\",\n\"name\": \"&lt;string&gt;\"\n},\n\"id\": \"&lt;string&gt; - a user which initially request ID, example: C042YPN1111\",\n\"is_admin\": \"&lt;boolean&gt; - is the user an admin\",\n\"is_owner\": \"&lt;boolean&gt; - is the workspace owner\",\n\"is_primary_owner\": \"&lt;boolean&gt;\",\n\"is_restricted\": \"&lt;boolean&gt;\",\n\"is_stranger\": \"&lt;boolean&gt;\",\n\"is_ultra_restricted\": \"&lt;boolean&gt;\",\n\"has_2fa\": \"&lt;boolean&gt;\"\n\"real_name\": \"&lt;string&gt;\",\n\"tz\": \"&lt;string&gt;\"\n}\n},\n\"spaces\": [{\n\"id\": \"&lt;string&gt; - an ID for a Space in spacelift\",\n\"labels\": \"&lt;stringArray&gt; - a list of labels attached to this space\",\n\"name\": \"&lt;string&gt; - name for a Space in spacelift\"\n}, {\n\"id\": \"&lt;string&gt;\",\n\"labels\": \"&lt;stringArray&gt;\",\n\"name\": \"&lt;string&gt;\"\n}]\n}\n</code></pre> <p>Info</p> <p>The <code>slack</code> object in the policy input data is built using Slack provided data. See their official documentation for always up-to-date and full explanation of the <code>slack</code> object fields.</p> <p>Using the above data we can write policies which only allow for a specific user or slack team to access specific spaces in which your stacks reside.</p> <p>For example here is a policy which would allow anyone from a specific slack team to alter stacks in a particular space:</p> <pre><code>package spacelift\n\n# Allow access for anyone in team X\nallow {\n  input.slack.team.id == \"X\"\n}\n\n# Deny access for everyone except team X\ndeny {\n  input.slack.team.id != \"\"\n  input.slack.team.id != \"X\"\n}\n\n# Grant write access to stacks in Space Y for anyone in team X\nspace_write[\"Y\"] {\n  input.slack.team.id == \"X\"\n}\n</code></pre>"},{"location":"integrations/chatops/slack.html#available-slash-commands","title":"Available slash commands","text":"<p>Warning</p> <p>It's recommended to instead use the notification policy in order to manage slack messages received from Spacelift. These slash commands are deprecated.</p> <p>Also, please note that slash commands only work if your Spacelift instance is publicly accessible by Slack. If your Spacelift installation uses an internal load balancer, for example, slash commands will not work.</p> <p>Three slash commands are currently available:</p> <ul> <li><code>/spacelift subscribe $stackId</code> - subscribes a particular Slack channel to run state changes for a given Stack - requires ;</li> <li><code>/spacelift unsubscribe $stackId</code> - unsubscribes a particular Slack channel from run state changes for a given Stack;</li> <li><code>/spacelift trigger $stackId</code> - triggers a tracked run for the specified Stack;</li> </ul>"},{"location":"integrations/cloud-providers/index.html","title":"Cloud Integrations","text":"<p>Cloud integrations allow Spacelift to manage your resources without the need for long-lived static credentials. When using infrastructure-as-code automation tools such as Terraform, AWS CloudFormation, or Pulumi, these tools typically require credentials to execute. Usually, these are very powerful credentials, administrative credentials, sometimes. And these can do a lot of damage. Typically, you'd provide those credentials statically - think AWS credentials, GCP service keys, etc. This is dangerous, and against security best practices.</p> <p>That's why Spacelift integrates with identity management systems from major cloud providers to dynamically generate short-lived access tokens that can be used to configure their corresponding Terraform providers.</p> <p>Currently AWS is natively supported. A generic OpenID Connect integration is also available to work with any compatible service provider.</p> <p>Hint</p> <p>This feature is designed for clients using the shared public worker pool. When hosting Spacelift workers on your infrastructure you can use your cloud providers' ambient credentials (eg. EC2 instance role or EKS worker role on AWS).</p>"},{"location":"integrations/cloud-providers/aws.html","title":"Amazon Web Services (AWS)","text":""},{"location":"integrations/cloud-providers/aws.html#lets-explain","title":"Let's Explain","text":"<p>The AWS integration allows either Spacelift runs or tasks to automatically assume an IAM role in your AWS account, and in the process, generate a set of temporary credentials. These credentials are then exposed as computed environment variables during the run/task that takes place on the particular Spacelift stack that the integration is attached to.</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_SECURITY_TOKEN</code></li> <li><code>AWS_SESSION_TOKEN</code></li> </ul> <p>This is enough for both the AWS Terraform provider and/or Amazon S3 state backend to generate a fully authenticated AWS session without further configuration. However, you will likely need to select one of the available regions with the former.</p>"},{"location":"integrations/cloud-providers/aws.html#usage","title":"Usage","text":"<p>To utilize the AWS integration, you need to set up at least one cloud integration, and then attach that integration to any stacks that need it. Please follow the Setup Guide for more information on this process.</p>"},{"location":"integrations/cloud-providers/aws.html#trust-policy","title":"Trust Policy","text":"<p>When setting up a Spacelift AWS Cloud Integration you need to specify the ARN of an IAM Role to use. The Trust Policy for this role must be configured to allow Spacelift to assume the role and generate temporary credentials.</p> <p>When completing the role assumption, Spacelift will pass extra information in the <code>ExternalId</code> attribute, allowing you to optionally add additional layers of security to your role.</p> <p>External ID Format: <code>&lt;spacelift-account-name&gt;@&lt;integration-id&gt;@&lt;stack-slug&gt;@&lt;read|write&gt;</code></p> <ul> <li><code>&lt;spacelift-account-name&gt;</code>: the name of the Spacelift account that initiated the role assumption.</li> <li><code>&lt;integration-id&gt;</code>: the ID of the AWS Cloud Integration that initiated the role assumption.</li> <li><code>&lt;stack-slug&gt;</code>: the slug of the stack that the AWS Cloud Integration is attached to, that initiated the role assumption.</li> <li><code>&lt;read|write&gt;</code>: set to either <code>read</code> or <code>write</code> based upon the event occurring that has initiated the role assumption. The Planning phase utilizes <code>read</code> while the Applying phase utilizes <code>write</code>.</li> </ul>"},{"location":"integrations/cloud-providers/aws.html#setup-guide","title":"Setup Guide","text":"<p>Prerequisites:</p> <ul> <li>The ability to create IAM Roles in your AWS account.</li> <li>Admin access to your Spacelift account.</li> </ul>"},{"location":"integrations/cloud-providers/aws.html#setup-a-role-in-aws","title":"Setup a Role in AWS","text":"<p>Before creating the Spacelift AWS integration, you need to have an AWS IAM Role within your AWS account that the cloud integration will use.</p> <p>Within your AWS account, navigate to AWS IAM and click the Create role button.</p> <p></p> <p>Hint</p> <p>To allow this integration to access multiple AWS accounts, you can extend this role to have cross-account permissions to the target accounts. See AWS documentation for more details.</p>"},{"location":"integrations/cloud-providers/aws.html#configure-trust-policy","title":"Configure Trust Policy","text":"<p>Next, we want to configure the Trust Policy for the role to allow Spacelift to assume the role.</p> <p>Here's an example trust policy statement you can use, that allows any stack within your Spacelift account to use this IAM Role:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Action\": \"sts:AssumeRole\",\n\"Condition\": {\n\"StringLike\": {\n\"sts:ExternalId\": \"yourSpaceliftAccountName@*\"\n}\n},\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"324880187172\"\n}\n}\n]\n}\n</code></pre> <p>Info</p> <p>Be sure to replace yourSpaceliftAccountName in the example above with your actual Spacelift account name.</p> <p></p>"},{"location":"integrations/cloud-providers/aws.html#optionally-configure-further-constraints-on-the-trust-policy","title":"Optionally Configure Further Constraints on the Trust Policy","text":"<p>Info</p> <p>By default, Spacelift passes the <code>ExternalId</code> value in this format: <code>&lt;spacelift-account-name&gt;@&lt;integration-id&gt;@&lt;stack-slug&gt;@&lt;read|write&gt;</code></p> <p>Knowing the format of the External ID passed by Spacelift, you can further secure your IAM Role trust policies if you desire a deeper level of granular security.</p> <p>For example, you may wish to lock down an IAM Role so that it can only be used by a specific stack. The following example shows how to lock down an IAM Role so that it can only be assumed by the stack <code>stack-a</code> in a Spacelift account called <code>example</code>:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Action\": \"sts:AssumeRole\",\n\"Condition\": {\n\"StringLike\": {\n\"sts:ExternalId\": \"example@*@stack-a@*\"\n}\n},\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"324880187172\"\n}\n}\n]\n}\n</code></pre>"},{"location":"integrations/cloud-providers/aws.html#configure-role-permissions","title":"Configure Role Permissions","text":"<p>Next, you need to attach at least one IAM Policy to your IAM Role to provide it with sufficient permissions to deploy any resources that your IaC code defines.</p> <p>Info</p> <p>For Terraform users that are managing their own state file, don't forget to give your role sufficient permissions to access your state (Terraform documents the permissions required for S3-managed state here, and for DynamoDB state locking here).</p>"},{"location":"integrations/cloud-providers/aws.html#create-iam-role","title":"Create IAM Role","text":"<p>Once you have your IAM Role's trust policy and IAM Policies configured, you can finish creating the role. Take a note of the IAM Role ARN, as you'll need this when setting up the integration in Spacelift in the next section.</p> <p></p>"},{"location":"integrations/cloud-providers/aws.html#navigate-to-cloud-integrations","title":"Navigate to Cloud Integrations","text":"<p>Now that you have an IAM Role created, navigate to the Cloud Integration page from the Spacelift navigation sidebar.</p> <p></p>"},{"location":"integrations/cloud-providers/aws.html#create-an-integration","title":"Create an Integration","text":"<p>Click the add your first integration button to begin the creation of your first integration.</p> <p></p> <p>When creating an integration, you will immediately notice that you need to specify two required fields: Name and Role ARN. Give the integration a name of your choosing, and paste in the ARN of the IAM Role that you just created.</p> <p>If you enable the assume role on worker option, the role assumption will be performed on your private worker rather than at Spacelift's end. When role assumption on the worker is enabled, you can also optionally specify a custom External ID to use during role assumption.</p> <p>Info</p> <p>When creating your role in AWS, you need to ensure the role has a trust policy that allows Spacelift to assume the role to generate temporary credentials for runs. Assuming you are following this guide, you should have configured this in the previous section.</p>"},{"location":"integrations/cloud-providers/aws.html#using-the-integration","title":"Using the Integration","text":"<p>Now that the integration has been created, you need to attach it to one or more stacks. To do this, navigate to a stack that you want to attach your integration to:</p> <p></p> <p>Next, go to the stack's settings:</p> <p></p> <p>Choose the integrations tab:</p> <p></p> <p>Select the AWS option from the drop down, choose your integration, and select whether it should be used for read, write or both read and write phases:</p> <p></p> <p>Info</p> <p>Once you have chosen your integration and specified whether it will be used for read or write phases, an example trust relationship statement will be displayed. This shows an example of how to configure your role for use by this exact stack, and based on whether the integration is being attached for read or write phases. This policy statement is provided for convenience only, and you can safely ignore it if you have already configured your trust relationship for your role.</p>"},{"location":"integrations/cloud-providers/aws.html#read-vs-write","title":"Read vs Write","text":"<p>You can attach an AWS integration as read, write or read-write, and you can attach at most two integrations to any single stack. Read indicates that this integration will be used during read phases of runs (for example, plans), and Write indicates that this integration will be used during write phases of runs (for example, applies).</p>"},{"location":"integrations/cloud-providers/aws.html#role-assumption-verification","title":"Role Assumption Verification","text":"<p>If the Cloud Integration has the \"Assume Role on Worker\" setting disabled, Spacelift will verify the role assumption as soon as you click the attach button. If role assumption succeeds, it will try to assume the role without the unique external ID, and this time it expects to fail. If Spacelift fails the latter check, we consider the integration is safely configured.</p> <p>Success</p> <p>This somewhat counterintuitive extra check is to prevent against malicious takeover of your account by someone who happens to know your AWS account ID, which isn't all that secret, really. The security vulnerability we're addressing here is known as the confused deputy problem.</p>"},{"location":"integrations/cloud-providers/aws.html#programmatic-setup","title":"Programmatic Setup","text":"<p>You can also use the Spacelift Terraform provider in order to create an AWS Cloud integration from an administrative stack, including the trust relationship. Note that in order to do that, your administrative stack will require AWS credentials itself, and ones powerful enough to be able to deal with IAM.</p> <p>Here's a little example of what that might look like to create a Cloud Integration programmatically:</p> <pre><code>data \"aws_caller_identity\" \"current\" {}\n\nlocals {\nrole_name = \"example-role\"\nrole_arn  = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\"\n}\n\nresource \"spacelift_stack\" \"this\" {\nname         = \"Example Stack\"\nrepository   = \"your-awesome-repo\"\nbranch       = \"main\"\n}\n\nresource \"spacelift_aws_integration\" \"this\" {\nname = local.role_name\n\n  # We need to set this manually rather than referencing the role to avoid a circular dependency\nrole_arn                       = local.role_arn\ngenerate_credentials_in_worker = false\n}\n\n# The spacelift_aws_integration_attachment_external_id data source is used to help generate a trust policy for the integration\ndata \"spacelift_aws_integration_attachment_external_id\" \"this\" {\nintegration_id = spacelift_aws_integration.this.id\nstack_id       = spacelift_stack.this.id\nread           = true\nwrite          = true\n}\n\nresource \"aws_iam_role\" \"this\" {\nname = local.role_name\n\nassume_role_policy = jsonencode({\nVersion = \"2012-10-17\"\nStatement = [\njsondecode(data.spacelift_aws_integration_attachment_external_id.this.assume_role_policy_statement),\n]\n})\n}\n\nresource \"aws_iam_role_policy_attachment\" \"this\" {\npolicy_arn = \"arn:aws:iam::aws:policy/PowerUserAccess\"\nrole       = aws_iam_role.this.name\n}\n\nresource \"spacelift_aws_integration_attachment\" \"this\" {\nintegration_id = spacelift_aws_integration.this.id\nstack_id       = spacelift_stack.this.id\nread           = true\nwrite          = true\n\n  # The role needs to exist before we attach since we test role assumption during attachment.\ndepends_on = [\naws_iam_role.this\n]\n}\n</code></pre> <p>Info</p> <p>Please always refer to the provider documentation for the most up-to-date documentation.</p>"},{"location":"integrations/cloud-providers/aws.html#attaching-a-role-to-multiple-stacks","title":"Attaching a Role to Multiple Stacks","text":"<p>The previous example explained how to use the <code>spacelift_aws_integration_attachment_external_id</code> data-source to generate the assume role policy for using the integration with a single stack, but what if you want to attach the integration to multiple stacks? The simplest option would be to create multiple instances of the data-source - one for each stack - but you can also use a Terraform <code>for_each</code> condition to reduce the amount of code required:</p> <pre><code>locals {\nrole_name        = \"multi-stack-integration\"\nrole_arn         = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\"\n\n  # Define the stacks we want to attach the integration to\nstacks_to_attach = [\"stack-1\", \"stack-2\", \"stack-3\"]\n}\n\ndata \"aws_caller_identity\" \"current\" {}\ndata \"spacelift_account\" \"current\" {}\n\nresource \"spacelift_aws_integration\" \"integration\" {\nname = local.role_name\nrole_arn                       = local.role_arn\ngenerate_credentials_in_worker = false\n}\n\n# Generate the External IDs required for creating our AssumeRole policy\ndata \"spacelift_aws_integration_attachment_external_id\" \"integration\" {\nfor_each = toset(local.stacks_to_attach)\n\nintegration_id = spacelift_aws_integration.integration.id\nstack_id       = each.key\nread           = true\nwrite          = true\n}\n\nresource \"aws_iam_role\" \"role\" {\nname = local.role_name\n\nassume_role_policy = jsonencode({\nVersion = \"2012-10-17\"\nStatement = [\n{\nEffect = \"Allow\",\n\"Principal\" = {\n\"AWS\" : data.spacelift_account.current.aws_account_id\n},\n\"Action\" = \"sts:AssumeRole\",\n\"Condition\" = {\n\"StringEquals\" = {\n            # Allow the external ID for any of the stacks to assume our role\n\"sts:ExternalId\" = [for i in values(data.spacelift_aws_integration_attachment_external_id.integration) : i.external_id],\n}\n}\n}\n],\n})\n}\n</code></pre> <p>This will generate a trust relationship that looks something like this:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"arn:aws:iam::324880187172:root\"\n},\n\"Action\": \"sts:AssumeRole\",\n\"Condition\": {\n\"StringEquals\": {\n\"sts:ExternalId\": [\n\"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-1@write\",\n\"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-2@write\",\n\"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-3@write\"\n]\n}\n}\n}\n]\n}\n</code></pre>"},{"location":"integrations/cloud-providers/aws.html#is-it-safe","title":"Is it safe?","text":"<p>Assuming roles and generating credentials on the private worker is perfectly safe. Those credentials are never leaked to us in any shape or form. Hence, the rest of this section discusses the trust relationship established between the Spacelift account and your AWS account for the purpose of dynamically generating short-lived credentials. So, how safe is that?</p> <p>Probably safer than storing static credentials in your stack environment. Unlike user keys that you'd normally have to use, role credentials are dynamically created and short-lived. We use the default expiration which is 1 hour, and do not store them anywhere. Leaking them accidentally through the logs is not an option either because we mask AWS credentials.</p> <p>The most tangible safety feature of the AWS integration is the breadcrumb trail it leaves in CloudTrail. Every resource change can be mapped to an individual Terraform run or task whose ID automatically becomes the username as the <code>sts:AssumeRole</code> API call with that ID as <code>RoleSessionName</code>. In conjunction with AWS tools like Config, it can be a very powerful compliance tool.</p> <p>Let's have a look at a CloudTrail event showing an IAM role being created by what seems to be a Spacelift run:</p> <p></p> <p><code>01DSJ63P40BAZY4VW8BXXG7M4K</code> is indeed a run ID we can then trace back even further:</p> <p></p>"},{"location":"integrations/cloud-providers/aws.html#roles-assuming-other-roles","title":"Roles assuming other roles","text":"<p>OK, we get it. Using everyone's favorite Inception meme:</p> <p></p> <p>Indeed, the AWS Terraform provider allows you to assume an IAM role during setup, effectively doing the same thing over again. This approach is especially useful if you want to control resources in multiple AWS accounts from a single Spacelift stack. This is totally fine - in IAM, roles can assume other roles, though what you need to do on your end is set up the trust relationship between the role you have Spacelift assume and the role for each provider instance to assume. But let's face it - at this level of sophistication, you sure know what you're doing.</p> <p>One bit you might not want to miss though, is the guaranteed ability to map the change to a particular run or task that we described in the previous section. One way of fixing that would be to use the <code>TF_VAR_spacelift_run_id</code> computed environment variable available to each Spacelift workflow. Conveniently, it's already a Terraform variable, so a setup like this should do the trick:</p> <pre><code>variable \"spacelift_run_id\" {}\n\n# That's our default provider with credentials generated by Spacelift.\nprovider \"aws\" {}\n\n# That's where Terraform needs to run sts:AssumeRole with your\n# Spacelift-generated credentials to obtain ones for the second account.\nprovider \"aws\" {\nalias = \"second-account\"\n\nassume_role {\nrole_arn     = \"&lt;up-to-you&gt;\"\nsession_name = var.spacelift_run_id\nexternal_id  = \"&lt;up-to-you&gt;\"\n}\n}\n</code></pre>"},{"location":"integrations/cloud-providers/aws.html#migrating-from-legacy-integrations","title":"Migrating from \"Legacy\" integrations","text":"<p>Originally, when our AWS integration was created it did not support account-level integrations. Instead, each integration was created separately on a per-stack basis. This section provides instructions on how to migrate to the new account-level integrations, with different sets of instructions provided depending on whether you manage your integrations via the UI, or via the Spacelift Terraform Provider.</p>"},{"location":"integrations/cloud-providers/aws.html#via-the-ui","title":"Via the UI","text":"<p>Once the new Cloud Integrations UI is enabled for your account, you will still be able to view your legacy integrations in the \u201cIntegrations\u201d section of your stacks, but it will look slightly different:</p> <p></p> <p>The first thing to do is to copy your Role ARN, then head over to the Cloud Integrations section. Once there, click on the \u201cAdd your first integration\u201d button to get started:</p> <p></p> <p>Give your role a name, and paste in your existing ARN:</p> <p></p> <p>The role name is completely up to you and is just used for managing the roles in your account. Maybe it\u2019s a role that allows management of S3 accounts, like in the example above, or maybe you have a single role that can manage your entire pre-production environment, in which case you might name it \u201cPreprod Writer\u201d.</p> <p>Now that you\u2019ve created an AWS integration, head back to your stack integration settings and click on the \u201cDelete\u201d button next to your legacy integration:</p> <p></p> <p>Now choose the \u201cAWS\u201d option from the \u201cSelect integration to set up\u201d drop down, select the AWS integration you just created, and check the read and write checkboxes:</p> <p></p> <p>You\u2019ll notice that we display an example trust relationship on this screen. You may or may not need to adjust the Trust Relationship of your role at this point. If you already use a <code>StringLike</code> condition with an <code>&lt;account-name&gt;@*</code> wildcard you shouldn\u2019t need to change anything.</p> <p>Click on the Attach button to attach your integration to your stack:</p> <p></p> <p>Congrats - you\u2019ve now successfully migrated!</p>"},{"location":"integrations/cloud-providers/aws.html#via-the-terraform-provider","title":"Via the Terraform Provider","text":"<p>Migrating via the Terraform Provider is also very simple. The Terraform Provider now contains two new resources, along with an additional data source that you can use:</p> <ul> <li>spacelift_aws_integration - creates an account level integration.</li> <li>spacelift_aws_integration_attachment - attaches an integration to a stack.</li> <li>spacelift_aws_integration_attachment_external_id - a data source that can help you generate the correct trust relationship for attaching an integration to a stack.</li> </ul>"},{"location":"integrations/cloud-providers/aws.html#starting-configuration","title":"Starting Configuration","text":"<p>To begin with, let\u2019s assume you have the following Terraform configuration that creates an IAM role, a Stack, and connects the role to the stack:</p> <pre><code>resource \"spacelift_stack\" \"this\" {\nname         = \"AWS Integration Mig Test\"\nrepository   = \"spacelift-local\"\nbranch       = \"main\"\nproject_root = \"stacks/aws-integration-testing\"\n}\n\nresource \"aws_iam_role\" \"this\" {\nname = \"adamc-v1-to-v2-migration-role\"\n\nassume_role_policy = jsonencode({\nVersion   = \"2012-10-17\"\nStatement = [jsondecode(spacelift_stack.this.aws_assume_role_policy_statement)]\n})\n}\n\nresource \"aws_iam_role_policy\" \"this\" {\nname = aws_iam_role.this.name\nrole = aws_iam_role.this.id\n\npolicy = jsonencode({\n    # Excluded for brevity })\n}\n\nresource \"spacelift_aws_role\" \"this\" {\nstack_id = spacelift_stack.this.id\nrole_arn = aws_iam_role.this.arn\n}\n</code></pre>"},{"location":"integrations/cloud-providers/aws.html#migration","title":"Migration","text":"<p>To migrate to the new integration format, we need to take the following steps:</p> <ol> <li>Add a new AWS integration.</li> <li>(optional) Generate an AssumeRole policy for attaching it to our stack.</li> <li>(optional) Update the <code>assume_role_policy</code> on our <code>aws_iam_role</code> resource.</li> <li>Remove the <code>spacelift_aws_role</code> resource.</li> <li>Attach your new integration to your stack via the <code>spacelift_aws_integration_attachment</code> resource.</li> </ol> <p>Steps 2 and 3 are completely optional, and may not be required if you are using wildcard matching for the external ID, for example.</p>"},{"location":"integrations/cloud-providers/aws.html#result","title":"Result","text":"<p>The following shows the finished result of the migrated Terraform:</p> <pre><code>data \"aws_caller_identity\" \"current\" {}\n\n# In this example, we're using some locals to calculate the role ARN. This is to avoid a circular\n# dependency between the aws_iam_role resource and the spacelift_aws_integration resource.\nlocals {\nrole_name = \"adamc-v1-to-v2-migration-role\"\nrole_arn  = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\"\n}\n\nresource \"spacelift_stack\" \"this\" {\nname         = \"AWS Integration Mig Test\"\nrepository   = \"spacelift-local\"\nbranch       = \"main\"\nproject_root = \"stacks/aws-integration-testing\"\n}\n\n# 1. Add the new integration.\nresource \"spacelift_aws_integration\" \"this\" {\nname                           = local.role_name\nrole_arn                       = local.role_arn\ngenerate_credentials_in_worker = false\n}\n\n# 2. Generate the AssumeRole policy for attaching it to our stack.\ndata \"spacelift_aws_integration_attachment_external_id\" \"this\" {\nintegration_id = spacelift_aws_integration.this.id\nstack_id       = spacelift_stack.this.id\nread           = true\nwrite          = true\n}\n\nresource \"aws_iam_role\" \"this\" {\nname = local.role_name\n\nassume_role_policy = jsonencode({\nVersion = \"2012-10-17\"\nStatement = [\n     # 3. Remove the old assume role policy statement, and add our new one.\n     # jsondecode(spacelift_stack.this.aws_assume_role_policy_statement),\njsondecode(data.spacelift_aws_integration_attachment_external_id.this.assume_role_policy_statement),\n]\n})\n}\n\nresource \"aws_iam_role_policy\" \"this\" {\nname = aws_iam_role.this.name\nrole = aws_iam_role.this.id\n\npolicy = jsonencode({\n   # Excluded for brevity\n})\n}\n\n# 4. Remove the old spacelift_aws_role resource\n# resource \"spacelift_aws_role\" \"this\" {\n#   stack_id = spacelift_stack.this.id\n#   role_arn = aws_iam_role.this.arn\n# }\n\n# 5. Add the new integration attachment\nresource \"spacelift_aws_integration_attachment\" \"this\" {\nintegration_id = spacelift_aws_integration.this.id\nstack_id       = spacelift_stack.this.id\nread           = true\nwrite          = true\n\n # The role needs to exist before we attach since we test role assumption during attachment.\ndepends_on = [\naws_iam_role.this\n]\n}\n</code></pre>"},{"location":"integrations/cloud-providers/oidc/index.html","title":"OpenID Connect (OIDC)","text":"<p>Hint</p> <p>For this feature to work, the service you are integrating with needs to be able to access your Spacelift instance. For example, if you have deployed your Spacelift instance with an internal rather than public-facing load balancer, you will not be able to use OIDC Federation for AWS role assumption.</p> <p>OpenID Connect is a federated identity technology that allows you to exchange short-lived Spacelift credentials for temporary credentials valid for external service providers like AWS, GCP, Azure, HashiCorp Vault etc. This allows you to use Spacelift to manage your infrastructure on these cloud providers without the need of using static credentials.</p> <p>OIDC is also an attractive alternative to our native AWS integration in that it implements a common protocol, requires no additional configuration on the Spacelift side, supports a wider range of external service providers and empowers the user to construct more sophisticated access policies based on JWT claims.</p> <p>It is not the purpose of this document to explain the details of the OpenID Connect protocol. If you are not familiar with it, we recommend you read the OpenID Connect specification or GitHub's excellent introduction to security hardening with OpenID Connect.</p>"},{"location":"integrations/cloud-providers/oidc/index.html#about-the-spacelift-oidc-token","title":"About the Spacelift OIDC token","text":"<p>The Spacelift OIDC token is a JSON Web Token that is signed by Spacelift and contains a set of claims that can be used to construct a set of temporary credentials for the external service provider. The token is valid for an hour and is available to every run in any paid Spacelift account. The token is available in the <code>SPACELIFT_OIDC_TOKEN</code> environment variable and in the <code>/mnt/workspace/spacelift.oidc</code> file.</p>"},{"location":"integrations/cloud-providers/oidc/index.html#standard-claims","title":"Standard claims","text":"<p>The token contains the following standard claims:</p> <ul> <li><code>iss</code> - the issuer of the token - the URL of your Spacelift account, for example <code>https://demo.app.spacelift.io</code>. This is unique for each Spacelift account;</li> <li><code>sub</code> - the subject of the token - some information about the Spacelift run that generated this token. The subject claim is constructed as follows: <code>space:&lt;space_id&gt;:(stack|module):&lt;stack_id|module_id&gt;:run_type:&lt;run_type&gt;:scope:&lt;read|write&gt;</code>. For example, <code>space:legacy:stack:infra:run_type:TRACKED:scope:write</code>. Individual values are also available as separate custom claims - see below;</li> <li><code>aud</code> - the audience of the token - the hostname of your Spacelift account. For example, <code>demo.app.spacelift.io</code>. This is unique for each Spacelift account;</li> <li><code>exp</code> - the expiration time of the token - the time at which the token will expire, in seconds since the Unix epoch. The token is valid for one hour;</li> <li><code>iat</code> - the time at which the token was issued, in seconds since the Unix epoch;</li> <li><code>jti</code> - the unique identifier of the token;</li> <li><code>nbf</code> - the time before which the token is not valid, in seconds since the Unix epoch. This is always set to the same value as <code>iat</code>;</li> </ul>"},{"location":"integrations/cloud-providers/oidc/index.html#custom-claims","title":"Custom claims","text":"<p>The token also contains the following custom claims:</p> <ul> <li><code>spaceId</code> - the ID of the space in which the run that owns the token was executed;</li> <li><code>callerType</code> - the type of the caller, ie. the entity that owns the run - either <code>stack</code> or <code>module</code>;</li> <li><code>callerId</code> - the ID of the caller, ie. the stack or module that generated the run;</li> <li><code>runType</code> - the type of the run (<code>PROPOSED</code>, <code>TRACKED</code>, <code>TASK</code>, <code>TESTING</code> or <code>DESTROY</code>;</li> <li><code>runId</code> - the ID of the run that owns the token;</li> <li><code>scope</code> - the scope of the token - either <code>read</code> or <code>write</code>.</li> </ul>"},{"location":"integrations/cloud-providers/oidc/index.html#about-scopes","title":"About scopes","text":"<p>Whether the token is given <code>read</code> or <code>write</code> scope depends on the type of the run that generated the token. Proposed runs get a <code>read</code> scope, while tracked, testing and destroy runs as well as tasks get a <code>write</code> scope. The only exception to that rule are tracked runs whose stack is not set to autodeploy. In that case, the token will have a <code>read</code> scope during the planning phase, and a <code>write</code> scope during the apply phase. This is because we know in advance that the tracked run requiring a manual approval should not perform write operations before human confirmation.</p> <p>Note that the scope claim, as well as other claims presented by the Spacelift token are merely advisory. It depends on you whether you want to control access to your external service provider based on the scope of the token or on some other claim like space, caller or run type. In other words, Spacelift just gives you the data and it's up to you to decide whether and how to use it.</p>"},{"location":"integrations/cloud-providers/oidc/index.html#using-the-spacelift-oidc-token","title":"Using the Spacelift OIDC token","text":"<p>You can follow guidelines under this section to see how to use the Spacelift OIDC token to authenticate with AWS, GCP, Azure, and HashiCorp Vault. In particular, we will focus on setting up the integration and using it from these services' respective Terraform providers</p>"},{"location":"integrations/cloud-providers/oidc/aws-oidc.html","title":"Amazon Web Services (AWS)","text":"<p>Warning</p> <p> While the Terraform AWS provider supports authenticating with OIDC, the AWS S3 state backend does not support it yet.</p> <p>If you need to use the AWS S3 state backend, you can use the following workaround:</p> <ul> <li>Add the following command as a <code>before_init</code> hook (make sure to replace <code>&lt;ROLE ARN&gt;</code> with your IAM role ARN)</li> </ul> <pre><code>export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" $(aws sts assume-role-with-web-identity --web-identity-token \"$(cat /mnt/workspace/spacelift.oidc)\" --role-arn &lt;ROLE ARN&gt; --role-session-name spacelift-run-${TF_VAR_spacelift_run_id} --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" --output text))\n</code></pre> <ul> <li>Comment out the <code>role_arn</code> argument in the <code>backend</code> block</li> <li>Comment out the <code>assume_role_with_web_identity</code> section in the AWS provider block</li> </ul> <p>Alternatively, you can use the dedicated AWS Cloud Integration that uses AWS STS to obtain temporary credentials.</p>"},{"location":"integrations/cloud-providers/oidc/aws-oidc.html#configuring-spacelift-as-an-identity-provider","title":"Configuring Spacelift as an Identity Provider","text":"<p>In order to be able to do that, you will need to set up Spacelift as a valid identity provider for your AWS account. This is done by creating an OpenID Connect identity provider . You can do it declaratively using any of the IaC providers, programmatically using the AWS CLI or simply use the console. For illustrative purposes, we will use the console:</p> <ol> <li>Go to the AWS console and select the IAM service;</li> <li>Click on the \"Identity providers\" link in the left-hand menu;</li> <li>Click on the \"Add provider\" button in the top bar </li> <li>Select \"OpenID Connect\" as the provider type </li> <li>Make sure to get the host thumbprint by clicking the \"Get thumbprint\" button. This is required by AWS and protects you from a certain class of MitM attacks.</li> </ol> <p>Hint</p> <p>You will need to add iss to Proviver URL and you will need to add aud to Audience. You will need to replace <code>demo.app.spacelift.io</code> with the hostname of your Spacelift account.</p> <p>Once created, the identity provider will be listed in the \"Identity providers\" table. You can click on the provider name to see the details. From here, you will also be able to assign an IAM role to this new identity provider:</p> <p></p> <p>A dialog will pop up, asking you to select whether you want to create a new role or use an existing one. Let's create a brand new role. The most important thing for us is to select the right trusted entity - the new Spacelift OIDC provider. Make sure you select the audience from the dropdown - there should be just one option to choose from:</p> <p></p> <p>The rest of the process is the same as for any other role creation. You will be asked to select the policies that you want to attach to the role. You can also add tags and a description. Once you're done, click the \"Create role\" button.</p> <p>If you go to your new role's details page, in the Trust relationships section you will notice that it is now associated with the Spacelift OIDC provider:</p> <p></p> <p>This trust relationship is very relaxed and will allow any stack or module in the <code>demo</code> Spacelift account to assume this role. If you want to be more restrictive, you will want to add more conditions. For example, we can restrict the role to be only assumable by stacks in the <code>production</code> space by adding the following condition:</p> <pre><code>\"StringLike\": {\n\"demo.app.spacelift.io:sub\": \"space:production:*\"\n}\n</code></pre> <p>Hint</p> <p>You will need to replace <code>demo.app.spacelift.io</code> with the hostname of your Spacelift account.</p> <p>You can also restrict the role to be assumable only by a specific stack by matching on the stack ID:</p> <pre><code>\"StringLike\": {\n\"demo.app.spacelift.io:sub\": \"*:stack:oidc-is-awesome:*\"\n}\n</code></pre> <p>You can mix and match these to get the exact constraints you need. It is not the purpose of this guide to go into the intricacies of AWS IAM conditions - you can learn all about these in the official doc. One important thing to remember though is that AWS does not seem to support custom claims so you will need to use the standard ones to do the matching - primarily <code>sub</code>, as shown above.</p>"},{"location":"integrations/cloud-providers/oidc/aws-oidc.html#configuring-the-terraform-provider","title":"Configuring the Terraform Provider","text":"<p>Once the Spacelift-AWS OIDC integration is set up, the provider can be configured without the need for any static credentials. The <code>aws_role_arn</code> variable should be set to the ARN of the role that you want to assume:</p> <pre><code>provider \"aws\" {\nassume_role_with_web_identity {\nrole_arn = var.aws_role_arn\nweb_identity_token_file = \"/mnt/workspace/spacelift.oidc\"\n}\n}\n</code></pre>"},{"location":"integrations/cloud-providers/oidc/azure-oidc.html","title":"Microsoft Azure","text":""},{"location":"integrations/cloud-providers/oidc/azure-oidc.html#configuring-workload-identity-federation","title":"Configuring workload identity federation","text":"<p>In order to enable Spacelift runs to access Azure resources, you need to set up Spacelift as a valid identity provider for your account. This is done using workload identity federation. The set up process involves creating an App Registration, and then adding federated credentials that tell Azure which Spacelift runs should be able to use which App Registrations. This process can be completed via the Azure Portal, Azure CLI or Terraform. For illustrative purposes we will use the Azure Portal.</p> <p>Info</p> <p>Note that the following instructions show how to setup federation using a Microsoft Entra App, but the same approach can also be used with a user-assigned managed identity.</p> <p>The first step is to go to the Azure AD section of the Azure Portal, go to App registrations, and then click on the New registration button:</p> <p></p> <p>Specify a name for your registration, select the Accounts in this organizational directory only option, and click on the Register button:</p> <p></p> <p>On the overview page, take a note of the Application (client) ID and Directory (tenant) ID - you will need them later when configuring the Terraform provider.</p> <p></p> <p>Next, go to the Certificates &amp; secrets section, select the Federated credentials tab and click on the Add credential button:</p> <p></p> <p>On the next screen, choose Other issuer as the Federated credential scenario:</p> <p></p> <p>The next step is to configure the trust relationship between Spacelift and Azure. In order to do this, we need to fill out the following pieces of information:</p> <ul> <li>Issuer - the URL of your Spacelift account, for example <code>https://myaccount.app.spacelift.io</code>.</li> <li>Subject identifier - the subject that a token must contain to be able to get credentials for your App. This uses the format mentioned in the Standard claims section.</li> <li>Name - a name for this credential.</li> <li>Audience - the hostname of your Spacelift account, for example <code>myaccount.app.spacelift.io</code>.</li> </ul> <p>Take a look at the following screenshot for an example allowing a proposed run to use our App:</p> <p></p> <p>Workload federation in Azure requires the subject claim of the OIDC token to exactly match the federated credential, and doesn't allow wildcards. Because of this you will need to repeat the same process and add a number of different federated credentials in order to support all the different types of runs for your Stack or module. For example for a stack called <code>azure-oidc-test</code> in the <code>legacy</code> space you need to add credentials for the following subjects:</p> <pre><code>space:legacy:stack:azure-oidc-test:run_type:TRACKED:scope:read\nspace:legacy:stack:azure-oidc-test:run_type:TRACKED:scope:write\nspace:legacy:stack:azure-oidc-test:run_type:PROPOSED:scope:read\nspace:legacy:stack:azure-oidc-test:run_type:TASK:scope:write\nspace:legacy:stack:azure-oidc-test:run_type:DESTROY:scope:write\n</code></pre> <p>And for a module called <code>my-module</code> in the <code>development</code> space you need to add the following:</p> <pre><code>space:development:stack:my-module:run_type:TESTING:scope:read\nspace:development:stack:my-module:run_type:TESTING:scope:write\n</code></pre> <p>After adding all the credentials for a stack, it should look something like this:</p> <p></p> <p>Info</p> <p>Please see the Standard claims section for more information about the subject format.</p>"},{"location":"integrations/cloud-providers/oidc/azure-oidc.html#configuring-the-terraform-provider","title":"Configuring the Terraform Provider","text":"<p>Once workload identity federation is set up, the AzureRM provider can be configured without the need for any static credentials. To do this, enable the <code>use_oidc</code> feature of the provider, and use the <code>oidc_token_file_path</code> setting to tell the provider where to find the token:</p> <pre><code>provider \"azurerm\" {\nfeatures {}\nuse_oidc             = true\noidc_token_file_path = \"/mnt/workspace/spacelift.oidc\"\n}\n</code></pre> <p>Next, add the following environment variables to your stack:</p> <ul> <li><code>ARM_CLIENT_ID</code> - the client ID of the App registration created in the previous section.</li> <li><code>ARM_TENANT_ID</code> - the tenant ID of the App registration created in the previous section.</li> <li><code>ARM_SUBSCRIPTION_ID</code> - the ID of the Azure subscription you want to use.</li> </ul> <p>Info</p> <p>Note - before you can use your App registration to manage Azure resources, you need to assign the correct RBAC permissions to it.</p>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html","title":"Google Cloud Platform (GCP)","text":""},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#configuring-workload-identity","title":"Configuring Workload Identity","text":"<p>In order to enable Spacelift runs to access GCP resources, you need to set up Spacelift as a valid identity provider for your account. To do this you need to perform a number of steps within GCP:</p> <ul> <li>Create a workload identity pool and set up the Spacelift OIDC provider as an identity provider for it;</li> <li>Create a service account that will be used by Spacelift;</li> <li>Connect the service account to the workload identity pool;</li> </ul> <p>Let's go through these steps one by one. First, you will want to go to the GCP console and select the IAM service, then click on the \"Workload Identity Federation\" link in the left-hand menu:</p> <p></p> <p>There, you will want to click on the Create pool button, which will take you to the pool creation form. First, give your new identity pool a name and optionally set a description. The next step is more interesting - you will need to set up an identity provider. The name is pretty much arbitrary but the rest of the fields are important to get right. The Issuer URL needs to be set to the URL of your Spacelift account (including the scheme). You will want to manually specify allowed audiences. There's just one you need - the hostname of your Spacelift account. Here is what a properly filled out form would look like:</p> <p></p> <p>Hint</p> <p>You will need to add iss to Issuer (URL) and you will need to add aud to Audience. You will need to replace <code>demo.app.spacelift.io</code> and <code>demo@spacelift.io</code> with the hostname of your Spacelift account.</p> <p>In the last step, you will need to configure a mapping between provider Spacelift token claims (assertions) and Google attributes. <code>google.subject</code> is a required mapping and should generally map to <code>assertion.sub</code>. Custom claims can be mapped to custom attributes, which need to start with the <code>attribute.</code> prefix. In the below example, we are also mapping Spacelift's <code>spaceId</code> claim to GCP's custom <code>space</code> attribute:</p> <p></p> <p>To restrict which identities can authenticate using your workload identity pool you can specify extra conditions using Google's Common Expression Language.</p> <p>Warning</p> <p>If your Stack ID is too long, it may exceed the threshold set by Google for the <code>google.subject</code> mapping. In that case, you can use a different Custom claim to create the mapping.</p> <p>Last but not least, we will want to grant the workload identity pool the ability to impersonate the service account we will be using. Assuming we already have a service account, let's allow any token claiming to originate from the <code>production</code> space in our Spacelift account to impersonate it:</p> <p></p>"},{"location":"integrations/cloud-providers/oidc/gcp-oidc.html#configuring-the-terraform-provider","title":"Configuring the Terraform Provider","text":"<p>Once the Spacelift-GCP OIDC integration is set up, the Google Cloud Terraform provider can be configured without the need for any static credentials. You will however want to provide a configuration file telling the provider how to authenticate. The configuration file can be created manually or generated by the <code>gcloud</code> utility and would look like this:</p> <pre><code>{\n\"type\": \"external_account\",\n\"audience\": \"//iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${WORKER_POOL_ID}/providers/${IDENTITY_PROVIDER_ID}\",\n\"subject_token_type\": \"urn:ietf:params:oauth:token-type:jwt\",\n\"token_url\": \"https://sts.googleapis.com/v1/token\",\n\"credential_source\": {\n\"file\": \"/mnt/workspace/spacelift.oidc\"\n},\n\"service_account_impersonation_url\": \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/${SERVICE_ACCOUNT_EMAIL}:generateAccessToken\",\n\"service_account_impersonation\": {\n\"token_lifetime_seconds\": 3600\n}\n}\n</code></pre> <p>Your Spacelift run needs to have access to this file, so you can check it in (there's nothing secret here), mount it on a stack or mount it in a context that is then attached to the stack.</p> <p>Note that you will also need to tell the provider how to find this configuration file. You can do this by creating a <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable, and setting it to the path to your credentials file.</p> <p>Here is an example of us using a Spacelift context to mount the file and configure the provider to be attached to an arbitrary number of stacks:</p> <p></p> <p>For more information about configuring the Terraform provider, please see the Google Cloud Terraform provider docs.</p>"},{"location":"integrations/cloud-providers/oidc/vault-oidc.html","title":"HashiCorp Vault","text":""},{"location":"integrations/cloud-providers/oidc/vault-oidc.html#configuring-spacelift-as-an-identity-provider","title":"Configuring Spacelift as an Identity Provider","text":"<p>In order to enable Spacelift runs to access Vault, you need to set up Spacelift as a valid identity provider for your Vault instance. This is done using Vault's OIDC auth method. The set up process involves creating a role in Vault that tells Vault which Spacelift runs should be able to access which Vault secrets. This process can be completed via the Vault CLI or Terraform. For illustrative purposes we will use the Vault CLI.</p> <p>If you haven't enabled the JWT auth method in your Vault instance, you need to do so first. To do this, run the following command:</p> <pre><code>vault auth enable jwt\n</code></pre> <p>In the next step, we need to add configuration for your Spacelift account as an identity provider. To do this, run the following command:</p> <pre><code>vault write auth/jwt/config \\\nbound_issuer=\"https://demo.app.spacelift.io\" \\\noidc_discovery_url=\"https://demo.app.spacelift.io\"\n</code></pre> <p>The <code>bound_issuer</code> parameter is the URL of your Spacelift account which is used as the issuer claim in the OIDC token you receive from Spacelift. The <code>oidc_discovery_url</code> parameter is the URL of the OIDC discovery endpoint for your Spacelift account, which is in this case identical to the <code>bound_issuer</code> parameter.</p> <p>Next, you will need to create a policy that will be used to determine which Spacelift runs can access which Vault secrets. For example, the following policy allows all Spacelift runs to read any secret in the <code>secrets/preprod</code> path:</p> <pre><code>vault policy write infra-preprod - &lt;&lt;EOF\npath \"secrets/preprod/*\" {\n  capabilities = [\"read\"]\n}\nEOF\n</code></pre> <p>Last but not least, you will need to create a role that binds the policy to the identity provider. The following command creates a role called <code>infra-preprod</code> that binds the <code>infra-preprod</code> policy to the JWT identity provider:</p> <pre><code>vault write auth/jwt/role/infra-preprod -&lt;&lt;EOF\n{\n  \"role_type\": \"jwt\",\n  \"user_claim\": \"iss\",\n  \"bound_audiences\": \"demo.app.spacelift.io\",\n  \"bound_claims\": { \"spaceId\": \"preprod\" },\n  \"policies\": [\"infra-preprod\"],\n  \"ttl\": \"10m\"\n}\nEOF\n</code></pre> <p>The <code>bound_audiences</code> parameter is the hostname of your Spacelift account, which is used as the audience claim in the OIDC token you receive from Spacelift. The <code>bound_claims</code> parameter is a JSON object that contains the claims that the OIDC token must contain in order to be able to access the Vault secrets. How you scope this will very much depend on your use case. In the above example, only runs belonging to a stack or module in the <code>spaceId</code> claim can assume \"infra-preprod\" Vault role. You can refer to this document to see the available standard and custom claims presented by the Spacelift OIDC token.</p>"},{"location":"integrations/cloud-providers/oidc/vault-oidc.html#configuring-the-terraform-provider","title":"Configuring the Terraform Provider","text":"<p>Once the Vault setup is complete, you need to configure the Terraform Vault provider to use the Spacelift OIDC JWT token to assume a particular role. To do this, you will provide the <code>auth_login_jwt</code> configuration block to the provider, and set the <code>role</code> parameter to the name of the role you created in the previous section:</p> <pre><code>provider \"vault\" {\n  # ... other configuration\nskip_child_token = true\nauth_login_jwt {\nrole = \"infra-preprod\"\n}\n}\n</code></pre> <p>Next, set the <code>TERRAFORM_VAULT_AUTH_JWT</code> environment variable to <code>${SPACELIFT_OIDC_TOKEN}</code>, either directly on your stack, or on one of the attached contexts. This approach uses interpolation to dynamically set the value of the variable the provider is looking for to the value of the environment variable that Spacelift provides.</p>"},{"location":"integrations/observability/index.html","title":"Observability","text":"<p>Spacelift can integrate with observability tools by sending information to a webhook endpoint, here are some of the observability tools we can integrate with:</p> <ul> <li>Datadog</li> <li>Prometheus</li> </ul>"},{"location":"integrations/observability/datadog.html","title":"Datadog integration","text":"<p>Spacelift can send data to Datadog to help you monitor your infrastructure and Spacelift stacks using Datadog's excellent monitoring and analytics tools. Our integration with Datadog focuses primarily on runs and lets you create dashboards and alerts to answer questions like:</p> <ul> <li>How many runs are failing?</li> <li>Which stacks see the most activity?</li> <li>How long does it take to plan a given stack?</li> <li>How long does it take to apply a stack?</li> <li>What is the load on my Spacelift private workers?</li> <li>How many resources am I changing?</li> <li>...and many more!</li> </ul> <p>Here's a very simple dashboard we've created based on this integration that shows the performance of our continuous regression tests:</p> <p></p>"},{"location":"integrations/observability/datadog.html#prerequisites","title":"Prerequisites","text":"<p>The Datadog integration is based on our notification policy feature, which requires at least an active Cloud tier subscription. While building a notification-based Datadog integration from scratch is possible, we've created a Terraform module that will set up all the necessary integration elements for you.</p> <p>This module will only create Spacelift assets:</p> <ul> <li>a notification policy that will send data to Datadog;</li> <li>a webhook endpoint that serve as a notification target for the policy;</li> <li>a webhook secret header that will securely authenticate the payload with Datadog;</li> </ul>"},{"location":"integrations/observability/datadog.html#setting-up-the-integration","title":"Setting up the integration","text":"<p>To set up the integration, you'll need to have a Datadog account and a Datadog API key. If you don't have a an administrative stack declaratively manage your Spacelift resources, we suggest you create one, and add module instantiation to it according to its usage instructions. We suggest that you pass the Datadog API key as a stack secret, or - even better - retrieve it from a remote secret store (eg. AWS Parameter Store) using Terraform.</p> <p>If you intend to monitor your entire account (our suggested approach), we suggest that the module is installed in the root space of your Spacelift account. If you only want to monitor a subset of your stacks, you can install the module in their respective space.</p>"},{"location":"integrations/observability/datadog.html#metrics","title":"Metrics","text":"<p>The following metrics are sent:</p> <ul> <li><code>spacelift.integration.run.count</code> (counter) - a simple count of runs;</li> <li><code>spacelift.integration.run.timing</code> (counter, nanoseconds) - the duration of different run states. In addition to common tags, this metric is also tagged with the state name, eg. <code>state:planning</code>, <code>state:applying</code>, etc.;</li> <li><code>spacelift.integration.run.resources</code> (counter) - the resources affected by a run. In addition to common tags, this metric is also tagged with the change type, eg. <code>change_type:added</code>, <code>change_type:changed</code>, etc.;</li> </ul>"},{"location":"integrations/observability/datadog.html#common-tags","title":"Common tags","text":"<p>Common tags for all metrics are the following:</p> <ul> <li><code>account</code> (string) : name of the Spacelift account generating the metric;</li> <li><code>branch</code> (string): name of the Git branch the run was triggered from;</li> <li><code>drift_detection</code> (boolean): whether the run was triggered by drift detection;</li> <li><code>run_type</code> (string): type of a run, eg. \"tracked\", \"proposed\", etc.;</li> <li><code>final_state</code> (string): the terminal state of the run, eg. \"finished\", \"failed\", etc.;</li> <li><code>space</code> (string): name of the Spacelift space the run belongs to;</li> <li><code>stack</code> (string): name of the Spacelift stack the run belongs to;</li> <li><code>worker_pool</code> (string): name of the Spacelift worker pool the run was executed on - for the public worker pool this value is always <code>public</code>;</li> </ul>"},{"location":"integrations/observability/datadog.html#extending-the-integration","title":"Extending the integration","text":"<p>The benefit of building this integration on top of a notification policy is that you can easily extend it to send additional data to Datadog, change the naming of your metrics, change the tags, etc. To do so, you'll need to edit the policy body generated by the module. You can do so by editing the policy in the Spacelift UI, or by forking the module and editing the policy body in the module's source code.</p> <p>Note that this is an advanced feature, and we recommend that you only do so if you're already familiar with Spacelift's notification policy feature and Datadog's API, or are willing to learn about them.</p>"},{"location":"integrations/observability/prometheus.html","title":"Prometheus integration","text":"<p>The Prometheus exporter allows you to monitor various metrics about your Spacelift account over time. You can then use tools like Grafana to visualize those changes and Alertmanager to take actions based on account metrics. Several metrics are available, and you can find the complete list of available metrics here.</p>"},{"location":"integrations/observability/prometheus.html#how-it-works","title":"How it works","text":"<p>The Prometheus exporter is an adaptor between Prometheus and the Spacelift GraphQL API. Whenever Prometheus asks for the current metrics, the exporter makes a GraphQL request and converts it into the metrics format Prometheus expects.</p> <p>Read more on our blog: Monitoring Your Spacelift Account via Prometheus.</p>"},{"location":"integrations/single-sign-on/index.html","title":"Single Sign-On","text":"<p>By default, Spacelift supports logging in using GitHub, GitLab, or Google. Some organizations however prefer a Single Sign-On approach, where access to resources is centralized. To accommodate this use-case, Spacelift supports Single Sign-On using SAML 2.0 or OIDC.</p>"},{"location":"integrations/single-sign-on/index.html#managed-identity-provider-vs-sso","title":"Managed Identity Provider vs SSO","text":"<p>Tip</p> <p>The SSO integration can only be configured once the Spacelift account has been created using one of the supported Identity Providers.</p> <p>To create a Spacelift account, a user needs to choose one of the supported managed identity providers. That user then becomes the \"Managed IdP Admin\".</p> <p>If SSO is configured, the managed identity provider used to create the account and the associated admin are disabled and the first user to successfully log in becomes the \"SSO Admin\".</p> <p>Login policies are not evaluated for Managed IdP and SSO admins so that they cannot lock themselves out. As a side effect, there won\u2019t be any Login policy samples for them in the Policy Workbench.</p> <p>If SSO is disabled later, the managed identity provider and associated admin are re-enabled automatically.</p>"},{"location":"integrations/single-sign-on/index.html#backup-credentials","title":"Backup Credentials","text":"<p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/index.html#managing-integrations","title":"Managing integrations","text":"<p>In order to manage Single Sign-On integrations on your Spacelift account, please go to the Settings section of your account view. Next, navigate to the Single Sign-On tab. If SSO is not enabled for your account, all you're going to see is instructions on how to get started. The first steps are always taken in your identity provider (Google Workspace, Okta, Auth0, ActiveDirectory, etc.). Navigate to your identity provider and create a dedicated SSO application filled with appropriate URLs taken from the Spacelift settings page presented below.</p> <p></p>"},{"location":"integrations/single-sign-on/index.html#setting-up-saml","title":"Setting up SAML","text":"<p>When setting up Spacelift on your identity provider, you may want to add three attribute mappings:</p> <ul> <li><code>FirstName</code> is used to build human-friendly user name;</li> <li><code>LastName</code> is used to build human-friendly user name;</li> <li><code>Teams</code> can be used by login and stack access policies to determine the level access to the Spacelift account and/or individual Stacks;</li> </ul> <p>Depending on your identity provider and your use case, your mapping may be different. Especially with regards to <code>Teams</code>, some identity providers (eg. Okta) will support an arbitrary list of memberships similar to GitHub teams out of the box, some will need extra customizations like (eg. Google Workspace) and as a courtesy, we will flush your login history.</p> <p>Some identity providers (eg. Okta) will allow you to provide a custom per-user SAML 2.0 Subject for SAML assertions. You could use this feature to map GitHub usernames to your identity provider users and thus get the exact same experience as when using GitHub as your identity provider.</p> <p>Warning</p> <p>When setting up SSO without this GitHub mapping, your future logins will appear as new users since Spacelift has no way of mapping those without your assistance. New users will count against your seat quota and you may run out of seats. If you run into this problem, you can contact us.</p> <p>Info</p> <p>Spacelift uses both HTTP-Redirect and HTTP-POST bindings for SAML 2.0. Most of the IdPs enable both by default, but if you run into any issues, please check your application settings.</p>"},{"location":"integrations/single-sign-on/index.html#nameid-format","title":"NameID format","text":"<p>The NameID format specifies the format that Spacelift requests user identifiers from your identity provider. The user identifier is used as the Spacelift login, and each unique identifier will count against your seat quota. Some identity providers allow you to configure this format, but certain providers (eg. Azure AD) do not.</p> <p>If your identity provider does not allow the NameID format to be configured at their end, you can choose from one of the following options:</p> <ul> <li>Transient - an opaque identifier that is not guaranteed to remain the same between logins.</li> <li>Email Address - an email address.</li> <li>Persistent - an opaque identifier that remains the same between logins.</li> </ul>"},{"location":"integrations/single-sign-on/index.html#saml-setup-guides","title":"SAML Setup Guides","text":"<p>The following are links to example implementations you can use as a reference/guide for setting up your own SAML integration.</p> <ul> <li>AWS IAM Identity Center (formerly known as AWS SSO)</li> </ul> <p>If you can't find your SAML provider in the list above, don't worry - we do support all SAML 2.0 providers.</p>"},{"location":"integrations/single-sign-on/index.html#setting-up-oidc","title":"Setting up OIDC","text":"<p>When setting up Spacelift on your identity provider, you must make sure it supports the <code>email</code> scope and returns the corresponding <code>email</code></p>"},{"location":"integrations/single-sign-on/index.html#additional-claims","title":"Additional claims","text":"<p>Spacelift dynamically checks integrated Identity Provider's Well-Known OpenID configuration for a list of supported scopes and, optionally, asks for <code>profile</code> and <code>groups</code> scopes if those are available.</p> <p>Warning</p> <p>In order to populate the <code>input.session.teams</code> value in the Login Policies Spacelift tries to fetch the <code>groups</code> claim. For many Identity Providers, this claim has to be manually set and configured. Bear in mind that some providers such as Google Workspace do not support retrieving groups of given users.</p>"},{"location":"integrations/single-sign-on/index.html#oidc-setup-guides","title":"OIDC Setup Guides","text":"<p>The following are links to example implementations you can use as a reference/guide for setting up your own OIDC integration.</p> <ul> <li>GitLab</li> <li>Okta</li> <li>OneLogin</li> <li>Azure AD</li> </ul> <p>If you can't find your OIDC provider in the list above, don't worry - we do support all OIDC providers as long as they support the email scope and return the user's email. Fortunately, most OIDC providers do.</p>"},{"location":"integrations/single-sign-on/index.html#idp-initiated-sso","title":"IdP-initiated SSO","text":"<p>While certainly more convenient, IdP-initiated SSO lacks some of the protections awarded by SP-initiated SSO and is thus inherently less safe. Since Spacelift manages some of your most valuable resources, we decided against supporting this feature.</p> <p>If our server detects an IdP-initiated SSO session, it simply redirects the browser using 303 See other HTTP status code to the endpoint that triggers a regular SP-initiated SSO flow. As a result, you can still access Spacelift by clicking on the link in your IdP catalog, but are not exposed to the vulnerabilities of the IdP-initiated SSO.</p>"},{"location":"integrations/single-sign-on/index.html#disabling-sso","title":"Disabling SSO","text":"<p>In order to disable SSO integration for your Spacelift account, or change the IdP provider, please click the Disable button to delete the integration. This change takes effect immediately for new logins, and will invalidate existing sessions. New sessions will be created using the new SSO identity provider or - if none is set up - Spacelift will utilize the default identity provider that was used to create the account originally.</p> <p>Warning</p> <p>Again, please note that new usernames will occupy new seats, even if they're the same users registered with a different identity provider.</p> <p></p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html","title":"AWS IAM Identity Center SAML Setup Guide","text":"<p>Info</p> <p>Note that SSO Integration with SAML 2.0 is an Enterprise plan feature.</p> <p>If you'd like to set up the ability to sign in to your Spacelift account using a SAML 2.0 integration with AWS IAM Identity Center (formerly known as AWS SSO), you've come to the right place. This example will walk you through the steps to get this set up, and you'll have Single Sign-On running in no time!</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Spacelift account, with access to admin permissions.</li> <li>AWS account which is a member of an AWS Organization, with permission to create AWS IAM Identity applications.</li> </ul>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#configure-the-aws-iam-identity-application","title":"Configure the AWS IAM Identity application","text":"<p>Log into the AWS account, go to the IAM Identity Center home page and finally, click on the \"Applications\" link.</p> <p></p> <p>On that screen, click on the \"Add a new application\" button.</p> <p></p> <p>Finally, click on the \"Add a custom SAML 2.0 application\" button.</p> <p></p> <p>Set the \"Display name\" field to \"Spacelift\". Then, copy the URL for the \"IAM Identity Center SAML metadata file\" and head to the settings in your Spacelift account.</p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#configure-spacelift-saml-integration","title":"Configure Spacelift SAML integration","text":"<p>From the navigation side bar menu, select \"Settings.\"</p> <p></p> <p>Next, you'll want to click the Set Up button underneath the \"SAML Settings\" section.</p> <p></p> <p>In the SAML settings:</p> <ul> <li>Set the value for \"NameID Format\" to \"Persistent\".</li> <li>Enable the \"Dynamic configuration\".</li> <li>Paste the URL you just copied in AWS in the \"IdP metadata URL\" field.</li> </ul> <p></p> <p>Danger</p> <p>Do not click on the \"Save\" button yet, otherwise Spacelift will try to activate SAML integration right away and we are not completely done with the setup yet.</p> <p>If you clicked on the button anyway, you will be presented with an AWS login page and you will likely be unable to log in at this time. Don't worry. Just open another tab in your browser and go to your Spacelift account. As an administrator, you will be able to log in via the Identity Provider your used to create the account. From there, you will be able to activate the SAML integration once you have completed all the remaining steps documented below.</p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#configure-the-aws-iam-identity-application-continued","title":"Configure the AWS IAM Identity application (Continued)","text":"<p>Go back to the AWS console. In the \"Application metadata\" section, click on the \"If you don't have a metadata file, you can manually type your metadata values.\" link.</p> <p></p> <p>Copy/paste the values for \"Single Sign-On URL\" and \"Entity ID (audience)\" from Spacelift to \"Application ACS URL\" and \"Application SAML audience\", respectively.</p> <p></p> <p></p> <p>Finally, click on the \"Save changes\" button.</p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#set-the-attribute-mappings","title":"Set the attribute mappings","text":"<p>Go to the \"Attribute mappings\" tab, set the values as described below and click on the \"Save changes\" button.</p> User attribute in the application Maps to this string value or user attribute in IAM Identity Center Format Subject ${user:subject} persistent FirstName ${user:givenName} basic LastName ${user:familyName} basic Teams ${user:groups} basic <p>Warning</p> <p>Please note that while available, <code>${user:groups}</code> is not officially supported by AWS IAM Identity Center and it will return the group ID (GUUID) and not the group name. There is currently no way to get the group name. </p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#assign-users-and-groups-to-the-application","title":"Assign users and groups to the application","text":"<p>Make sure to assign users and/or groups to the SAML application in the \"Assigned users\" tab.</p> <p></p>"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#activate-the-spacelift-saml-integration","title":"Activate the Spacelift SAML integration","text":"<p>Back to Spacelift for the final step. You can finally click on the \"Save\" button on the SAML integration page.</p> <p>The page will reload and the AWS login page will be displayed. Use the credentials for a user that has access to the SAML application and you should be able to log into Spacelift.</p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html","title":"Azure AD OIDC Setup Guide","text":"<p>If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with Azure AD, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time!</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p> <p>Warning</p> <p>Due to a limitation in Azure AD, when you map the teams to spacelift it will only map the IDs of the teams. A workaround would be to rewrite the teams in the login policy.</p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Spacelift account, with access to admin permissions</li> <li>Azure account, with an existing Azure Active Directory</li> <li>You'll need permissions to create an App Registration within your Azure AD</li> </ul> <p>Info</p> <p>Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.</p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#configure-account-settings","title":"Configure Account Settings","text":"<p>You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\"</p> <p></p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#setup-oidc","title":"Setup OIDC","text":"<p>Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from Azure. For now, copy the authorized redirect URL as we will need to provide Azure this URL when configuring our Azure App Registration within your Azure AD.</p> <p></p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-portal-navigate-to-azure-active-directory","title":"Azure Portal: Navigate to Azure Active Directory","text":"<p>Within your Azure Account, navigate to your Azure Active Directory where you'd like to setup the OIDC integration for. In this guide, we are using a Default Directory for example purposes.</p> <p></p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-create-an-app-registration","title":"Azure AD: Create an App Registration","text":"<p>While you are within your Active Directory's settings, click on App registrations from the navigation, and then select New registration.</p> <p></p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-app-registration-configuration","title":"Azure AD: App Registration Configuration","text":"<p>Give your application a name - Spacelift sounds like a good one.</p> <p>Configure your supported account types as per your login requirements. In this example, we are allowing Accounts in this organizational directory access to Spacelift.</p> <p>Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Redirect URI input as shown. Make sure you select Web for the type.</p> <p>Click Register.</p> <p></p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-add-upn-claim","title":"Azure AD: Add UPN Claim","text":"<p>Start by navigating to the Token configuration section of your application.</p> <p></p> <p>Click the Add optional claim button, choose the ID token type, and select the upn claim:</p> <p></p> <p>Click the Add button, making sure to enable the Turn on the Microsoft Graph profile permission checkbox on the popup that appears:</p> <p></p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-configure-app-credentials","title":"Azure AD: Configure App Credentials","text":"<p>Navigate to the Certificates &amp; secrets section of your application.</p> <p></p> <p>Click the New client secret button.</p> <p></p> <p>Give your secret a description and an expiration:</p> <p></p> <p>Info</p> <p>In this example, we are using 6 months for Expires. This means you will need to generate a new client secret in 6 months for your OIDC integration.</p> <p>Click Add.</p> <p>Now that your secret has been created, copy its Value:</p> <p></p> <p>Take this secret value that you have just copied and paste this into our Spacelift OIDC settings within the Secret input:</p> <p></p> <p>Info</p> <p>Don't click Save in Spacelift just yet, we still need to get the Client ID and Provider URL for your application.</p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-client-id-and-provider-url","title":"Azure AD: Client ID and Provider URL","text":"<p>To complete your configuration, you need two more pieces of information:</p> <ul> <li>The Client ID.</li> <li>The Provider URL.</li> </ul> <p>You can get both of these pieces of information from the Overview section of your Azure AD application. The Application (client) ID field should be used as the Client ID, and the Provider URL can be found in the OpenID Connect metadata document field after clicking on the Endpoints button:</p> <p></p> <p>In summary, here are the values that should be copied over to Spacelift:</p> <ul> <li>Application (client) ID within Azure AD =&gt; Client ID on Spacelift</li> <li>Secret Value you generated =&gt; Secret input on Spacelift</li> <li>OpenID Connect metadata document URL =&gt; Provider URL on Spacelift</li> </ul> <p>Click Save.</p>"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-oidc-setup-completed","title":"Azure AD OIDC Setup Completed","text":"<p>That's it! Your OIDC integration with Azure AD should now be configured (as per this example). Feel free to make any changes to your liking within your Azure AD App Registration configuration for the app that you just created.</p>"},{"location":"integrations/single-sign-on/backup-credentials.html","title":"Backup Credentials","text":"<p>Spacelift is SSO-first. This means that you can only create a Spacelift account using one of the supported identity providers, and can then further configure custom SAML or OIDC integrations.</p> <p>However, usually you'd also like to set up a set of backup credentials that can be safely stored, in case you ever need to access Spacelift without going through your SSO provider. This might be useful in situations such as accidentally misconfiguring the SSO provider and locking oneself out, or the SSO provider having an outage.</p>"},{"location":"integrations/single-sign-on/backup-credentials.html#creating-backup-credentials","title":"Creating Backup Credentials","text":"<p>In order to create a set of backup credentials, just create an API Key with admin permissions, as described in Spacelift API Key &gt; Token. Then, securely store the API Key ID, as well as the first secret token in the downloaded secret file, the one right after <code>Please use the following API secret when communicating with Spacelift programmatically:</code>.</p>"},{"location":"integrations/single-sign-on/backup-credentials.html#logging-in-with-backup-credentials","title":"Logging in with Backup Credentials","text":"<p>In order to log in using an API Key open <code>https://&lt;your-account&gt;.app.spacelift.io/apikeytoken</code>. There, you'll be able to provide the previously stored API key ID and secret.</p> <p> </p> <p>After clicking EXCHANGE, you'll be logged into the Spacelift UI.</p>"},{"location":"integrations/single-sign-on/backup-credentials.html#additional-use-cases","title":"Additional Use-Cases","text":"<p>Since API Keys go through the login policy and you can specify the teams an API Key should be on, you can use the above functionality with a non-admin key to easily debug how other users see your account, for the purposes of i.e. debugging your login policy.</p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html","title":"GitLab OIDC Setup Guide","text":"<p>If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with GitLab, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time!</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Spacelift account, with access to admin permissions</li> <li>GitLab account, with permission to create GitLab Applications</li> </ul> <p>Info</p> <p>Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.</p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#configure-account-settings","title":"Configure Account Settings","text":"<p>You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\"</p> <p></p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#setup-oidc","title":"Setup OIDC","text":"<p>Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from GitLab. For now copy the authorized redirect URL as we will need to provide GitLab this URL when configuring our GitLab application.</p> <p></p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#gitlab-create-gitlab-application","title":"GitLab: Create GitLab Application","text":"<p>Within your GitLab account, visit the Applications section of your account.</p> <p></p> <p>Create your GitLab Application as shown, the application's Name can be whatever you'd like. Spacelift sounds like a great name to use though.</p> <p>Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Redirect URI input as shown.</p> <p>Ensure that the openId, profile and email scopes are check'd.</p> <p>Click Save Application.</p> <p></p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#configure-oidc-settings","title":"Configure OIDC Settings","text":"<p>Now that we have the GitLab Application setup, we'll need to take the Application ID and Secret to configure the Spacelift OIDC Settings.</p> <p>Application ID = Spacelift's Client ID</p> <p>Secret = Spacelift's Secret</p> <p>In Spacelift, the Provider URL depends on where you are using GitLab, if you are using GitLab.com this value can be set as <code>https://gitlab.com</code></p> <p>Info</p> <p>When setting your Provider URL within Spacelift, do not include a trailing slash \"/\" at the end of your URL or you may receive an error.</p> <p></p> <p></p>"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#gitlab-oidc-setup-completed","title":"GitLab OIDC Setup Completed","text":"<p>That's it! Your OIDC integration with GitLab should now be fully configured.</p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html","title":"Okta OIDC Setup Guide","text":"<p>If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with Okta, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time!</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Spacelift account, with access to admin permissions</li> <li>Okta account, with permission to create Okta App Integrations</li> </ul> <p>Info</p> <p>Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.</p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#configure-account-settings","title":"Configure Account Settings","text":"<p>You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\"</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#setup-oidc","title":"Setup OIDC","text":"<p>Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from Okta. For now copy the authorized redirect URL as we will need to provide Okta this URL when configuring our Okta App Integration.</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-select-applications","title":"Okta: Select Applications","text":"<p>In a new browser tab, open your Okta account. Select the Applications link from the navigation.</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-create-app-integration","title":"Okta: Create App Integration","text":"<p>Click the \"Create App Integration\" button. For the sign in type, ensure you select OIDC - for the application type, select Web Application.</p> <p></p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-configure-app-integration","title":"Okta: Configure App Integration","text":"<p>Give your app integration a name - Spacelift sounds like a good one.</p> <p>Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Sign-in redirect URIs input as shown.</p> <p>As far as the assignments for this app integration, that's up to you at the end of the day. This determines what users from your Okta account will be able to access Spacelift. Click Save.</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-configure-group-claim","title":"Okta: Configure Group Claim","text":"<p>Click on the Sign On tab within your newly created Okta App Integration,</p> <p>You'll need to edit the groups claim type to return groups you consider useful in Spacelift Login Policies. For testing purposes, you could set it to Matches regex with .* for the regex value.</p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#configure-oidc-settings","title":"Configure OIDC Settings","text":"<p>Switch back to the General tab. Now that we have the Okta App Integration setup, we'll need to take the Client ID, Client Secret, and Okta domain, to configure the Spacelift OIDC Settings.</p> <p>Info</p> <p>The Okta Domain will be set as the \"Provider URL\" in your Spacelift OIDC settings. Ensure that you prefix this URL with <code>https://</code>.</p> <p></p> <p></p>"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-oidc-setup-completed","title":"Okta OIDC Setup Completed","text":"<p>That's it! Your OIDC integration with Okta should now be fully configured. Feel free to make any changes to your liking within your Okta App Integration configuration for the app that you just created.</p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html","title":"OneLogin OIDC Setup Guide","text":"<p>If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with OneLogin, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time!</p> <p>Warning</p> <p>Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.</p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Spacelift account, with access to admin permissions</li> <li>OneLogin account, with permission to create OneLogin Applications</li> </ul> <p>Info</p> <p>Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.</p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#configure-account-settings","title":"Configure Account Settings","text":"<p>You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\"</p> <p></p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#setup-oidc","title":"Setup OIDC","text":"<p>Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from OneLogin. For now copy the authorized redirect URL as we will need to provide OneLogin this URL when configuring our OneLogin application.</p> <p></p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-select-applications","title":"OneLogin: Select Applications","text":"<p>In a new browser tab, open your OneLogin account and visit the Administration page. Select the Applications link from the navigation.</p> <p></p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-add-application","title":"OneLogin: Add Application","text":"<p>Click the Add App button.</p> <p></p> <p>Search for OpenId Connect and select the result as shown.</p> <p></p> <p>Give your new OneLogin App a name, Spacelift sounds like a good one.</p> <p>In regards to \"Visible in portal\" this is a OneLogin configuration decision that's up to you. In this example, we are enabled this value.</p> <p></p> <p>In the app navigation, navigate to the Configuration section. Input your Login URL for example \"https://AccountName.app.spacelift.io\" Make sure to replace AccountName with your actual account name.</p> <p>Next, paste the previously copied authorized redirect URL into the Redirect URI's input field. Once done, remember to click on the Save button.</p> <p></p> <p>In the app navigation, click on the SSO section. Now that we have the OneLogin App setup, we'll need to take the Client ID, Client Secret, and Issuer URL, to configure the Spacelift OIDC Settings</p> <p></p> <p></p> <p>Info</p> <p>Important: You'll need to ensure your OneLogin user has access to the OneLogin App you just created, or else you will receive an unauthorized error when clicking save.</p>"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-oidc-setup-completed","title":"OneLogin OIDC Setup Completed","text":"<p>That's it! OIDC integration with OneLogin should now be fully configured. Feel free to make any changes to your liking within your OneLogin App configuration.</p> <p>You'll of course need to configure any users/groups within your OneLogin account to have access to this newly created app.</p>"},{"location":"integrations/source-control/index.html","title":"Source Control","text":"<p>Spacelift supports the following source control tools:</p> <ul> <li>GitHub</li> <li>GitLab</li> <li> <p>Bitbucket</p> <ul> <li>Cloud</li> <li>Datacenter/Server</li> </ul> </li> <li> <p>Azure DevOps</p> </li> <li>Raw Git</li> </ul>"},{"location":"integrations/source-control/azure-devops.html","title":"Azure DevOps","text":"<p>Spacelift supports using Azure DevOps as the source of code for your stacks and modules.</p>"},{"location":"integrations/source-control/azure-devops.html#setting-up-the-integration","title":"Setting up the integration","text":"<p>In order to set up the integration from the Spacelift side, please navigate to the VCS providers section of the admin Settings page, find the Azure DevOps integration and click the Set up button:</p> <p></p> <p>This should open a form like this one:</p> <p></p>"},{"location":"integrations/source-control/azure-devops.html#finding-your-organization-url","title":"Finding your Organization URL","text":"<p>Now you'll have to fill in the Organization URL, which is the main URL of your Azure DevOps organization.</p> <p></p> <p>Info</p> <p>The Azure DevOps Organization URL usually has the following format:</p> <ul> <li><code>https://dev.azure.com/{my-organization-name}</code></li> </ul> <p>Depending on when your Azure DevOps organization was created, it may use a different format, for example:</p> <ul> <li><code>https://{my-organization-name}.visualstudio.com</code></li> </ul> <p>You can find out more about Azure DevOps URLs here.</p>"},{"location":"integrations/source-control/azure-devops.html#creating-a-personal-access-token","title":"Creating a Personal Access Token","text":"<p>In order to create a Personal access token you need to:</p> <p>1. Go to User settings -&gt; Personal access tokens (in the top right section of the Azure DevOps page)</p> <p></p> <p>2. On the Personal Access Tokens page click + New Token</p> <p></p> <p>3. Create a new personal access token. There, you will need to set the name of your token, expiration and scope. For Spacelift give it Code (Source code, repositories, pull requests and notifications) Read &amp; write access.</p> <p></p> <p>4. Once the token is created, put it into the Personal access token field on the Spacelift Azure DevOps setup form.</p> <p></p>"},{"location":"integrations/source-control/azure-devops.html#creating-the-integration","title":"Creating the Integration","text":"<p>After doing all this you should have all fields filled in.</p> <p></p> <p>If all the data is correct, after saving you should see two notifications in the bottom right part of Spacelift: New integration was created; and Connecting to Azure DevOps succeeded.</p>"},{"location":"integrations/source-control/azure-devops.html#configuring-webhooks","title":"Configuring Webhooks","text":"<p>In order for Spacelift to be notified of any changes made in your Azure DevOps repos, you need to setup webhooks in Azure DevOps. You can find your webhook endpoint and webhook password under the Azure DevOps VCS integration section:</p> <p></p> <p>For each Azure DevOps project you want to use with Spacelift, you now have to go into its Project settings -&gt; Service hooks -&gt; Create subscription. Within the services list choose Web Hooks and click Next.</p> <p></p> <p>You will need to create a Web Hooks Service Hook for the following events:</p> <ul> <li>Code pushed.</li> <li>Pull request created.</li> <li>Pull request merge attempted.</li> <li>Pull request updated.</li> <li>Pull request commented on.</li> </ul> <p>Let's first create the Code pushed webhook.</p> <p>Once on the Trigger page of New Service Hooks Subscription window, select Code pushed in the Trigger on this type of event dropdown and click Next.</p> <p></p> <p>After clicking Next you should see the Action page. Under the Settings section fill in the Spacelift Webhook endpoint URL. Leave Basic authentication username empty and put the Webhook password under Basic authentication password and click Finish.</p> <p></p> <p>Once done you should see the list of configured Service Hooks. Repeat the same process for the others. Afterwards you should see the configured webhooks on the Service Hooks settings page.</p> <p></p>"},{"location":"integrations/source-control/azure-devops.html#using-the-integration","title":"Using the Integration","text":"<p>When creating a Stack, you will now be able to choose the Azure DevOps provider and a repository inside of it:</p> <p></p>"},{"location":"integrations/source-control/azure-devops.html#unlinking-the-integration","title":"Unlinking the Integration","text":"<p>If you no longer need the integration, you can remove it via the Unlink button on the VCS settings page.</p> <p></p> <p>Please also remember to remove any Spacelift webhooks from your repositories.</p>"},{"location":"integrations/source-control/bitbucket-cloud.html","title":"Bitbucket Cloud","text":"<p>Spacelift supports using Bitbucket Cloud as the source of code for your stacks and modules.</p>"},{"location":"integrations/source-control/bitbucket-cloud.html#setting-up-the-integration","title":"Setting up the integration","text":"<p>In order to set up the integration from the Spacelift side, please navigate to the VCS providers section of the admin Settings page, find the Bitbucket Cloud integration and click the Set up button:</p> <p></p> <p>This should open a form like this one:</p> <p></p> <p>Now you'll have to fill in the Username, which is a username of your Bitbucket Cloud account.</p> <p>In order to get the App password you'll need to go to the Bitbucket Cloud site and navigate to Personal settings -&gt; App passwords (it's under Access management) -&gt; Create app password. There, you will need to give your new app password a label and give it read access to repositories and pull requests:</p> <p></p> <p>This will give you an app password token which you can put into the App Password field in the integration configuration.</p> <p></p> <p>After doing all this you should have all fields filled in.</p> <p></p> <p>After saving, you'll receive your webhook endpoint:</p> <p></p> <p>For each repository you want to use with Spacelift, you now have to go into its Repository settings -&gt; Webhooks -&gt; Add webhook, and configure the webhook accordingly, by activating the following events:</p> <ul> <li>Repository &gt; Push</li> <li>Pull Request &gt; Created</li> <li>Pull Request &gt; Updated</li> <li>Pull Request &gt; Approved</li> <li>Pull Request &gt; Approval removed</li> <li>Pull Request &gt; Merged</li> <li>Pull Request &gt; Comment created</li> </ul> <p>It should look something like the following:</p> <p></p> <p>The last step is to install the Pull Request Commit Links app to be able to use this API. This is done automatically when you go to the commit's details and then click \"Pull requests\" link.</p> <p></p> <p>When creating a Stack, you will now be able to choose the Bitbucket Cloud provider and a repository inside of it:</p> <p></p>"},{"location":"integrations/source-control/bitbucket-cloud.html#unlinking-the-integration","title":"Unlinking the Integration","text":"<p>If you no-longer need the integration, you can remove it via the Unlink button on the VCS settings page.</p> <p></p> <p>Please also remember to remove any Spacelift webhooks from your repositories.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html","title":"Bitbucket Datacenter/Server","text":"<p>Spacelift supports using an on-premises Bitbucket installation as the source of code for your stacks and modules.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#setting-up-the-integration","title":"Setting up the integration","text":""},{"location":"integrations/source-control/bitbucket-datacenter-server.html#creating-the-integration","title":"Creating the integration","text":"<p>In order to set up the integration from the Spacelift side, please navigate to the VCS Providers section of the admin Settings page and click the Set up button next to the Bitbucket Data Center integration:</p> <p></p> <p>This should open a form like this one:</p> <p></p> <p>Now you'll have to fill in the API host URL, which is the URL on which Spacelift will access the Bitbucket server.</p> <p>The user facing host URL is the address on which users of your Bitbucket instance access it. This could be an internal address inside of your company network, but could also be a public address if your Bitbucket instance is publicly available.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#creating-an-access-token","title":"Creating an access token","text":"<p>In order to use the integration, you need a user account for Spacelift to use, and you need to generate an access token for that account. The user account requires the following access:</p> <ul> <li>Read access to any projects Spacelift needs to be able to access.</li> <li>Write access to the repositories within those projects that Spacelift should have access to.</li> </ul> <p>Once you have a user account created, login as that user and go to Manage account -&gt; Personal access tokens -&gt; create. There, you will need to give your new access token a name and give it write access to repositories:</p> <p></p> <p>This will give you an access token which you can put into the Access token field in the integration configuration.</p> <p></p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#saving-the-integration","title":"Saving the integration","text":"<p>Once you have your access token, enter it into Spacelift. At this point all the fields should be filled out:</p> <p></p> <p>You can now save the integration.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#configuring-webhooks","title":"Configuring webhooks","text":"<p>Once you have saved your integration, you'll receive your webhook secret and endpoint:</p> <p></p> <p>For each repository you want to use with Spacelift, you need to go into its Repository settings -&gt; Webhooks -&gt; Create webhook, and configure the webhooks accordingly, by activating the following events:</p> <ul> <li>Repository &gt; Push</li> <li>Pull Request &gt; Opened</li> <li>Pull Request &gt; Source branch updated</li> <li>Pull Request &gt; Modified</li> <li>Pull Request &gt; Approved</li> <li>Pull Request &gt; Unapproved</li> <li>Pull Request &gt; Merged</li> <li>Pull Request &gt; Comment added</li> </ul> <p>It should look something like this:</p> <p></p> <p>Warning</p> <p>Don't forget to enter a secret when configuring your webhook. Bitbucket will allow you to create your webhook with no secret specified, but any webhook requests to Spacelift will fail without one configured.</p>"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#using-the-integration","title":"Using the integration","text":"<p>When creating a Stack, you will now be able to choose the Bitbucket Datacenter provider and a repository inside of it:</p> <p></p>"},{"location":"integrations/source-control/github.html","title":"GitHub","text":"<p>One of the things we're most proud of at Spacelift is the deep integration with everyone's favorite version control system - GitHub.</p> <p>Warning</p> <p>This integration is typically set up automatically for users who have selected GitHub as their login source, but if you are logging into Spacelift using a different identity provider (e.g. Google, GitLab, or another SSO provider), and do not have GitHub configured as a VCS Provider, see the following section on Setting up the custom application. This is also applicable for users who want to connect Spacelift to their GitHub Enterprise instance or to multiple GitHub accounts.</p>"},{"location":"integrations/source-control/github.html#setting-up-the-integration","title":"Setting up the integration","text":""},{"location":"integrations/source-control/github.html#setting-up-the-custom-application","title":"Setting up the custom application","text":"<p>In some cases, using the Spacelift application from the Marketplace is not an option, or you might already have it installed and want to link another GitHub account with your Spacelift account. For these advanced uses cases, you can use the custom Spacelift application.</p>"},{"location":"integrations/source-control/github.html#creating-the-custom-application","title":"Creating the custom application","text":"<p>In order to do so, navigate to the Source Code section of the Account Settings page in your Spacelift account, then click the Set Up button next to the GitHub (custom App) option:</p> <p></p> <p>You will be presented with two options:</p> <p></p> WizardManual setup <p>The easiest and recommended way to install the custom Spacelift application in your GitHub account is by using the wizard.</p> <p></p> <p>Answer the questions and you should be set up in no time.</p> <p>You can create the custom Spacelift application for GitHub manually.</p> <p>Warning</p> <p>This should be used as a last resort when the other methods can not be used as it is more tedious and error-prone.</p> <p>After selecting the option to enter your details manually, you should see the following form:</p> <p></p> <p>Before you can complete this step you need to create a GitHub App within GitHub. Start by navigating to the GitHub Apps page in the Developer Settings for your account/organization, and clicking on New GitHub App.</p> <p>You will need the Webhook endpoint and Webhook secret while creating your App, so take a note of them.</p> <p>You can either create the App in an individual user account or within an organization account:</p> <p></p> <p>Give your app a name and homepage URL (these are only used for informational purposes within GitHub):</p> <p></p> <p>Enter your Webhook URL and secret:</p> <p></p> <p>Set the following Repository permissions:</p> Permission Access Checks Read &amp; write Contents Read-only Deployments Read &amp; write Metadata Read-only Pull requests Read &amp; write Webhooks Read &amp; write Commit statuses Read &amp; write <p>Set the following Organization permissions:</p> Permission Access Members Read-only <p>Subscribe to the following events:</p> <ul> <li>Organization</li> <li>Pull request</li> <li>Pull request review</li> <li>Push</li> <li>Repository</li> </ul> <p>Finally, choose whether you want to allow the App to be installed on any account or only on the account it is being created in and click on Create GitHub App:</p> <p></p> <p>Once your App has been created, make a note of the App ID in the About section:</p> <p></p> <p>Now scroll down to the Private keys section of the page and click on Generate a private key:</p> <p></p> <p>This will download a file onto your machine containing the private key for your GitHub app. The file will be named <code>&lt;app-name&gt;.&lt;date&gt;.private-key.pem</code>, for example <code>spacelift.2021-05-11.private-key.pem</code>.</p> <p>Now that your GitHub App has been created, go back to the integration configuration screen in Spacelift, and enter your API host URL (the URL to your GitHub server), the App ID, and paste the contents of your private key file into the Private key box:</p> <p>Info</p> <p>If you are using <code>github.com</code> set your API host URL as:  https://api.github.com</p> <p></p> <p>Click on the Save button to save your integration settings.</p> <p>Congratulations! You are almost done! \ud83c\udf89</p> <p>The last step is to install the application you just created so that Spacelift can interact with GitHub. This is what the next section is about.</p>"},{"location":"integrations/source-control/github.html#installing-the-custom-application","title":"Installing the custom application","text":"<p>Now that you've created a GitHub App and configured it in Spacelift, the last step is to install your App in one or more accounts or organizations you have access to. To do this, go back to GitHub, find your App in the GitHub Apps page in your account settings, and click on the Edit button next to it:</p> <p></p> <p>Go to the Install App section, and click on the Install button next to the account your want Spacelift to access:</p> <p></p> <p>Choose whether you want to allow Spacelift access to all the repositories in the account, or only certain ones:</p> <p></p> <p>Congrats, you've just linked your GitHub account to Spacelift!</p>"},{"location":"integrations/source-control/github.html#using-github-with-stacks-and-modules","title":"Using GitHub with stacks and modules","text":"<p>If your Spacelift account is integrated with GitHub, the stack or module creation and editing forms will show a dropdown from which you can choose the VCS provider to use. GitHub will always come first, assuming that you've integrated it with Spacelift for a good reason:</p> <p></p> <p>The rest of the process is exactly the same as with creating a GitHub-backed stack or module, so we won't go into further details.</p>"},{"location":"integrations/source-control/github.html#team-based-access","title":"Team-based access","text":"<p>In order to spare you the need to separately manage access to Spacelift, you can reuse GitHub's native teams. If you're using GitHub as your identity provider (which is the default), upon login, Spacelift uses GitHub API to determine organization membership level and team membership within an organization and persists it in the session token which is valid for one hour. Based on that you can set up login policies to determine who can log in to your Spacelift account, and stack access policies that can grant an appropriate level of access to individual Stacks.</p> <p>Info</p> <p>The list of teams is empty for individual/private GitHub accounts.</p>"},{"location":"integrations/source-control/github.html#notifications","title":"Notifications","text":""},{"location":"integrations/source-control/github.html#commit-status-notifications","title":"Commit status notifications","text":"<p>Commit status notifications are triggered for proposed runs to provide feedback on the proposed changes to your stack - running a preview command (eg. <code>terraform plan</code> for Terraform) with the source code of a short-lived feature branch with the state and config of the stack that's pointing to another, long-lived branch. Here's what such a notification looks like:</p> <p>...when the run is in progress (initializing):</p> <p></p> <p>...when it succeeds without changes:</p> <p></p> <p>...when it succeeds with changes:</p> <p></p> <p>...and when it fails:</p> <p></p> <p>In each case, clicking on the Details link will take you to the GitHub check view showing more details about the run:</p> <p></p> <p>The Check view provides high-level information about the changes introduced by the push, including the list of changing resources, including cost data if Infracost is set up.</p> <p>From this view you can also perform two types of Spacelift actions:</p> <ul> <li>Preview - execute a proposed run against the tested commit;</li> <li>Deploy - execute a tracked run against the tested commit;</li> </ul>"},{"location":"integrations/source-control/github.html#pr-pre-merge-deployments","title":"PR (Pre-merge) Deployments","text":"<p>The Deploy functionality has been introduced in response to customers used to the Atlantis approach, where the deployment happens from within a Pull Request itself rather than on merge, which we see as the default and most typical workflow.</p> <p>If you want to prevent users from deploying directly from GitHub, you can add a simple plan policy to that effect, based on the fact that the run trigger always indicates GitHub as the source (the exact format is <code>github/$username</code>).</p> <pre><code>package spacelift\n\ndeny[\"Do not deploy from GitHub\"] {\n  input.spacelift.run.type == \"TRACKED\"\n  startswith(input.spacelift.run.triggered_by, \"github/\")\n}\n</code></pre> <p>The effect is as follows:</p> <p></p>"},{"location":"integrations/source-control/github.html#using-spacelift-checks-to-protect-branches","title":"Using Spacelift checks to protect branches","text":"<p>You can use commit statuses to protect your branches tracked by Spacelift stacks by ensuring that proposed runs succeed before merging their Pull Requests:</p> <p></p> <p>This is is an important part of our proposed workflow - please refer to this section for more details.</p>"},{"location":"integrations/source-control/github.html#deployment-status-notifications","title":"Deployment status notifications","text":"<p>Deployments and their associated statuses are created by tracked runs to indicate that changes are being made to the Terraform state. A GitHub deployment is created and marked as Pending when the planning phase detects changes and a tracked run either transitions to Unconfirmed state or automatically starts applying the diff:</p> <p></p> <p>If the user does not like the proposed changes during the manual review and discards the tracked run, its associated GitHub deployment is immediately marked as a Failure. Same happens when the user confirms the tracked run but the Applying phase fails:</p> <p></p> <p>If the Applying phase succeeds (fingers crossed!), the deployment is marked as Active:</p> <p></p> <p>The whole deployment history broken down by stack can be accessed from your repo's Environments section - a previously obscure feature that's recently getting more and more love from GitHub:</p> <p></p> <p>That's what it looks like for our test repo, with just a singe stack pointing at it:</p> <p></p> <p>GitHub deployment environment names are derived from their respective stack names. This can be customized by setting the <code>ghenv:</code> label on the stack. For example, if you have a stack named <code>Production</code> and you want to name the deployment environment <code>I love bacon</code>, you can set the <code>ghenv:I love bacon</code> label on the stack. You can also disable the creation of a GitHub deployments by setting the <code>ghenv:-</code> label on the stack.</p> <p>Info</p> <p>The Deployed links lead to their corresponding Spacelift tracked runs.</p>"},{"location":"integrations/source-control/github.html#pull-requests","title":"Pull Requests","text":"<p>In order to help you keep track of all the pending changes to your infrastructure, Spacelift also has a PRs tab that lists all the active Pull Request against your tracked branch. Each of the entries shows the current status of the change as determined by Spacelift, and a link to the most recent Run responsible for determining that status:</p> <p></p> <p>Note that this view is read-only - you can't change a Pull Request through here, but clicking on the name will take you to GitHub where you can make changes.</p> <p>Once a Pull Request is closed, whether with or merging or without merging, it disappears from this list.</p>"},{"location":"integrations/source-control/github.html#proposed-workflow","title":"Proposed workflow","text":"<p>In this section, we'd like to propose a workflow that has worked for us and many other DevOps professionals working with infrastructure-as-code. Its simplest version is based on a single stack tracking a long-lived branch like main, and short-lived feature branches temporarily captured in Pull Requests. A more sophisticated version can involve multiple stacks and a process like GitFlow.</p> <p>Tip</p> <p>These are mere suggestions and Spacelift will fit pretty much any Git workflow, but feel free to experiment and find what works best for you.</p>"},{"location":"integrations/source-control/github.html#single-stack-version","title":"Single stack version","text":"<p>Let's say you have a single stack called Infra. Let's have it track the default <code>master</code> branch in the repository called... <code>infra</code>. Let's say you want to introduce some changes - define an Amazon S3 bucket, for example. What we suggest is opening a short-lived feature branch, making your change there, and opening a Pull Request from that branch to <code>master</code>.</p> <p>At this point, a proposed run is triggered by the push notification, and the result of running <code>terraform plan</code> with the new code but existing state and config is reported to the Pull Request. First, we should ensure that the Pull Request does not get merged to master without a successful run, so we'd protect the branch by requiring a successful status check from your stack.</p> <p>Second, we can decide whether we just need a tick from Spacelift, or we'd rather require a manual review. We generally believe that more eyes is always better, but sometimes that's not practicable. Still, it's possible to protect the tracked branch in a way that requires manual Pull Request approval before merging.</p> <p>We're almost there, but let's also consider a scenario where our coworkers are also busy modifying the same stack. One way of preventing snafus as a team and get meaningful feedback from Spacelift is to require that branches are up to date before merging. If the current feature branch is behind the PR target branch, it needs to be rebased, which triggers a fresh Spacelift run that will ultimately produce the newest and most relevant commit status.</p>"},{"location":"integrations/source-control/github.html#multi-stack-version","title":"Multi-stack version","text":"<p>One frequent type of setup involves two similar or even identical environments - for example, staging and production. One approach would be to have them in a single repository but in different directories, setting <code>project_root</code> runtime configuration accordingly. This approach means changing the staging directory a lot and using as much or as little duplication as necessary to keep things moving, and a lot of commits will necessarily be no-ops for the production stack. This is a very flexible approach, and we generally like it, but it leaves Git history pretty messy and some people really don't like that.</p> <p>If you're in that group, you can create two long-lived Git branches, each linked to a different stack - the default <code>staging</code> branch linked to the staging stack, and a <code>production</code> branch linked to the production stack. Most development thus occurs on the staging branch and once the code is perfected there over a few iterations, a Pull Request can be opened from the <code>staging</code> to <code>production</code> branch, incorporating all the changes. That's essentially how we've seen most teams implement GitFlow. This approach keeps the history of the <code>production</code> branch clear and allows plenty of experimentation in the <code>staging</code> branch.</p> <p>With the above GitFlow-like setup, we propose protecting both <code>staging</code> and <code>production</code> branches in GitHub. To maximize flexibility, <code>staging</code> branch may require a green commit status from its associated stack but not necessarily a manual review. In the meantime, <code>production</code> branch should probably require both a manual approval and a green commit status from its associated stack.</p>"},{"location":"integrations/source-control/github.html#webhook-integrations","title":"Webhook integrations","text":"<p>Below is the list of some of the GitHub webhooks we subscribe to with a brief explanation of what we do with those.</p>"},{"location":"integrations/source-control/github.html#push-events","title":"Push events","text":"<p>Any time we receive a repository code push notification, we match it against Spacelift repositories and - if necessary - create runs. We'll also stop proposed runs that have been superseded by a newer commit on their branch.</p>"},{"location":"integrations/source-control/github.html#app-installation-created-or-deleted","title":"App installation created or deleted","text":"<p>When the Spacelift GitHub app is installed on an account, we create a corresponding Spacelift account. When the installation is deleted, we deleted the corresponding Spacelift account and all its data.</p>"},{"location":"integrations/source-control/github.html#organization-renamed","title":"Organization renamed","text":"<p>If a GitHub organization name is changed, we change the name of the corresponding account in Spacelift.</p> <p>Warning</p> <p>This is only applicable for accounts that were created using GitHub originally.</p>"},{"location":"integrations/source-control/github.html#pull-request-events","title":"Pull Request events","text":"<p>Whenever a Pull Request is opened or reopened, we generate a record in our database to show it on the Stack's PRs page. When it's closed, we delete that record. When it's synchronized (eg. new push) or renamed, we update the record accordingly. This way, what you see in Spacelift should be consistent with what you see in GitHub.</p>"},{"location":"integrations/source-control/github.html#pull-request-review-events","title":"Pull Request Review events","text":"<p>Whenever a review is added or dismissed from a Pull Request, we check whether a new run should be triggered based on any push policies attached to your stacks. This allows you to make decisions about whether or not to trigger runs based on the approval status of your Pull Request.</p>"},{"location":"integrations/source-control/github.html#repository-renamed","title":"Repository renamed","text":"<p>If a GitHub repository is renamed, we update its name in all the stacks pointing to it.</p>"},{"location":"integrations/source-control/github.html#github-action","title":"GitHub Action","text":"<p>You can use the Setup Spacectl GitHub Action to install our spacectl CLI tool to easily interact with Spacelift.</p>"},{"location":"integrations/source-control/github.html#git-checkout-support","title":"Git checkout support","text":"<p>By default Spacelift uses the GitHub API to download a tarball containing the source code for your stack or module. We are introducing experimental support for downloading the code using a standard Git checkout. If you would like to enable this for your stacks/modules, there are currently two options available:</p> <ol> <li>Add a label called <code>feature:enable_git_checkout</code> to each stack or module that you want to use Git checkout on. This allows you to test the new support without switching over all your stacks at once.</li> <li>Contact our support team and ask us to enable the feature for all stacks/modules in your account.</li> </ol>"},{"location":"integrations/source-control/github.html#unlinking-github-and-spacelift","title":"Unlinking GitHub and Spacelift","text":"Uninstalling the Marketplace applicationUninstalling the custom application <p>If you wish to uninstall the Spacelift application you installed from the GitHub Marketplace, go to the GitHub account settings and select the Applications menu item.</p> <p></p> <p>Click on the Configure button for the spacelift.io application.</p> <p></p> <p>Finally, click on the Uninstall button.</p> <p></p> <p>Go to the Developer settings of the GitHub account, then in the GitHub Apps section click on the Edit button for the Spacelift application.</p> <p></p> <p>On the page for the Spacelift application, go to the Advanced section and click on the Delete GitHub App button. Confirm by typing the name of the application and it is gone.</p> <p></p> <p>You can now remove the integration via the Unlink button on the Source Code page:</p> <p></p>"},{"location":"integrations/source-control/gitlab.html","title":"GitLab","text":"<p>Spacelift supports using GitLab as the source of code for your stacks and modules. While we support both managed (<code>gitlab.com</code>) and self-hosted GitLab installations just the same, only one GitLab server and its associated token can be used by a single Spacelift account.</p>"},{"location":"integrations/source-control/gitlab.html#setup-guide","title":"Setup Guide","text":"<p>In order to set up the GitLab integration from the Spacelift side, please navigate to the Settings page of Spacelift, select source code and click the Set up button next to the GitLab:</p> <p></p> <p></p> <p>This should open a form like this one:</p> <p></p> <p>In this step you will need to provide the API host URL of your GitLab server, the User facing host URL, and an API token generated for Spacelift to communicate with the GitLab API.</p>"},{"location":"integrations/source-control/gitlab.html#creating-an-access-token","title":"Creating an Access Token","text":"<p>Let's assume we don't have token handy, so let's navigate to our GitLab server (we'll just use <code>gitlab.com</code>) to create one from the Access Tokens section of your User Settings page:</p> <p></p> <p>Please give the personal access token a descriptive name and grant it <code>api</code> scope. Note that while we will only write commit statuses, merge request comments and environment deployments, GitLab's permissions are coarse enough to require us to take write on the whole thing.</p> <p>Warning</p> <p>Please note, when creating tokens bound to a GitLab user, the user is required to have \"Maintainer\" level access to any projects you require Spacelift to be able to access.</p>"},{"location":"integrations/source-control/gitlab.html#enabling-the-integration","title":"Enabling the Integration","text":"<p>Once you've created your personal API token, please pass it - along with the server API host - to the integration form in Spacelift and click the Save button:</p> <p></p> <p>Congrats, you've just linked your GitLab account to Spacelift. You should be taken to the integration settings page where you can retrieve the webhook data - secret and endpoint - which you will need to integrate Spacelift stacks with GitLab projects. Don't worry, this data will be accessible again to Spacelift admins, so there's no need to persist it separately:</p> <p></p> <p>Warning</p> <p>Unlike GitHub credentials which are specific to an organization rather than an individual, the GitLab integration uses personal credentials, which makes it more fragile in situations where an individual leaves the organization and deletes the access token.</p> <p>Thus, it may be a good idea to create \"virtual\" (machine) users in GitLab as a source of more stable credentials. Note however that this is a general concern, not one specific to Spacelift.</p>"},{"location":"integrations/source-control/gitlab.html#using-gitlab-with-stacks-and-modules","title":"Using GitLab with stacks and modules","text":"<p>If your Spacelift account is integrated with GitLab, the stack or module creation and editing forms will show a dropdown from which you can choose the VCS provider to use. GitLab will always come first, assuming that you've integrated it with Spacelift for a good reason:</p> <p></p> <p>The rest of the process is exactly the same as with creating a GitHub-backed stack or module, so we won't be going into further details.</p>"},{"location":"integrations/source-control/gitlab.html#setting-up-webhooks","title":"Setting up Webhooks","text":"<p>An important thing though is that for every GitLab project that's being used by a Spacelift project (stack or module), you will need to set up a webhook to notify Spacelift about the project changes. That's where you will use the webhooks data from the previous step - the URL and webhook secret.</p> <p>Spacelift is interested in pushes, tags and merge requests, so make sure you add triggers for all these types of events:</p> <p></p> <p>If that sounds like hassle (it sure does to us), you can do the same thing automatically using GitLab's Terraform provider.</p> <p>Warning</p> <p>Note that you only need to set it up one hook for each repo used by Spacelift, regardless of how many stacks it is used by. Setting up multiple hooks for a single repo may lead to unintended behavior.</p> <p>Regardless of whether you've created it manually or programmatically, once your project webhook is set up, your GitLab-powered stack or module is ready to use.</p>"},{"location":"integrations/source-control/gitlab.html#namespaces","title":"Namespaces","text":"<p>When utilizing the Terraform provider to provision Spacelift Stacks for GitLab, you are required to specify a <code>namespace</code>.</p> <p>The <code>namespace</code> value should be set to the the grouping mechanism that your project (repository) is within. For example, if you are simply referencing a project (repository) within your GitLab account, that is not within any group, then the namespace value should be set to your GitLab username.</p> <p>If your project lives within a group, then the namespace should be set to the group slug that the project is within. For example, if you have <code>project-a</code> within <code>group-1</code> the namespace would be <code>group-1</code>. When using subgroups, you will also need to include these within your namespace references.</p> <p>GitLab provides a Namespaces API which you can use to find information about your project's namespace. The <code>full_url</code> attribute value is what you'll want to reference as this namespace for a given project.</p>"},{"location":"integrations/source-control/gitlab.html#spacelift-in-gitlab","title":"Spacelift in GitLab","text":"<p>Spacelift provides feedback to GitLab in a number of ways.</p>"},{"location":"integrations/source-control/gitlab.html#commits-and-merge-requests","title":"Commits and merge requests","text":"<p>When a webhook containing a push or tag event is received by Spacelift, it may trigger a test run. Test runs provide feedback though GitLab's pipeline functionality. When viewed from a merge request, the pipeline looks like this:</p> <p></p> <p>You can see all the Spacelift jobs executed as part of this pipeline by clicking through to its dedicated view:</p> <p></p> <p>As you can see, the test job passed and gave some brief information about the proposed change - that - if applied - it would add a single resource.</p> <p>Also, for every merge request affected by the commit there will be a comment showing the exact change:</p> <p></p>"},{"location":"integrations/source-control/gitlab.html#environments","title":"Environments","text":"<p>Each Spacelift stack creates an Environment in GitLab where we report the status of each tracked run:</p> <p></p> <p>For example, this successful run:</p> <p></p> <p>...is thus reflected in its respective GitLab environment:</p> <p></p> <p>This functionality allows you to track Spacelift history directly from GitLab.</p>"},{"location":"integrations/source-control/raw-git.html","title":"Raw Git","text":"<p>Spacelift supports a whole range of version control systems but all of them require setup.</p> <p>This integration enables you to create stacks right from a publicly accessible Git repository, which also include GitHub gists.</p> <p>Info</p> <p>Using Raw Git gives you a one-way connection to the repository. Pushes, PRs, etc. are not supported.</p> <p>This provider is best suited for consuming public content.</p>"},{"location":"integrations/source-control/raw-git.html#using-raw-git-with-stacks-and-modules","title":"Using Raw Git with stacks and modules","text":"<p>The stack or module creation and editing forms will show a dropdown from which you can choose the Raw Git VCS provider. What's different from other VCS providers, you need to specify the repository URL.</p> <p></p> <p>The rest of the process is exactly the same as with creating a GitHub-backed stack or module, so we won't go into further details.</p>"},{"location":"integrations/source-control/raw-git.html#using-github-gists","title":"Using GitHub gists","text":"<p>To create a stack backed by a GitHub gist, you enter the gist's URL into the repository URL field.</p> <p></p>"},{"location":"integrations/source-control/raw-git.html#triggering-runs-on-updates","title":"Triggering runs on updates","text":"<p>This provider gives you a one-way connection to the repository. New runs won't get triggered automatically on updates to the repository.</p> <p>The run can be triggered manually, though.</p> <p></p> <ol> <li>You can use the Sync button to check for updates,</li> <li>If there are any updates since the last sync or creation of the stack, the tracked commit will change,</li> <li>You can use the Trigger button to invoke a run.</li> </ol>"},{"location":"legal/cookie-policy.html","title":"Cookie Policy","text":"<p>Last updated: June 10, 2022</p> <p>This Cookies Policy explains what Cookies are and how We use them. You should read this policy so You can understand what type of cookies We use, the information We collect using Cookies, and how that information is used.</p> <p>Cookies do not typically contain any information that personally identifies a user, but personal information that we store about You may be linked to the information stored in and obtained from Cookies. For further information on how We use, store and keep your personal data secure, see our Privacy Policy.</p> <p>We do not store sensitive personal information, such as mailing addresses, account passwords, etc., in the Cookies We use.</p>"},{"location":"legal/cookie-policy.html#interpretation-and-definitions","title":"Interpretation and Definitions","text":""},{"location":"legal/cookie-policy.html#interpretation","title":"Interpretation","text":"<p>The words in which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in the plural.</p>"},{"location":"legal/cookie-policy.html#definitions","title":"Definitions","text":"<p>For the purposes of this Cookies Policy:</p> <ul> <li>Company (referred to as either \"the Company\", \"We\", \"Us\", or \"Our\" in this Cookies Policy) refers to SPACELIFT, INC., 541 Jefferson Ave. Suite 100, Redwood City, CA 94063.</li> <li>Cookies are small files that are placed on Your computer, mobile device, or any other device by a website, containing details of your browsing history on that website among its many uses.</li> <li>Website refers to Spacelift.io, accessible from https://spacelift.io</li> <li>You mean the individual accessing or using the Website, or a company, or any legal entity on behalf of which such individual is accessing or using the Website, as applicable.</li> </ul>"},{"location":"legal/cookie-policy.html#the-use-of-the-cookies","title":"The use of the Cookies","text":""},{"location":"legal/cookie-policy.html#type-of-cookies-we-use","title":"Type of Cookies We Use","text":"<p>Cookies can be \"Persistent\" or \"Session\" Cookies. Persistent Cookies remain on your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close your web browser.</p> <p>We use both session, and persistent Cookies for the purposes set out below:</p> <ul> <li> <p>Necessary / Essential Cookies</p> <p>Type: Session Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.</p> </li> <li> <p>Cookies Policy / Notice Acceptance Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies identify if users have accepted the use of cookies on the Website.</p> </li> <li> <p>Functionality Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.</p> </li> <li> <p>Tracking and Performance Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Third-Parties</p> <p>Purpose: These Cookies are used to track information about traffic to the Website and how users use the Website. The information gathered via these Cookies may directly or indirectly identify you as an individual visitor. This is because the information collected is typically linked to a pseudonymous identifier associated with the device you use to access the Website. We may also use these Cookies to test new advertisements, pages, features or new functionality of the Website to see how our users react to them.</p> </li> <li> <p>Targeting and Advertising Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Third-Parties</p> <p>Purpose: These Cookies track your browsing habits to enable Us to show advertising which is more likely to be of interest to You. These Cookies use information about your browsing history to group You with other users who have similar interests. Based on that information, and with Our permission, third party advertisers can place Cookies to enable them to show adverts that We think will be relevant to your interests while You are on third-party websites.</p> </li> <li> <p>Social Media Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Third-Parties</p> <p>Purpose: In addition to Our own Cookies, We may also use various third parties Cookies to report usage statistics of the Website, deliver advertisements on and through the Website, and so on. These Cookies may be used when You share information using a social media networking website such as Facebook, Instagram, Twitter, or Google+.</p> </li> </ul>"},{"location":"legal/cookie-policy.html#your-choices-regarding-cookies","title":"Your Choices Regarding Cookies","text":"<p>If You prefer to avoid using Cookies on the Website, first, You must disable the use of Cookies in your browser and then delete the Cookies saved in your browser associated with this website. You may use this option to prevent Cookies use at any time.</p> <p>If You do not accept Our Cookies, You may experience some inconvenience in your use of the Website, and some features may not function properly.</p> <p>If You'd like to delete Cookies or instruct your web browser to delete or refuse Cookies, please visit the help pages of your web browser.</p> <ul> <li>For the Chrome web browser, please visit this page from Google: https://support.google.com/accounts/answer/32050</li> <li>For the Internet Explorer web browser, please visit this page from Microsoft: http://support.microsoft.com/kb/278835</li> <li>For the Firefox web browser, please visit this page from Mozilla: https://support.mozilla.org/en-US/kb/delete-cookies-remove-info-websites-stored</li> <li>For the Safari web browser, please visit this page from Apple: https://support.apple.com/guide/safari/manage-cookies-and-website-data-sfri11471/mac</li> </ul> <p>For any other web browser, please visit your web browser's official web pages.</p>"},{"location":"legal/cookie-policy.html#more-information-about-cookies","title":"More Information about Cookies","text":"<p>You can learn more about cookies: Cookies: What Do They Do?.</p>"},{"location":"legal/cookie-policy.html#contact-us","title":"Contact Us","text":"<p>If you have any questions about this Cookies Policy, You can contact us by email at privacy@spacelift.io.</p>"},{"location":"legal/privacy.html","title":"Privacy","text":"<p>Thank you for choosing to be part of our community at Spacelift (\u201ccompany\u201d, \u201cwe\u201d, \u201cus\u201d, or \u201cour\u201d). We are committed to protecting your personal information and your right to privacy. If you have any questions or concerns about our policy, or our practices with regards to your personal information, please contact us at privacy@spacelift.io.</p> <p>When you visit our and use our services, you trust us with your personal information. We take your privacy very seriously. In this privacy notice, we describe our privacy policy. We seek to explain to you in the clearest way possible what information we collect, how we use it and what rights you have in relation to it. We hope you take some time to read through it carefully, as it is important. If there are any terms in this privacy policy that you do not agree with, please discontinue use of our and our services.</p> <p>This privacy policy applies to all information collected through our and/or any related services, sales, marketing or events (we refer to them collectively in this privacy policy as the \"Sites\").</p> <p>Please read this privacy policy carefully as it will help you make informed decisions about sharing your personal information with us.</p>"},{"location":"legal/privacy.html#1-what-information-do-we-collect","title":"1. What information do we collect?","text":""},{"location":"legal/privacy.html#personal-information-you-disclose-to-us","title":"Personal information you disclose to us","text":"<p>We collect personal information that you voluntarily provide to us when registering at the expressing an interest in obtaining information about us or our products and services, when participating in activities on the or otherwise contacting us.</p> <p>The personal information that we collect depends on the context of your interactions with us and the site, the choices you make and the products and features you use. The personal information we collect can include the following:</p> <p>Name and Contact Data. We may collect your first and last name, email address, and other similar contact data from contact forms.</p> <p>Social Media Login Data. We provide you with the option to register using social media account details, like your GitHub. If you choose to register in this way, we will collect the Information described in the relevant section below.</p> <p>All personal information that you provide to us must be true, complete and accurate, and you must notify us of any changes to such personal information.</p>"},{"location":"legal/privacy.html#information-automatically-collected","title":"Information automatically collected","text":"<p>We automatically collect certain information when you visit, use or navigate the site. This information does not reveal your specific identity (like your name or contact information) but may include device and usage information, such as your IP address, browser and device characteristics, operating system, language preferences, referring URLs, device name, country, location, information about how and when you use our and other technical information. This information is primarily needed to maintain the security and operation of our site, and for our internal analytics and reporting purposes.</p> <p>Like many businesses, we also collect information through cookies and similar technologies.</p>"},{"location":"legal/privacy.html#2-will-your-information-be-shared-with-anyone","title":"2. Will your information be shared with anyone?","text":"<p>We may process or share data based on the following legal basis:</p> <ul> <li> <p>Consent: We may process your data if you have given us specific consent to use your personal information in a specific purpose.</p> </li> <li> <p>Legitimate Interests: We may process your data when it is reasonably necessary to achieve our legitimate business interests.</p> </li> <li> <p>Performance of a Contract: Where we have entered into a contract with you, we may process your personal information to fulfil the terms of our contract.</p> </li> <li> <p>Legal Obligations: We may disclose your information where we are legally required to do so in order to comply with applicable law, governmental requests, a judicial proceeding, court order, or legal process, such as in response to a court order or a subpoena (including in response to public authorities to meet national security or law enforcement requirements).</p> </li> <li> <p>Vital Interests: We may disclose your information where we believe it is necessary to investigate, prevent, or take action regarding potential violations of our policies, suspected fraud, situations involving potential threats to the safety of any person and illegal activities, or as evidence in litigation in which we are involved.</p> </li> </ul> <p>More specifically, we may need to process your data or share your personal information in the following situations:</p> <ul> <li> <p>Vendors, Consultants and Other Third-Party Service Providers. We may share your data with third party vendors, service providers, contractors or agents who perform services for us or on our behalf and require access to such information to do that work. Examples include: payment processing, data analysis, email delivery, hosting services, customer service and marketing efforts. We may allow selected third parties to use tracking technology on the , which will enable them to collect data about how you interact with the over time. This information may be used to, among other things, analyze and track data, determine the popularity of certain content and better understand online activity. Unless described in this Policy, we do not share, sell, rent or trade any of your information with third parties for their promotional purposes.</p> </li> <li> <p>Business Transfers. We may share or transfer your information in connection with, or during negotiations of, any merger, sale of company assets, financing, or acquisition of all or a portion of our business to another company.</p> </li> </ul>"},{"location":"legal/privacy.html#3-do-we-use-cookies-and-other-tracking-technologies","title":"3. Do we use cookies and other tracking technologies?","text":"<p>We may use cookies and similar tracking technologies (like web beacons and pixels) to access or store information. Specific information about how we use such technologies and how you can refuse certain cookies is set out in our Cookie Policy.</p>"},{"location":"legal/privacy.html#4-how-do-we-handle-your-social-logins","title":"4. How do we handle your social logins?","text":"<p>Our offer you the ability to register and login using your third party social media account details (like your GitHub login). Where you choose to do this, we will receive certain profile information about you from your social media provider. The profile Information we receive may vary depending on the social media provider concerned, but will often include your name, e-mail address, organization membership, profile picture as well as other information you choose to make public.</p> <p>We will use the information we receive only for the purposes that are described in this privacy policy or that are otherwise made clear to you on the site. Please note that we do not control, and are not responsible for, other uses of your personal information by your third party social media provider. We recommend that you review their privacy policy to understand how they collect, use and share your personal information, and how you can set your privacy preferences on their sites and apps.</p>"},{"location":"legal/privacy.html#5-is-your-information-transferred-internationally","title":"5. Is your information transferred internationally?","text":"<p>Our servers are located in Ireland. If you are accessing our site from outside, please be aware that your information may be transferred to, stored, and processed by us in our facilities and by those third parties with whom we may share your personal information, in and other countries.</p> <p>If you are a resident in the European Economic Area, then these countries may not have data protection or other laws as comprehensive as those in your country. We will however take all necessary measures to protect your personal information in accordance with this privacy policy and applicable law.</p>"},{"location":"legal/privacy.html#6-how-long-do-we-keep-your-information","title":"6. How long do we keep your information?","text":"<p>We will only keep your personal information for as long as it is necessary for the purposes set out in this privacy policy, unless a longer retention period is required or permitted by law (such as tax, accounting or other legal requirements). No purpose in this policy will require us keeping your personal information for longer than 30 days.</p> <p>When we have no ongoing legitimate business need to process your personal information, we will either delete or anonymize it, or, if this is not possible (for example, because your personal information has been stored in backup archives), then we will securely store your personal information and isolate it from any further processing until deletion is possible.</p>"},{"location":"legal/privacy.html#7-how-do-we-keep-your-information-safe","title":"7. How do we keep your information safe?","text":"<p>We have implemented appropriate technical and organisational security measures designed to protect the security of any personal information we process. However, please also remember that we cannot guarantee that the internet itself is 100% secure. Although we will do our best to protect your personal information, transmission of personal information to and from our is at your own risk. You should only access the services within a secure environment.</p>"},{"location":"legal/privacy.html#8-do-we-collect-information-from-minors","title":"8. Do we collect information from minors?","text":"<p>We do not knowingly solicit data from or market to children under 13 years of age. By using the , you represent that you are at least 13 or that you are the parent or guardian of such a minor and consent to such minor dependent\u2019s use of the site. If we learn that personal information from users less than 18 years of age has been collected, we will deactivate the account and take reasonable measures to promptly delete such data from our records. If you become aware of any data we have collected from children under age 13, please contact us at privacy@spacelift.io.</p>"},{"location":"legal/privacy.html#9-what-are-your-privacy-rights","title":"9. What are your privacy rights?","text":"<p>In some regions (like the European Economic Area), you have certain rights under applicable data protection laws. These may include the right (i) to request access and obtain a copy of your personal information, (ii) to request rectification or erasure; (iii) to restrict the processing of your personal information; and (iv) if applicable, to data portability. In certain circumstances, you may also have the right to object to the processing of your personal information. To make such a request, please use the contact details provided below. We will consider and act upon any request in accordance with applicable data protection laws.</p> <p>If we are relying on your consent to process your personal information, you have the right to withdraw your consent at any time. Please note however that this will not affect the lawfulness of the processing before its withdrawal.</p> <p>If you are resident in the European Economic Area and you believe we are unlawfully processing your personal information, you also have the right to complain to your local data protection supervisory authority. You can find their contact details here: http://ec.europa.eu/justice/data-protection/bodies/authorities/index_en.htm</p>"},{"location":"legal/privacy.html#account-information","title":"Account Information","text":"<p>If you would at any time like to review or change the information in your account or terminate your account, you can:</p> <p>Upon your request to terminate your account, we will deactivate or delete your account and information from our active databases. However, some information may be retained in our files to prevent fraud, troubleshoot problems, assist with any investigations, enforce our Terms of Use and/or comply with legal requirements.</p>"},{"location":"legal/privacy.html#10-do-california-residents-have-specific-privacy-rights","title":"10. Do California residents have specific privacy rights?","text":"<p>California Civil Code Section 1798.83, also known as the \u201cShine The Light\u201d law, permits our users who are California residents to request and obtain from us, once a year and free of charge, information about categories of personal information (if any) we disclosed to third parties for direct marketing purposes and the names and addresses of all third parties with which we shared personal information in the immediately preceding calendar year. If you are a California resident and would like to make such a request, please submit your request in writing to us using the contact information provided below.</p> <p>If you are under 18 years of age, reside in California, and have a registered account with the site, you have the right to request removal of unwanted data that you publicly post on the site. To request removal of such data, please contact us using the contact information provided below, and include the email address associated with your account and a statement that you reside in California. We will make sure the data is not publicly displayed on the , but please be aware that the data may not be completely or comprehensively removed from our systems.</p>"},{"location":"legal/privacy.html#11-how-can-you-contact-us-about-this-policy","title":"11. How can you contact us about this policy?","text":"<p>If you have questions or comments about this policy, you may email us at privacy@spacelift.io or by post to:</p> <p>Spacelift, Inc. 541 Jefferson Ave. Suite 100 Redwood City CA 94063</p> <p>For GDPR related:</p> <p>Spacelift Podbipi\u0119ty 47A 02-737 Warsaw Poland</p>"},{"location":"legal/privacy.html#12-how-can-you-review-update-or-delete-the-data-we-collect-from-you","title":"12. How can you review, update, or delete the data we collect from you?","text":"<p>Based on the laws of some countries, you may have the right to request access to the personal information we collect from you, change that information, or delete it in some circumstances. To request to review, update, or delete your personal information, please email us at privacy@spacelift.io. We will respond to your request within 30 days.</p>"},{"location":"legal/refund-policy.html","title":"Refund Policy","text":""},{"location":"legal/refund-policy.html#general-provisions","title":"GENERAL PROVISIONS","text":"<p>Refunds will be made on the terms and conditions set out in this Policy in the event that the Customer withdraws from the Software purchase Agreement.</p>"},{"location":"legal/refund-policy.html#definitions","title":"DEFINITIONS","text":"<p>The following definitions are set out for the purposes of this Policy:</p> <ol> <li>Policy - these regulations for making refunds.</li> <li>Company \u2013 Spacelift, Inc., a Delaware corporation with offices located at 541 Jefferson Ave. Suite 100, Redwood City, CA 94063.</li> <li>Customer - an entity who purchases software from the Company on the basis of the Agreement.</li> <li>Software - software offered by the Company, purchased by the Customer, which is the subject of the Agreement.</li> <li>Agreement - the agreement between the Company and the Customer concerning the purchase of the Software.</li> <li>Funds - funds paid by the Customer to the Company under the Agreement for the purchase of the Software.</li> </ol>"},{"location":"legal/refund-policy.html#refund-process","title":"REFUND PROCESS","text":"<p>The Customer may withdraw from the Agreement and claim a refund of funds within 14 days after the completion of the purchase of the Software provided that the purchased Software has not been activated during that period.</p> <p>Upon receipt of notification of withdrawal, access to the Software will be blocked and verification of compliance with the conditions described in Section 3 above will begin. The verification process may take up to 10 days.</p> <p>In the case of positive verification, referred to in Section 4 above, the procedure of returning the Funds will be initiated.</p> <p>The Funds will be returned using the original payment method.</p> <p>The time taken to return the Funds may be affected by the operation of the payment provider.</p>"},{"location":"legal/refund-policy.html#final-provisions","title":"FINAL PROVISIONS","text":"<p>The current version of the Policy is available here</p> <p>This Policy is valid from 1st November 2021.</p> <p>In matters not regulated by this Policy, the Terms and Conditions shall apply.</p>"},{"location":"legal/terms.html","title":"Terms and Conditions","text":"<p>Last updated: March 7, 2023</p> <p>This Terms and Conditions is between the entity you represent, or, if you do not indicate an entity in connection with the Services, you individually (\u201cClient\u201d, \u201cyou\u201d or \u201cyour\u201d), and Spacelift, Inc. with its principal office at 541 Jefferson Ave. Suite 100, Redwood City CA 94063, United States of America (\u201cSpacelift\u201d, \u201cthe Company\u201d \u201cwe\u201d, \u201cus\u201d, or \u201cour\u201d). It consists of the terms and conditions below, the Privacy Policy, the Refund Policy and the Cookie Policy (together, the \u201cAgreement\u201d).</p> <p>The Agreement does not govern the use of:</p> <ul> <li>the Services under Enterprise Plan provided that a separate and mutually agreed contract has been executed,</li> <li>the Services purchased via AWS Marketplace, which are subject to Standard Contract for AWS Marketplace and any amendments,</li> <li>any self-hosted services provided by Spacelift, Inc. which are subject to a separate agreement.</li> </ul>"},{"location":"legal/terms.html#1-definitions","title":"1. DEFINITIONS","text":"<p>1.1. \u201cAuthorized User\u201d means Client\u2019s employees, consultants, contractors, agents, and Workers (a) who Client authorizes to access and use the Services under the rights granted to Client under this Agreement; and (b) for whom access to the Services has been purchased hereunder.</p> <p>1.2. \u201cClient Data\u201d means information, data, and other content, in any form or medium, that is collected, downloaded, or otherwise received from the Client or an Authorized User by or through the Services. For the avoidance of doubt, Client Data does not include Resultant Data or any other information reflecting the access or use of the Services by or on behalf of Client or any Authorized User.</p> <p>1.3. \u201cConfidential Information\u201d means all nonpublic information, including, but not limited to, source code, software, trade secrets, know-how, technical drawings, algorithms, ideas, inventions, other technical, business or sales information, negotiations or proposals, disclosed by us in whatever form and which is known by the Client or its Authorized User to be confidential or under circumstances by which the receiving party should reasonably understand such information is to be treated as confidential, whether or not marked as \u201cConfidential\u201d.</p> <p>1.4. \u201cDocumentation\u201d means any manuals, instructions, including technical specifications, or other documents or materials describing the features and functionality of the Services, which are located on Website or provided to Clients, and may be updated from time to time.</p> <p>1.5. \u201cIntellectual Property Rights\u201d or \u201cIPR\u201d means any registered and unregistered rights granted, applied for, or otherwise now or hereafter in existence under or related to any patent, copyright, trademark, trade secret, database protection, or other intellectual property rights laws in any part of the world.</p> <p>1.6. \u201cMetrics\u201d - means the measurements used for quantifying the Services usage with the following meaning:</p> <p>\u00a0\u00a0\u00a0\u00a0\u200b1.6.1. \u201cPrivate Minute(s)\u201d means minute(s) of Private Workers\u2019 Services usage in a given month;</p> <p>\u00a0\u00a0\u00a0\u00a01.6.2. \u201cPublic Minutes\u201d means minutes of Public Worker\u2019s Services usage in a given month;</p> <p>\u00a0\u00a0\u00a0\u00a016.3. \u201cSeat(s)\u201d means an Authorized User(s) who actively logged in to the Services in a given month;</p> <p>\u00a0\u00a0\u00a0\u00a01.6.4. \u201cWorker(s)\u201d means a predefined set(s) of computing resources that are specifically optimized for the development and provisioning and deployment of cloud-based infrastructures based on IaC; a Worker can be either a self-hosted agent performing infrastructure management in a Client-controlled environment (\u201cPrivate Worker\u201d) or any other software agent, provided and managed by Spacelift in a common secure worker pool (\u201cPublic Worker\u201d).</p> <p>1.7. \u201cServices\u201d means the Spacelift\u2019s specialized, continuous integration and deployment (CI/CD) platform for infra-as-code available at https://spacelift.io as SaaS;</p> <p>1.8. \u201cSubscription\u201d means enrollment for Services for a Subscription Plan on Subscription Terms as defined in the Agreement;</p> <p>1.9. \u201cSubscription Plan(s)\u201d means available Subscription offer(s) for use of the Services as described on the Website, including Trial, Free, Cloud, and Enterprise;</p> <p>1.10. \u201cSubscription Term(s)\u201d means the conditions under which a Subscription is made under the Agreement, including the Subscription Period, Metrics, and Subscription Fees, as described in Section 5;</p> <p>1.11. \u201cWebsite\u201d means https://spacelift.io website managed by Spacelift.</p>"},{"location":"legal/terms.html#2-general","title":"2. GENERAL","text":"<p>2.1. Execution of the Agreement. Accepting this Agreement is a condition of using the Services provided by Spacelift. \u200b\u200bBY COMPLETING THE REGISTRATION PROCESS, ACCESSING OR USING THE SERVICES, YOU ACKNOWLEDGE AND AGREE THAT (I) YOU HAVE READ, UNDERSTOOD, AND ACCEPTED THIS AGREEMENT, AND (II) YOU HEREBY REPRESENT AND WARRANT THAT YOU ARE AUTHORIZED TO ENTER OR ACT ON BEHALF OF THE CLIENT, AND BIND TO THIS AGREEMENT. If you do not have the legal authority to enter this Agreement, do not understand this Agreement, or do not agree to the Agreement, you should not accept the Terms and Conditions, or use the Services.</p> <p>2.2. Conflict of Provisions. In the event of any inconsistencies or conflict between the documents that make up this Agreement, the documents will prevail in the following order: (a) any written amendment agreed upon by the parties (such as order forms); (b) Privacy Policy; (c) these Terms and Conditions and (d) the Refund Policy.</p> <p>2.3. Compliance. You are responsible for (a) compliance with the provisions of the Agreement by you and your Authorized Users and for any and all acts and omissions of Authorized Users connected with their use and access to the Services and for any breach of this Agreement by Authorized Users; and (b) any delay or failure of performance caused in whole or in part by your delay in performing, or failure to perform, any of your obligations under this Agreement. Without limiting the foregoing, you are solely responsible for ensuring that your use of the Services is compliant with all applicable laws and regulations, as well as any and all privacy policies, agreements, or other obligations you may maintain or enter into.</p> <p>2.4. Amendments. Any individual amendment to this Agreement must be made in writing (expressly stating that it is amending this Agreement) and signed by both parties.</p>"},{"location":"legal/terms.html#3-license-intellectual-property-rights-and-ownership","title":"3. LICENSE, INTELLECTUAL PROPERTY RIGHTS, AND OWNERSHIP","text":"<p>3.1. Ownership. The Services, Documentation, and Website, all copies and portions thereof, and all IPR therein, including, but not limited to source code, databases, functionality, software, website designs, audio, video, text, photographs, graphics, or derivative works therefrom, are owned by us or licensed to us. You are not authorized to use (and will not permit any third party to use) the Services, Website, Documentation, or any portion thereof except as expressly authorized by this Agreement. Specifically, no part of the Services, Documentation, or Website may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, modified, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission.</p> <p>3.2. Confidential Information. All our Confidential Information and derivations thereof will remain our sole and exclusive property. You should not disclose, use or publish Confidential Information without our prior written consent. You must hold all our Confidential Information in strict confidence and safeguard the Confidential Information from unauthorized use, access, or disclosure using at least the degree of care you use to protect your similarly sensitive information and in no event less than a reasonable degree of care.</p> <p>3.3. License. Spacelift makes the Services available to you during the Subscription Period, subject to the terms and conditions of this Agreement and Subscription Terms. Spacelift grants a limited, non-exclusive, non-sublicensable, non-transferable right to access and use the Services and its Documentation during the Subscription Period, solely for your internal business purposes or your personal use.</p> <p>3.4. Restrictions. You will not, and will not permit any other person to, access or use the Services except as expressly permitted by this Agreement. For purposes of clarity and without limiting the generality of the foregoing, you will not, except as this Agreement expressly permits: (a) copy, modify, or create derivative works or improvements of the Services or Documentation; (b) rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer, or otherwise make available any Services or Documentation to any person; (c) reverse engineer, disassemble, decompile, decode, adapt, or otherwise attempt to derive or gain access to the source code of the Services, in whole or in part; (d) bypass or breach any security device or protection used by the Services or access or use the Services other than by an Authorized User through the use of his or her own then valid access credentials; (e) input, upload, transmit, or otherwise provide to or through the Services or Documentation, any information or materials that are unlawful or injurious, or contain, transmit, or activate any harmful code; (f) damage, destroy, disrupt, disable, impair, interfere with, or otherwise impede or harm in any manner the Services or Documentation, or our provision of services to any third party, in whole or in part; (g) remove, delete, alter, or obscure any trademarks, Documentation, warranties, or disclaimers, or IPR notices from any Services; (h) access or use the Services or Documentation in any manner or for any purpose that infringes, misappropriates, or otherwise violates any IPR or other right of any third party or that violates any applicable law; or (i) access or use the Services or Documentation for purposes of competitive analysis of the Services, the development, provision, or use of a competing software service or product.</p> <p>3.5. Client Data. You are and will remain the sole and exclusive owner of all rights, title, and interest in and to all Client Data, including all IPR, subject to the rights and permissions granted in the Agreement. You have exclusive control and responsibility for determining what data you submit to the Services, for obtaining all necessary consents and permissions for the submission of Client Data, and for the accuracy, quality, and legality of Client Data.</p> <p>3.6. Consent to Use Client Data. You irrevocably grant all such rights and permissions in or relating to Client Data as are reasonably necessary or useful to us to enforce this Agreement and exercise our rights and perform our obligations hereunder.</p> <p>3.7. Use of Resultant Data. We may collect data and information related to your use of the Services that is used by us in an aggregate manner, including to compile statistical and performance information related to the provision and operation of the Services (\u201cResultant Data\u201d). You unconditionally and irrevocably grant to us an assignment of all rights, title, and interest in and to the Resultant Data, including all IPR relating thereto, if any.</p>"},{"location":"legal/terms.html#4-provision-of-services","title":"4. PROVISION OF SERVICES","text":"<p>4.1. Metrics. Use of the Services is subject to usage limits reflected in Metrics, as set forth in the Subscription Plan. We will monitor your use of the Services in order to verify whether you comply with the presented limits.</p> <p>4.2. Services Modifications. We reserve the right to make any changes to the Services or Documentation that we deem necessary or useful to: (a) maintain or enhance: (i) the quality or delivery of Services to you and other clients; (ii) the competitive strength of or market for Services; or (iii) the Services' cost efficiency or performance; or (b) to comply with applicable law.</p> <p>4.3. Privacy. When applicable, we will comply with all applicable laws, regulations, and government orders relating to personally identifiable information and data privacy with respect to any such Client Data that we receive or have access to under the Agreement or in connection with the performance of the Services. In particular, regulations for the protection of personally identifiable information are indicated in the Privacy Policy incorporated herein by reference.</p> <p>4.4. Access and Security. You will employ all physical, administrative, and technical controls, screening, security procedures, and other safeguards necessary to: (a) securely administer the distribution and use of all access credentials and protect against any unauthorized access to or use of the Services; and (b) control the content and use of Client Data, including the uploading or other provision of Client Data for processing by the Services.</p> <p>4.5. Security. We maintain industry-standard security and privacy certification, such as a SOC II certification. We will use appropriate technical and organizational measures designed to prevent unauthorized access, use, alteration, or disclosure of the Services or Client Data.</p> <p>4.6. Incidents. We will notify you in case of any security incident as soon as possible, provided that you have indicated your contact data in the Services under the address: https://*.app.spacelift.io/settings/security (* being the domain name chosen by you to access Services).</p> <p>4.7. Downtimes. We will use commercially reasonable efforts to give you at least 24 hours prior notice of all scheduled outages of the Services. You can check the current Services\u2019 availability status at https://spacelift.statuspage.io/</p> <p>4.8. Services and Website Management. We reserve the right at our sole discretion, to (a) monitor the Services for breaches of the Agreement; (b) take appropriate legal action against anyone in breach of applicable laws or the Agreement; (c) refuse, restrict access to, or availability of, or disable (to the extent technologically feasible) any of Client Data; (d) remove from the Services or Website or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (e) otherwise manage the Website and Services in a manner designed to protect our rights and property and to facilitate the proper functioning of the Website and Services.</p> <p>4.9. Third-Party Materials. The Website and/or Services may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third-party websites or applications or the third-party operator. We are not responsible for and do not endorse any third-party websites or applications or their availability or content.</p> <p>4.10. Access by Third-Party Accounts. You may register and login to the Services using your third-party service providers account details, like a Google or GitHub account (\u201cThird-Party Account\u201d). When you do so, we will receive certain profile information varying on the identity provider and the information you decided to include in your Third-Party Account. You will have the ability to disable the connection between your Services account and your Third Party Accounts at any time. Please note that your relationship with the third-party service providers associated with your Third-Party Accounts is governed solely by your agreement(s) with such third-party service providers. If a Third Party Account or associated service becomes unavailable or the access to such Third Party Account is terminated by the third-party service provider, then your access using such Third Party Account may no longer be available.</p> <p>4.11. Support Services. During the Subscription Period, we will provide support services depending on the Subscription Plan, as described in https://docs.spacelift.io/product/support/</p> <p>4.12. Client Systems and Cooperation. You will at all times during the period of Subscription: (a) set up, maintain, and operate in good repair and in accordance with the Documentation all your systems (meaning information technology infrastructure, including computers, software, databases, electronic systems, database management systems, and networks) on or through which the Services are accessed or used; (b) provide us with such access to your data or systems as is necessary for us to perform the Services in accordance with the Agreement and Documentation; (c) use reasonable measures to prevent and promptly notify us of any unauthorized access to Authorized User accounts of which you become aware of, and (d) provide all cooperation and assistance as we may reasonably request to enable us to exercise our rights and perform our obligations under and in connection with this Agreement.</p> <p>4.13. Quality of Services. You are aware that the quality of the Services and your use of the Services might be affected by a number of factors outside our control, including but not limited to force majeure, technical conditions, hardware or software (including third-party software and network) issues. Any delay or default affecting the availability, functionality, correctness, or timely performance of the Services caused by such circumstances will not constitute a breach of the Agreement.</p> <p>4.14. No Data Backup. We do not store or backup any Client Data. The Services do not replace the need for you to maintain regular data backups or redundant data archives. We have no obligation or liability for any loss, alteration, destruction, damage, corruption, or recovery of Client Data.</p> <p>4.15. Disclaimer. The content on the Website is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Website.</p>"},{"location":"legal/terms.html#5-subscription-plans-and-terms","title":"5. SUBSCRIPTION PLANS AND TERMS","text":"<p>5.1. Effective Date and Term. This Agreement commences on the effective date being the day of your registration, access to, or use of the Services, whichever happens first (\u201cEffective Date\u201d). Unless earlier terminated pursuant to the terms of this Section, the Agreement will continue through the Subscription Period of a chosen Subscription Plan.</p> <p>5.2. Subscription Plans. The Services are available under the following Subscription Plans with the relevant Subscription Terms:</p> Plan Subscription Period Termination Subscription Fee Trial 14 days Any time None Free Non-definite term Any time None Cloud Default: 1 month, monthly renewal.Individual arrangements may include an annual or  a multi-year Subscription Period. Any time during the Subscription Period, having its effect on the last day of the given Subscription Period. Current fees are set forth in https://spacelift.io/pricing Enterprise As agreed by the parties in a separate agreement or order form As agreed by the parties in a separate agreement or order form As agreed by the parties in a separate agreement or order form <p>5.3. The Trial Plan. The Subscription Terms for the Trial Plan are as follows:</p> <p>\u00a0\u00a0\u00a0\u00a05.3.1. Scope and Metrics. The Trial Plan offers access to Services to get to know Services before starting the Free, Cloud, or Enterprise Plan. Any usage limitations are indicated on https://spacelift.io/pricing.</p> <p>\u00a0\u00a0\u00a0\u00a05.3.2. Subscription Period. The Trial Plan expires 14 days after your registration to the Services.</p> <p>\u00a0\u00a0\u00a0\u00a05.3.3. Termination. You may cancel the Trial Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io.</p> <p>\u00a0\u00a0\u00a0\u00a05.3.4. Next Steps. Upon the end of Trial Plan, you may: (a) stop using the Services and delete your account thus terminating the Agreement, (b) continue to use the Services under Free Plan, (c) subscribe to Cloud Plan if you provide payment details to make a Subscription Fee according to the currently effective rates presented on https://spacelift.io/pricing, or (d) contact sales@spacelift.io to discuss the Enterprise Plan which is subject to separate agreement.</p> <p>5.4. The Free Plan. The Subscription Terms for the Free Plan are as follows:</p> <p>\u00a0\u00a0\u00a0\u00a05.4.1. Scope and Metrics. The Free Plan offers access to the Services with usage limitations indicated on https://spacelift.io/pricing. In order to expand the usage limitations, upgrade to Cloud Plan or Enterprise Plan.</p> <p>\u00a0\u00a0\u00a0\u00a05.4.2. Subscription Period and Termination. The Free Plan is available for an indefinite period of time and might be terminated at any time. You may cancel the Free Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io.</p> <p>5.5. The Cloud Plan. The Subscription Terms for the Cloud Plan are as follows:</p> <p>\u00a0\u00a0\u00a0\u00a05.5.1. Scope and Metrics. The Cloud Plan offers access to the Services with usage limitations indicated on https://spacelift.io/pricing. This plan can be supplemented with additional Seats and/or Workers as required, at an additional cost calculated on the basis of current rates.</p> <p>\u00a0\u00a0\u00a0\u00a05.5.2. Subscription Period and Billing. If you activate the Cloud Plan, you authorize Spacelift to periodically charge, on a going-forward basis and until cancellation of either the recurring payments or your account, all accrued sums on or before the payment due date for the accrued sums. The \"Subscription Billing Date\" is the date when you purchase your first Subscription to the Services. Your account will be charged automatically on the Subscription Billing Date for all applicable fees for the next Subscription Period. The subscription will continue unless and until you cancel your Subscription or we terminate it. You must cancel your Subscription before it renews in order to avoid billing the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide to us during registration (or to a different payment method if you change your payment information).</p> <p>\u00a0\u00a0\u00a0\u00a05.5.3. Termination. You may cancel the Cloud Plan at any time by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io. The termination will be effective on the last day of the given Subscription Period.</p> <p>5.6. Withdrawal and Refund. You may withdraw from the Agreement and claim a refund of funds within 14 days after its execution provided that the Services have not been activated during that period. You can find all the details regarding the refund in our Refund Policy.</p> <p>5.7. Plan Adjustments and Upgrades. The Subscription Terms for each plan, including Subscription Period, Fees, and Metrics may be adjusted by written agreement of the parties. If you wish to upgrade your Subscription Plan, contact sales@spacelift.io to discuss the available options.</p> <p>5.8. The Enterprise Plan. In most cases, Subscription Terms of the Enterprise Plan are individually discussed by the parties and bind the parties on the basis of a separately executed agreement. In case a separate agreement is not executed between the parties, the written arrangements (such as order forms) regarding Subscription Terms apply and in the remaining scope terms for the use of the Services will be subject to conditions set forth in this Agreement.</p> <p>5.9. Services Usage during Negotiations. If you wish to actively use the Services in the course of negotiating the separate agreement, the parties may agree on the temporary terms of use of the Services, including the relevant Metrics, period, and fees, and in the remaining scope terms for the use of the Services will be subject to conditions set forth in this Agreement.</p>"},{"location":"legal/terms.html#6-subscription-fees","title":"6. SUBSCRIPTION FEES","text":"<p>6.1. Terms of Payment. Unless otherwise agreed by the parties, Subscription Fees will be payable in USD via a credit card on a going-forward basis and will be subject to this Section 6.</p> <p>6.2. Taxes. All Subscription Fees and other amounts payable by you under this Agreement are exclusive of taxes and similar assessments. Without limiting the foregoing, you are responsible for all sales, use, excise taxes, and any other similar taxes, duties, and charges of any kind, other than any taxes imposed on our income.</p> <p>6.3. Late Payment. If you fail to make any payment when due then, in addition to all other remedies that may be available: (a) we may charge interest on the past due amount at the rate of 1.5% per month calculated daily and compounded monthly or, if lower, the highest rate permitted under applicable law; (b) you will reimburse us for all reasonable costs incurred by us in collecting any late payments or interest, including attorneys' fees, court costs, and collection agency fees; and (c) if such failure continues for fourteen (14) days following written notice, we may suspend performance of the Services until all past due amounts and interest have been paid, without incurring any obligation or liability to you or any other person by reason of such suspension.</p> <p>6.4. Subscription Fees Increases. Separately from any changes in Subscription Fees due to the upgrade of the relevant Metrics, we may increase Fees for any Subscription Period before its start by providing you a notice prior to the commencement of the next Subscription Period. Your continued use of the Services constitutes your acceptance of such changed Subscription Fees.</p>"},{"location":"legal/terms.html#7-suspension-and-termination","title":"7. SUSPENSION AND TERMINATION","text":"<p>7.1. Suspension. Without limiting any other provision of the Agreement, we reserve the right to, in our sole discretion and without notice or liability, deny access to and use of the Services (including blocking certain IP addresses), to any person for any reason including but not limited to (a) proven or suspected breach of any representation, warranty or covenant contained in the Agreement or of any applicable law or regulation; (b) your use of the Services poses a risk to the Services, our other customers, or us (including our infrastructure, security, and third-party relationships); (c) your use of the Services could subject us to liability or (d) you are past due in the payment of Subscription Fee. We will provide you with prompt notice of any suspension.</p> <p>7.2. Effect of Suspension. If we suspend your access to the Services for any reason set out in the Agreement, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.</p> <p>7.3. Termination for Cause. Notwithstanding the termination for convenience as described in Section 5, Either Party may terminate this Agreement, effective on written notice to the other party, if the other party (a) materially breaches this Agreement, and such breach: (i) is incapable of cure; or (ii) being capable of cure, remains uncured 30 days after the non-breaching party provides the breaching party with written notice of such breach; (b) becomes insolvent or is generally unable to pay, or fails to pay, its debts as they become due; (c) files, or has filed against it, a petition for voluntary or involuntary bankruptcy or otherwise becomes subject, voluntarily or involuntarily, to any proceeding under any domestic or foreign bankruptcy or insolvency law; (d) makes or seeks to make a general assignment for the benefit of its creditors; or (e) applies for or has appointed a receiver, trustee, custodian, or similar agent appointed by order of any court of competent jurisdiction to take charge of or sell any material portion of its property or business.</p> <p>7.4. Effect of Termination. Upon any termination of this Agreement, except as expressly otherwise provided in this Agreement: (a) all rights, licenses, consents, and authorizations granted by either party to the other will immediately terminate; (b) we will immediately cease all use of any Client Data and at your request destroy, all documents and tangible materials containing or based on Client Data and erase all Client Data from all our systems, provided that, for clarity, our obligations under this Section 7.4 do not apply to any Resultant Data or other data that is required to establish proof of a right or a contract, which will be stored for the duration provided by enforceable law; (c) you will immediately cease all use of any Services and within sixty (60) days destroy, all documents and tangible materials containing or based on any our materials, including Documentation and erase all our materials from the systems you directly or indirectly control. You acknowledge and agree that you are responsible to retrieve Client Data from the Services prior to the termination of this Agreement.</p> <p>7.5. Surviving Terms. The provisions set forth in the following sections, and any other right or obligation of the parties in this Agreement that, by its nature, should survive termination or expiration of this Agreement, will survive any expiration or termination of this Agreement: 3.1, 3.2, 3.4, 3.7, 7.4, 7.5, 8.4, 9, 10, 12.</p>"},{"location":"legal/terms.html#8-representations-and-warranties","title":"8. REPRESENTATIONS AND WARRANTIES","text":"<p>8.1. Mutual Representations and Warranties. Each party represents and warrants to the other party that it has the full right, power, and authority to enter into and perform its obligations and grant the rights, licenses, consents, and authorizations it grants or is required to grant under this Agreement.</p> <p>8.2. Additional Spacelift Representations, Warranties, and Covenants. We represent, warrant, and covenant to you that (a) we will perform the Services using personnel of required skill, experience, and qualifications and in a professional and workmanlike manner in accordance with generally recognized industry standards and will devote adequate resources to meet its obligations under this Agreement; (b) the Services will be performed materially in accordance with the applicable Documentation; (c) to the best of our knowledge, the Services is free from any viruses, worms, malware, or other malicious source code.</p> <p>8.3. Additional Client Representations, Warranties, and Covenants. You represent, warrant, and covenant to us that (a) you own or otherwise have and will have the necessary rights and consents in and relating to the Client Data so that, as received by us and processed in accordance with this Agreement, they do not and will not infringe, misappropriate, or otherwise violate any IPR, or any privacy or other rights of any third party or violate any applicable law; (b) all registration information you submit will be true, accurate, current, and complete and relate to you and not a third party; (c) you will maintain the accuracy of such information and promptly update such information as necessary; (d) you will keep your access credentials confidential and will be responsible for all use of your access credentials; (e) you are aware that you may not access or use the Services for any purpose other than that for which we make the Services available and (f) you are at least eighteen years of age.</p> <p>8.4. DISCLAIMER OF WARRANTIES. EXCEPT FOR THE EXPRESS WARRANTIES SET FORTH IN THIS SECTION 8, ALL SERVICES, DOCUMENTATION, AND WEBSITE ARE PROVIDED \"AS IS.\" WE SPECIFICALLY DISCLAIM ALL IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT, AND ALL WARRANTIES ARISING FROM THE COURSE OF DEALING, USAGE, OR TRADE PRACTICE. WITHOUT LIMITING THE FOREGOING, WE MAKE NO WARRANTY OF ANY KIND THAT THE SERVICES, DOCUMENTATION OR WEBSITE, OR ANY PRODUCTS OR RESULTS OF THE USE THEREOF, WILL MEET YOUR OR ANY OTHER PERSON'S REQUIREMENTS, OPERATE WITHOUT INTERRUPTION, ACHIEVE ANY INTENDED RESULT, BE COMPATIBLE OR WORK WITH ANY SOFTWARE, SYSTEM, OR OTHER SERVICES, OR BE SECURE, ACCURATE, COMPLETE, FREE OF HARMFUL CODE, OR ERROR-FREE. ALL THIRD-PARTY MATERIALS ARE PROVIDED \"AS IS\" AND ANY REPRESENTATION OR WARRANTY OF OR CONCERNING ANY THIRD-PARTY MATERIALS IS STRICTLY BETWEEN THE CLIENT AND THE THIRD-PARTY OWNER OR DISTRIBUTOR OF THE THIRD-PARTY MATERIALS.</p>"},{"location":"legal/terms.html#9-indemnification","title":"9. INDEMNIFICATION","text":"<p>9.1. Spacelift Indemnification. Subject to the remainder of this Section 9 and the liability limitations set forth in Section 10, we will indemnify, defend, and hold you harmless from and against any and all losses incurred by you resulting from any action by a third party that your use of the Services (excluding Client Data and any third-party materials) in accordance with this Agreement infringes or misappropriates IPR. The foregoing obligation does not apply to the extent that the alleged infringement arises from (a) any third-party materials or Client Data; (b) access to or use of the Services in combination with any hardware, system, software, network, or other materials or service not provided by us; (c) failure to timely implement any modifications, upgrades, replacements, or enhancements made available to you by us or on our behalf; or (d) use of the Services other than in accordance with the terms and conditions of this Agreement and the Documentation. THIS SECTION 9 SETS FORTH THE CLIENT\u2019S SOLE REMEDIES AND SPACELIFT\u2019S SOLE LIABILITY AND OBLIGATION FOR ANY ACTUAL, THREATENED, OR ALLEGED CLAIMS THAT THE SERVICES AND ANY OTHER PROVIDER MATERIALS OR ANY SUBJECT MATTER OF THIS AGREEMENT INFRINGES, MISAPPROPRIATES OR OTHERWISE VIOLATES ANY INTELLECTUAL PROPERTY RIGHTS OF ANY THIRD PARTY.</p> <p>9.2. Mitigation. If the Services or any of the other Spacelift\u2019s materials are, or in our opinion are likely to be, claimed to infringe, misappropriate, or otherwise violate any third-party IPR, or if your or your Authorized User's use of the Services or other Spacelift\u2019s materials is enjoined or threatened to be enjoined, we may, at our option and sole cost and expense: (a) obtain the right for you to continue to use the Services and said materials materially as contemplated by this Agreement, or (b) modify or replace the Services and said materials.</p> <p>9.3. Client Indemnification. You will indemnify, defend, and hold us harmless from and against any and all losses incurred by us resulting from any action by a third party to the extent that such losses arise out of or result from, or are alleged to arise out of or result from: (a) your use of the Services; (b) Client Data, including any processing of Client Data by us or on our behalf in accordance with this Agreement; (c) any other materials or information (including any documents, data, or technology) provided by you or on your behalf, (d) your breach of any of its representations, warranties, covenants, or obligations under this Agreement; or (e) negligence or more culpable act or omission (including recklessness or willful misconduct) by you, any Authorized User, or any third party acting on your behalf or any Authorized User, in connection with this Agreement, provided, that Client will have no obligation under this Section 9.3 to the extent the applicable claim arises from Spacelift\u2019s breach of this Agreement.</p> <p>9.4. Indemnification Procedure. Each party\u2019s indemnification obligations are conditioned on the indemnified party: (a) promptly giving written notice of the claim to the indemnifying party; (b) giving the indemnifying party sole control of the defense and settlement of the claim; and (c) providing to the indemnifying party all available information and assistance in connection with the claim, at the indemnifying party\u2019s request and expense. The indemnified party may participate in the defense of the claim, at the indemnified party\u2019s sole expense (not subject to reimbursement). Neither party may admit liability for or consent to any judgment or concede or settle or compromise any claim unless such admission or concession or settlement or compromise includes a full and unconditional release of the other Party from all liabilities in respect of the such claim.</p>"},{"location":"legal/terms.html#10-liability","title":"10. LIABILITY","text":"<p>10.1. Exclusion of Liability In no event will Spacelift have any obligation or liability arising from (a) use or inability to use any Services if modified or combined with materials not provided by us; (b) statements or conduct of any third party on or in the Services, (c) any Client Data, (d) any failure by Client to comply with the Agreement; and (e) damages suffered by the Client or Authorized Users, or any other person having arisen due to the third-party claims (other than described in Section 9.1), suspension or termination of the Services, or for other reasons arising from the Client\u2019s fault.</p> <p>10.2. EXCLUSION OF DAMAGES. EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4 AND TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL SPACELIFT OR ANY OF ITS LICENSORS OR SERVICE PROVIDERS BE LIABLE UNDER OR IN CONNECTION WITH THIS AGREEMENT OR ITS SUBJECT MATTER UNDER ANY LEGAL OR EQUITABLE THEORY, INCLUDING BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, AND OTHERWISE, FOR ANY: (a) LOSS OF PRODUCTION, USE, BUSINESS, REVENUE, OR PROFIT OR DIMINUTION IN VALUE; (b) IMPAIRMENT, INABILITY TO USE OR LOSS, INTERRUPTION, OR DELAY OF THE SERVICES; (c) LOSS, DAMAGE, CORRUPTION, OR RECOVERY OF DATA, OR BREACH OF DATA OR SYSTEM SECURITY; (d) COST OF REPLACEMENT GOODS OR SERVICES; (e) LOSS OF GOODWILL OR REPUTATION; OR (f) CONSEQUENTIAL, INCIDENTAL, INDIRECT, EXEMPLARY, SPECIAL, ENHANCED, OR PUNITIVE DAMAGES, REGARDLESS OF WHETHER SUCH PERSONS WERE ADVISED OF THE POSSIBILITY OF SUCH LOSSES OR DAMAGES OR SUCH LOSSES OR DAMAGES WERE OTHERWISE FORESEEABLE, AND NOTWITHSTANDING THE FAILURE OF ANY AGREED OR OTHER REMEDY OF ITS ESSENTIAL PURPOSE.</p> <p>10.3. CAP ON MONETARY LIABILITY. SPACELIFT WILL ONLY BE LIABLE FOR DIRECT DAMAGES EXCLUDING ANY SITUATION FOR WHICH WE ARE NOT RESPONSIBLE OR WHICH ARE CAUSED BY EVENTS OUTSIDE OUR REASONABLE CONTROL. HOWEVER, EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4, IN NO EVENT WILL THE COLLECTIVE AGGREGATE LIABILITY OF SPACELIFT ARISING OUT OF OR RELATED TO THIS AGREEMENT, WHETHER ARISING UNDER OR RELATED TO BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, OR ANY OTHER LEGAL OR EQUITABLE THEORY, EXCEED THE GREATER OF (a) THE TOTAL AMOUNTS PAID TO SPACELIFT UNDER THIS AGREEMENT IN THE 6 MONTH PERIOD PRECEDING THE EVENT GIVING RISE TO THE CLAIM OR (b) THE AMOUNT OF 5000 USD. THE FOREGOING LIMITATIONS APPLY EVEN IF ANY REMEDY FAILS OF ITS ESSENTIAL PURPOSE.</p> <p>10.4. Exceptions. NOTHING IN THIS SECTION 10 WILL BE DEEMED TO LIMIT EITHER PARTY\u2019S LIABILITY FOR WILLFUL MISCONDUCT, GROSS NEGLIGENCE, FRAUD, OR INFRINGEMENT BY ONE PARTY OF THE OTHER\u2019S INTELLECTUAL PROPERTY RIGHTS.</p>"},{"location":"legal/terms.html#11-provisions-relating-to-consumers","title":"11. PROVISIONS RELATING TO CONSUMERS","text":"<p>11.1. Right to Withdraw. If you are a natural person and have your habitual residence within a Member State of the European Union or the European Economic Area and are entering into the Agreement as a consumer (i.e. for purposes which are outside your trade, business, craft or profession), you have the right to withdraw from the contract as described below.</p> <p>11.2. Withdrawal Period. You have the right to withdraw from this Agreement (concluded under any Subscription Plan) within 14 days without giving any reason. The withdrawal right will expire after 14 days from the day of the conclusion of the Agreement.</p> <p>11.3. Exercise of the Right to Withdraw. To exercise the right of withdrawal, you must inform us, Spacelift, Inc, of your decision to withdraw from this Agreement by an unequivocal statement (e.g. an e-mail sent to legal@spacelift.io). To meet the withdrawal deadline, it is sufficient for you to send your communication concerning your exercise of the right of withdrawal before the withdrawal period expires.</p> <p>11.4. Model Withdrawal Form. To exercise your right of withdrawal, you may use the model withdrawal form, included in Appendix No. 2 to the Act on Consumer Rights of May 20, 2014, but this is not obligatory.</p> <p>11.5. Effect of the Withdrawal. If you withdraw from this Agreement, we will reimburse you all payments received from you, without undue delay and in any event not later than 14 days from the day on which we are informed about your decision to withdraw from this Agreement. We will carry out such reimbursement using the same means of payment as you used for the initial transaction unless you have expressly agreed otherwise; in any event, you will not incur any fees as a result of such reimbursement.</p> <p>11.6. Consumer Rights. Nothing in the Agreement will affect your legal rights as a consumer. If any provision of the Agreement does not comply with the relevant law for you as a consumer, the relevant law will apply instead of this provision. The severability clause equally applies. In case of any concerns, questions, or doubts, contact us at legal@spacelift.io.</p> <p>11.7. Complaints. If you have a complaint about Services, you should contact us at contact@spacelift.io, providing as much detail as possible about the complaint, together with your name, date of execution of the Agreement, and expected means of settling a complaint. We will respond by confirming receipt and will investigate the matter. Upon receiving the complaint, we will investigate the complaint internally, taking into account the importance and complexity of the issue raised, and get back to you no later than 30 days from the receipt of the complaint.</p> <p>11.8. ADR. If you are a consumer, you may consider Alternative Dispute Resolution means in the event of a dispute with us, including referring to the trade inspection, a consumer ombudsman, or an organization whose statutory tasks include consumer protection.</p>"},{"location":"legal/terms.html#12-final-provisions","title":"12. FINAL PROVISIONS","text":"<p>12.1. Current Version of Agreement. Usage of the Services is subject to the then-current version of the Agreement posted on the Website and we advise you to periodically review the latest currently effective Agreement. We reserve the right to update the provisions of the Agreement from time to time at our sole discretion. The updated Agreement version supersedes all prior versions, as well as is effective and binding immediately after posting on the Website. Your continued use of the Services on or after the date of the updated version of the Agreement is effective and constitutes your acceptance of such updated terms. If you do not agree to our updated Agreement, you can terminate the Subscription in accordance with Section 5.</p> <p>12.2. Applicable Law and Jurisdiction. This Agreement is governed by and construed in accordance with the Applicable Law without giving effect to any choice or conflict of law provision of any jurisdiction. Any legal suit, action, or proceeding arising out of or related to this Agreement will be subject to the exclusive jurisdiction of the Applicable Jurisdiction as provided in the following table:</p> Client Applicable Law Applicable Jurisdiction Consumers residing in the Member State of the European Union or the European Economic Area Poland Warsaw, Poland Other Clients State of Delaware, US County New Castle, Delaware, US <p>Each party irrevocably submits to the exclusive jurisdiction of such courts in any such suit, action, or proceeding.</p> <p>12.3. Contact details. In order to resolve a complaint regarding the Services, receive further information regarding the use of the Services, or send any notice to Spacelift, please contact us by email at contact@spacelift.io.</p> <p>12.4. Notices. Except as otherwise expressly set forth in this Agreement, any notice, request, consent, claim, demand, waiver, or other communications under this Agreement have legal effect and will be deemed effectively given: (a) when received, if delivered by hand or with signed confirmation of receipt; (b) when received, if sent by a nationally recognized overnight courier or by certified or registered mail, signature required; or (c) when sent, if by email, if sent during the addressee's normal business hours, and on the next business day, if sent after the addressee's normal business hours.</p> <p>12.5. Feedback. If you provide us with any suggestions, comments, recommendations, opinions, or other information relating to the Services or Website (\u201cFeedback\u201d), you grant us a royalty-free, non-exclusive, irrevocable, perpetual, worldwide right and license to use the Feedback on our websites or in marketing materials. We reserve the right to remove any Feedback posted on the Website if, in our opinion, such Feedback does not comply with the Agreement or applicable law.</p> <p>12.6. Logo usage. You grant us the right to use your name and other indicia, such as logo or trademark in our list of current or former clients in promotional materials and on our websites. Any other announcement, statement, press release, or other publicity or marketing materials relating to your use of Services will be subject to your consent.</p> <p>12.7. Export Laws. Each Party will comply with the export laws and regulations of the United States and other applicable jurisdictions in providing and using the Services. Without limiting the generality of the foregoing, Client represents that it is not named on any U.S. government denied-party list and will not make the Services available to any user or entity that is located in a country that is subject to a U.S. government embargo, or is listed on any U.S. government list of prohibited or restricted parties.</p> <p>12.8. Non-waiver. Our failure to exercise or enforce any right or provision of the Agreement will not operate as a waiver of such right or provision.</p> <p>12.9. Assignment. We may assign any or all of our rights and obligations to others at any time. We will notify you of any assignment.</p> <p>12.0. Severability. If any term or provision of this Agreement is invalid, illegal, or unenforceable in any jurisdiction, such invalidity, illegality, or unenforceability will not affect any other term or provision of this Agreement or invalidate or render unenforceable such term or provision in any other jurisdiction.</p> <p>12.11. No relationship. There is no joint venture, partnership, employment, or agency relationship created between you and us as a result of the Agreement or use of the Services.</p>"},{"location":"legal/archive/terms.html","title":"Terms and conditions","text":"<p>Effective until March 6, 2023</p>"},{"location":"legal/archive/terms.html#1-agreement-to-terms","title":"1. Agreement to Terms","text":"<p>1.1 These Terms and Conditions constitute a legally binding agreement made between you, whether personally or on behalf of an entity (you), and Spacelift Inc., registered at 651 N. Broad Street, Suite 206B Middletown, County of New Castle DE 19709 United States, doing business as Spacelift (we, us), concerning your access to and use of the Spacelift (https://spacelift.io) website as well as any related applications (the Site).</p> <p>The Site provides the following services: a specialized, Terraform-compatible continuous integration and deployment (CI/CD) platform for infra-as-code (Services). You agree that by accessing the Site and/or Services, you have read, understood, and agree to be bound by all of these Terms and Conditions.</p> <p>If you do not agree with all of these Terms and Conditions, then you are prohibited from using the Site and Services and you must discontinue use immediately. We recommend that you print a copy of these Terms and Conditions for future reference.</p> <p>1.2 The supplemental policies set out in Section 1.7 below, as well as any supplemental terms and conditions or documents that may be posted on the Site from time to time, are expressly incorporated by reference.</p> <p>1.3 We may make changes to these Terms and Conditions at any time. The updated version of these Terms and Conditions will be indicated by an updated \u201cRevised\u201d date and the updated version will be effective as soon as it is accessible. You are responsible for reviewing these Terms and Conditions to stay informed of updates. Your continued use of the Site represents that you have accepted such changes.</p> <p>1.4 We may update or change the Site from time to time to reflect changes to our products, our users' needs and/or our business priorities.</p> <p>1.5 The information provided on the Site is not intended for distribution to or use by any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject us to any registration requirement within such jurisdiction or country.</p> <p>1.6 The Site is intended for users who are at least 18 years old.  If you are under the age of 18, you are not permitted to register for the Site or use the Services without parental permission.</p> <p>1.7 Subscription</p> <p>The Services may include automatically recurring payments for periodic charges (\"Subscription Service\"). If you activate a Subscription Service, you authorize Spacelift to periodically charge, on a going-forward basis and until cancellation of either the recurring payments or your account, all accrued sums on or before the payment due date for the accrued sums. The \"Subscription Billing Date\" is the date when you purchase your first subscription to the Service. Your account will be charged automatically on the Subscription Billing Date all applicable fees for the next subscription period. The subscription will continue unless and until you cancel your subscription or we terminate it. You must cancel your subscription before it renews in order to avoid billing of the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide to us during registration (or to a different payment method if you change your payment information). You may cancel the Subscription Service by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io .</p> <p>In Subscription Service:</p> <ul> <li>Seat means each user who actively logged in to the Site in the last month.</li> <li>Private worker is a single worker installed on-premise that allows you to execute Spacelift workflows on your end. You can read more about it here.</li> </ul> <p>1.8 Additional policies which also apply to your use of the Site include:</p> <ul> <li> <p>Our Privacy Notice, which sets out the terms on which we process any personal data we collect from you, or that you provide to us. By using the Site, you consent to such processing and you warrant that all data provided by you is accurate.</p> </li> <li> <p>Our Cookie Policy, which sets out information about the cookies on the Site.</p> </li> </ul>"},{"location":"legal/archive/terms.html#2-acceptable-use","title":"2. Acceptable Use","text":"<p>2.1You may not access or use the Site for any purpose other than that for which we make the site and our services available. The Site may not be used in connection with any commercial endeavors except those that are specifically endorsed or approved by us.</p> <p>2.2 As a user of this Site, you agree not to:</p> <ul> <li> <p>Systematically retrieve data or other content from the Site to a compile database or directory without written permission from us;</p> </li> <li> <p>Make any unauthorized use of the Site, including collecting usernames and/or email addresses of users to send unsolicited email or creating user accounts under false pretenses;</p> </li> <li> <p>Use the Site to advertise or sell goods and services;</p> </li> <li> <p>Circumvent, disable, or otherwise interfere with security-related features of the Site, including features that prevent or restrict the use or copying of any content or enforce limitations on the use;</p> </li> <li> <p>Trick, defraud, or mislead us and other users, especially in any attempt to learn sensitive account information such as user passwords;</p> </li> <li> <p>Make improper use of our support services, or submit false reports of abuse or misconduct;</p> </li> <li> <p>Interfere with, disrupt, or create an undue burden on the Site or the networks and services connected to the Site;</p> </li> <li> <p>Attempt to impersonate another user or person, or use the username of another user;</p> </li> <li> <p>Sell or otherwise transfer your profile;</p> </li> <li> <p>Use any information obtained from the Site in order to harass, abuse, or harm another person;</p> </li> <li> <p>Harass, annoy, intimidate, or threaten any of our employees, agents, or other users;</p> </li> <li> <p>Delete the copyright or other proprietary rights notice from any of the content;</p> </li> <li> <p>Upload or transmit (or attempt to upload or to transmit) viruses, Trojan horses, or other material that interferes with any party\u2019s uninterrupted use and enjoyment of the Site, or any material that acts as a passive or active information collection or transmission mechanism;</p> </li> <li> <p>Use the Site in a manner inconsistent with any applicable laws or regulations;</p> </li> <li> <p>Falsely imply a relationship with us or another company with whom you do not have a relationship;</p> </li> </ul>"},{"location":"legal/archive/terms.html#3-information-you-provide-to-us","title":"3. Information you provide to us","text":"<p>3.1 You represent and warrant that: (a) all registration information you submit will be true, accurate, current, and complete and relate to you and not a third party; (b) you will maintain the accuracy of such information and promptly update such information as necessary; (c) you will keep your password confidential and will be responsible for all use of your password and account; (d) you have the legal capacity and you agree to comply with these Terms and Conditions; and (e) you are not a minor in the jurisdiction in which you reside, or if a minor, you have received parental permission to use the Site.</p> <p>If you know or suspect that anyone other than you knows your user information (such as an identification code or user name) and/or password you must promptly notify us at contact@spacelift.io.</p> <p>3.2 If you provide any information that is untrue, inaccurate, not current or incomplete, we may suspend or terminate your account. We may remove or change a user name you select if we determine that such user name is inappropriate.</p> <p>3.3 As part of the functionality of the Site, you may link your account with online accounts you may have with third party service providers (each such account, a Third Party Account) by either: (a) providing your Third Party Account login information through the Site; or (b) allowing us to access your Third Party Account, as is permitted under the applicable terms and conditions that govern your use of each Third Party Account.</p> <p>You represent that you are entitled to disclose your Third Party Account login information to us and/or grant us access to your Third Party Account without breach by you of any of the terms and conditions that govern your use of the applicable Third Party Account and without obligating us to pay any fees or making us subject to any usage limitations imposed by such third party service providers.</p> <p>3.4 By granting us access to any Third Party Accounts, you understand that (a) we may access, make available and store (if applicable) any content that you have provided to and stored in your Third Party Account (the \u201cSocial Network Content\u201d) so that it is available on and through the Site via your account; and (b) we may submit and receive additional information to your Third Party Account to the extent you are notified when you link your account with the Third Party Account.</p> <p>Depending on the Third Party Accounts you choose and subject to the privacy settings that you have set in such Third Party Accounts, personally identifiable information that you post to your Third Party Accounts may be available on and through your account on the Site. Please note that if a Third Party Account or associated service becomes unavailable or our access to such Third Party Account is terminated by the third party service provider, then Social Network Content may no longer be available on and through the Site.</p> <p>You will have the ability to disable the connection between your account on the Site and your Third Party Accounts at any time. Please note that your relationship with the third party service providers associated with your third party accounts is governed solely by your agreement(s) with such third party service providers. We make no effort to review any Social Network Content for any purpose, including but not limited to, for accuracy, legality or non-infringement, and we are not responsible for any Social Network Content.</p>"},{"location":"legal/archive/terms.html#4-content-you-provide-to-us","title":"4. Content you provide to us","text":"<p>4.1 There may be opportunities for you to post content to the Site or send feedback to us (User Content). You understand and agree that your User Content may be viewed by other users on the Site, and that they may be able to see who has posted that User Content.</p> <p>4.2 In posting User Content, including reviews or making contact with other users of the Site you shall comply with our Acceptable Use Policy.</p> <p>4.3 You warrant that any User Content does comply with our Acceptable Use Policy, and you will be liable to us and indemnify us for any breach of that warranty. This means you will be responsible for any loss or damage we suffer as a result of your breach of this warranty.</p> <p>4.4 We have the right to remove any User Content you put on the Site if, in our opinion, such User Content does not comply with the Acceptable Use Policy.</p> <p>4.5 We are not responsible and accept no liability for any User Content including any such content that contains incorrect information or is defamatory or loss of User Content. We accept no obligation to screen, edit or monitor any User Content but we reserve the right to remove, screen and/or edit any User Content without notice and at any time. User Content has not been verified or approved by us and the views expressed by other users on the Site do not represent our views or values.</p> <p>4.6 If you wish to complain about User Content uploaded by other users please contact us at contact@spacelift.io.</p>"},{"location":"legal/archive/terms.html#5-our-content","title":"5. Our content","text":"<p>5.1 Unless otherwise indicated, the Site and Services including source code, databases, functionality, software, website designs, audio, video, text, photographs, and graphics on the Site (Our Content) are owned or licensed to us, and are protected by copyright and trade mark laws.</p> <p>5.2 Except as expressly provided in these Terms and Conditions, no part of the Site, Services or Our Content may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission.</p> <p>5.3 Provided that you are eligible to use the Site, you are granted a limited licence to access and use the Site and Our Content and to download or print a copy of any portion of the Content to which you have properly gained access solely for your personal, non-commercial use.</p> <p>5.4 You shall not (a) try to gain unauthorised access to the Site or any networks, servers or computer systems connected to the Site; and/or (b) make for any purpose including error correction, any modifications, adaptations, additions or enhancements to the Site or Our Content, including the modification of the paper or digital copies you may have downloaded.</p> <p>5.5 We shall (a) prepare the Site and Our Content with reasonable skill and care; and (b) use industry standard virus detection software to try to block the uploading of content to the Site that contains viruses.</p> <p>5.6 The content on the Site is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Site.</p> <p>5.7 Although we make reasonable efforts to update the information on our site, we make no representations, warranties or guarantees, whether express or implied, that Our Content on the Site is accurate, complete or up to date.</p>"},{"location":"legal/archive/terms.html#6-link-to-third-party-content","title":"6. Link to third party content","text":"<p>6.1 The Site may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third party websites or applications or the third party operator. We are not responsible for and do not endorse any third party websites or applications or their availability or content.</p> <p>6.2 We accept no responsibility for adverts contained within the Site. If you agree to purchase goods and/or services from any third party who advertises in the Site, you do so at your own risk. The advertiser, and not us, is responsible for such goods and/or services and if you have any questions or complaints in relation to them, you should contact the advertiser.</p>"},{"location":"legal/archive/terms.html#7-site-management","title":"7. Site Management","text":"<p>7.1 We reserve the right at our sole discretion, to (1) monitor the Site for breaches of these Terms and Conditions; (2) take appropriate legal action against anyone in breach of applicable laws or these Terms and Conditions; (3) refuse, restrict access to or availability of, or disable (to the extent technologically feasible) any of your Contributions; (4) remove from the Site or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (5) otherwise manage the Site in a manner designed to protect our rights and property and to facilitate the proper functioning of the Site and Services.</p> <p>7.2 We do not guarantee that the Site will be secure or free from bugs or viruses.</p> <p>7.3 You are responsible for configuring your information technology, computer programs and platform to access the Site and you should use your own virus protection software.</p>"},{"location":"legal/archive/terms.html#8-modifications-to-and-availability-of-the-site","title":"8. Modifications to and availability of the Site","text":"<p>8.1 We reserve the right to change, modify, or remove the contents of the Site at any time or for any reason at our sole discretion without notice. We also reserve the right to modify or discontinue all or part of the Services without notice at any time.</p> <p>8.2 We cannot guarantee the Site and Services will be available at all times. We may experience hardware, software, or other problems or need to perform maintenance related to the Site, resulting in interruptions, delays, or errors. You agree that we have no liability whatsoever for any loss, damage, or inconvenience caused by your inability to access or use the Site or Services during any downtime or discontinuance of the Site or Services. We are not obliged to maintain and support the Site or Services or to supply any corrections, updates, or releases.</p> <p>8.3 There may be information on the Site that contains typographical errors, inaccuracies, or omissions that may relate to the Services, including descriptions, pricing, availability, and various other information. We reserve the right to correct any errors, inaccuracies, or omissions and to change or update the information at any time, without prior notice.</p>"},{"location":"legal/archive/terms.html#9-disclaimerlimitation-of-liability","title":"9. Disclaimer/Limitation of Liability","text":"<p>9.1  The Site and Services are provided on an as-is and as-available basis. You agree that your use of the Site and/or Services will be at your sole risk except as expressly set out in these Terms and Conditions. All warranties, terms, conditions and undertakings, express or implied (including by statute, custom or usage, a course of dealing, or common law) in connection with the Site and Services and your use thereof including, without limitation, the implied warranties of satisfactory quality, fitness for a particular purpose and non-infringement are excluded to the fullest extent permitted by applicable law.</p> <p>We make no warranties or representations about the accuracy or completeness of the Site\u2019s content and are not liable for any (1) errors or omissions in content: (2) any unauthorized access to or use of our servers and/or any and all personal information and/or financial information stored on our server; (3) any interruption or cessation of transmission to or from the site or services; and/or (4) any bugs, viruses, trojan horses, or the like which may be transmitted to or through the site by any third party. We will not be responsible for any delay or failure to comply with our obligations under these Terms and Conditions if such delay or failure is caused by an event beyond our reasonable control.</p> <p>9.2 Our responsibility for loss or damage suffered by you:</p> <p>Whether you are a consumer or a business user:</p> <ul> <li> <p>We do not exclude or limit in any way our liability to you where it would be unlawful to do so. This includes liability for death or personal injury caused by our negligence or the negligence of our employees, agents or subcontractors and for fraud or fraudulent misrepresentation;</p> </li> <li> <p>If we fail to comply with these Terms and Conditions, we will be responsible for loss or damage you suffer that is a foreseeable result of our breach of these Terms and Conditions, but we would not be responsible for any loss or damage that were not foreseeable at the time you started using the Site/Services;</p> </li> </ul> <p>Notwithstanding anything to the contrary contained in the Disclaimer/Limitation of Liability section, our liability to you for any cause whatsoever and regardless of the form of the action, will at all times be limited to a total aggregate amount equal to the greater of (a) the sum of PLN 5000 or (b) the amount paid, if any, by you to us for the Services/Site during the six (6) month period prior to any cause of action arising.</p> <p>If you are a business user:</p> <p>We will not be liable to you for any loss or damage, whether in contract, tort (including negligence), breach of statutory duty, or otherwise, even if foreseeable, arising under or in connection with:</p> <ul> <li> <p>use of, or inability to use, our Site/Services; or</p> </li> <li> <p>use of or reliance on any content displayed on our Site.</p> </li> </ul> <p>In particular, we will not be liable for:</p> <ul> <li> <p>loss of profits, sales, business, or revenue;</p> </li> <li> <p>business interruption;</p> </li> <li> <p>loss of anticipated savings;</p> </li> <li> <p>loss of business opportunity, goodwill or reputation; or</p> </li> <li> <p>any indirect or consequential loss or damage.</p> </li> </ul> <p>If you are a consumer user:</p> <ul> <li> <p>Please note that we only provide our Site for domestic and private use. You agree not to use our Site for any commercial or business purposes, and we have no liability to you for any loss of profit, loss of business, business interruption, or loss of business opportunity;</p> </li> <li> <p>If defective digital content that we have supplied, damages a device or digital content belonging to you and this is caused by our failure to use reasonable care and skill, we will either repair the damage or pay you compensation.</p> </li> <li> <p>You have legal rights in relation to goods that are faulty or not as described. Advice about your legal rights is available from your local Citizens' Advice Bureau or Trading Standards office. Nothing in these Terms and Conditions will affect these legal rights.</p> </li> </ul>"},{"location":"legal/archive/terms.html#10-term-and-termination","title":"10. Term and Termination","text":"<p>10.1 These Terms and Conditions shall remain in full force and effect while you use the Site or Services or are otherwise a user of the Site, as applicable. You may terminate your use or participation at any time, for any reason, by following the instructions for terminating user accounts, if available, or by contacting us at contact@spacelift.io.</p> <p>10.2 Without limiting any other provision of these Terms and Conditions, we reserve the right to, in our sole discretion and without notice or liability, deny access to and use of the Site and the Services (including blocking certain IP addresses), to any person for any reason including without limitation for breach of any representation, warranty or covenant contained in these Terms and Conditions or of any applicable law or regulation.</p> <p>If we determine, in our sole discretion, that your use of the Site/Services is in breach of these Terms and Conditions or of any applicable law or regulation, we may terminate your use or participation in the Site and the Services or delete your profile and any content or information that you posted at any time, without warning, in our sole discretion.</p> <p>10.3 If we terminate or suspend your account for any reason set out in this Section 9, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to terminating or suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.</p>"},{"location":"legal/archive/terms.html#11-general","title":"11. General","text":"<p>11.1 Visiting the Site, sending us emails, and completing online forms constitute electronic communications. You consent to receive electronic communications and you agree that all agreements, notices, disclosures, and other communications we provide to you electronically, via email and on the Site, satisfy any legal requirement that such communication be in writing.</p> <p>You hereby agree to the use of electronic signatures, contracts, orders and other records and to electronic delivery of notices, policies and records of transactions initiated or completed by us or via the Site. You hereby waive any rights or requirements under any statutes, regulations, rules, ordinances or other laws in any jurisdiction which require an original signature or delivery or retention of non-electronic records, or to payments or the granting of credits by other than electronic means.</p> <p>11.2 These Terms and Conditions and any policies or operating rules posted by us on the Site or in respect to the Services constitute the entire agreement and understanding between you and us.</p> <p>11.3 Our failure to exercise or enforce any right or provision of these Terms and Conditions shall not operate as a waiver of such right or provision.</p> <p>11.4 We may assign any or all of our rights and obligations to others at any time.</p> <p>11.5 We shall not be responsible or liable for any loss, damage, delay or failure to act caused by any cause beyond our reasonable control.</p> <p>11.6 If any provision or part of a provision of these Terms and Conditions is unlawful, void or unenforceable, that provision or part of the provision is deemed severable from these Terms and Conditions and does not affect the validity and enforceability of any remaining provisions.</p> <p>11.7 There is no joint venture, partnership, employment or agency relationship created between you and us as a result of these Terms and Conditions or use of the Site or Services.</p> <p>11.8 For consumers only  - Please note that these Terms and Conditions, their subject matter and their formation, are governed by Polish law. You and we both agree that the courts of Poland will have exclusive jurisdiction. If you have any complaint or wish to raise a dispute under these Terms and Conditions or otherwise in relation to the Site please follow this link.</p> <p>11.9 For business users only - If you are a business user, these Terms and Conditions, their subject matter and their formation (and any non-contractual disputes or claims) are governed by Polish Law. We both agree to the exclusive jurisdiction of the courts of Poland.</p> <p>11.10 A person who is not a party to these Terms and Conditions shall have no right to enforce any term of these Terms and Conditions.</p> <p>11.11 In order to resolve a complaint regarding the Services or to receive further information regarding use of the Services, please contact us by email at contact@spacelift.io or by post to:</p> <p>Spacelift Inc. 651 N. Broad Street, Suite 206B Middletown, County of New Castle DE 19709, United States</p>"},{"location":"product/disaster-continuity.html","title":"Disaster Continuity","text":""},{"location":"product/disaster-continuity.html#preparation","title":"Preparation","text":"<p>The following preparation items are recommended to be followed to ensure that you are able to continue with your infrastructure as code deployments in the event of a Spacelift outage.</p>"},{"location":"product/disaster-continuity.html#state-management","title":"State Management","text":"<p>Here is a list of best practices that should be followed in regards to managing the state of your infrastructure.</p> <ul> <li>Store your state externally (e.g. Amazon S3 Bucket)</li> <li>Enable versioning on your state file to keep a record of changes</li> <li>Replicate your state file across regions</li> <li>Enable MFA on deletion to prevent accidental loss of your state file</li> </ul>"},{"location":"product/disaster-continuity.html#deployment-roles","title":"Deployment Roles","text":"<p>Within your Spacelift configuration, each Spacelift stack utilizes a given Role for deployment purposes. We will be referring to this Role as the deployment role.</p> <p>In the event of a disaster, Spacelift will presumably not be accessible or usable. You should ensure that you have appropriate access to your deployment role, to provide yourself the ability to assume it for deployment purposes, or have plans to use another role for deployment purposes.</p> <ul> <li>Keep a record of all Roles used by your Spacelift Stacks that are used for deployment purposes</li> <li>Ensure that you've done one of the following:</li> <li>Provided yourself access to the deployment role that Spacelift is using</li> <li>Have a plan to create, or have already created a break-glass role that you can use for disaster purposes</li> </ul>"},{"location":"product/disaster-continuity.html#terraform-break-glass-example-procedure","title":"Terraform Break Glass Example Procedure","text":""},{"location":"product/disaster-continuity.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Access to assume your deployment role(s)</li> <li>Terraform installed locally</li> <li>Managing your state externally (not Spacelift-managed state)</li> </ul>"},{"location":"product/disaster-continuity.html#assume-deployment-role-locally","title":"Assume deployment role locally","text":"<p>Using your favorite cloud provider, generate temporary credentials for your deployment role. With Amazon Web Services for example, this would be done using the following command:</p> <pre><code>aws sts assume-role \\\n--role-arn &lt;your-deployment-role-arn&gt; \\\n--role-session-name local-infra-deployment\n</code></pre> <p>Using the output from the assume-role command, set your credentials in your shell.</p> <pre><code>export AWS_ACCESS_KEY_ID=&lt;value for access key id&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;value for secret access key&gt;\nexport AWS_SESSION_TOKEN=&lt;value for session token&gt;\n</code></pre>"},{"location":"product/disaster-continuity.html#run-deployment-commands-as-required","title":"Run deployment commands as required","text":"<p>Initialize your code locally:</p> <pre><code>terraform init\n</code></pre> <p>To preview changes to be deployed:</p> <pre><code>terraform plan\n</code></pre> <p>To deploy changes:</p> <pre><code>terraform apply\n</code></pre>"},{"location":"product/migrating-to-spacelift.html","title":"Migrating to Spacelift","text":"<p>Migrating from one Infrastructure as Code CI/CD provider to another can feel daunting. This is why we created a migration kit that takes care of the heavy lifting.</p> <p>Edit a few settings, and let it do the hard work. Then review, and possibly tweak, the generated code, and finally have your Spacelift entities created.</p> <p>There is no one-size-fits-all for this kind of migration. This is why we designed this tool to be flexible and easy to hack to meet your specific needs. Feel free to reach out to our support team if you need any help or guidance.</p>"},{"location":"product/migrating-to-spacelift.html#overview","title":"Overview","text":"<p>The migration process is as follows:</p> <ul> <li>Export the definition for your resources at your current vendor.</li> <li>Generate the Terraform code to recreate similar resources at Spacelift using the Terraform provider.</li> <li>Review and possibly edit the generated Terraform code.</li> <li>Commit the Terraform code to a repository.</li> <li>Create a manager Spacelift stack that points to the repository with the Terraform code.</li> </ul> <p>Tip</p> <p>Currently, only Terraform Cloud and Terraform Enterprise are supported as sources. The instructions below apply to both.</p>"},{"location":"product/migrating-to-spacelift.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Terraform</li> </ul>"},{"location":"product/migrating-to-spacelift.html#instructions","title":"Instructions","text":""},{"location":"product/migrating-to-spacelift.html#preparation","title":"Preparation","text":"<ul> <li>Clone the Spacelift Migration Kit repository locally.</li> <li>Use the <code>terraform login spacelift.io</code> command to ensure that Terraform can interact with your Spacelift account.</li> </ul> <p>Depending on the exporter used, you may need additional steps:</p> <ul> <li>Terraform Cloud/Enterprise: Use the <code>terraform login</code> command to ensure that Terraform can interact with your Terraform Cloud/Enterprise account.</li> </ul>"},{"location":"product/migrating-to-spacelift.html#pre-migration-cleanup","title":"Pre-Migration Cleanup","text":"<p>In order to start fresh, clean up files and folders from previous runs.</p> <pre><code>rm -rf ./out ./{exporters/tfc,generator,manager-stack}/.terraform ./{exporters/tfc,generator,manager-stack}/.terraform.lock.hcl ./{exporters/tfc,generator,manager-stack}/terraform.tfstate ./{exporters/tfc,generator,manager-stack}/terraform.tfstate.backup\n</code></pre>"},{"location":"product/migrating-to-spacelift.html#export-the-resource-definitions-and-terraform-state","title":"Export the resource definitions and Terraform state","text":"<ul> <li>Choose an exporter and copy the example <code>.tfvars</code> file for it into <code>exporter.tfvars</code>.</li> <li>Edit that file to match your context.</li> <li>Run the following commands:</li> </ul> <pre><code>cd exporters/&lt;EXPORTER&gt;\nterraform init\nterraform apply -auto-approve -var-file=../../exporter.tfvars\n</code></pre> <p>A new <code>out</code> folder should have been created. The <code>data.json</code> files contains the mapping of your vendor resources to the equivalent Spacelift resources, and the <code>state-files</code> folder contains the files for the Terraform state of your stacks, if the state export was enabled.</p> <p>Please note that once exported the Terraform state files can be imported into Spacelift or to any backend supported by Terraform.</p>"},{"location":"product/migrating-to-spacelift.html#generate-the-terraform-code","title":"Generate the Terraform code","text":"<ul> <li>If you want to customize the template that generates the Terraform code, run <code>cp ../../generator/generator.tftpl ../generator.tftpl</code>, and edit the <code>generator.tftpl</code> file at the root of the repository. If present, it will be used automatically.</li> <li>Run the following commands:</li> </ul> <pre><code>cd ../../generator\nterraform init\nterraform apply -auto-approve -var-file=../out/data.json\n</code></pre>"},{"location":"product/migrating-to-spacelift.html#review-and-edit-the-generated-terraform-code","title":"Review and edit the generated Terraform code","text":"<p>A <code>main.tf</code> should have been generated in the <code>out</code> folder. It contains all the Terraform code for your Spacelift resources.</p> <p>Mapping resources from a vendor to Spacelift resources is not an exact science. There are gaps in functionality and caveats in the mapping process.</p> <p>Please carefully review the generated Terraform code and make sure that it looks fine. If it does not, repeat the process with a different configuration or edit the Terraform code.</p>"},{"location":"product/migrating-to-spacelift.html#commit-the-terraform-code","title":"Commit the Terraform code","text":"<p>When the Terraform code is ready, commit it to a repository.</p>"},{"location":"product/migrating-to-spacelift.html#create-a-manager-spacelift-stack","title":"Create a manager Spacelift stack","text":"<p>It is now time to create a Spacelift stack that will point to the committed Terraform code that manages your Spacelift resources.</p> <ul> <li>Copy the example <code>manager-stack.example.tfvars</code> file into <code>manager-stack.tfvars</code> .</li> <li>Edit that file to match your context.</li> <li>Run the following commands:</li> </ul> <pre><code>cd ../manager-stack\nterraform init\nterraform apply -auto-approve -var-file=../manager-stack.tfvars\n</code></pre> <p>After the stack has been created, a tracked run will be triggered automatically. That run will create the defined Spacelift resources.</p>"},{"location":"product/migrating-to-spacelift.html#post-migration-cleanup","title":"Post-Migration Cleanup","text":"<p>Before you can use Spacelift to manage your infrastructure,  you may need to make changes to the Terraform code for your infrastructure, depending on the Terraform state is managed.</p> <p>If the Terraform state is managed by Spacelift,perform the following actions, otherwise you can skip this section:</p> <ul> <li>Remove any backend/cloud block from the Terraform code that manages your infrastructure to avoid a conflict with Spacelift's backend.</li> <li>Delete the <code>import_state_file</code> arguments from the Terraform code that manages your Spacelift resources.</li> <li>After the manager stack has successfully run, the mounted Terraform state files are not needed anymore and can be deleted by setting the <code>import_state</code> argument to <code>false</code> in the <code>manager-stack.tfvars</code> file and run <code>terraform apply -auto-approve -var-file=../manager-stack.tfvars</code> in the <code>manager-stack</code> folder.</li> </ul>"},{"location":"product/migrating-to-spacelift.html#sources","title":"Sources","text":""},{"location":"product/migrating-to-spacelift.html#terraform-cloudenterprise","title":"Terraform Cloud/Enterprise","text":""},{"location":"product/migrating-to-spacelift.html#known-limitations","title":"Known Limitations","text":"<p>The limitations listed below come from the original provider. We are actively looking for workarounds.</p> <ul> <li>The variable sets are not exposed so they cannot be listed and exported.</li> <li>The name of the Version Control System (VCS) provider for a stack is not returned so it has to be set in the exporter configuration file.</li> <li>When the branch for the stack is the repository default branch, the value is empty. You can set the value for the default branch in the exporter configuration file, or edit the generated Terraform code.</li> </ul>"},{"location":"product/migrating-to-spacelift.html#glossary","title":"Glossary","text":"Terraform Cloud/Enterprise Spacelift Agent Pool Worker Pool Organization Account Policy Policy Project Space Variable Environment Variable Variable Set Context Workspace Stack"},{"location":"product/notifications.html","title":"Notifications","text":"<p>As nicely stated by Murphy's law: \"Anything that can go wrong will go wrong.\". Some issues will blow in your face and be obvious, and others will be sneakier.</p> <p>In the background, Spacelift interacts with different systems (VCS providers, cloud providers, Slack, etc.), which can fail in various ways. The Notification Inbox section gives you visibility into issues arising from those interactions.</p>"},{"location":"product/notifications.html#visibility","title":"Visibility","text":"<p>Notifications are only available to admins.</p> <p>They can be checked either at the account level, which includes all the stacks.</p> <p></p> <p>Or, they can be checked for a specific stack.</p> <p></p>"},{"location":"product/notifications.html#available-categories","title":"Available Categories","text":"<p>Currently, the Notifications only include VCS provider issues, but more categories will be added soon.</p>"},{"location":"product/notifications.html#retention","title":"Retention","text":"<p>Notifications are kept for 14 days.</p>"},{"location":"product/security.html","title":"Security","text":"<p>At Spacelift, your security is our first and foremost priority. We're aware of the utmost importance of security in our service and we're grateful for your trust. Here's what we're doing to earn and maintain this trust, and to keep Spacelift secure by design.</p>"},{"location":"product/security.html#certifications","title":"Certifications","text":"<p>SOC2 Type II Certified</p> <p>Certification performed by an independent external auditor, who confirms the effectiveness of internal controls in terms of Spacelift Security: Confidentiality, Integrity, Availability, and Privacy of customer data.</p>"},{"location":"product/security.html#security-audits","title":"Security Audits","text":"<p>Spacelift regularly engages with external security firms to perform audits and penetration testing at least once per year. Additionally, the Spacelift Security Team conducts internal security audits regularly in combination with automated security tooling.</p>"},{"location":"product/security.html#encryption","title":"Encryption","text":"<p>All of our data is encrypted at rest and in transit. With the exception of intra-VPC traffic between the web server and the load balancer protected by a restrictive AWS security group, all other traffic is handled using secure transport protocols. All the data sources (Amazon S3, database, Amazon SNS topics and Amazon SQS queues) are encrypted at rest using AWS KMS keys with restricted and audited access.</p> <p>Customer secrets are extra encrypted at rest in a way that should withstand even an internal attacker.</p>"},{"location":"product/security.html#security-features","title":"Security Features","text":""},{"location":"product/security.html#single-sign-on-sso","title":"Single Sign-On (SSO)","text":"<p>In addition to the default login providers (currently GitHub, GitLab, and Google), Spacelift also supports the ability to configure Single Sign-On (SSO) via SAML or OIDC using your favorite identity provider. Using SSO, Spacelift can be configured in a password-less approach, helping your company follow a zero-trust approach. As long as your Identity Provider supports SAML or OIDC, and passing the <code>email</code> scope, you're good to go! You can learn more about our Single Sign-On support here.</p>"},{"location":"product/security.html#environment-variables","title":"Environment Variables","text":"<p>Spacelift allows for granular control of environment variables on your Stacks either by setting environment variables on a per-stack basis, or creating collections of variables as a Context. These environment variables can be created in two types: plain or secret.</p>"},{"location":"product/security.html#policies","title":"Policies","text":"<p>Spacelift policies provide a way to express rules as code to manage your infrastructure as a code environment. Users can build policies to control Spacelift login permissions, access controls, deployment workflows, and even govern the infrastructure itself to be deployed. Policies are based on the Open Policy Agent project and can be defined using its rule language Rego. You can learn more about policies here.</p>"},{"location":"product/security.html#responsible-disclosure","title":"Responsible disclosure","text":"<p>If you discover a vulnerability, we would like to know about it so we can take steps to address it as quickly as possible. We would like to ask you to help us better protect our clients and our systems.</p> <p>Please do the following:</p> <ul> <li> <p>email your findings to security@spacelift.io;</p> </li> <li> <p>do not take advantage of the vulnerability or problem you have discovered, for example by downloading more data than necessary to demonstrate the vulnerability or deleting or modifying other people's data;</p> </li> <li> <p>do not reveal the problem to others until it has been resolved;</p> </li> <li> <p>do not use attacks on physical security, social engineering, distributed denial of service, spam or applications of third parties, and;</p> </li> <li> <p>do provide sufficient information to reproduce the problem, so we will be able to resolve it as quickly as possible;</p> </li> </ul> <p>What we promise:</p> <ul> <li> <p>we will respond to your report within 3 business days with our evaluation of the report and an expected resolution date;</p> </li> <li> <p>if you have followed the instructions above, we will not take any legal action against you in regard to the report;</p> </li> <li> <p>we will handle your report with strict confidentiality, and not pass on your personal details to third parties without your permission;</p> </li> <li> <p>we will keep you informed of the progress towards resolving the problem;</p> </li> <li> <p>in the public information concerning the problem reported, we will give your name as the discoverer of the problem (unless you desire otherwise), and;</p> </li> <li> <p>as a token of our gratitude for your assistance, we offer a reward for every report of a security problem that was not yet known to us. The amount of the reward will be determined based on the severity of the leak and the quality of the report;</p> </li> </ul> <p>We strive to resolve all problems as quickly as possible, and we would like to play an active role in the ultimate publication on the problem after it is resolved.</p>"},{"location":"product/administration/advanced-installations.html","title":"Advanced Installations","text":""},{"location":"product/administration/advanced-installations.html#custom-vpc","title":"Custom VPC","text":"<p>In certain situations you may want to have full control of the network that Spacelift runs in, and the default VPC and security groups created by Spacelift don't fit your circumstances. In these situations, you can create your own networking components and supply them during the installation process.</p> <p>If you choose to do this, you will need to create the following resources:</p> <ul> <li>A VPC.</li> <li>A set of private subnets.</li> <li>A set of public subnets (the same subnets IDs can be used if you don't need separate private and public subnets).</li> <li>Security groups for the various Spacelift components.</li> </ul> <p>The following sections explain the requirements for each component.</p> <p>Also, see the section on HTTP Proxies if you need to use a proxy to allow the Spacelift components to make HTTP requests.</p>"},{"location":"product/administration/advanced-installations.html#vpc","title":"VPC","text":"<p>The VPC needs to have a CIDR block large enough to run the Spacelift server and drain instances along with the database and a few other networking components. We would recommend using a minimum network prefix of <code>/27</code>.</p>"},{"location":"product/administration/advanced-installations.html#private-subnets","title":"Private Subnets","text":"<p>The number of private subnets depends on the number of availability zones you want to deploy Spacelift to.</p>"},{"location":"product/administration/advanced-installations.html#public-subnets","title":"Public Subnets","text":"<p>The public subnets are used to place the load balancer for the Spacelift server. If you don't need separate public and private subnets you can use the same subnets for both. Just use the same subnet IDs to populate both the private and public subnet configuration options.</p>"},{"location":"product/administration/advanced-installations.html#security-groups","title":"Security Groups","text":"<p>Security groups need to be created for the following components:</p> <ul> <li>Drain.</li> <li>Server.</li> <li>Load Balancer.</li> <li>Installation Task.</li> <li>Database.</li> </ul> <p>The next sections explain the requirements of each group.</p>"},{"location":"product/administration/advanced-installations.html#drain","title":"Drain","text":"<p>Needs to be able to access the following components:</p> <ul> <li>Your VCS system (e.g. GitHub, GitLab, etc).</li> <li>Various AWS APIs.</li> <li>The Spacelift database.</li> </ul> <p>Our default CloudFormation template for the drain security group looks like the following, and allows unrestricted egress (for accessing VCS systems):</p> <pre><code>DrainSecurityGroup:\nType: AWS::EC2::SecurityGroup\nProperties:\nGroupName: \"drain_sg\"\nGroupDescription: \"The security group for the Spacelift async-processing service\"\nSecurityGroupEgress:\n- Description: \"Unrestricted egress\"\nFromPort: 0\nToPort: 0\nIpProtocol: \"-1\"\nCidrIp: \"0.0.0.0/0\"\nVpcId: {Ref: VPC}\n</code></pre>"},{"location":"product/administration/advanced-installations.html#server","title":"Server","text":"<p>Needs to be able to access the following components:</p> <ul> <li>Your VCS system (e.g. GitHub, GitLab, etc).</li> <li>Your identity provider for SSO.</li> <li>Various AWS APIs.</li> <li>The Spacelift database.</li> </ul> <p>The server also needs to allow ingress from its load balancer.</p> <p>Our default CloudFormation template for the server security group looks like the following, and allows unrestricted egress along with ingress via the load balancer:</p> <pre><code>ServerSecurityGroup:\nType: AWS::EC2::SecurityGroup\nProperties:\nGroupName: \"server_sg\"\nGroupDescription: \"The security group for the Spacelift HTTP server\"\nSecurityGroupEgress:\n- Description: \"Unrestricted egress\"\nFromPort: 0\nToPort: 0\nIpProtocol: \"-1\"\nCidrIp: \"0.0.0.0/0\"\nSecurityGroupIngress:\n- Description: \"Only accept HTTP connections on port 1983 from the load balancer\"\nFromPort: 1983\nToPort: 1983\nIpProtocol: \"tcp\"\nSourceSecurityGroupId: {Ref: LoadBalancerSecurityGroup}\nVpcId: {Ref: VPC}\n</code></pre>"},{"location":"product/administration/advanced-installations.html#load-balancer","title":"Load Balancer","text":"<p>The load balancer needs to be able to accept traffic from any clients (e.g. users logging into Spacelift via a browser or using <code>spacectl</code>), and also needs to accept incoming webhooks from your VCS system. In addition it needs to be able to access the server container.</p> <p>Our default CloudFormation template for the load balancer security group looks like the following:</p> <pre><code>LoadBalancerSecurityGroup:\nType: AWS::EC2::SecurityGroup\nProperties:\nGroupName: \"load_balancer_sg\"\nGroupDescription: \"The security group for the load balancer sitting in front of the Spacelift HTTP server\"\nSecurityGroupIngress:\n- Description: \"Accept HTTPS connections on port 443\"\nFromPort: 443\nToPort: 443\nIpProtocol: \"tcp\"\nCidrIp: \"0.0.0.0/0\"\nVpcId: {Ref: VPC}\n\nLoadBalancerToServerEgress:\nType: AWS::EC2::SecurityGroupEgress\nProperties:\nDescription: \"Allow the server load balancer to access server app containers\"\nDestinationSecurityGroupId: {Ref: ServerSecurityGroup}\nFromPort: 1983\nToPort: 1983\nIpProtocol: \"tcp\"\nGroupId: {Ref: LoadBalancerSecurityGroup}\n</code></pre>"},{"location":"product/administration/advanced-installations.html#installation-task","title":"Installation Task","text":"<p>The installation task security group allows one-off tasks that are run during the installation process to access the Spacelift database. Our default CloudFormation template looks like the following:</p> <pre><code>InstallationTaskSecurityGroup:\nType: AWS::EC2::SecurityGroup\nProperties:\nGroupName: \"installation_task_sg\"\nGroupDescription: \"The security group for tasks that run as part of the installation process\"\nSecurityGroupEgress:\n- Description: \"Unrestricted egress\"\nFromPort: 0\nToPort: 0\nIpProtocol: \"-1\"\nCidrIp: \"0.0.0.0/0\"\nVpcId: {Ref: VPC}\n</code></pre>"},{"location":"product/administration/advanced-installations.html#database","title":"Database","text":"<p>The database security group needs to allow inbound access from the server, the drain and installation tasks. Our default CloudFormation template looks like the following:</p> <pre><code>DatabaseSecurityGroup:\nType: AWS::EC2::SecurityGroup\nProperties:\nGroupName: \"database_sg\"\nGroupDescription: \"The security group defining what services can access the Spacelift database\"\nSecurityGroupIngress:\n- Description: \"Only accept TCP connections on appropriate port from the drain\"\nFromPort: 5432\nToPort: 5432\nIpProtocol: \"tcp\"\nSourceSecurityGroupId: {Ref: DrainSecurityGroup}\n- Description: \"Only accept TCP connections on appropriate port from the server\"\nFromPort: 5432\nToPort: 5432\nIpProtocol: \"tcp\"\nSourceSecurityGroupId: {Ref: ServerSecurityGroup}\n- Description: \"Only accept TCP connections on appropriate port from the installation tasks\"\nFromPort: 5432\nToPort: 5432\nIpProtocol: \"tcp\"\nSourceSecurityGroupId: {Ref: InstallationTaskSecurityGroup}\nVpcId: {Ref: VPC}\n</code></pre>"},{"location":"product/administration/advanced-installations.html#performing-a-custom-vpc-installation","title":"Performing a custom VPC installation","text":"<p>To install Spacelift into a custom VPC, create your VPC along with all the other required components like security groups, then edit the <code>vpc_config</code> section of your config.json file, making sure to set <code>use_custom_vpc</code> to <code>true</code>. A correctly populated <code>vpc_config</code> will look like this:</p> <pre><code>{\n\"vpc_config\": {\n\"use_custom_vpc\": true,\n\"vpc_id\": \"vpc-091e6f4d35908e7c1\",\n\"private_subnet_ids\": \"subnet-01a25f47c5a7e94fc,subnet-035169be40fbfbbbf,subnet-09c72e8ab5499eed1\",\n\"public_subnet_ids\": \"subnet-08aa756ab626d690f,subnet-0d03bb49f32922d93,subnet-0d85c4a80db226099\",\n\"drain_security_group_id\": \"sg-045061f7120343acd\",\n\"load_balancer_security_group_id\": \"sg-086a38a75894c4fc5\",\n\"server_security_group_id\": \"sg-0cb943fd285fc5c85\",\n\"installation_task_security_group_id\": \"sg-03b9b0e17cce91d3a\",\n\"database_security_group_id\": \"sg-0b67dd8ad00e237fd\",\n\"availability_zones\": \"eu-west-1a,eu-west-1b,eu-west-1c\"\n}\n}\n</code></pre> <p>Once you have populated your configuration, just run the installer as described in the installation guide.</p>"},{"location":"product/administration/advanced-installations.html#http-proxies","title":"HTTP Proxies","text":"<p>If you need to use an HTTP proxy to allow the Spacelift components to access the internet, you can specify this via the config.json file:</p> <pre><code>{\n\"proxy_config\": {\n\"http_proxy\": \"\",\n\"https_proxy\": \"\",\n\"no_proxy\": \"\"\n}\n}\n</code></pre> <p>These three settings correspond to the <code>HTTP_PROXY</code>, <code>HTTPS_PROXY</code> and <code>NO_PROXY</code> environment variables, respectively. These environment variables are automatically added to the Spacelift ECS containers during installation when values are specified in the configuration file.</p> <p>Any variable that isn't populated will not be added. For example, if you use the following configuration all three environment variables will be added to the containers:</p> <pre><code>{\n\"proxy_config\": {\n\"http_proxy\": \"http://my.http.proxy\",\n\"https_proxy\": \"https://my.https.proxy\",\n\"no_proxy\": \"safe.domain\"\n}\n}\n</code></pre> <p>However if you only need to specify the <code>HTTPS_PROXY</code> environment variable you can use the following configuration:</p> <pre><code>{\n\"proxy_config\": {\n\"http_proxy\": \"\",\n\"https_proxy\": \"https://my.https.proxy\",\n\"no_proxy\": \"\"\n}\n}\n</code></pre> <p>NOTE: you must include the protocol with your proxy URL (e.g. <code>http://</code> or <code>https://</code>), otherwise the proxy configuration can fail to parse and prevent the Spacelift ECS services from starting correctly.</p>"},{"location":"product/administration/advanced-installations.html#using-a-tls-connection-between-the-application-load-balancer-and-spacelift-server","title":"Using a TLS connection between the Application Load Balancer and Spacelift Server","text":"<p>The Spacelift server is served over TLS by default. This is achieved by using an AWS Application Load Balancer (ALB) to terminate the TLS connection and forward the request to the Spacelift server running in an ECS container. That is through HTTP.</p> <pre><code>client -&gt; (https) -&gt; Load Balancer -&gt; (http) -&gt; ECS\n</code></pre> <p>If you want the server to use HTTPS as well, you can do so by specifying the TLS certificate in <code>config.json</code>. <code>tls_config.server_certificate_secrets_manager_arn</code> is the ARN of the SecretsManager secret that holds both the private and public keys of your TLS certificate in the following format:</p> <pre><code>{\"privateKey\": \"&lt;base64-encoded-private-key&gt;\", \"publicKey\": \"&lt;base64-encoded-public-key&gt;\"}\n</code></pre> <p>Example <code>config.json</code>:</p> <pre><code>{\n\"tls_config\": {\n\"server_certificate_secrets_manager_arn\": \"arn:aws:secretsmanager:eu-west-1:123456789012:secret:spacelift-server-tls-cert-123456\",\n\"ca_certificates\": []\n}\n}\n</code></pre>"},{"location":"product/administration/advanced-installations.html#using-custom-ca-certificates","title":"Using custom CA certificates","text":"<p>If you use a custom certificate authority to issue TLS certs for components that Spacelift will communicate with, for example your VCS system, you need to provide your custom CA certificates. You can provide these via the <code>config.json</code> file via the <code>tls_config.ca_certificates</code> property:</p> <pre><code>{\n\"tls_config\": {\n\"server_certificate_secrets_manager_arn\": \"\",\n\"ca_certificates\": [\n\"&lt;base64-encoded certificate 1&gt;\",\n\"&lt;base64-encoded certificate 2&gt;\"\n]\n}\n}\n</code></pre> <p>Please note that each certificate should be base64-encoded and formatted onto a single line. For example, if we had the following certificate:</p> <pre><code>-----BEGIN CERTIFICATE-----\nMIIFsTCCA5mgAwIBAgIUDD/4VBfLx5K/tAY+SckH05TJ8i8wDQYJKoZIhvcNAQEL\nBQAwaDELMAkGA1UEBhMCR0IxETAPBgNVBAgMCFNjb3RsYW5kMRAwDgYDVQQHDAdH\nbGFzZ293MRkwFwYDVQQKDBBBZGFtIEMgUm9vdCBDQSAxMRkwFwYDVQQDDBBBZGFt\nIEMgUm9vdCBDQSAxMB4XDTIzMDMxMzExMzYxMVoXDTI1MTIzMTExMzYxMVowaDEL\nMAkGA1UEBhMCR0IxETAPBgNVBAgMCFNjb3RsYW5kMRAwDgYDVQQHDAdHbGFzZ293\nMRkwFwYDVQQKDBBBZGFtIEMgUm9vdCBDQSAxMRkwFwYDVQQDDBBBZGFtIEMgUm9v\ndCBDQSAxMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAxjv/+sInXiQ+\n2Fb+itF8ndlpmmYUoZwYN4dx+2wrcbOVngTvy4sE+33nGBzH4vt4pOhKTWwaYXFI\n0CzqoIoazi8Zl0medyrwtIUDZ1pNcVugb4KAFb9Jbq40Ik3xG6t16maxQJGTiAG2\n/xVtsuYdhnBGx//61SEbEwSpR145/Qf1cba8RlRQMz4QUWNe8XXo3SYaX2kxiw2V\n1Op+fQxg2jf1AyzQXX1ch1jyG5RLESPUMFkBiQwi7LOSCaavfJEUzwqeoORgd7Ti\nuyMV+4Gsb1XAnK7KXYwisGeP5/QNFPAByfAdPjR20rMYYHfxqEDth4Najjmu/iyF\nPGk4CobRhitTtJXT/QxWcvtrRu1BCVnedyESMyiya4Q9dn27rFjjg3ZARqWOZhyq\nOTWHo2mO2FzEJuxhvYNe2iYVp2s8wMTB02nP3wpWoYwje2yDwcjkIl8uXKzEZ9Gf\nFATJaCLoO8o5J2HXsgOIqXlpzU9tUtEew/xTzZqX5A34o8/+NgUtm0F7joWa5mDC\nQB7L8cKfACydfpekJx/gFUGSy/5vdfBzOczc6Bmh66yHPBRDcgyDFnnx34m/XVQa\nrBwwIDDbqu3sscdOgm9v8csCJd0YlXGb/x4oAA61IITnsNd9NCw0GJIquSEcYiCE\nA0YrQTKVfRAXuhSZ1VPIuxXiF2K3XTMCAwEAAaNTMFEwHQYDVR0OBBYEFD55R4mt\n0hNOJUgPL0JBKZB1jybSMB8GA1UdIwQYMBaAFD55R4mt0hNOJUgPL0JBKZB1jybS\nMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggIBAHecVjMklTkS2Py5\nXNpJ9cdzG66GuPDw8aQZIunrxqYud74CA1Y0K26kyDJkLnWzVa7nT+F0d8Qn3tov\nvFwI3xy5l+4upmuZ3u1jFEMiSk8C2FPohLDnDo3rwEUCGvJ6a4Gas7YyHPGL3DrJ\n0dcu9wsX9cYB2YJ27QosZ5s6zmmUvBGTI30JNvPnSoC7kzqD3ArxvTEW9WaUqoJt\n88lsMnn6+ps9A6exb/fK909ZWaEJWRd9cdMET0fna7EhhkO+Cqz415RgMxlK7ggT\n97CvkjvvLNeFT5naHbzUANqfMVRRcUaP3PjTC9z5cDo9CaPaFjV/+Uxax2mAlARk\nfqYyWoqvZH90czpvFG1jUo6P4NpyxZS8layJwD24qX+EON43WYApLsl/jE2A/JmQ\nMdgWNhOy4HP8U8+aANr0Ev7gWWNi6VcR8T6PT/rbAGjnPmVmoZ4rc7CdoS8ZQZJh\nK8ELA17+pnMTgo7wxfARqL+p+mqgtUxRbiWitev8F2hUVB/SwP8hpcGrdhTEN7td\npSW1ykPeGJFKSBo5QHanqqPFCzqtFeoL9DhYx5/xE6FpKMLg3vVcFsHu6glS8iMV\n4Hvb2fXuhXxLTBCbD1+5lLP/bHXogQKmp2H6Oj0e6WBmQ0xqGou4Il6bavsZCx2v\nADWvlue5jXdNu5xPZdsNVNAluAne\n-----END CERTIFICATE-----\n</code></pre> <p>We would base64-encode it and then use a tls_config.json file that looks something like the following:</p> <pre><code>{\n\"ServerCertificateSecretsManagerArn\": \"\",\n\"CACertificates\": [\n\"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\"\n]\n}\n</code></pre>"},{"location":"product/administration/advanced-installations.html#installing-into-a-private-vpc","title":"Installing into a Private VPC","text":"<p>If you want to install Spacelift into a private VPC that does not have any internet access, you need to setup VPC endpoints for the following AWS services:</p> <ul> <li>ECR and ECR Docker endpoint.</li> <li>IoT.</li> <li>KMS.</li> <li>License manager.</li> <li>Logs.</li> <li>Monitoring.</li> <li>S3.</li> <li>Secrets manager.</li> <li>SQS.</li> <li>Xray.</li> </ul> <p>In addition, if you want to deploy a worker pool into a VPC with no internet access and are using our CloudFormation template to deploy the pool, you need to create a VPC endpoint for the following service:</p> <ul> <li>EC2 autoscaling.</li> </ul> <p>The following sections contain example CloudFormation definitions describing each endpoint that needs to be created.</p>"},{"location":"product/administration/advanced-installations.html#proxy-configuration","title":"Proxy Configuration","text":"<p>As well as setting up VPC endpoints, you will also need to configure an HTTP Proxy. This is required because AWS does not currently provide an endpoint for the IoT control plane, meaning that these requests cannot use a private VPC endpoint.</p> <p>When using VPC endpoints, you should include the following in your <code>NO_PROXY</code> environment variable to ensure that requests to the required AWS services are routed via your VPC endpoints rather than via the proxy (making sure to substitute <code>&lt;region&gt;</code> with your install region):</p> <pre><code>s3.&lt;region&gt;.amazonaws.com,license-manager.&lt;region&gt;.amazonaws.com,a3mducvsqca9re-ats.iot.&lt;region&gt;.amazonaws.com,logs.&lt;region&gt;.amazonaws.com,monitoring.&lt;region&gt;.amazonaws.com,sqs.&lt;region&gt;.amazonaws.com,xray.&lt;region&gt;.amazonaws.com,secretsmanager.&lt;region&gt;.amazonaws.com,kms.&lt;region&gt;.amazonaws.com,ecr.&lt;region&gt;.amazonaws.com,api.ecr.&lt;region&gt;.amazonaws.com\n</code></pre>"},{"location":"product/administration/advanced-installations.html#vpc-endpoint-security-group","title":"VPC Endpoint Security Group","text":"<p>You need to provide a security group for the VPC endpoints to use. This security group should allow inbound access on port 443. For example you could use something like the following:</p> <pre><code>VPCEndpointSecurityGroup:\nType: AWS::EC2::SecurityGroup\nProperties:\nGroupName: \"vpc_endpoint_sg\"\nGroupDescription: \"The sg to use for VPC endpoints\"\nSecurityGroupIngress:\n- Description: \"Allow inbound HTTPS access to VPC endpoints from VPC\"\nFromPort: 443\nToPort: 443\nIpProtocol: \"tcp\"\nCidrIp: \"&lt;replace-with-your-own-address-range&gt;\"\nVpcId: {Ref: VPC}\n</code></pre>"},{"location":"product/administration/advanced-installations.html#ecr-and-ecr-docker-endpoint","title":"ECR and ECR Docker endpoint","text":"<p>The following VPC endpoints need to be created:</p> <pre><code>ECRInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.ecr.api\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n\nECRDockerInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.ecr.dkr\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n</code></pre>"},{"location":"product/administration/advanced-installations.html#iot","title":"IoT","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>IoTInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.iot.data\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1}\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: false\n</code></pre> <p>Note that the <code>PrivateDnsEnabled</code> option is set to false. For the IoT data endpoint you need to manually create the correct DNS entry to allow Spacelift workers to connect to the IoT broker for your account.</p> <p>First, run the following command to find the correct IoT endpoint for your region:</p> <pre><code>aws iot describe-endpoint --endpoint-type iot:Data-ATS --region &lt;region&gt; --no-cli-pager --output json\n</code></pre> <p>This should output something like the following:</p> <pre><code>{\n\"endpointAddress\": \"b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com\"\n}\n</code></pre> <p>Next, go to the Route53 console, and create a private hosted zone for your endpoint address. In the example above, this would be <code>b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com</code>.</p> <p>Finally, create an A record for your endpoint address (for example <code>b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com</code>), and use an alias to point it at your IoT VPC endpoint.</p> <p>NOTE: make sure that you create your private hosted zone for your full endpoint address, and not for <code>iot.&lt;region&gt;.amazonaws.com</code>. If you create a hosted zone for <code>iot.&lt;region&gt;.amazonaws.com</code> it will prevent the Spacelift server and drain processes from being able to access the IoT control plane.</p>"},{"location":"product/administration/advanced-installations.html#kms","title":"KMS","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>KMSInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.kms\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n</code></pre>"},{"location":"product/administration/advanced-installations.html#license-manager","title":"License manager","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>LicenseManagerInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.license-manager\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n</code></pre>"},{"location":"product/administration/advanced-installations.html#logs","title":"Logs","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>LogsInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.logs\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n</code></pre>"},{"location":"product/administration/advanced-installations.html#monitoring","title":"Monitoring","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>MonitoringInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.monitoring\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n</code></pre>"},{"location":"product/administration/advanced-installations.html#s3","title":"S3","text":"<p>The following VPC endpoint needs to be created. Also note that each of your Spacelift subnets will also need a route table attached that can be referenced in the endpoint definition:</p> <pre><code>S3GatewayEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.s3\"}\nVpcEndpointType: Gateway\nVpcId: {Ref: VPC}\nRouteTableIds:\n# Attach a route table corresponding to each of the subnets being used for Spacelift\n- {Ref: PrivateSubnet1RouteTable}\n- {Ref: PrivateSubnet2RouteTable}\n- {Ref: PrivateSubnet3RouteTable}\nPolicyDocument:\nVersion: 2012-10-17\nStatement:\n- Effect: Allow\nPrincipal: '*'\nAction: '*'\nResource: '*'\n</code></pre>"},{"location":"product/administration/advanced-installations.html#secrets-manager","title":"Secrets manager","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>SecretsManagerInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.secretsmanager\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n</code></pre>"},{"location":"product/administration/advanced-installations.html#sqs","title":"SQS","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>SQSInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.sqs\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n</code></pre>"},{"location":"product/administration/advanced-installations.html#xray","title":"Xray","text":"<p>The following VPC endpoint needs to be created:</p> <pre><code>XrayInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.xray\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n</code></pre>"},{"location":"product/administration/advanced-installations.html#ec2-autoscaling","title":"EC2 Autoscaling","text":"<p>The following optional VPC endpoint can be created if using our CloudFormation worker pool template:</p> <pre><code>AutoscalingInterfaceEndpoint:\nType: AWS::EC2::VPCEndpoint\nProperties:\nVpcEndpointType: Interface\nServiceName: {Fn::Sub: \"com.amazonaws.${AWS::Region}.autoscaling\"}\nVpcId: {Ref: VPC}\nSubnetIds:\n- {Ref: PrivateSubnet1} # Replace this with at least one of your subnets\nSecurityGroupIds:\n- {Ref: VPCEndpointSecurityGroup}\nPrivateDnsEnabled: true\n</code></pre>"},{"location":"product/administration/changelog.html","title":"Changelog","text":""},{"location":"product/administration/changelog.html#changes-between-v0011-and-v0012","title":"Changes between v0.0.11 and v0.0.12","text":""},{"location":"product/administration/changelog.html#features","title":"Features","text":"<ul> <li>OpenTofu v1.6.0 support</li> <li>PRs as notification targets</li> <li>Run prioritization through Push Policy (<code>prioritize</code> keyword)</li> <li>Add state size (in bytes) to <code>ManagedStateVersion</code> type in GraphQL</li> </ul>"},{"location":"product/administration/changelog.html#fixes","title":"Fixes","text":"<ul> <li>Various backend and frontend fixes and improvements</li> </ul>"},{"location":"product/administration/changelog.html#changes-between-v0010-and-v0011","title":"Changes between v0.0.10 and v0.0.11","text":""},{"location":"product/administration/changelog.html#features_1","title":"Features","text":"<ul> <li>New stack creation view</li> <li>Auto Attaching Contexts</li> <li>Context Hooks</li> <li>Additional project globs</li> <li>Pull request default behaviour change<ul> <li>Spacelift will start handling pull request events and creating proposed runs if no push policy is set as the default behaviour</li> </ul> </li> </ul>"},{"location":"product/administration/changelog.html#fixes_1","title":"Fixes","text":"<ul> <li>Various backend and frontend fixes and improvements</li> </ul>"},{"location":"product/administration/changelog.html#changes-between-v009-and-v0010","title":"Changes between v0.0.9 and v0.0.10","text":""},{"location":"product/administration/changelog.html#features_2","title":"Features","text":"<ul> <li>Stack Dependencies with output/input references.</li> <li>Ready run state.</li> <li>Targeted replan support.</li> <li>New detailed terraform changes view.</li> <li>Worker Pool Management views.</li> <li>Add OpenTofu and custom workflows support for terraform.</li> </ul>"},{"location":"product/administration/changelog.html#fixes_2","title":"Fixes","text":"<ul> <li>Do not re-create SAML certificate during each install</li> </ul>"},{"location":"product/administration/changelog.html#changes-between-v008-and-v009","title":"Changes between v0.0.8 and v0.0.9","text":""},{"location":"product/administration/changelog.html#features_3","title":"Features","text":"<ul> <li>Increase worker default disk size to 40GB.</li> <li>Adding support for Terraform versions up to v1.5.7.</li> <li>Update frontend and backend to the latest versions.</li> </ul>"},{"location":"product/administration/changelog.html#fixes_3","title":"Fixes","text":"<ul> <li>Enforce bucket policy to prevent objects getting fetched not using HTTPS.</li> <li>Updated no account ID message to indicate that it is caused by missing AWS credentials in the install script.</li> </ul>"},{"location":"product/administration/changelog.html#changes-between-v007-and-v008","title":"Changes between v0.0.7 and v0.0.8","text":""},{"location":"product/administration/changelog.html#features_4","title":"Features","text":"<ul> <li>Update CloudFormation worker pool template to allow a custom instance role to be provided.</li> <li>Update CloudFormation worker pool template to allow poweroff on crash to be disabled to aid debugging.</li> <li>Update CloudFormation worker pool template to allow custom user data to be provided.</li> <li>Update frontend and backend to the latest versions.</li> <li>Adding support for Terraform versions up to v1.5.4 and kubectl up to v1.27.4.</li> <li>Added support for External Dependencies.</li> <li>Added support for Raw Git source code provider.</li> </ul>"},{"location":"product/administration/changelog.html#removals","title":"Removals","text":"<ul> <li>Remove the unused <code>ecs-state-handler</code> Lambda.</li> </ul>"},{"location":"product/administration/changelog.html#fixes_4","title":"Fixes","text":"<ul> <li>Improve warning message during installation when changeset contains no changes.</li> <li>Fix role assumption and automatic ECR login in GovCloud regions.</li> <li>Don't incorrectly attempt to report errors to Bugsnag in Self-Hosting (errors were never reported, but this could cause some misleading log entries).</li> <li>Fix crash on run startup if the runner image was missing the <code>ps</code> command.</li> <li>Increase default worker pool size to <code>t3.medium</code>.</li> <li>Increase minimum drain instances to 3 to provide more resilience.</li> </ul>"},{"location":"product/administration/install.html","title":"Installation Guide","text":"<p>This guide contains instructions on installing a self-hosted copy of Spacelift in an AWS account you control.</p>"},{"location":"product/administration/install.html#pre-requisites","title":"Pre-requisites","text":"<p>Before proceeding with the installation, you need to satisfy the following pre-requisites:</p> <ul> <li>You need access to an AWS account you wish to install Spacelift into.</li> <li>You need to choose a hostname that you wish to use for your Spacelift installation, for example <code>spacelift.example.com</code>. This needs to be on a domain that you control and can add DNS records to.</li> <li>You need to create an ACM certificate for your chosen domain in the same account that you want to install Spacelift in.</li> </ul>"},{"location":"product/administration/install.html#requirements","title":"Requirements","text":"<p>The installation process requires the following tools:</p> <ul> <li>A Mac or Linux machine to run the installation script from.</li> <li>A copy of the AWS CLI v2, configured to access the account you wish to install Spacelift into.</li> <li>jq version 1.6.</li> <li>Standard unix utilities including bash, base64, cat, read, openssl.</li> <li>Docker.</li> </ul>"},{"location":"product/administration/install.html#spacelift-infrastructure","title":"Spacelift infrastructure","text":""},{"location":"product/administration/install.html#server-and-drain","title":"Server and drain","text":"<p>The Spacelift infrastructure has two core parts: the server and the drain. The server is a web application that provides a GraphQL API that serves frontend and every HTTPS request in general. The drain is a worker that processes all asynchronous tasks such as VCS webhooks, run scheduling, worker handling, etc. The drain consumes messages from several SQS queues. The server serves requests through a load balancer.</p> <p>As of today, both the server and the drain are ECS services running on Fargate. We have autoscaling set up for the server so ideally you don't need to worry about scaling it.</p> <p></p>"},{"location":"product/administration/install.html#worker-pool-infrastructure","title":"Worker pool infrastructure","text":"<p>A worker pool is a group of workers that can be used to run workloads. During the startup the worker will attempt to connect to the regional AWS IoT Core broker endpoint and register itself. The drain and the server will then be able to communicate with the worker via AWS IoT Core, specifically via MQTT.</p> <p></p>"},{"location":"product/administration/install.html#running-costs","title":"Running Costs","text":"<p>We estimate the baseline running costs of a Spacelift Self-Hosted instance to be around $15 per day (roughly $450 per month). Note that this is with no activity in your account, so your costs may be higher after factoring in things like bandwidth.</p> <p>These baseline costs include all of the resources deployed as part of your Spacelift install, for example Aurora RDS, Fargate cluster, and KMS keys.</p>"},{"location":"product/administration/install.html#installation","title":"Installation","text":"<p>This section explains the installation process for Spacelift. You may also be interested in the following pages that explain how to configure the Slack integration as well as advanced installations:</p> <ul> <li>Slack integration setup - explains how to configure the Slack integration for your Spacelift instance.</li> <li>Advanced installations - explains how to configure advanced options like providing a custom VPC configuration, or specifying HTTP proxy settings.</li> </ul>"},{"location":"product/administration/install.html#aws-requirements","title":"AWS Requirements","text":"<p>Before you start the installation process, make sure the following requirements are met:</p> <ul> <li>The region that you wish to install self-hosting in has at least 3 EIPs available. The default quota per region in an AWS account only allows 5 EIPs to be created, so you may need to choose another region or ask for an increase. These EIPs are used as part of NAT Gateways to allow outbound traffic from Spacelift. If you want full control over your networking setup, please see advanced installations.</li> </ul>"},{"location":"product/administration/install.html#accepting-your-license","title":"Accepting your License","text":"<p>When you sign up for self-hosting, a license will be issued to your AWS account via AWS License Manager. Before you can use your license, you need to accept it.</p> <p>Navigate to AWS License Manager in your AWS Console, and go to Granted licenses:</p> <p></p> <p>Note: if this is your first time accessing License Manager, you may need to grant permissions to AWS before you can use it. If this is the case you will automatically be prompted to grant permission.</p> <p>Click on your license ID, and then choose the Accept &amp; activate license option on the license page:</p> <p></p> <p>Follow the instructions on the popup that appears to activate your license:</p> <p></p> <p>That\u2019s it - you\u2019re now ready to proceed with the rest of the installation!</p>"},{"location":"product/administration/install.html#release-archive","title":"Release Archive","text":"<p>Spacelift self-hosted is distributed as an archive containing everything needed to install Spacelift into your AWS account. The archive has the following structure:</p> <ul> <li><code>config.json</code> - the configuration file containing all necessary configuration options. Some options come prepopulated with default values.</li> <li><code>bin</code> - contains binaries including a copy of the launcher built for self-hosting.</li> <li><code>cloudformation</code> - contains CloudFormation templates used to deploy Spacelift.</li> <li><code>container-images</code> - contains container images for running the Spacelift backend as well as a launcher image.</li> <li><code>install.sh</code> - the installation script.</li> <li><code>uninstall.sh</code> - the uninstallation script.</li> <li><code>scripts</code> - contains other scripts called by the installation script.</li> <li><code>version</code> - contains the version number.</li> </ul>"},{"location":"product/administration/install.html#signature-validation","title":"Signature Validation","text":"<p>Along with the release archive, we also provide a SHA-256 checksum of the archive as well as a GPG signature. The fingerprint of our GPG key is <code>380BD7699053035B71D027B173EBA0CF3B3F4A46</code>, and you can import it using the following command:</p> <pre><code>gpg --recv-keys 380BD7699053035B71D027B173EBA0CF3B3F4A46\n</code></pre> <p>You can verify the integrity of the release archive using the following command:</p> <pre><code>sha256sum -c self-hosted-&lt;version&gt;.tar.gz_SHA256SUMS\n</code></pre> <p>And you can verify the authenticity using the following command:</p> <pre><code>gpg --verify self-hosted-&lt;version&gt;.tar.gz_SHA256SUMS.sig\n</code></pre>"},{"location":"product/administration/install.html#extraction","title":"Extraction","text":"<p>First, extract the release artifacts and move to the extracted directory (replacing <code>&lt;version&gt;</code> with the version you are installing):</p> <pre><code>tar -zxf self-hosted-&lt;version&gt;.tar.gz\ncd self-hosted-&lt;version&gt;\n</code></pre>"},{"location":"product/administration/install.html#configuration","title":"Configuration","text":"<p>The included <code>config.json</code> file provides an easy way to provide additional and required configuration for the resources created during the deployment.</p> <p>The mandatory fields are:</p> <ul> <li><code>account_name</code> - the name of your Spacelift account. Note: the URL of your Spacelift installation doesn't necessarily need to match this name but the account name affects the URL of the module registry.</li> <li><code>aws_region</code> - the AWS region you wish to install Spacelift into.</li> <li><code>load_balancer.certificate_arn</code> - the ARN of the ACM certificate you created in the pre-requisites.</li> <li><code>spacelift_hostname</code> - the hostname you wish to use for your Spacelift installation without the protocol or trailing slash, for example <code>spacelift.mycorp.com</code>.</li> <li><code>sso_config.admin_login</code> - the email address of the user you wish to use as the initial admin user for your Spacelift installation.</li> <li><code>sso_config.sso_type</code> - the type of SSO you wish to use. Valid values are <code>OIDC</code> and <code>SAML</code>.</li> <li><code>sso_config.oidc_args</code> - if <code>sso_type</code> is <code>OIDC</code>, all fields are mandatory:<ul> <li><code>client_id</code> - the OIDC client ID.</li> <li><code>client_credentials</code> - the OIDC client secret.</li> <li><code>identity_provider_host</code> - the OIDC identity provider host with protocol. Example: <code>\"https://mycorp.okta.com\"</code>.</li> </ul> </li> <li><code>sso_config.saml_args</code> - if <code>sso_type</code> is <code>SAML</code>, <code>dynamic</code> and <code>metadata</code> fields are mandatory:<ul> <li><code>dynamic</code> - <code>true</code> or <code>false</code>. if <code>true</code> then <code>metadata</code> must be a URL to the SAML IDP metadata. If <code>false</code> then <code>metadata</code> must be the SAML IDP metadata.</li> <li><code>metadata</code> - either the full SAML IDP metadata or the URL to the SAML IDP metadata.</li> <li><code>name_id_format</code> - SAML name identifier format, can be left empty. Valid values are <code>TRANSIENT</code>, <code>EMAIL_ADDRESS</code> or <code>PERMANENT</code>. Defaults to <code>TRANSIENT</code>.</li> </ul> </li> </ul> <p>NOTE: we recommend that you store your config.json file so that you can reuse it when upgrading to newer Spacelift versions.</p>"},{"location":"product/administration/install.html#optional-configuration-options","title":"Optional configuration options","text":""},{"location":"product/administration/install.html#load-balancer","title":"Load balancer","text":"<p>Load balancer configuration has a mix of required and optional fields alongside some which should already be prefilled. The object itself looks like this:</p> <pre><code>\"load_balancer\": {\n\"certificate_arn\": \"\",\n\"scheme\": \"internet-facing\",\n\"ssl_policy\": \"ELBSecurityPolicy-TLS-1-2-2017-01\",\n\"tag\": {\n\"key\": \"\",\n\"value\": \"\"\n}\n}\n</code></pre> <p>The prefilled fields are valid defaults and can be left unchanged, while the <code>certificate_arn</code> field is required and must be set, and the <code>tag</code> object is optional and can be used to set a custom tag against the load balancer resource.</p> <p>The <code>scheme</code> can be either <code>internet-facing</code> or <code>internal</code> and defaults to <code>internet-facing</code>: internal load balancers can only route requests from clients with access to the VPC for the load balancer, while internet-facing load balancers can route requests from clients over the internet.</p> <p><code>ssl_policy</code> is the name of the security policy that defines ciphers and protocols. The default value is <code>ELBSecurityPolicy-TLS-1-2-2017-01</code> which is the most recent security policy that supports TLS 1.2. For more information, see Security policies.</p>"},{"location":"product/administration/install.html#database","title":"Database","text":"<p>You can configure the following options for the Spacelift Postgres database:</p> <ul> <li><code>database.delete_protection_enabled</code> - whether to enable deletion protection for the database (defaults to <code>true</code>). Note: <code>uninstall.sh</code> script will disable this option before deleting the database.</li> <li><code>database.instance_class</code> - the instance class of the database (defaults to <code>db.t4g.large</code>).</li> </ul>"},{"location":"product/administration/install.html#monitoring-alerting","title":"Monitoring &amp; Alerting","text":"<p>As part of the self-hosting installation, we deploy a monitoring dashboard to help you monitor the health of your Spacelift installation. It is accessible via the CloudWatch UI.</p> <p>Additionally, we can also create a few preconfigured alerts for you. You can configure the following options for the CloudWatch alarms:</p> <pre><code>\"alerting\": {\n\"sns_topic_arn\": \"\"\n}\n</code></pre> <p>If an SNS topic ARN is configured, we'll create an SNS subscription for each alarm. If left empty, we won't create any alarms, only the monitoring dashboard.</p> <p>Important! Your SNS topic's access policy must allow the <code>cloudwatch.amazonaws.com</code> service principal to publish to the topic. An example access policy:</p> <pre><code>{\n\"Sid\": \"Allow_Publish_Alarms\",\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": \"cloudwatch.amazonaws.com\"\n},\n\"Action\": \"sns:Publish\",\n\"Resource\": \"&lt;sns-topic-arn&gt;\"\n}\n</code></pre>"},{"location":"product/administration/install.html#global-tags","title":"Global tags","text":"<p>You can add additional tags to all the resources created by the installer by adding your desired tags to the <code>global_resources_tags</code> array in the config.json:</p> <pre><code>\"global_resource_tags\": [\n{\n\"key\": \"selfhost\",\n\"value\": \"spacelift\"\n}\n]\n</code></pre>"},{"location":"product/administration/install.html#identity-provider","title":"Identity Provider","text":""},{"location":"product/administration/install.html#urls","title":"URLs","text":"<p>You may need certain URLs when configuring an application in your identity provider. For SAML, use the following URLs:</p> <ul> <li>Single Sign-On URL: <code>https://&lt;spacelift-hostname&gt;/saml/acs</code></li> <li>Entity ID (audience): <code>https://&lt;spacelift-hostname&gt;/saml/metadata</code></li> </ul> <p>For OIDC, use the following URL:</p> <ul> <li>Authorized redirect URL: <code>https://&lt;spacelift-hostname&gt;/oidc/exchange</code></li> </ul> <p>NOTE: please make sure to substitute <code>&lt;spacelift-hostname&gt;</code> with the hostname you plan to use for your Spacelift install.</p>"},{"location":"product/administration/install.html#saml-metadata","title":"SAML Metadata","text":"<p>If you are using non-dynamic SAML metadata rather than using a dynamic metadata URL, you need to ensure that the metadata provided is a valid JSON-escaped string. One way to do this is to use <code>jq</code> to escape your metadata. For example, if your metadata was contained in a file called metadata.xml, you could run the following command:</p> <pre><code>cat metadata.xml| jq -R -s '.'\n</code></pre> <p>You would then enter the escaped string into your config.json file:</p> <pre><code>\"saml_args\": {\n\"metadata\": \"&lt;?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?&gt;&lt;EntityDescriptor ID=\\\"_90756ab2...\",\n\"dynamic\": false,\n\"name_id_format\": \"EMAIL_ADDRESS\"\n}\n</code></pre>"},{"location":"product/administration/install.html#running-the-installer","title":"Running the installer","text":"<p>This section covers simple installations using a Spacelift-created VPC and a public facing HTTP load balancer. For information about using an existing VPC please see advanced installations.</p> <p>To run the installer, pass in the path of the configuration file:</p> <pre><code>./install.sh [-c \"&lt;configuration file&gt;\"]\n</code></pre> <p>The <code>-c</code> flag is optional, it defaults to <code>config.json</code> if not specified.</p> <p>When the installer starts, it will check it can connect to your AWS account, and will ask for confirmation to continue. Please check the AWS account ID is correct, and if so enter <code>yes</code>:</p> <pre><code>./install.sh\n[2023-01-24T12:17:52+0000] INFO: installing version v0.0.6 of Spacelift into AWS account 123456789012\n\nAre you sure you want to continue? Only 'yes' will be accepted: yes\n</code></pre> <p>Please note, the installation process can take between 10 and 20 minutes to create all of the required infrastructure for Spacelift.</p>"},{"location":"product/administration/install.html#troubleshooting","title":"Troubleshooting","text":"<p>If you see the following error message indicating that the account could not be found, please check the credentials for your AWS CLI are correctly configured:</p> <pre><code>ERROR: could not find AWS account ID. Cannot continue with Spacelift install. Please check that your AWS CLI credentials are correct and have not expired.\n</code></pre> <p>This error message can also be displayed if your AWS credentials are connected to a GovCloud account but a non-GovCloud region has been specified in the <code>aws_region</code> configuration option.</p>"},{"location":"product/administration/install.html#setting-up-dns-entries","title":"Setting up DNS entries","text":"<p>Once the installer has completed, you should see output similar to the following, providing you with the DNS address of the load balancer along with the Launcher container image URL:</p> <pre><code>Installation info:\n\n  * Load balancer DNS: spacelift-server-1234567890.eu-west-1.elb.amazonaws.com\n  * Launcher container image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/spacelift-launcher:v0.0.6\n\n[2023-01-24T11:30:59+0000] INFO: Spacelift version v0.0.6 has been successfully installed!\n</code></pre> <p>Please use the Load balancer DNS to setup a CNAME entry or an A record using an alias. This entry should point from the hostname you want to use for Spacelift (e.g. <code>spacelift.saturnhead.io</code>) to the Spacelift Load Balancer.</p> <p>Once your DNS changes propagate, you should be able to login to your instance by navigating to its hostname (for example <code>https://spacelift.saturnhead.io</code>). Assuming everything has been successful, you should see a welcome screen similar to the following:</p> <p></p>"},{"location":"product/administration/install.html#creating-your-first-worker-pool","title":"Creating your first worker pool","text":"<p>Before you can create stacks or trigger any runs, you need a worker pool. For more information please see our worker pools page.</p>"},{"location":"product/administration/install.html#updating-existing-sso-configuration","title":"Updating existing SSO configuration","text":"<p>If you already have an existing SSO configuration and want to update it, you need to update the <code>sso_config</code> section of the configuration file and run the update script:</p> <pre><code>./scripts/update-sso-settings.sh [-c \"&lt;configuration file&gt;\"]\n</code></pre> <p>It will run the ECS task that will update the SSO configuration.</p>"},{"location":"product/administration/install.html#upgrading","title":"Upgrading","text":"<p>To upgrade to the latest version of self-hosting, follow these steps:</p> <ol> <li>Make sure your config.json file is fully configured to match your existing installation. Ideally you should use the config file from your previous installation.</li> <li>Run <code>./install.sh</code>.</li> <li>Deploy the latest version of the CloudFormation worker pool template, and restart any workers connected to your Spacelift installation to make sure they're running the latest version.</li> </ol>"},{"location":"product/administration/install.html#uninstalling","title":"Uninstalling","text":"<p>If you want to completely uninstall Spacelift, you can use the <code>uninstall.sh</code> script. By default, the script will retain S3 buckets, database and KMS keys so that they can be restored later.</p> <p>To run the uninstall script, use the following command, specifying your AWS region:</p> <pre><code>./uninstall.sh [-c &lt;config-file&gt;] [-f | -n | -h]\n</code></pre> <p>Flags:</p> <ul> <li><code>-c &lt;config-file&gt;</code>: path to the config file (default: <code>config.json</code>)</li> <li><code>-f</code>: force uninstallation, do not prompt for confirmation</li> <li><code>-n</code>: do not retain S3 buckets, database or KMS keys. complete uninstallation.</li> <li><code>-h</code>: show help</li> </ul> <p>For example:</p> <pre><code>./uninstall.sh\n</code></pre>"},{"location":"product/administration/slack-integration-setup.html","title":"Slack integration setup","text":"<p>If you want to use the Slack integration in your self-hosting instance, you need to create your own Slack app and add its details to the config.json file before running the self-hosting installer. This section explains how to do that, along with describing limitations of the Slack integration in self-hosting.</p>"},{"location":"product/administration/slack-integration-setup.html#known-limitations","title":"Known Limitations","text":"<p>The Slack integration relies on Slack being able to communicate with Spacelift in order to provide support for Slash commands. This means that Slack must be able to access your Spacelift load balancer in order for it to be able to make requests to <code>https://&lt;your-spacelift-domain&gt;/webhooks/slack</code>.</p> <p>This means that if you are using an internal load balancer for Spacelift, Slack will not be able to access this endpoint.</p>"},{"location":"product/administration/slack-integration-setup.html#creating-your-slack-app","title":"Creating your Slack app","text":"<p>First, create a new Slack app in your workspace by navigating to https://api.slack.com/apps and following these instructions:</p> <ul> <li>Click \"Create New App\"</li> <li>Choose \"From an app manifest\"</li> <li>Select your workspace</li> <li>Paste following manifest, replacing <code>&lt;your-domain&gt;</code> with the domain you want to host the self-hosted Spacelift instance on.</li> </ul> <pre><code>{\n\"display_information\": {\n\"name\": \"Spacelift\",\n\"description\": \"Taking your infra-as-code to the next level\",\n\"background_color\": \"#131417\",\n\"long_description\": \"Spacelift is a sophisticated and compliant infrastructure delivery platform for Terraform (including Terragrunt), Pulumi, CloudFormation, Ansible, and Kubernetes.\\r\\n\\r\\n\u2022 No lock-in. Under the hood, Spacelift uses your choice of Infrastructure as Code providers: open-source projects with vibrant ecosystems and a multitude of existing providers, modules, and tutorials.\\r\\n\\r\\n\u2022 Works with your Git flow. Spacelift integrates with GitHub (and other VCSes) to provide feedback on commits and Pull Requests, allowing you and your team to preview the changes before they are applied.\\r\\n\\r\\n\u2022 Drift detection. Spacelift natively detects drift, and can optionally revert it, to provide visibility and awareness to those \\\"changes\\\" that will inevitably happen.\\r\\n\\r\\n\u2022 Policy as a Code. With Open Policy Agent (OPA) Rego, you can programmatically define policies, approval flows, and various decision points within your Infrastructure as Code flow.\\r\\n\\r\\n\u2022 Customize your runtime. Spacelift uses Docker to run its workflows, which allows you to fully control your execution environment.\\r\\n\\r\\n\u2022 Share config using contexts. Spacelift contexts are collections of configuration files and environment variables that can be attached to multiple stacks.\\r\\n\\r\\n\u2022 Look ma, no credentials. Spacelift integrates with identity management systems from major cloud providers; AWS, Azure, and Google Cloud, allowing you to set up limited temporary access to your resources without the need to supply powerful static credentials.\\r\\n\\r\\n\u2022 Manage programmatically. With the Terraform provider, you can manage Spacelift resources as code.\\r\\n\\r\\n\u2022 Protect your state. Spacelift supports a sophisticated state backend and can optionally manage the state on your behalf.\"\n},\n\"features\": {\n\"bot_user\": {\n\"display_name\": \"Spacelift\",\n\"always_online\": true\n},\n\"slash_commands\": [\n{\n\"command\": \"/spacelift\",\n\"url\": \"https://&lt;your-domain&gt;/webhooks/slack\",\n\"description\": \"Get notified about Spacelift events\",\n\"usage_hint\": \"subscribe, unsubscribe or help\",\n\"should_escape\": false\n}\n]\n},\n\"oauth_config\": {\n\"redirect_urls\": [\n\"https://&lt;your-domain&gt;/slack_oauth\"\n],\n\"scopes\": {\n\"bot\": [\n\"channels:read\",\n\"chat:write\",\n\"chat:write.public\",\n\"commands\",\n\"links:write\",\n\"team:read\",\n\"users:read\"\n]\n}\n},\n\"settings\": {\n\"event_subscriptions\": {\n\"request_url\": \"https://&lt;your-domain&gt;/webhooks/slack\",\n\"bot_events\": [\n\"app_uninstalled\"\n]\n},\n\"interactivity\": {\n\"is_enabled\": true,\n\"request_url\": \"https://&lt;your-domain&gt;/webhooks/slack\"\n},\n\"org_deploy_enabled\": false,\n\"socket_mode_enabled\": false,\n\"token_rotation_enabled\": false\n}\n}\n</code></pre>"},{"location":"product/administration/slack-integration-setup.html#configuring-the-spacelift-installer","title":"Configuring the Spacelift installer","text":"<p>Once it's done, you just need to copy the relevant information from the \"Basic Information\" tab of your Slack App to the configuration file:</p> <pre><code>{\n\"slack_config\": {\n\"enabled\": true,\n\"client_id\": \"&lt;client id here&gt;\",\n\"client_secret\": \"&lt;client secret here&gt;\",\n\"signing_secret\": \"&lt;signing secret here&gt;\"\n}\n}\n</code></pre> <p>Once you have populated your configuration, just run the installer as described in the installation guide.</p> <p>After the installation script finishes. You can now go to your selfhosted Spacelift instance, to Settings -&gt; Slack and install the Slack app into your workspace.</p>"},{"location":"product/support/index.html","title":"Support","text":"<p>Spacelift offers a variety of support options depending on your needs. You should be able to find help using the resources linked below, regardless of how you use Spacelift.</p>"},{"location":"product/support/index.html#have-you-tried","title":"Have you tried\u2026","text":"<p>Before reaching out for support, have you tried:</p> <ul> <li> <p>Searching our documentation. Most answers can be found there.</p> </li> <li> <p>Reviewing the Scope of Support section to understand what is within the scope of Spacelift support.</p> </li> </ul>"},{"location":"product/support/index.html#contacting-support","title":"Contacting Support","text":"<p>For technical questions related to the Spacelift product, open a support ticket in the shared Slack channel (preferred, when available), or email support@spacelift.io if using Slack is not possible.</p> <p>Questions related to billing, purchasing, or subscriptions should be sent to ar@spacelift.io.</p>"},{"location":"product/support/index.html#support-sla","title":"Support SLA","text":"<p>The SLA times listed below are the timeframes in which you can expect the first response. Spacelift will make the best effort to resolve any issues to your satisfaction as quickly as possible. However, the SLA times are not to be considered as an expected time-to-resolution.</p> Severity First Response Time Working Hours Critical 1 hour 24x7 Major 8 hours 4 am - 8 pm (ET), Mon - Fri Minor 48 hours 4 am - 8 pm (ET), Mon - Fri General Guidance 72 hours 4 am - 8 pm (ET), Mon - Fri <p>Spacelift has support engineers in Europe and the US. They observe local holidays, so the working hours might change on those days. We have engineers on-call 24/7 for Critical incidents, so those are not impacted by holidays.</p>"},{"location":"product/support/index.html#definitions-of-severity-level","title":"Definitions of Severity Level","text":"<ul> <li>Severity 1 - Critical: A critical incident with very high impact (e.g., A customer-facing service is down for all customers).</li> <li>Severity 2 - Major: A major incident with significant impact. (e.g., A customer-facing service is down for a sub-set of customers).</li> <li>Severity 3 - Minor: A minor incident with low impact:<ul> <li>Spacelift use has a minor loss of operational functionality, regardless of the environment or usage (e.g., A system bug creates a minor inconvenience to customers).</li> <li>Important Spacelift features are unavailable or somewhat slowed, but a workaround is available.</li> </ul> </li> <li>Severity 4 - General Guidance: Implementation or production use of Spacelift is continuing, and work is not impeded (e.g., Information, an enhancement, or documentation clarification is requested, but there is no impact on the operation of Spacelift).</li> </ul> <p>Severity is assessed by Spacelift engineers based on the information at their disposal. Make sure to clearly and thoroughly communicate the extent and impact of an incident when reaching out to support to ensure it gets assigned the appropriate severity.</p>"},{"location":"product/support/index.html#scope-of-support","title":"Scope of Support","text":"<p>The scope of support, in the simplest terms, is what we support and what we do not. Ideally, we would support everything. However, without reducing the quality of our support or increasing the price of our product, this would be impossible. These \"limitations\" help create a more consistent and efficient support experience.</p> <p>Please understand that any support that might be offered beyond the scope defined here is done at the discretion of the Support Engineer and is provided as a courtesy.</p>"},{"location":"product/support/index.html#spacelift-features-and-adjacent-technologies","title":"Spacelift Features and Adjacent Technologies","text":"<p>Of course, we provide support for all Spacelift features, but also for adjacent parts of the third parties we integrate with.</p> <p>Here are some examples of what is in scope and what is not for some of the technologies we support:</p> Technology In Scope Out of Scope Cloud Provider Helping configure the permissions used with a Cloud Integration Helping architecture a cloud account IaC Tool Helping troubleshoot a failed deployment Helping architecture your source code VCS Provider Helping troubleshoot events not triggering Spacelift runs Advising how to best configure a VCS provider repository"},{"location":"product/support/index.html#requirements","title":"Requirements","text":"<p>Spacelift cannot provide training on the use of the underlying technologies that Spacelift integrates with. Spacelift is a product aimed at technical users, and we expect our users to be versed in the basic usage of the technologies related to features that they seek support for.</p> <p>For example, a customer looking for help with a Kubernetes integration should understand Kubernetes to the extent that they can retrieve log files or perform other essential tasks without in-depth instruction.</p> <p>For Self-Hosted, we do not provide support for the underlying cloud account that hosts Spacelift. We expect the network, security, and other components to be configured and maintained in a way that is compatible with Spacelift requirements</p>"},{"location":"product/support/index.html#feature-preview","title":"Feature Preview","text":""},{"location":"product/support/index.html#alpha-features","title":"Alpha Features","text":"<p>Alpha features are not yet thoroughly tested for quality and stability, may contain bugs or errors, and be prone to see breaking changes in the future. You should not depend on them, and the functionality is subject to change. As such, support is provided on a best-effort basis.</p>"},{"location":"product/support/index.html#beta-features","title":"Beta Features","text":"<p>We provide support for Beta features on a commercially-reasonable effort basis. Because they are not yet thoroughly tested for quality and stability, we may not yet have identified all the corner cases and may be prone to see breaking changes in the future. Also, troubleshooting might require more time and assistance from the Engineering team.</p>"},{"location":"vendors/ansible/index.html","title":"Ansible","text":"<p>You can find more details in the subpages:</p> <ul> <li>Getting Started</li> <li>Reference</li> <li>Using Policies with Ansible stacks</li> <li>Ansible Galaxy</li> </ul>"},{"location":"vendors/ansible/index.html#why-use-ansible","title":"Why use Ansible?","text":"<p>Ansible is a versatile and battle-tested infrastructure configuration tool. It can do anything from software provisioning to configuration management and application deployment.</p> <p>You can tap into the wealth of roles, playbooks, and collections available online to get started in no time.</p>"},{"location":"vendors/ansible/index.html#why-use-spacelift-with-ansible","title":"Why use Spacelift with Ansible?","text":"<p>Spacelift helps you manage the complexities and compliance challenges of using Ansible. It brings with it a GitOps flow, so your infrastructure repository is synced with your Ansible Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies, which lets you automate compliance checks and build complex multi-stack workflows.</p> <p>You can also use Spacelift to mix and match Terraform, Pulumi, AWS CloudFormation, Kubernetes, and Ansible Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like a set of AWS EC2 instances with all their dependencies) and then connect that to an Ansible Stack which then transactionally configures these EC2 instances using trigger policies.</p>"},{"location":"vendors/ansible/ansible-galaxy.html","title":"Ansible Galaxy","text":"<p>If you followed previous examples in our Ansible documentation, you might have noticed that we do not do much in the Initialization phase.</p> <p></p> <p>If it comes to Ansible stacks, during that phase we try to auto-detect the requirements.yml file that will be used to install dependencies. We will look for it in the following locations:</p> <ul> <li><code>requirements.yml</code> in the root directory</li> <li><code>roles/requirements.yml</code> for roles requirements</li> <li><code>collections/requirements.yml</code> for collections requirements</li> </ul> <p>Tip</p> <p>We also check for the alternative <code>.yaml</code> extension for the paths listed above.</p> <p>As an example, try using an example <code>requirements.yml</code> file.</p> Example requirements.yml file<pre><code>---\ncollections:\n- name: community.grafana\nversion: 1.3.1\n</code></pre> <p>After our Initialization phase detects this file, it will use Ansible Galaxy to install those dependencies.</p> <p></p>"},{"location":"vendors/ansible/getting-started.html","title":"Getting Started","text":""},{"location":"vendors/ansible/getting-started.html#prerequisites","title":"Prerequisites","text":"<p>To follow this tutorial, you should have an EC2 instance together with an SSH private key that can be used to access the instance ready.</p>"},{"location":"vendors/ansible/getting-started.html#initial-setup","title":"Initial Setup","text":"<p>Start by forking our Ansible example repository</p> <p>Looking at the code, you'll find that it configures a simple Apache HTTP Server on an EC2 instance. We are also using the AWS EC2 inventory plugin to find the hosts to configure. Feel free to modify <code>aws_ec2.yml</code> inventory file to fit your needs.</p> <p>Also, please take notice of the Spacelift runtime config file, that defines the runner image used on this stack and the <code>ANSIBLE_CONFIG</code> environment variable. Remember you can always define runner image in stack settings, and environment variables within environment settings.</p>"},{"location":"vendors/ansible/getting-started.html#creating-a-stack","title":"Creating a stack","text":"<p>In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift.</p> <p>In the first tab, you should select the repository you've just forked, as can be seen in the picture.</p> <p></p> <p>In the next tab, you should choose the Ansible backend. There, fill in the Playbook field with the playbook you want to run. In the case of our example, it is playbook.yml. You could also configure an option to skip the planning phase, but we will leave that disabled for now.</p> <p></p> <p>On the next tab (Define Behavior), setting up a runner image with Ansible dependencies is required. You may use your image (with the Ansible version you choose and all the required dependencies) or use one of our default ones. In this example we are using an AWS-based runner image defined in the runtime configuration file: <code>public.ecr.aws/spacelift/runner-ansible-aws:latest</code>.</p> <p>If you have a private worker pool you'd like to use, you can specify it there instead of the default public one as well.</p> <p>Finally, choose a name for your Spacelift Stack on the last page. We'll use Ansible Example again.</p> <p></p> <p></p>"},{"location":"vendors/ansible/getting-started.html#triggering-the-stack","title":"Triggering the Stack","text":""},{"location":"vendors/ansible/getting-started.html#making-sure-the-inventory-plugin-works","title":"Making sure the inventory plugin works","text":"<p>You can now click Trigger to create a new Spacelift Run.</p> <p>You should see the run finishing with no hosts matched. This is because the AWS EC2 inventory plugin did not detect valid AWS credentials.</p> <p></p> <p>Info</p> <p>You need to configure the AWS integration to give Spacelift access to your AWS account. You can find the details here.</p>"},{"location":"vendors/ansible/getting-started.html#configuring-ssh-keys","title":"Configuring SSH keys","text":"<p>After triggering a run again, you will see we could successfully find EC2 hosts (provided they could be localized with aws_ec2.yml inventory file filters), but we cannot connect to them using SSH. The reason for that is we did not configure SSH keys yet.</p> <p></p> <p>Let's configure the correct credentials using the Environment.</p> <p>Go to the Environment tab and add the private key that can be used to access the machine as a secret mounted file.</p> <p></p> <p>You should also specify the location of the SSH private key for Ansible, and you can do that using the <code>ANSIBLE_PRIVATE_KEY_FILE</code> environment variable.</p> <p></p>"},{"location":"vendors/ansible/getting-started.html#investigating-planned-changes","title":"Investigating planned changes","text":"<p>Triggering a run again, you should successfully see it get through the planning phase and end up in the unconfirmed state.</p> <p></p> <p>In the plan, you can see detailed information about each resource that is supposed to be created.</p> <p>At this point, you can investigate the changes Ansible playbook will apply and to which hosts.</p> <p>When you're happy with the planned changes, click Confirm to apply them.</p> <p></p> <p>By now, the machine should be configured with a simple Apache HTTP server with a sample website on port 8000.</p> <p>You can switch to the Resources tab to see the hosts you have configured, together with the history of when the host was last created and updated.</p> <p></p>"},{"location":"vendors/ansible/getting-started.html#conclusion","title":"Conclusion","text":"<p>That's it! You can find more details about the available configuration settings in the reference. Apart from configuration options available within Spacelift remember you can configure Ansible however you'd like using native Ansible configuration capabilities.</p>"},{"location":"vendors/ansible/policies.html","title":"Spacelift Policies with Ansible","text":""},{"location":"vendors/ansible/policies.html#plan-policy-with-ansible","title":"Plan Policy with Ansible","text":"<p>Using our native Plan Policy you could implement advanced handling of different situations that can happen while running playbooks on multiple hosts.</p> <p>One good example is if you'd like to manually review situations when some of the hosts were unreachable, but still be able to apply the change regardless.</p> <p>You could test it using the following Plan Policy:</p> <pre><code>package spacelift\n\nwarn[\"Some hosts were unreachable\"] {\n  input.ansible.dark != {}\n}\n\nsample { true }\n</code></pre> <p>Once you attach the above Plan Policy to an Ansible stack that is configured in a way not to fail when finding unreachable hosts, you could automatically detect unreachable hosts using a Plan Policy and require approval using the Approval Policy.</p> <p>Please find an example policy evaluation below: </p>"},{"location":"vendors/ansible/policies.html#linking-terraform-and-ansible-workflows","title":"Linking Terraform and Ansible workflows","text":"<p>You can use our Trigger Policy to link multiple stacks together. This applies also to stacks from different vendors.</p> <p>One of the use cases is to link Terraform and Ansible workflows so that you could use Ansible to configure EC2 instances you've just created using Terraform.</p> <p>We provide an extensive example of one way to set something like this up in our Terraform-Ansible workflow demo repository.</p>"},{"location":"vendors/ansible/reference.html","title":"Reference","text":""},{"location":"vendors/ansible/reference.html#stack-settings","title":"Stack Settings","text":"<ul> <li>Playbook - A playbook file to run on a stack.</li> <li>Skip Plan - Runs on Spacelift stacks typically have a planning and an applying phase. In Ansible for the planning phase we are running Ansible in check mode. However, not all Ansible modules support check mode and it can result in a run failure. You could configure your playbook to ignore certain errors (e.g. using <code>ignore_errors: \"{{ ansible_check_mode }}\"</code>) or choose to skip the planning phase entirely (e.g. in situations when handling check failures at the playbook level is not an option). On Ansible stacks, this phase can be skipped without execution by setting the <code>SPACELIFT_SKIP_PLANNING</code> environment variable to true in the stack's environment variables.</li> </ul>"},{"location":"vendors/ansible/reference.html#other-settings","title":"Other settings","text":"<p>For most of the settings below, there is usually more than one way to configure it (usually either through environment variables or through <code>ansible.cfg</code> file). More on Ansible configuration can be found in official Ansible docs.</p>"},{"location":"vendors/ansible/reference.html#ssh-private-key-location","title":"SSH private key location","text":"<p>If you want to use SSH to connect to your hosts you will need to provide a path to  the SSH private key. You can do that using the <code>ANSIBLE_PRIVATE_KEY_FILE</code> environment variable.</p>"},{"location":"vendors/ansible/reference.html#forcing-color-mode-for-ansible","title":"Forcing color mode for Ansible","text":"<p>By default, Ansible will not color the output when running without TTY. You could enable colored output using the <code>ANSIBLE_FORCE_COLOR</code> environment variable.</p>"},{"location":"vendors/ansible/reference.html#debugging-ansible-runs","title":"Debugging Ansible runs","text":"<p>When running into issues with Ansible playbooks a good way to debug the runs is to increase the Ansible verbosity level using the <code>ANSIBLE_VERBOSITY</code> environment variable.</p>"},{"location":"vendors/ansible/reference.html#controlling-ssh-controlpath-parameter","title":"Controlling SSH ControlPath parameter","text":"<p>Ansible uses <code>ControlMaster</code> and <code>ControlPath</code> SSH options to speed up playbook execution. On some occasions, you might want to modify default values to make them compatible with your execution environment. Depending on your exact setup, you might want to adjust some of the SSH settings Ansible uses.</p> <p>The default value for <code>ANSIBLE_SSH_CONTROL_PATH_DIR</code> is <code>/tmp/.ansible/cp</code>.</p>"},{"location":"vendors/ansible/reference.html#specifying-additional-cli-flags","title":"Specifying additional CLI flags","text":"<p>You can specify addtional CLI flags using the following environment variables:</p> <ul> <li><code>SPACELIFT_ANSIBLE_CLI_ARGS</code> - will take effect for both planning and applying phases</li> <li><code>SPACELIFT_ANSIBLE_CLI_ARGS_plan</code> - will take effect only for the planning phase</li> <li><code>SPACELIFT_ANSIBLE_CLI_ARGS_apply</code> - will take effect only for the applying phase</li> </ul> <p>If both phase specific and generic flags are used, both will take effect.</p> <p></p>"},{"location":"vendors/ansible/reference.html#file-permissions","title":"File permissions","text":"<p>There are a few nuances with certain files' permissions when using Ansible.</p>"},{"location":"vendors/ansible/reference.html#ansiblecfg","title":"ansible.cfg","text":"<p>If you use <code>ansible.cfg</code> file within a repository (or - more generally - within the current working directory) make sure that permissions on that file (and parent directory) are set properly. You can find more details in official Ansible documentation in the section on avoiding security risks with <code>ansible.cfg</code></p>"},{"location":"vendors/ansible/reference.html#ssh-private-key-files","title":"SSH private key files","text":"<p>If you are using SSH to connect to your hosts, then you need to make sure that private keys delivered to the worker have the correct permissions.</p> <p>As the ssh man page states:</p> <p>These files contain sensitive data and should be readable by the user but not accessible by others (read/write/execute). <code>ssh</code> will simply ignore a private key file if it is accessible by others.</p> <p>Typically, you would like to deliver private keys directly at the worker level where you can fully manage your environment. If that is not an option, you can always use our read-only mounted files or any other option you find suitable.</p>"},{"location":"vendors/cloudformation/index.html","title":"AWS CloudFormation","text":"<p>You can find more details in the subpages:</p> <ul> <li>Getting Started</li> <li>Reference</li> <li>Integrating with AWS Serverless Application Model (SAM)</li> <li>Integrating with the Serverless Framework</li> </ul>"},{"location":"vendors/cloudformation/index.html#why-use-cloudformation","title":"Why use CloudFormation?","text":"<p>CloudFormation is an excellent Infrastructure-as-Code tool that supports transactional deploys (automatically rolling back on failure), has a rich construct library, and does not require separate state management like Terraform or Pulumi.</p> <p>Even if you don't want to write YAML/JSON files directly, there are multiple frameworks that let you write your CloudFormation config in more ergonomic, general-purpose languages.</p>"},{"location":"vendors/cloudformation/index.html#why-use-spacelift-with-cloudformation","title":"Why use Spacelift with CloudFormation?","text":"<p>Spacelift helps you manage the complexities and compliance challenges of using CloudFormation. It brings with it a GitOps flow, so your infrastructure repository is synced with your CloudFormation Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies, which lets you automate compliance checks and build complex multi-stack workflows.</p> <p>You can also use Spacelift to mix and match Terraform, Pulumi, and CloudFormation Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like an ECS/EKS cluster with all its dependencies) and then connect that to a CloudFormation Stack which then transactionally deploys your services there using trigger policies and the Spacelift provider run resources for workflow orchestration and Contexts to export Terraform outputs as CloudFormation input parameters.</p>"},{"location":"vendors/cloudformation/index.html#does-spacelift-support-cloudformation-frameworks","title":"Does Spacelift support CloudFormation frameworks?","text":"<p>Yes! We support AWS CDK, AWS Serverless Application Model (SAM), and the Serverless Framework. You can read more about it in the relevant subpages of this document.</p>"},{"location":"vendors/cloudformation/getting-started.html","title":"Getting Started","text":""},{"location":"vendors/cloudformation/getting-started.html#initial-setup","title":"Initial Setup","text":"<p>Start by forking our AWS CloudFormation example repository</p> <p>Looking at the code, you'll find that it creates two simple Lambda Functions in nested Stacks and a common API Gateway REST API, which provides access to both of them.</p> <p>In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift.</p> <p>In the first screen, you should select the repository you've just forked, as can be seen in the picture.</p> <p></p> <p>In the next screen, you should choose the CloudFormation backend. There, fill in the Region field with the AWS region you want to create the CloudFormation Stack in. You should also create an Amazon S3 bucket for template storage and provide its name in the Template Bucket field. We won't automatically create this bucket.</p> <p>The Entry Template File should be set to main.yaml (based on the code in our repository) and the Stack Name to a unique CloudFormation Stack name in your AWS account. We'll use cloudformation-example in the pictures.</p> <p></p> <p>You can leave the settings on the next page (Define Behavior) unchanged. If you have a private worker pool you'd like to use, specify it there instead of the default public one.</p> <p>Finally, choose a name for your Spacelift Stack on the last page. We'll use cloudformation-example again.</p> <p></p> <p></p> <p>You'll also have to configure the AWS integration to give Spacelift access to your AWS account. You can find the details here: AWS</p>"},{"location":"vendors/cloudformation/getting-started.html#deploying-the-stack","title":"Deploying the Stack","text":"<p>You can now click Trigger to create a new Spacelift Run.</p> <p>And... oh no! It failed! However, the error message is quite straightforward. We're lacking the relevant capability.</p> <p></p> <p>We can acknowledge this capability by setting the <code>CF_CAPABILITY_IAM</code> environment variable to <code>1</code>.</p> <p>There's a bunch of optional settings for CloudFormation Stacks we expose this way. You can read up on all of them in the reference.</p> <p></p> <p>Triggering a run again, you should successfully see it get through the planning phase and end up in the unconfirmed state.</p> <p>In the plan, you can see detailed information about each resource that is supposed to be created.</p> <p></p> <p>You can also click the ADD +10 tab to see a concise overview of the resources to be created.</p> <p></p> <p>When you're happy with the planned changes, click Confirm to apply them.</p> <p>This will show you a feed of the creation and update events happening in the root Stack and all nested Stacks, which will stop after the creation finishes.</p> <p></p> <p>Great! The resources have successfully been created.</p> <p>You can now switch to the Outputs tab and find the URLBase output. You can curl that URL with a hello1 or hello2 suffix to get responses from your Lambda Functions.</p> <p></p> <p></p> <p>You can also switch to the Resources tab to explore the resources you've created.</p> <p></p>"},{"location":"vendors/cloudformation/getting-started.html#conclusion","title":"Conclusion","text":"<p>That's it! You can find more details about the available configuration settings in the reference, or you can check out how to use AWS Serverless Application Model (SAM) or the Serverless Framework to generate your CloudFormation templates.</p>"},{"location":"vendors/cloudformation/integrating-with-cdk.html","title":"Integrating with AWS Cloud Development Kit (CDK)","text":"<p>To use AWS Cloud Development Kit in an AWS CloudFormation stack you'll need to do two things: create a Docker image with AWS CDK and invoke it in <code>before_plan</code> hooks.</p>"},{"location":"vendors/cloudformation/integrating-with-cdk.html#preparing-the-image","title":"Preparing the image","text":"<p>Since our base image doesn't support AWS CDK, you will have to build your image that includes it as well as any tooling needed to run whatever language your infrastructure declaration is written in.</p> <p>The example below shows a Dockerfile with attached <code>cdk</code> and <code>go</code>:</p> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\nUSER root\n\n# Install packages\nRUN apk update &amp;&amp; apk add --update --no-cache npm\n# Update NPM\nRUN npm update -g\n# Install cdk\nRUN npm install -g aws-cdk\nRUN cdk --version\n\n# Add Go\nCOPY --from=golang:1.19-alpine /usr/local/go/ /usr/local/go/\n\nENV PATH=\"/usr/local/go/bin:${PATH}\"\nRUN go version\n</code></pre> <p>You should build it, push it to a repository, and set it as the Runner Image of your Stack.</p>"},{"location":"vendors/cloudformation/integrating-with-cdk.html#adding-before_plan-hooks","title":"Adding <code>before_plan</code> hooks","text":"<p>For the AWS CDK code to be properly interpreted by Spacelift, you have to customize the default stack workflow by enriching them with hooks. To create a CloudFormation template that can be interpreted by Spacelift, you will have to add these hooks to the <code>before_plan</code> stage:</p> <ul> <li><code>cdk bootstrap</code> - to bootstrap your AWS CDK project.</li> <li><code>cdk synth</code> - to create a CloudFormation template.</li> </ul>"},{"location":"vendors/cloudformation/integrating-with-cdk.html#limitations","title":"Limitations","text":""},{"location":"vendors/cloudformation/integrating-with-cdk.html#multiple-template-aws-cdk-definition","title":"Multiple template AWS CDK definition","text":"<p>The default CloudFormation integration in Spacelift uses a single CloudFormation template. That means that AWS CDK definitions that generate multiple templates will only have a single template picked up for further processing. To mitigate this, consider unifying the AWS CDK definition to generate a single template file.</p>"},{"location":"vendors/cloudformation/integrating-with-cdk.html#deploying-lambdas","title":"Deploying Lambdas","text":"<p>Our integration doesn't use <code>cdk deploy</code>, but rather uses template definitions created by <code>cdk synth</code>.</p> <p><code>cdk deploy</code> deploys Lambda assets to S3 which are used to deploy Lambdas by CloudFormation. Our process won't upload assets, so deploying Lambdas via a Spacelift stack configured to handle AWS CDK will result in errors.</p>"},{"location":"vendors/cloudformation/integrating-with-sam.html","title":"Integrating with AWS Serverless Application Model (SAM)","text":"<p>In order to use AWS Serverless Application Model (SAM) in an AWS CloudFormation Stack you'll need to do two things: create a Docker image with SAM included and invoke SAM in <code>before_init</code> hooks.</p> <p>The first one can be done using a Dockerfile akin to this one:</p> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform\nUSER root\nWORKDIR /home/spacelift\nRUN apk add --update --no-cache curl py-pip\nRUN apk -v --no-cache --update add \\\nmusl-dev \\\ngcc \\\npython3 \\\npython3-dev\nRUN python3 -m ensurepip --upgrade \\\n&amp;&amp; pip3 install --upgrade pip\nRUN pip3 install --upgrade awscli aws-sam-cli\nRUN pip3 uninstall --yes pip \\\n&amp;&amp; apk del python3-dev gcc musl-dev\nRUN sam --version\nUSER spacelift\n</code></pre> <p>You should build it, push it to a repository and set it as the Runner Image of your Stack.</p> <p>You'll also have to invoke SAM in order to generate raw CloudFormation files and upload Lambda artifacts to S3. You can do this by adding the following to your before initialization hooks:</p> <pre><code>sam package --region ${CF_METADATA_REGION} --s3-bucket ${CF_METADATA_TEMPLATE_BUCKET} --s3-prefix sam-artifacts --output-template-file ${CF_METADATA_ENTRY_TEMPLATE_FILE}\n</code></pre>"},{"location":"vendors/cloudformation/integrating-with-the-serverless-framework.html","title":"Integrating with the Serverless Framework","text":"<p>In order to use the Serverless Framework in an AWS CloudFormation Stack you'll need to do a few things: create a Docker image with the Serverless Framework included, invoke the serverless CLI in <code>before_init</code> hook, sync your artifacts with Amazon S3, and make sure the serverless config has your template bucket configured as the artifact location.</p> <p>The first one can be done using a Dockerfile akin to this one:</p> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform\nUSER root\nWORKDIR /home/spacelift\nRUN apk add --update --no-cache curl nodejs npm\nRUN npm install -g serverless\nRUN serverless --version\nUSER spacelift\n</code></pre> <p>You should build it, push it to a repository and set it as the Runner Image of your Stack.</p> <p>You'll also have to invoke the serverless CLI in order to generate raw CloudFormation files. You can do this by adding the following to your before initialization hooks:</p> <p><code>serverless package --region ${CF_METADATA_REGION}</code></p> <p>You can add the following script as a mounted file:</p> <pre><code>#!/bin/bash\n\nset -eu\nset -o pipefail\n\nSTATE_FILE=.serverless/serverless-state.json\nS3_PREFIX=$(jq -r '.package.artifactDirectoryName' &lt; \"$STATE_FILE\")\nARTIFACT=$(jq -r '.package.artifact' &lt; \"$STATE_FILE\")\n\naws s3 cp .serverless/$ARTIFACT s3://$CF_METADATA_TEMPLATE_BUCKET/$S3_PREFIX/$ARTIFACT\n</code></pre> <p>and invoke it in your before initialization hooks: <code>sh sync.sh</code></p> <p>Finally, specify the S3 bucket for artifacts in your serverless.yml configuration file:</p> <pre><code>provider:\ndeploymentBucket: your-s3-bucket\n</code></pre> <p>When creating the CloudFormation Stack, make sure that you set the entry template file to <code>.serverless/cloudformation-template-update-stack.json</code>, which is the file generated by the <code>serverless package</code> command.</p>"},{"location":"vendors/cloudformation/reference.html","title":"Reference","text":""},{"location":"vendors/cloudformation/reference.html#stack-settings","title":"Stack Settings","text":"<ul> <li>Region - AWS region in which to create and execute the AWS CloudFormation Stack.</li> <li>Stack Name - The name for the CloudFormation Stack controlled by this Spacelift Stack.</li> <li>Entry Template File - The path to the JSON or YAML file describing your root CloudFormation Stack. If you're generating CloudFormation code using a tool like AWS Serverless Application Model (SAM) or AWS Cloud Development Kit (AWS CDK), point this to the file containing the generated template.</li> <li>Template Bucket - Amazon S3 bucket to store CloudFormation templates in. Each created object will be prefixed by the current run ID like this: <code>&lt;run id&gt;/artifact_name</code></li> </ul>"},{"location":"vendors/cloudformation/reference.html#special-environment-variables","title":"Special Environment Variables","text":""},{"location":"vendors/cloudformation/reference.html#cloudformation-stack-parameters","title":"CloudFormation Stack Parameters","text":"<p>Use this if your CloudFormation template requires parameters to be specified.</p> <p>Each environment variable of the form <code>CF_PARAM_xyz</code> will be interpreted as the value for the parameter <code>xyz</code>.</p> <p>For example, with this template snippet:</p> <pre><code>Parameters:\nInstanceTypeParameter:\nType: String\nAllowedValues:\n- t2.micro\n- t2.small\nDescription: Enter t2.micro or t2.small.\n</code></pre> <p>In order to specify the InstanceTypeParameter add an environment variable to your Stack <code>CF_PARAM_InstanceTypeParameter</code> and set its value to i.e. <code>t2.micro</code></p>"},{"location":"vendors/cloudformation/reference.html#cloudformation-stack-capabilities","title":"CloudFormation Stack Capabilities","text":"<p>Some functionalities available to CloudFormation Stacks need to be explicitly acknowledged using capabilities. You can configure capabilities in Spacelift using environment variables of the form <code>CF_CAPABILITY_xyz</code> and set them to 1.</p> <p>As of the time of writing this page, available capabilities are <code>CF_CAPABILITY_IAM</code>, <code>CF_CAPABILITY_NAMED_IAM,</code> and <code>CF_CAPABILITY_AUTO_EXPAND</code>. Detailed descriptions can be found in the AWS API documentation.</p>"},{"location":"vendors/cloudformation/reference.html#available-computed-environment-variables","title":"Available Computed Environment Variables","text":"<p>To ease writing reusable scripts and hooks for your CloudFormation Stacks, the following environment variables are computed for each run: <code>CF_METADATA_REGION</code>, <code>CF_METADATA_STACK_NAME</code>, <code>CF_METADATA_ENTRY_TEMPLATE_FILE</code>, <code>CF_METADATA_TEMPLATE_BUCKET</code>.</p> <p>Their values are set to the respective Stack settings.</p>"},{"location":"vendors/cloudformation/reference.html#permissions","title":"Permissions","text":"<p>You need to provide Spacelift with access to your AWS account. You can either do this using the AWS Integration, provide ambient credentials on private workers, or pass environment variables directly.</p>"},{"location":"vendors/kubernetes/index.html","title":"Kubernetes","text":""},{"location":"vendors/kubernetes/index.html#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. For more information about Kubernetes, see the reference documentation.</p>"},{"location":"vendors/kubernetes/index.html#how-does-spacelift-work-with-kubernetes","title":"How does Spacelift work with Kubernetes?","text":"<p>Spacelift supports Kubernetes via <code>kubectl</code>.</p>"},{"location":"vendors/kubernetes/index.html#what-is-kubectl","title":"What is <code>kubectl</code>?","text":"<p>The Kubernetes command-line tool, <code>kubectl</code>, allows you to run commands against Kubernetes clusters. You can use <code>kubectl</code> to deploy applications, inspect and manage cluster resources, and view logs. For more information including a complete list of <code>kubectl</code> operations, see the <code>kubectl</code> reference documentation.</p>"},{"location":"vendors/kubernetes/index.html#why-use-spacelift-with-kubernetes","title":"Why use Spacelift with Kubernetes?","text":"<p>Spacelift helps you manage the complexities and compliance challenges of using Kubernetes. It brings with it a GitOps flow, so your Kubernetes Deployments are synced with your Kubernetes Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies, which lets you automate compliance checks and build complex multi-stack workflows.</p> <p>You can also use Spacelift to mix and match Terraform, Pulumi, AWS CloudFormation, and Kubernetes Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision the required infrastructure (like an ECS/EKS cluster with all its dependencies) and then deploy the following via a Kubernetes Stack.</p> <p>Anything that can be run via <code>kubectl</code> can be run within a Spacelift stack.</p> <p>To find out more about Kubernetes Workload Resources, read the reference documentation.</p>"},{"location":"vendors/kubernetes/authenticating.html","title":"Authenticating","text":"<p>The Kubernetes integration relies on using <code>kubectl</code>'s native authentication to connect to your cluster. You can use the <code>$KUBECONFIG</code> environment variable to find the location of the Kubernetes configuration file, and configure any credentials required.</p> <p>You should perform any custom authentication as part of a before init hook to make sure that <code>kubectl</code> is configured correctly before any commands are run in after init and subsequent hooks.</p> <p>The following sections provide examples of how to configure the integration manually, as well as using cloud provider-specific tooling.</p>"},{"location":"vendors/kubernetes/authenticating.html#manual-configuration","title":"Manual Configuration","text":"<p>Manual configuration allows you to connect to any Kubernetes cluster accessible by your Spacelift workers, regardless of whether your cluster is on-prem or hosted by a cloud provider. The Kubernetes integration automatically sets the <code>$KUBECONFIG</code> environment variable to point at <code>/mnt/workspace/.kube/config</code>, giving you a number of options:</p> <ul> <li>You can use a mounted file to mount a pre-prepared config file into your workspace at <code>/mnt/workspace/.kube/config</code>.</li> <li>You can use a before init hook to create a kubeconfig file, or to download it from a trusted location.</li> </ul> <p>Please refer to the Kubernetes documentation for more information on configuring kubectl.</p>"},{"location":"vendors/kubernetes/authenticating.html#aws","title":"AWS","text":"<p>The simplest way to connect to an AWS EKS cluster is using the AWS CLI tool. To do this, add the following before init hook to your Stack:</p> <pre><code>aws eks update-kubeconfig --region $REGION_NAME --name $CLUSTER_NAME\n</code></pre> <p>Info</p> <ul> <li>The <code>$REGION_NAME</code> and <code>$CLUSTER_NAME</code> environment variables must be defined in your Stack's environment.</li> <li>This relies on either using the Spacelift AWS Integration, or ensuring that your workers have permission to access the EKS cluster.</li> </ul>"},{"location":"vendors/kubernetes/custom-resources.html","title":"Custom Resources","text":"<p>Spacelift supports the use of Kubernetes Custom Resources. To find out more about these supported extensions, review the official documentation.</p>"},{"location":"vendors/kubernetes/getting-started.html","title":"Getting Started","text":""},{"location":"vendors/kubernetes/getting-started.html#repository-creation","title":"Repository Creation","text":"<p>Start by creating a new deployment repository and name the file as <code>deployment.yaml</code> with the following code:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nspec:\nselector:\nmatchLabels:\napp: nginx\nreplicas: 2 # tells deployment to run 2 pods matching the template\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\n</code></pre> <p>You can learn more about this example deployment by navigating to the Run a Stateless Application Using a Deployment website from the official Kubernetes documentation.</p> <p>Looking at the code, you will find that it deploys a single instance of Nginx.</p>"},{"location":"vendors/kubernetes/getting-started.html#create-a-new-stack","title":"Create a new Stack","text":"<p>In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift.</p> <p></p>"},{"location":"vendors/kubernetes/getting-started.html#integrate-vcs","title":"Integrate VCS","text":"<p>Select the repository you created in the initial step, as seen in the picture.</p> <p></p>"},{"location":"vendors/kubernetes/getting-started.html#configure-backend","title":"Configure Backend","text":"<p>Choose Kubernetes from the dropdown list and type out the namespace.</p> <p>Info</p> <p>Spacelift does not recommend leaving the Namespace blank due to the elevated level of access privilege required.</p> <p></p>"},{"location":"vendors/kubernetes/getting-started.html#define-behavior","title":"Define Behavior","text":"<p>Select the arrow next to Show Advanced Options to expose the advanced configuration options.</p> <p>To ensure the success of a Kubernetes deployment, the following options should be reviewed and validated.</p> <ul> <li>Runner Image: Use a custom one that has <code>kubectl</code> installed.</li> <li>Customize workflow: During the initialization phase, you must specify the necessary command to ensure kubeconfig is updated and authenticated to the Kubernetes Cluster you will be authenticating against.</li> </ul> <p>Info</p> <p>Spacelift can authenticate against any Kubernetes cluster, including local or cloud provider managed instances.</p> <p></p> <p>In the example above, I am authenticating to an AWS EKS Cluster and used the following command to update the kubeconfig for the necessary cluster.</p> <pre><code>aws eks update-kubeconfig --region $region-name --name $cluster-name\n</code></pre> <p>Info</p> <p>Update the previous variables according to your deployment.</p> <ul> <li><code>$region-name:</code> AWS region where your Kubernetes cluster resides</li> <li><code>$cluster-name:</code> Name of your Kubernetes clusters</li> </ul> <p>The above allows the worker to authenticate to the proper cluster before running the specified Kubernetes deployment in the repository that we created earlier.</p> <p>Warning</p> <p>Authentication with a Cloud Provider is required.</p> <p>After you Name the Stack, follow the Cloud Integrations section to ensure Spacelift can authenticate to your Kubernetes Cluster.</p>"},{"location":"vendors/kubernetes/getting-started.html#name-the-stack","title":"Name the Stack","text":"<p>Provide the name of your Stack. Labels and Description are not required but recommended.</p> <p></p> <p>Saving the Stack will redirect you to its Tracked Runs (Deployment) page.</p> <p></p>"},{"location":"vendors/kubernetes/getting-started.html#configure-integrations","title":"Configure Integrations","text":"<p>To authenticate against a Kubernetes cluster provided by Cloud Provider managed service, Spacelift requires integration with the associated Cloud Provider.</p> <p>Navigate to the Settings, Integrations page and select the dropdown arrow to access the following selection screen:</p> <p></p> <p>Warning</p> <p>Necessary permissions to the Kubernetes Cluster are required.</p> <p>The following links will help you set up the necessary integration with your Cloud Provider of choice.</p> <ul> <li>AWS</li> <li>OIDC</li> </ul> <p>Once you have configured the necessary integration, navigate the Stack landing page and Trigger a Run.</p>"},{"location":"vendors/kubernetes/getting-started.html#trigger-a-run","title":"Trigger a Run","text":"<p>To Trigger a Run, select Trigger on the right side of the Stacks view.</p> <p></p> <p>Spacelift Label</p> <p>To help identify resources deployed to your Kubernetes cluster, Spacelift will add the following label to all resources: <code>spacelift-stack=&lt;stack-slug&gt;</code></p>"},{"location":"vendors/kubernetes/getting-started.html#triggered-run-status","title":"Triggered Run Status","text":"<p>Please review the documentation for a detailed view of each Run Phase and Status associated with Kubernetes.</p>"},{"location":"vendors/kubernetes/getting-started.html#unconfirmed","title":"Unconfirmed","text":"<p>After you manually trigger the Run in the Stack view, Spacelift will deploy a runner image, initialize the Cloud Provider, Authenticate with the Kubernetes Cluster and run the Deployment specified in the repository.</p> <p>After a successful planning phase, you can check the log to see the planned changes.</p> <p></p> <p>Planning Phase</p> <p>Spacelift utilizes the dry run functionality of <code>kubectl apply</code> to compare your code to the current state of the cluster and output the list of changes to be made.</p> <p>A slightly different dry run mode depending on the scenario:</p> <ul> <li><code>--dry-run=server</code>: Utilized when resources are available</li> <li><code>--dry-run=client</code>: Utilized when no resources are available</li> </ul> <p>To confirm the Triggered run, click the CONFIRM button.</p>"},{"location":"vendors/kubernetes/getting-started.html#finished-deployment","title":"Finished Deployment","text":"<p>The following screen highlights the Finished Run and output from a successful deployment to your Kubernetes cluster.</p> <p></p> <p>Applying</p> <p>The default timeout is set to 10 minutes (10m). If a Kubernetes Deployment is expected to take longer, you can customize that using the <code>KUBECTL_ROLLOUT_TIMEOUT</code> environment variable.</p> <p>Review the documentation to find out more about Spacelift environment variables..</p>"},{"location":"vendors/kubernetes/getting-started.html#default-removal-of-deployments","title":"Default Removal of Deployments","text":"<p>Info</p> <p>By default; if a YAML file is removed from your repository, the resources with an attached <code>spacelift-stack=&lt;stack-slug&gt;</code> label will be removed from the Kubernetes cluster.</p> <p>The <code>--prune</code> flag will be utilized.</p>"},{"location":"vendors/kubernetes/helm.html","title":"Helm","text":"<p>There is no native support within Spacelift for Helm, but you can use the <code>helm template</code> command in a before plan hook to generate the Kubernetes resource definitions to deploy.</p> <p>Please note, the following caveats apply:</p> <ul> <li>Using <code>helm template</code> means that you are not using the full Helm workflow, which may cause limitations or prevent certain Charts from working.</li> <li>You need to use a custom Spacelift worker image that has Helm installed, or alternatively you can install Helm using a before init hook.</li> </ul> <p>The rest of this page will go through an example of deploying the Spacelift Workerpool Helm Chart using the Kubernetes integration. See here for an example repository.</p>"},{"location":"vendors/kubernetes/helm.html#prerequisites","title":"Prerequisites","text":"<p>The following prerequisites are required to follow the rest of this guide:</p> <ul> <li>A Kubernetes cluster that you can authenticate to from a Spacelift stack.</li> <li>A namespace called <code>spacelift-worker</code> that exists within that cluster.</li> </ul>"},{"location":"vendors/kubernetes/helm.html#repository-creation","title":"Repository Creation","text":"<p>Start by creating a new repository for your Helm stack. This repository only needs to contain a single item - a kustomization.yaml file:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: spacelift-worker\n\nresources:\n# spacelift-worker-pool.yaml will be generated in a pre-plan hook\n- spacelift-worker-pool.yaml\n</code></pre> <p>The kustomization file is used to tell <code>kubectl</code> where to find the file containing the output of the <code>helm template</code> command, and prevents <code>kubectl</code> from attempting to apply every yaml file in your repository. This is important if you want to commit a <code>values.yaml</code> file to your repository.</p> <p>Info</p> <p>Our example repository contains a values.yaml file used to configure some of the chart values. This isn't required, and is simply there for illustrative purposes.</p>"},{"location":"vendors/kubernetes/helm.html#create-a-new-stack","title":"Create a new Stack","text":""},{"location":"vendors/kubernetes/helm.html#define-behavior","title":"Define Behavior","text":"<p>Follow the same steps to create your stack as per the Getting Started guide, but when you get to the Define Behavior step, add the following commands as before plan hooks:</p> <pre><code>helm repo add spacelift https://downloads.spacelift.io/helm\nhelm template spacelift-worker-pool spacelift/spacelift-worker --values values.yaml --set \"replicaCount=$SPACELIFT_WORKER_REPLICAS\" --set \"credentials.token=$SPACELIFT_WORKER_POOL_TOKEN\" --set \"credentials.privateKey=$SPACELIFT_WORKER_POOL_PRIVATE_KEY\" &gt; spacelift-worker-pool.yaml\n</code></pre> <p>Also, make sure to specify your custom Runner image that has Helm installed if you are not installing Helm using a before init hook.</p> <p>Once you've completed both steps, you should see something like this:</p> <p></p>"},{"location":"vendors/kubernetes/helm.html#configure-environment","title":"Configure Environment","text":"<p>Once you have successfully created your Stack, add values for the following environment variables to your Stack environment:</p> <ul> <li><code>SPACELIFT_WORKER_REPLICAS</code> - the number of worker pool replicas to create.</li> <li><code>SPACELIFT_WORKER_POOL_TOKEN</code> - the token downloaded when creating your worker pool.</li> <li><code>SPACELIFT_WORKER_POOL_PRIVATE_KEY</code> - your base64-encoded private key.</li> </ul> <p>Your Stack environment should look something like this:</p> <p></p>"},{"location":"vendors/kubernetes/helm.html#configure-integrations","title":"Configure Integrations","text":"<p>Configure any required Cloud Provider integrations as per the Getting Started guide.</p>"},{"location":"vendors/kubernetes/helm.html#trigger-a-run","title":"Trigger a Run","text":"<p>This example assumes that a Kubernetes namespace called <code>spacelift-worker</code> already exists. If it doesn't, create it using <code>kubectl create namespace spacelift-worker</code> before triggering a run.</p> <p>Info</p> <p>You can use a Spacelift Task to run the <code>kubectl create namespace</code> command.</p> <p>Triggering runs works exactly the same as when not using Helm. Once the planning stage has completed, you should see a preview of your changes, showing the Chart resources that will be created:</p> <p></p> <p>After approving the run, you should see the changes applying, along with a successful rollout of your Chart resources:</p> <p></p>"},{"location":"vendors/kubernetes/kustomize.html","title":"Kustomize","text":"<p>Kubernetes support in Spacelift is driven by Kustomize with native support in <code>kubectl</code>.</p> <p>We used Kustomize to make all resources created using the Spacelift Kubernetes support have unique labels attached to them:</p> <ul> <li><code>spacelift-stack: &lt;stack-slug&gt;</code></li> <li><code>app.kubernetes.io/managed-by: spacelift</code></li> </ul> <p>All operations Spacelift does will be done only on resources with the <code>spacelift-stack: &lt;stack-slug&gt;</code>.</p> <p>If you are not using Kustomize, Spacelift will transparently create a <code>kustomization.yaml</code> file that will reference all yaml files in the Stack's project root and subdirectories.</p> <p>If you are using Kustomize, then we will only add a label transformer to your kustomization.yaml file.</p>"},{"location":"vendors/kubernetes/workflow-tool.html","title":"Workflow Tool","text":"<p>The Workflow Tool stack setting allows you to choose between two options:</p> <ul> <li>Kubenetes.</li> <li>Custom.</li> </ul> <p>The Kubernetes option gives you out of the box support where all you need to do is choose the version you want to use and you're good to go.</p> <p>The rest of this page explains the Custom option. This option allows you to customize the commands that are executed as part of Spacelift's Kubernetes workflow. This can be useful if you want to run a custom binary instead of one of the kubectl versions supported by our Kubernetes integration out the box.</p> <p>Info</p> <p>Note that any custom binary is considered third-party software and you need to make sure you have the necessary rights (e.g. license) to use it.</p>"},{"location":"vendors/kubernetes/workflow-tool.html#how-does-it-work","title":"How does it work?","text":"<p>Each stage of the Kubernetes workflow uses certain commands to perform tasks such as listing API resources availible at the server or applying the resources. We provide a built-in set of commands to use for Kubernetes, but you can also specify your own custom commands. You do this via a file called <code>.spacelift/workflow.yml</code>.</p> <p>The following is an example of what the workflow commands look like for Kubernetes:</p> <pre><code># Used to create a resource from standard input. We use this\n# query to make authorization SelfSubjectAccessReview request.\ncreateFromStdin: \"kubectl create -f - -o yaml\"\n\n\n# Used to list api resources supported on the server.\n#\n# Available template parameters:\n# - .NoHeaders    - controls the --no-headers argument of kubectl.\n# - .IsNamespaced - controls arguments to provide if we use a namespace.\n# - .OutputFormat - output format of a reequest, can be set to two values: name or wide.\napiResources: \"kubectl api-resources {{ if .NoHeaders }}--no-headers {{ end }}--verbs=list,create {{ if .IsNamespaced }}--namespaced {{ end }}-o {{ .OutputFormat }}\"\n\n# Used to both plan and apply changes.\n#\n# Available template parameters:\n# - .PruneWhiteList - provides list of arguments to the --prune flag.\n# - .StackSlug      - the slug of a current stack.\n# - .DryRunStrategy - dry run sttrategy, can be set to values: none, client, server.\n# - .OutputFormat   - output format. Can be set to empty value or json.\n# - .Namespace      - the namespace used.\napply: \"kubectl apply -k ./ --prune {{ .PruneWhiteList }} -l \\\"spacelift-stack={{ .StackSlug }}\\\" --dry-run={{ .DryRunStrategy }}{{ if .OutputFormat }} -o {{ .OutputFormat }}{{ end }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ end}}\"\n\n# Rollout status command used to check the rollout status of resources.\n#\n# Available template parameters:\n# - .Namespace                - the namespace used.\n# - .RolloutTimeoutInSeconds  - a timeout in seconds that will be passed to the command.\n# - .Id                       - id of the resource to watch.\nrolloutStatus: \"kubectl rollout status{{ if .Namespace }} --namespace={{ .Namespace }}{{ end}} --timeout {{ .RolloutTimeoutInSeconds }}s -w {{ .Id }}\"\n\n# List a pods based on selector passed. We use getPods command\n# to display status of pods while they are being deployed.\n#\n# Available template parameters:\n# - .Namespace - the namespace used.\n# - .Pods      - a selector passed to get pods command.\ngetPods: \"kubectl get pods{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} -w -l '{{ .Pods }}'\"\n\n# Used to tear down any resources as part of deleting a stack.\n#\n# Available template parameters:\n# - .StackSlug          - the slug of a current stack.\n# - .Namespace          - the namespace used.\n# - .ResourcesToDelete  - list of resources to delete.\ndelete: \"kubectl delete --ignore-not-found -l spacelift-stack={{ .StackSlug }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} {{ .ResourcesToDelete }}\"\n\n# APPLE\n#\n# Available template parameters:\n# - .StackSlug            - the slug of a current stack.\n# - .Namespace            - the namespace used.\n# - .ResourceKindsToGet   - list of kinds of resources to get.\nget: \"kubectl get -o json --show-kind --ignore-not-found -l spacelift-stack={{ .StackSlug }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} {{ .ResourceKindsToGet }}\"\n</code></pre>"},{"location":"vendors/kubernetes/workflow-tool.html#how-to-configure-a-custom-tool","title":"How to configure a custom tool","text":"<p>To use a custom tool, three things are required:</p> <ol> <li>A way of providing the tool to your runs.</li> <li>A way of specifying the commands that should be executed.</li> <li>Indicating that you want to use a custom tool on your stack/module.</li> </ol>"},{"location":"vendors/kubernetes/workflow-tool.html#providing-a-tool","title":"Providing a tool","text":"<p>There are two main ways of providing a custom tool:</p> <ul> <li>Using a custom runner image.</li> <li>Using a before_init hook to download your custom tool.</li> </ul>"},{"location":"vendors/kubernetes/workflow-tool.html#specifying-the-commands","title":"Specifying the commands","text":"<p>Your custom workflow commands need to be provided in a <code>workflow.yml</code> file stored in the <code>.spacelift</code> folder at the root of your workspace. There are three main ways of providing this:</p> <ol> <li>Via a mounted file in the stack's environment.</li> <li>Via a mounted file stored in a context</li> <li>Directly via your Git repo.</li> </ol> <p>The option you choose will depend on your exact use-case, for example using the stack's environment allows you to quickly test out a new custom tool, using a context allows you to easily share the same configuration across multiple stacks, and storing the configuration in your Git repo allows you to track your settings along with the rest of your code.</p> <p>Here is an example configuration to use a fictional tool called <code>my-custom-tool</code>:</p> <pre><code>createFromStdin: \"my-custom-tool create -f - -o yaml\"\napiResources: \"my-custom-tool api-resources {{ if .NoHeaders }}--no-headers {{ end }}--verbs=list,create {{ if .IsNamespaced }}--namespaced {{ end }}-o {{ .OutputFormat }}\"\napply: \"my-custom-tool apply -k ./ --prune {{ .PruneWhiteList }} -l \\\"spacelift-stack={{ .StackSlug }}\\\" --dry-run={{ .DryRunStrategy }}{{ if .OutputFormat }} -o {{ .OutputFormat }}{{ end }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ end}}\"\nrolloutStatus: \"my-custom-tool rollout status{{ if .Namespace }} --namespace={{ .Namespace }}{{ end}} --timeout {{ .RolloutTimeoutInSeconds }}s -w {{ .Id }}\"\ngetPods: \"my-custom-tool get pods{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} -w -l '{{ .Pods }}'\"\ndelete: \"my-custom-tool delete --ignore-not-found -l spacelift-stack={{ .StackSlug }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} {{ .ResourcesToDelete }}\"\nget: \"my-custom-tool get -o json --show-kind --ignore-not-found -l spacelift-stack={{ .StackSlug }}{{ if .Namespace }} --namespace={{ .Namespace }}{{ else }} --all-namespaces{{ end}} {{ .ResourceKindsToGet }}\"\n</code></pre>"},{"location":"vendors/kubernetes/workflow-tool.html#using-a-custom-tool-on-a-stack","title":"Using a custom tool on a Stack","text":"<p>To update your Stack to use a custom workflow tool, edit the Workflow tool setting in the Backend settings:</p> <p></p> <p>When you choose the Custom option, the version selector will disappear:</p> <p></p> <p>This is because you are responsible for ensuring that the tool is available to the Run, and Spacelift will not automatically download the tool for you.</p>"},{"location":"vendors/kubernetes/workflow-tool.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"vendors/kubernetes/workflow-tool.html#workflow-file-not-found","title":"Workflow file not found","text":"<p>If no workflow.yml file has been created but your Stack has been configured to use a custom tool, you may get an error message like the following:</p> <p></p> <p>If this happens, please ensure you have added a <code>.spacelift/workflow.yml</code> file to your Git repository, or attached it to your Stack's environment via a mounted file.</p>"},{"location":"vendors/kubernetes/workflow-tool.html#commands-missing","title":"Commands missing","text":"<p>If your <code>.spacelift/workflow.yml</code> does not contain all the required command definitions, or if any commands are empty, you will get an error message like the following:</p> <p></p> <p>This check is designed as a protection mechanism in case new commands are added but your workflow hasn't been updated. In this case, please provide an implementation for the specified commands (in the example the <code>init</code> and <code>workspaceSelect</code> commands),</p>"},{"location":"vendors/kubernetes/workflow-tool.html#tool-not-found","title":"Tool not found","text":"<p>If your custom tool binary cannot be found you will get an error message like the following:</p> <p></p> <p>In this situation, please ensure that you are providing a custom workflow tool via a custom runner image or workflow hook.</p>"},{"location":"vendors/pulumi/index.html","title":"Pulumi","text":"<p>Info</p> <p>Feature previews are subject to change, may contain bugs, and have not yet been ironed out based on real production usage.</p> <p>On a high level, Pulumi has a very similar flow to Terraform. It uses a state backend, provides dry run functionality, reconciles the actual world with the desired state. In this article we'll dive into how each of the concepts in Spacelift translates into working with Pulumi.</p> <p>However, if you're the type that prefers to start with doing, instead of reading too much, there are quickstarts for each of the runtimes supported by Pulumi:</p> <ul> <li>C#</li> <li>Go</li> <li>Javascript</li> <li>Python</li> </ul> <p>In case you're just getting started with Pulumi, we'd recommend you to start with Javascript. Believe it or not, it's actually the most pleasant experience we had with Pulumi! Later you can also easily switch to languages which compile to Javascript, like TypeScript or ClojureScript.</p> <p>The high level concepts of Spacelift don't change when used with Pulumi. Below, we'll cover a few lower level details, which may be of interest.</p>"},{"location":"vendors/pulumi/index.html#run-execution","title":"Run Execution","text":""},{"location":"vendors/pulumi/index.html#initialization","title":"Initialization","text":"<p>Previously described in Run Initializing, in Pulumi the initialization will run:</p> <ul> <li><code>pulumi login</code> with your configured login URL</li> <li><code>pulumi stack select --create --select</code> with your configured Pulumi stack name (the one you set in vendor-specific settings, not the Spacelift Stack name)</li> </ul> <p>It will then commence to run all pre-initialization hooks.</p>"},{"location":"vendors/pulumi/index.html#planning","title":"Planning","text":"<p>We run <code>pulumi preview --refresh --diff --show-replacement-steps</code> in order to show planned changes.</p>"},{"location":"vendors/pulumi/index.html#applying","title":"Applying","text":"<p>We run <code>pulumi up --refresh --diff --show-replacement-steps</code> in order to apply changes.</p>"},{"location":"vendors/pulumi/index.html#policies","title":"Policies","text":"<p>Most policies don't change at all. The one that changes most is the plan policy. Instead of the terraform raw plan in the <code>terraform</code> field, you'll get a <code>pulumi</code> field with the raw Pulumi plan and the following schema:</p> <pre><code>{\n\"pulumi\": {\n\"steps\": [\n{\n\"new\": {\n\"custom\": \"boolean\",\n\"id\": \"string\",\n\"inputs\": \"object - input properties\",\n\"outputs\": \"object - output properties\",\n\"parent\": \"string - parent resource of this resource\",\n\"provider\": \"string - provider this resource stems from\",\n\"type\": \"string - resource type\",\n\"urn\": \"string - urn of this resource\"\n},\n\"old\": {\n\"custom\": \"boolean\",\n\"id\": \"string\",\n\"inputs\": \"object - input properties\",\n\"outputs\": \"object - output properties\",\n\"parent\": \"string - parent resource of this resource\",\n\"provider\": \"string - provider this resource stems from\",\n\"type\": \"string - resource type\",\n\"urn\": \"string - urn of this resource\"\n},\n\"op\": \"string - same, refresh, create, update, delete, create-replacement or delete-replaced\",\n\"provider\": \"string - provider this resource stems from\",\n\"type\": \"string - resource type\",\n\"urn\": \"string - urn of this resource\"\n}\n]\n},\n\"spacelift\": {\"...\": \"...\"}\n}\n</code></pre> <p>Pulumi secrets are detected and encoded as <code>[secret]</code> instead of the actual value, that's why there's no other string sanitization going on with Pulumi plans.</p>"},{"location":"vendors/pulumi/index.html#modules","title":"Modules","text":"<p>Spacelift module CI/CD isn't currently available for Pulumi.</p>"},{"location":"vendors/pulumi/c-sharp.html","title":"C#","text":"<p>In order to follow along with this article, you'll need an AWS account.</p> <p>Start with forking the Pulumi examples repo, we'll be setting up an example directory from there, namely aws-cs-webserver.</p> <p>In the root of the repository (not the aws-cs-webserver directory), add a new file:</p> .spacelift/config.yml<pre><code>version: \"1\"\n\nstack_defaults:\nbefore_apply:\n- dotnet clean\n- rm -rf bin\n- rm -rf obj\n</code></pre> <p><code>before_apply</code> is not yet exposed through the interface like <code>before_init</code>, so you have to set it through the config file. When compiling, the dotnet CLI creates global state which is lost after confirmation. This will mostly clean the workspace before applying, so everything will be cleanly recompiled. Why mostly? This you will see in a sec.</p> <p>Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values:</p> <ul> <li>Set the project root to <code>aws-cs-webserver</code>, as we want to run Pulumi in this subdirectory only.</li> <li>Set the runner image to <code>public.ecr.aws/spacelift/runner-pulumi-dotnet:latest</code></li> <li>Pinning to a specific Pulumi version is possible too, using a tag like <code>v2.15.4</code> - you can see the available versions  here.</li> </ul> <p></p> <p>In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose:</p> <ul> <li>A state backend, aka login URL. This can be a cloud storage bucket, like <code>s3://pulumi-state-bucket</code>, but it can also be a Pulumi Service endpoint.</li> <li>A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like <code>my-dotnet-pulumi-spacelift-stack</code>.</li> </ul> <p></p> <p>Info</p> <p>You can use <code>https://api.pulumi.com</code> as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the <code>PULUMI_ACCESS_TOKEN</code> environment variable.</p> <p>You'll now have to set up the AWS integration for the Stack, as is described in AWS.</p> <p>Go into the Environment tab in your screen, add an <code>AWS_REGION</code> environment variable and set it to your region of choice, i.e. <code>eu-central-1</code>.</p> <p>Previously I said <code>dotnet clean</code> mostly clears the state, this is because you'll also have to add the <code>NUGET_PACKAGES</code> environment variable, and set it to a directory persisted with the workspace, i.e. <code>/mnt/workspace/nuget_packages</code>.</p> <p></p> <p>You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes.</p> <p></p> <p>Confirm the run to let it apply the changes, after applying it should look like this:</p> <p></p> <p>We can see the PublicDns stack output. If we try to curl it, lo and behold:</p> <pre><code>~&gt; curl ec2-18-184-92-34.eu-central-1.compute.amazonaws.com\nHello, World!\n</code></pre> <p>In order to clean up, open the Tasks tab, and perform <code>pulumi destroy --non-interactive --yes</code> there.</p> <p></p> <p>Which will destroy all created resources.</p> <p></p>"},{"location":"vendors/pulumi/golang.html","title":"Go","text":"<p>In order to follow along with this article, you'll need an AWS account.</p> <p>Start with forking the Pulumi examples repo, we'll be setting up an example directory from there, namely aws-go-s3-folder.</p> <p>Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values:</p> <ul> <li>Set the project root to <code>aws-go-s3-folder</code>, as we want to run Pulumi in this subdirectory only.</li> <li>Set the runner image to <code>public.ecr.aws/spacelift/runner-pulumi-golang:latest</code></li> <li>Pinning to a specific Pulumi version is possible too, using a tag like <code>v2.15.4</code> - you can see the available versions here.</li> </ul> <p></p> <p>In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose:</p> <ul> <li>A state backend, aka login URL. This can be a cloud storage bucket, like <code>s3://pulumi-state-bucket</code>, but it can also be a Pulumi Service endpoint.</li> <li>A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like <code>my-golang-pulumi-spacelift-stack</code>.</li> </ul> <p></p> <p>Info</p> <p>You can use <code>https://api.pulumi.com</code> as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the <code>PULUMI_ACCESS_TOKEN</code> environment variable.</p> <p>You'll now have to set up the AWS integration for the Stack, as is described in AWS.</p> <p>Go into the Environment tab in your screen, add an <code>AWS_REGION</code> environment variable and set it to your region of choice, i.e. <code>eu-central-1</code>.</p> <p></p> <p>You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes.</p> <p></p> <p>Confirm the run to let it apply the changes, after applying it should look like this:</p> <p></p> <p>We can see the websiteUrl stack output. If we try to curl it, lo and behold:</p> <pre><code>~&gt; curl s3-website-bucket-b47a23a.s3-website.eu-central-1.amazonaws.com\n&lt;html&gt;&lt;head&gt;\n    &lt;title&gt;Hello Amazon S3&lt;/title&gt;&lt;meta charset=\"UTF-8\"&gt;\n    &lt;link rel=\"shortcut icon\" href=\"/favicon.png\" type=\"image/png\"&gt;\n&lt;/head&gt;\n&lt;body&gt;&lt;p&gt;Hello, world!&lt;/p&gt;&lt;p&gt;Made with \u2764\ufe0f with &lt;a href=\"https://pulumi.com\"&gt;Pulumi&lt;/a&gt;&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>In order to clean up, open the Tasks tab, and perform <code>pulumi destroy --non-interactive --yes</code> there.</p> <p></p> <p>Which will destroy all created resources.</p> <p></p>"},{"location":"vendors/pulumi/javascript.html","title":"Javascript","text":"<p>In order to follow along with this article, you'll need an AWS account.</p> <p>Start with forking the Pulumi examples repo, we'll be setting up an example directory from there, namely aws-js-webserver.</p> <p>Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values:</p> <ul> <li>Set the project root to <code>aws-js-webserver</code>, as we want to run Pulumi in this subdirectory only.</li> <li>Add one before init script: <code>npm install</code>, which will install all necessary dependencies, before initializing Pulumi itself. The outputs will be persisted in the workspace and be there for the Planning and Applying phases.</li> <li>Set the runner image to <code>public.ecr.aws/spacelift/runner-pulumi-javascript:latest</code></li> <li>Pinning to a specific Pulumi version is possible too, using a tag like <code>v2.15.4</code> - you can see the available versions here.</li> </ul> <p></p> <p>In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose:</p> <ul> <li>A state backend, aka login URL. This can be a cloud storage bucket, like <code>s3://pulumi-state-bucket</code>, but it can also be a Pulumi Service endpoint.</li> <li>A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like <code>my-javascript-pulumi-spacelift-stack</code>.</li> </ul> <p></p> <p>Info</p> <p>You can use <code>https://api.pulumi.com</code> as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the <code>PULUMI_ACCESS_TOKEN</code> environment variable.</p> <p>You'll now have to set up the AWS integration for the Stack, as is described in AWS.</p> <p>Go into the Environment tab in your screen, add an <code>AWS_REGION</code> environment variable and set it to your region of choice, i.e. <code>eu-central-1</code>.</p> <p></p> <p>You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes.</p> <p></p> <p>Confirm the run to let it apply the changes, after applying it should look like this:</p> <p></p> <p>We can see the publicHostName stack output. If we try to curl it, lo and behold:</p> <pre><code>~&gt; curl ec2-18-184-240-9.eu-central-1.compute.amazonaws.com\nHello, World!\n</code></pre> <p>In order to clean up, open the Tasks tab, and perform <code>pulumi destroy --non-interactive --yes</code> there.</p> <p></p> <p>Which will destroy all created resources.</p> <p></p>"},{"location":"vendors/pulumi/python.html","title":"Python","text":"<p>In order to follow along with this article, you'll need an AWS account.</p> <p>Start with forking the Pulumi examples repo, we'll be setting up an example directory from there, namely aws-py-webserver.</p> <p>In the root of the repository (not the aws-py-webserver directory), add a new file:</p> .spacelift/config.yml<pre><code>version: \"1\"\n\nstack_defaults:\nbefore_apply:\n- pip install -r requirements.txt\n</code></pre> <p><code>before_apply</code> is not yet exposed through the interface like <code>before_init</code>, so you have to set it through the config file.</p> <p>Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values:</p> <ul> <li>Set the project root to <code>aws-py-webserver</code>, as we want to run Pulumi in this subdirectory only.</li> <li>Add one before init script: <code>pip install -r requirements.txt</code>, which will install all necessary dependencies, before initializing Pulumi itself. This will need to run both when initializing and before applying.</li> <li>Set the runner image to <code>public.ecr.aws/spacelift/runner-pulumi-python:latest</code></li> <li>Pinning to a specific Pulumi version is possible too, using a tag like <code>v2.15.4</code> - you can see the available versions here.</li> </ul> <p></p> <p>In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose:</p> <ul> <li>A state backend, aka login URL. This can be a cloud storage bucket, like <code>s3://pulumi-state-bucket</code>, but it can also be a Pulumi Service endpoint.</li> <li>A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like <code>my-python-pulumi-spacelift-stack</code>.</li> </ul> <p></p> <p>Info</p> <p>You can use <code>https://api.pulumi.com</code> as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the <code>PULUMI_ACCESS_TOKEN</code> environment variable.</p> <p>You'll now have to set up the AWS integration for the Stack, as is described in AWS.</p> <p>Go into the Environment tab in your screen, add an <code>AWS_REGION</code> environment variable and set it to your region of choice, i.e. <code>eu-central-1</code>.</p> <p></p> <p>You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes.</p> <p></p> <p>Confirm the run to let it apply the changes, after applying it should look like this:</p> <p></p> <p>We can see the <code>public_dns</code> stack output. If we try to curl it, lo and behold:</p> <pre><code>~&gt; curl ec2-3-125-48-55.eu-central-1.compute.amazonaws.com\nHello, World!\n</code></pre> <p>In order to clean up, open the Tasks tab, and perform <code>pulumi destroy --non-interactive --yes</code> there.</p> <p></p> <p>Which will destroy all created resources.</p> <p></p>"},{"location":"vendors/terraform/index.html","title":"Terraform","text":""},{"location":"vendors/terraform/index.html#why-use-terraform","title":"Why use Terraform?","text":"<p>Terraform is a full-featured, battle-tested Infrastructure as Code tool. It has a vast ecosystem of providers to interact with many vendors from cloud providers such as AWS, Azure and GCP to monitoring such as New Relic and Datadog, and many many more.</p> <p>There are also plenty of community-managed modules and tools to get you started in no time.</p>"},{"location":"vendors/terraform/index.html#why-use-spacelift-with-terraform","title":"Why use Spacelift with Terraform?","text":"<p>Spacelift helps you manage the complexities and compliance challenges of using Terraform. It brings with it a GitOps flow, so your infrastructure repository is synced with your Terraform Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies, which lets you automate compliance checks and build complex multi-stack workflows.</p> <p>You can also use Spacelift to mix and match Terraform, Pulumi, and CloudFormation Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like an ECS/EKS cluster with all its dependencies) and then connect that to a CloudFormation Stack which then transactionally deploys your services there using trigger policies and the Spacelift provider run resources for workflow orchestration and Contexts to export Terraform outputs as CloudFormation input parameters.</p>"},{"location":"vendors/terraform/index.html#does-spacelift-support-terraform-wrappers","title":"Does Spacelift support Terraform wrappers?","text":"<p>Yes! We support Terragrunt and Cloud Development Kit for Terraform (CDKTF). You can read more about it in the relevant subpages of this document.</p>"},{"location":"vendors/terraform/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Module registry</li> <li>Provider registry (beta)</li> <li>External modules</li> <li>Provider</li> <li>State management</li> <li>External state access</li> <li>Terragrunt</li> <li>Version management</li> <li>Handling .tfvars</li> <li>CLI Configuration</li> <li>Cost Estimation</li> <li>Resource Sanitization</li> <li>Storing Complex Variables</li> <li>Debugging Guide</li> <li>Dependency Lock File</li> <li>Cloud Development Kit for Terraform (CDKTF)</li> </ul>"},{"location":"vendors/terraform/cdktf.html","title":"Cloud Development Kit for Terraform (CDKTF)","text":"<p>The Cloud Development Kit for Terraform (CDKTF) generates JSON Terraform configuration from code in C#, Python, TypeScript, Java, or Go. Spacelift fully supports CDKTF.</p>"},{"location":"vendors/terraform/cdktf.html#building-a-custom-runner-image","title":"Building a custom runner image","text":"<p>CDKTF requires packages and tools that are not included in the default Terraform runner. These dependencies are different for each supported programming language.</p> <p>Luckily, extending the default runner Docker image to include these dependencies is easy. You will need to:</p> <ul> <li>Create a <code>Dockerfile</code> file that installs the required tools and packages for the specific programming language you want to use (see below).</li> <li>Build and publish the Docker image.</li> <li>Configure the runner image to use in the stack settings.</li> </ul> TypeScriptPythonGo <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\n\nUSER root\nRUN apk add --no-cache nodejs npm\nRUN npm install --global cdktf-cli@latest\nUSER spacelift\n</code></pre> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\n\nUSER root\nRUN apk add --no-cache nodejs npm python3\nRUN npm install --global cdktf-cli@latest\nRUN python3 -m ensurepip \\\n&amp;&amp; python3 -m pip install --upgrade pip setuptools\n\nUSER spacelift\nRUN pip3 install --user pipenv\nENV PATH=\"/home/spacelift/.local/bin:$PATH\"\n</code></pre> <pre><code>FROM public.ecr.aws/spacelift/runner-terraform:latest\n\nUSER root\nRUN apk add --no-cache go nodejs npm\nRUN npm install --global cdktf-cli@latest\nUSER spacelift\n</code></pre>"},{"location":"vendors/terraform/cdktf.html#synthesizing-terraform-code","title":"Synthesizing Terraform code","text":"<p>Before Terraform can plan and apply changes to your infrastructure, CDKTF must turn your C#, Python, TypeScript, Java, or Go code into Terraform configuration code. That process is called synthesizing.</p> <p>This step needs to happen before the Initializing phase of a run. This can be easily done by adding a few <code>before_init</code> hooks:</p> TypeScriptPythonGo <ul> <li><code>npm install</code></li> <li><code>cdktf synth</code></li> <li><code>cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json .</code></li> </ul> <ul> <li><code>pipenv install</code></li> <li><code>cdktf synth</code></li> <li><code>cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json .</code></li> </ul> <ul> <li><code>cdktf synth</code></li> <li><code>cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json .</code></li> </ul> <p>Warning</p> <p>If the Terraform state is managed by Spacelift, make sure to disable the local backend that CDKTF automatically adds if none is configured by adding the following command after the hooks mentioned above:</p> <pre><code>jq '.terraform.backend.local = null' cdk.tf.json &gt; cdk.tf.json.tmp &amp;&amp; mv cdk.tf.json.tmp cdk.tf.json\n</code></pre>"},{"location":"vendors/terraform/cli-configuration.html","title":"CLI Configuration","text":"<p>For some of our Terraform users, a convenient way to configure the Terraform CLI behavior is through customizing the <code>~/.terraformrc</code> file.</p> <p>During the preparing phase of a run, Spacelift creates a configuration file at the default location: <code>~/.terraformrc</code>. This file contains credentials that are needed to communicate with remote services.</p>"},{"location":"vendors/terraform/cli-configuration.html#extending-cli-configuration","title":"Extending CLI configuration","text":"<p>Extending the Terraform CLI behavior can be done by using mounted files:</p>"},{"location":"vendors/terraform/cli-configuration.html#using-mounted-files","title":"Using mounted files","text":"<p>Any mounted files with names ending in <code>.terraformrc</code> will be appended to <code>~/.terraformrc</code>.</p> <p>Info</p> <p>The Terraform CLI configuration file syntax supports these settings</p>"},{"location":"vendors/terraform/debugging-guide.html","title":"Debugging Guide","text":""},{"location":"vendors/terraform/debugging-guide.html#setting-environment-variables","title":"Setting Environment Variables","text":"<p>Environment variables are commonly used for enabling advanced logging levels for Terraform and Terragrunt. There are two ways environment variables can be set for runs.</p> <ol> <li> <p>Set Environment Variable(s) directly on a Stack's Environment (easiest method).</p> </li> <li> <p>Set Environment Variable(s) in a Context, and then attach that Context to your Spacelift Stack(s).</p> </li> </ol>"},{"location":"vendors/terraform/debugging-guide.html#terraform-debugging","title":"Terraform Debugging","text":"<p>Terraform providing an advanced logging mode that can be enabled using the <code>TF_LOG</code> environment variable. As of the writing of this documentation, <code>TF_LOG</code> has 5 different logging levels: <code>TRACE</code> <code>DEBUG</code> <code>INFO</code> <code>WARN</code> and <code>ERROR</code></p> <p>Please refer to the Setting Environment Variables section for more information on how to set these variables on your Spacelift Stack(s).</p> <p>For more information on Terraform logging, please refer directly to the Terraform documentation.</p>"},{"location":"vendors/terraform/debugging-guide.html#terragrunt-debugging","title":"Terragrunt Debugging","text":"<p>Please refer to the Debugging Terragrunt section of our Terragrunt documentation.</p>"},{"location":"vendors/terraform/dependency-lock-file.html","title":"Dependency Lock File","text":"<p>Recent versions of Terraform can optionally track dependency selections using a Dependency Lock File named <code>.terraform.lock.hcl</code>, in a similar fashion to npm's <code>package-lock.json</code> file.</p> <p>If this file is present in the project root for your stack, Terraform will use it. Otherwise, it will dynamically determine the dependencies to use.</p>"},{"location":"vendors/terraform/dependency-lock-file.html#generating-updating-the-file","title":"Generating &amp; Updating the File","text":"<p>Terraform recommends including the Dependency Lock File file in your version control repository, alongside your infrastructure code.</p> <p>You can generate or update this file by running <code>terraform init</code> locally and committing it into your repository.</p> <p>An alternative option would be to run the <code>terraform init</code> in a Task, print it to the Task logs, copy/paste the content from the Task logs into the <code>.terraform.lock.hcl</code> file, and commit it into your repository.</p>"},{"location":"vendors/terraform/external-modules.html","title":"External modules","text":"<p>Those of our customers who are not yet using our private module registry may want to pull modules from various external sources supported by Terraform. This article discusses a few most popular types of module sources and how to use them in Spacelift.</p>"},{"location":"vendors/terraform/external-modules.html#cloud-storage","title":"Cloud storage","text":"<p>The easiest option is accessing modules from Amazon S3 buckets. Access to S3 can be granted using our AWS integration. Alternatively if your worker pool has the correct IAM permissions, you may not require any authentication at all!</p>"},{"location":"vendors/terraform/external-modules.html#git-repositories","title":"Git repositories","text":"<p>Git is by far the most popular external module source. This example will focus on GitHub as the most popular one, but the advice applies to other VCS providers. In general, Terraform retrieves Git-based modules using one of the two supported transports - HTTPS or SSL. Assuming your repository is private, you will need to give Spacelift credentials required to access it.</p>"},{"location":"vendors/terraform/external-modules.html#using-https","title":"Using HTTPS","text":"<p>Git with HTTPS is slightly simpler than SSH - all you need is a personal access token, and you need to make sure that it ends up in the <code>~/.netrc</code> file, which Terraform will use to log in to the host that stores your source code.</p> <p>Assuming you already have a token you can use, create a file like this:</p> <pre><code>machine github.com\nlogin $yourLogin\npassword $yourToken\n</code></pre> <p>Then, upload this file to your stack's Spacelift environment as a mounted file. In this example, we called that file <code>github.netrc</code>:</p> <p></p> <p>Add the following commands as \"before init\" hooks to append the content of this file to the <code>~/.netrc</code> proper:</p> <pre><code>cat /mnt/workspace/github.netrc &gt;&gt; ~/.netrc\nchmod 600 ~/.netrc\n</code></pre>"},{"location":"vendors/terraform/external-modules.html#using-ssh","title":"Using SSH","text":"<p>Using SSH isn't much more complex, but it requires a bit more preparation. Once you have a public-private key pair (whether it's a personal SSH key or a single-repo deploy key), you will need to pass it to Spacelift and make sure it's used to access your VCS provider. Once again, we're going to use the mounted file functionality to pass the private key called <code>id_ed25519</code> to your stack's environment:</p> <p></p> <p>Add the following commands as \"before init\" hooks to \"teach\" our SSH agent to use this key for GitHub:</p> <pre><code>mkdir -p ~/.ssh\ncp /mnt/workspace/id_ed25519 ~/.ssh/id_ed25519\nchmod 400 ~/.ssh/id_ed25519\nssh-keyscan -t rsa github.com &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>The above example warrants a little explanation. First, we're making sure that the <code>~/.ssh</code> directory exists - otherwise, we won't be able to put anything in there. Then we copy the private key file mounted in our workspace to the SSH configuration directory and give it proper permissions. Last but not least, we're using the <code>ssh-keyscan</code> utility to retrieve the public SSH host key for <code>github.com</code> and add it to the list of known hosts - this will avoid your code checkout failing due to what would otherwise be an interactive prompt asking you whether to trust that key.</p> <p>Tip</p> <p>If you get an error referring to error in libcrypto, please try adding a newline to the end of the mounted file and trying again.</p>"},{"location":"vendors/terraform/external-modules.html#dedicated-third-party-registries","title":"Dedicated third-party registries","text":"<p>For users storing their modules in dedicated external private registries, like Terraform Cloud's one, you will need to supply credentials in the <code>.terraformrc</code> file - this approach is documented in the official documentation.</p> <p>In order to facilitate that, we've introduced a special mechanism for extending the CLI configuration that does not even require using <code>before_init</code> hooks. You can read more about it here.</p>"},{"location":"vendors/terraform/external-modules.html#to-mount-or-not-to-mount","title":"To mount or not to mount?","text":"<p>That is the question. And there isn't a single right answer. Instead, there is a list of questions to consider. By mounting a file, you're giving us access to its content. No, we're not going to read it, and yes, we have it encrypted using a fancy multi-layered mechanism, but still - we have it. So the main question is how sensitive the credentials are. Read-only deploy keys are probably the least sensitive - they only give read access to a single repository, so these are the ones where convenience may outweigh other concerns. On the other hand, personal access tokens may be pretty powerful, even if you generate them from service users. The same thing goes for personal SSH keys. Guard these well.</p> <p>So if you don't want to mount these credentials, what are your options? First, you can put these credentials directly into your private runner image. But that means that anyone in your organization who uses the private runner image gets access to your credentials - and that may or may not be what you wanted.</p> <p>The other option is to store the credentials externally in one of the secrets stores - like AWS Secrets Manager or HashiCorp Vault and retrieve them in one of your <code>before_init</code> scripts before putting them in the right place (<code>~/.netrc</code> file, <code>~/.ssh</code> directory, etc.).</p> <p>Info</p> <p>If you decide to mount, we advise that you store credentials in contexts and attach these to stacks that need them. This way you can avoid credentials sprawl and leak.</p>"},{"location":"vendors/terraform/external-state-access.html","title":"External state access","text":"<p>External state access allows you to read the state of the stack from outside authorized runs and tasks. In particular, this enables sharing the outputs between stacks using the Terraform mechanism of remote state or even accessing the state offline for analytical or compliance purposes.</p> <p>If enabled for a particular stack, any user or stack with Write permission to that stack's space will be able to access its state.</p> <p>This feature is off by default.</p>"},{"location":"vendors/terraform/external-state-access.html#enabling-external-access","title":"Enabling external access","text":"<p>Info</p> <p>Only administrative stacks or users with write permission to this Stack's space can access the state.</p> <p>You can enable the external access in a couple of ways.</p> <ul> <li>through the UI</li> </ul> <p></p> <ul> <li>using the Terraform provider.</li> </ul> <pre><code>resource \"spacelift_stack\" \"example\" {\nname                            = \"example\"\nrepository                      = \"spacelift-stacks\"\nbranch                          = \"main\"\nterraform_external_state_access = true\n}\n</code></pre>"},{"location":"vendors/terraform/external-state-access.html#sharing-outputs-between-stacks","title":"Sharing outputs between stacks","text":"<p>Sharing the outputs between stacks can be achieved using the Terraform mechanism of remote state.</p> <p>Given an account called spacecorp, and a stack named deep-thought, having the following Terraform configuration.</p> <pre><code>output \"answer\" {\nvalue = 42\n}\n</code></pre> <p>You can read that output from a different stack by using the <code>terraform_remote_state</code> data source.</p> <pre><code>data \"terraform_remote_state\" \"deepthought\" {\nbackend = \"remote\"\n\nconfig = {\nhostname     = \"spacelift.io\" # (1)\norganization = \"spacecorp\"    # (2)\n\nworkspaces = {\nname = \"deep-thought\"       # (3)\n}\n}\n}\n\noutput \"ultimate_answer\" {\nvalue = data.terraform_remote_state.deepthought.outputs.answer\n}\n</code></pre> <ol> <li>The hostname of the Spacelift system.</li> <li>The name of the account.</li> <li>The ID of the stack you wish to retrieve outputs from.</li> </ol>"},{"location":"vendors/terraform/external-state-access.html#offline-state-access","title":"Offline state access","text":"<p>Given an account called <code>spacecorp</code>, and a stack named <code>deep-thought</code>, having the following Terraform configuration.</p> <pre><code>output \"answer\" {\nvalue = 42\n}\n</code></pre> <p>Before you will be able to access the state of the stack, you need to retrieve an authentication token for spacelift.io.</p> <pre><code>terraform login spacelift.io\n</code></pre> <p>Next, you need a remote backend configuration.</p> <pre><code>terraform {\nbackend \"remote\" {\nhostname     = \"spacelift.io\" # (1)\norganization = \"spacecorp\"    # (2)\n\nworkspaces {\nname = \"deep-thought\"       # (3)\n}\n}\n}\n</code></pre> <ol> <li>The hostname of the Spacelift system.</li> <li>The name of the account.</li> <li>The ID of the stack you wish to retrieve outputs from.</li> </ol> <p>Finally, you can download the state of the stack.</p> <pre><code>terraform state pull &gt; terraform.tfstate\n</code></pre>"},{"location":"vendors/terraform/handling-tfvars.html","title":"Handling .tfvars","text":"<p>For some of our Terraform users, the most convenient solution to configure a stack is to specify its input values in a variable definitions file that is then passed to Terraform executions in plan and apply phases.</p> <p>Spacelift supports this approach but does not provide a separate mechanism, depending instead on a combination of Terraform's built-in mechanisms and Spacelift-provided primitives like:</p> <ul> <li>environment variables;</li> <li>mounted files</li> <li><code>before_init</code>scripts;</li> </ul>"},{"location":"vendors/terraform/handling-tfvars.html#using-environment-variables","title":"Using environment variables","text":"<p>In Terraform, special environment variables can be used to pass extra flags to executed commands like plan or apply. These are the more generic <code>TF_CLI_ARGS</code> and <code>TF_CLI_ARGS_name</code> that only affect a specific command. In Spacelift, environment variables can be defined directly on stacks and modules, as well as on contexts attached to those. As an example, let's declare the following environment variable:</p> <p></p> <p>In our particular case we don't have this file checked in, so the run will fail:</p> <p></p> <p>But we can supply this file dynamically using mounted files functionality.</p>"},{"location":"vendors/terraform/handling-tfvars.html#using-mounted-files","title":"Using mounted files","text":"<p>If the variable definitions file is not part of the repo, we can inject it dynamically. The above example can be fixed by supplying the variables file at the requested path:</p> <p></p> <p>Note that there are \"magical\" names you can give to your variable definitions files that always get autoloaded, without the need to supply extra CLI arguments. According to the documentation, Terraform automatically loads a number of variable definitions files if they are present:</p> <ul> <li>Files named exactly <code>terraform.tfvars</code> or <code>terraform.tfvars.json</code>.</li> <li>Any files with names ending in <code>.auto.tfvars</code> or <code>.auto.tfvars.json</code>.</li> </ul> <p>The above can be used in conjunction with another Spacelift building block, <code>before_init</code> hooks.</p>"},{"location":"vendors/terraform/handling-tfvars.html#using-before_init-hooks","title":"Using <code>before_init</code> hooks","text":"<p>If you need to use different variable definitions files for different projects, would like to have them checked into the repo, but would also want to avoid supplying extra CLI arguments, you could just dynamically move files - whether as a move, copy or a symlink to one of the autoloaded locations. This should happen in one of the <code>before_init</code> steps. Example:</p> <p></p>"},{"location":"vendors/terraform/infracost.html","title":"Cost Estimation","text":"<p>The Infracost integration allows you to run an Infracost breakdown during Spacelift runs, providing feedback on PRs, and allowing you to integrate cost data with plan policies.</p> <p>This allows you to understand how infrastructure changes will impact costs, and to build automatic guards to help prevent costs from spiraling out of control.</p>"},{"location":"vendors/terraform/infracost.html#setting-up-the-integration","title":"Setting up the integration","text":"<p>To enable Infracost on any stack you need to do the following:</p> <ul> <li>Add the <code>infracost</code> label to the stack.</li> <li>Add an <code>INFRACOST_API_KEY</code> environment variable containing your Infracost API key.</li> </ul> <p>Info</p> <p>Creating a context for your Infracost API key means you can attach your key to any stacks that need to have Infracost enabled.</p> <p>If Infracost has been configured successfully, you should see some messages during the initialization phase of your runs indicating that Infracost is enabled and that the environment variable has been found:</p> <p></p>"},{"location":"vendors/terraform/infracost.html#additional-cli-arguments","title":"Additional CLI Arguments","text":"<p>If you need to pass any additional CLI arguments to the Infracost breakdown command, you can add them to the <code>INFRACOST_CLI_ARGS</code> environment variable. Anything found in this variable is automatically appended to the command. This allows you to do things like specifying the path to your Infracost usage file.</p>"},{"location":"vendors/terraform/infracost.html#ignore-failures","title":"Ignore Failures","text":"<p>By default, a failure executing Infracost, or a non-zero exit code being returned from the command will cause runs to fail.</p> <p>This behavior can be changed by setting the <code>INFRACOST_WARN_ON_FAILURE</code> environment variable to <code>true</code>. When enabled, Infracost errors will produce a warning message, but will not cause run failures.</p>"},{"location":"vendors/terraform/infracost.html#using-the-integration","title":"Using the integration","text":"<p>Once the integration is configured, Spacelift will automatically run Infracost breakdowns during the planning and applying stages. The following sections explain the functionality provided by the integration.</p>"},{"location":"vendors/terraform/infracost.html#pull-requests","title":"Pull Requests","text":"<p>Spacelift automatically posts the usage summary to your pull requests once Infracost is enabled:</p> <p></p>"},{"location":"vendors/terraform/infracost.html#plan-policies","title":"Plan Policies","text":"<p>Spacelift includes the full Infracost breakdown report in JSON format as part of the input to your plan policies. This is contained in <code>third_party_metadata.infracost</code>. The following shows an example plan input:</p> <pre><code>{\n\"cloudformation\": null,\n\"pulumi\": null,\n\"spacelift\": { ... },\n\"terraform\": { ... },\n\"third_party_metadata\": {\n\"infracost\": {\n\"projects\": [{\n\"breakdown\": {\n\"resources\": [...],\n\"totalHourlyCost\": \"0.0321506849315068485\",\n\"totalMonthlyCost\": \"23.47\"\n},\n\"diff\": {\n\"resources\": [...],\n\"totalHourlyCost\": \"0.0321506849315068485\",\n\"totalMonthlyCost\": \"23.47\"\n},\n\"metadata\": {},\n\"pastBreakdown\": {\n\"resources\": [],\n\"totalHourlyCost\": \"0\",\n\"totalMonthlyCost\": \"0\"\n},\n\"path\": \"/tmp/spacelift-plan923575332\"\n}],\n\"resources\": [...],\n\"summary\": {\n\"unsupportedResourceCounts\": {}\n},\n\"timeGenerated\": \"2021-06-09T14:14:44.146230883Z\",\n\"totalHourlyCost\": \"0.0321506849315068485\",\n\"totalMonthlyCost\": \"23.47\",\n\"version\": \"0.1\"\n}\n}\n}\n</code></pre> <p>This means that you can take cost information into account when deciding whether to ask for human approval or to block changes entirely. The following policy provides a simple example of this:</p> <pre><code>package spacelift\n\n# Prevent any changes that will cause the monthly cost to go above a certain threshold\ndeny[sprintf(\"monthly cost greater than $%d ($%.2f)\", [threshold, monthly_cost])] {\n  threshold := 100\n  monthly_cost := to_number(input.third_party_metadata.infracost.projects[0].breakdown.totalMonthlyCost)\n  monthly_cost &gt; threshold\n}\n\n# Warn if the monthly costs increase more than a certain percentage\nwarn[sprintf(\"monthly cost increase greater than %d%% (%.2f%%)\", [threshold, percentage_increase])] {\n  threshold := 5\n  previous_cost := to_number(input.third_party_metadata.infracost.projects[0].pastBreakdown.totalMonthlyCost)\n  previous_cost &gt; 0\n\n  monthly_cost := to_number(input.third_party_metadata.infracost.projects[0].breakdown.totalMonthlyCost)\n  percentage_increase := ((monthly_cost - previous_cost) / previous_cost) * 100\n\n  percentage_increase &gt; threshold\n}\n</code></pre>"},{"location":"vendors/terraform/infracost.html#resources-view","title":"Resources View","text":"<p>Infracost provides information about how individual resources contribute to the overall cost of the stack. Spacelift combines this information with our resources view to allow you to view the cost information for each resource:</p> <p></p>"},{"location":"vendors/terraform/module-registry.html","title":"Module registry","text":""},{"location":"vendors/terraform/module-registry.html#intro","title":"Intro","text":"<p>In Terraform, modules help you abstract away common functionality in your infrastructure.</p> <p>The name of a module managed by Spacelift is of the following form:</p> <pre><code>spacelift.io/&lt;organization&gt;/&lt;module_name&gt;/&lt;provider&gt;\n</code></pre> <p>In this name we have:</p> <ul> <li>The source module registry - <code>spacelift.io</code> is used here;</li> <li>The organization which owns and maintains the module;</li> <li>The module name, this will usually be the best shorthand descriptor of what the module actually does, i.e. it could be starting a machine with an HTTP server running.</li> <li>The main Terraform provider this module is meant to work with, i.e. the provider for the cloud service the resources should be created on.</li> </ul> <p>You can use a module in your Terraform configuration this way:</p> <pre><code>module \"my-birthday-cake\" {\nsource  = \"spacelift.io/spacelift-io/cake/oven\"\nversion = \"4.2.0\"\n\n  # Inputs.\neggs  = 5\nflour = \"200g\"\n}\n\noutput \"my-birthday-cake\" {\nvalue = {\nweight = module.my-birthday-cake.weight\nallergens = module.my-birthday-cake.allergens\n}\n}\n</code></pre> <p>As you can see, we've explicitly used a module which can make cakes using an oven. We can specify variables the module depends on, and finally use the outputs the cake module exports.</p> <p>Spacelift obviously lets you host modules, but it also does much more, providing you with robust CI/CD for your modules, leading us to the question...</p>"},{"location":"vendors/terraform/module-registry.html#why-host-your-modules-on-spacelift","title":"Why host your Modules on Spacelift?","text":"<p>Spacelift provides everything you need to make your module easily maintainable and usable. There is CI/CD for multiple specified versions of Terraform, which \"runs\" your module on each commit. You get an autogenerated page describing your Module and its intricacies, so your users can explore them and gather required information at a glimpse. It's also deeply integrated with all the features Stacks use which you know and love, like Environments, Policies, Contexts and Worker Pools.</p>"},{"location":"vendors/terraform/module-registry.html#setting-up-a-module","title":"Setting up a Module","text":""},{"location":"vendors/terraform/module-registry.html#git-repository-structure","title":"Git repository structure","text":"<p>You will have to set up a repository for your module, the structure of the repository should be as follows:</p> <pre><code>.\n\u251c\u2500\u2500 .spacelift\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config.yml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 output.tf\n\u2514\u2500\u2500 variables.tf\n</code></pre> <p>Each module must have a <code>config.yml</code> file in its <code>.spacelift</code> directory, containing information about the module along with any test cases. Details of the format of this file can be found in the module configuration section of this page.</p> <p>You can check out an example module here: https://github.com/spacelift-io/terraform-spacelift-example</p> <p>Info</p> <p>The source code for a module can be stored in a subdirectory of your repository because you can specify the project root when configuring your module in Spacelift. An example of a repository containing multiple modules can be found here: https://github.com/spacelift-io/multimodule</p>"},{"location":"vendors/terraform/module-registry.html#spacelift-setup","title":"Spacelift setup","text":"<p>In order to add a module to Spacelift, navigate to the Terraform registry section of the account view, and click the Add module button:</p> <p></p> <p>The setup steps are pretty similar to the ones for stacks. First you you point Spacelift at the right repo and choose the \"tracked\" branch - note that repositories whose name don't follow the convention are filtered out:</p> <p></p> <p>In the behavior section there are just three settings: administrative, worker pool and  project root. You will only need to set administrative to <code>true</code> if your module manages Spacelift resources (and most likely it does not). Setting worker pool to the one you manage yourself makes sense if the module tests will be touching resources or accounts you don't want Spacelift to access directly. Plus, your private workers may have more bandwidth than the shared ones, so you may get feedback faster. The project root let's you specify the module source code root inside of your repository:</p> <p></p> <p>Last but not least, you will be able to add a name, provider, labels and description.</p> <p></p> <p>The name and provider will be inferred from your repository name if it follows the <code>terraform-&lt;provider&gt;-&lt;name&gt;</code> convention. However, if it can't be inferred or you want a custom name, then you can specify them directly. The final module slug will then be based on the name.</p>"},{"location":"vendors/terraform/module-registry.html#environment-contexts-and-policies","title":"Environment, contexts and policies","text":"<p>Environment and context management in modules is identical to that for stacks. The only thing worth noting here is the fact that environment variables and mounted files set either through the module environment directly, or via one of its attached contexts will be passed to each of the test cases for the module.</p> <p>Attaching policies works in a similar way. One thing worth pointing out is that the behavior of Trigger policies are slightly different for modules. Instead of being provided with the list of all other accessible stacks, module trigger policies receive a list of the current consumers of the module. This allows you to automatically trigger dependent stacks when new module versions are published.</p>"},{"location":"vendors/terraform/module-registry.html#module-configuration","title":"Module configuration","text":"<p>While by convention a single Git repository hosts a single module, that root module can have multiple submodules. Thus, we've created a way to create a number of test cases:</p> <pre><code># The version of the configuration file format\nversion: 1\n# Your module version - must be changed to release a new version\nmodule_version: 0.1.1\n\n# Any default settings that should be used for all test cases\ntest_defaults:\nbefore_init: [\"terraform fmt -check\"]\nrunner_image: your/runner:image\n\n# The set of tests to run to verify your module works correctly\ntests:\n- name: Test the module with 0.12.7\nterraform_version: 0.12.7\nenvironment:\nTF_VAR_bacon: tasty\n\n- name: Test the submodule with 0.13.0\n# project_root can be set if your test case is not stored in the root directory\nproject_root: submodule\nterraform_version: 0.13.0\nenvironment:\nTF_VAR_cabbage: awful\n\n- name: Ensure that the submodule can fail\n# You can use negative to indicate that the test case is expected to fail\nnegative: true\nproject_root: submodule\nterraform_version: 0.13.0\n</code></pre> <p>This configuration is nearly identical to the one described in the Runtime configuration section, with both <code>test_defaults</code> and each test case accepting the same configuration block. Note that settings explicitly specified in each test case will override those in the <code>test_defaults</code> section. Also, notice that each test case has a name, which is a required field.</p> <p>Info</p> <p>While we don't check for name uniqueness, it's always good idea to give your test cases descriptive names, as these are then used to report job status on your commits and pull requests.</p>"},{"location":"vendors/terraform/module-registry.html#tests","title":"Tests","text":"<p>In order to verify that your module is working correctly, Spacelift can run a number of test cases for your module. Note how the configuration above allows you to set up different runtime environment (Docker image, Terraform version) etc. If you want to test the module with different inputs, these can be passed as Terraform variables (starting with <code>TF_VAR_</code>) through the test-level <code>environment</code> configuration option - see above for an example.</p> <p>While coverage is not yet calculated or enforced, we suggest that tests set up all resources defined by the module and submodules. It's generally a good idea to provide examples in the <code>examples/</code> directory of your repository showing users how they can use the module in practice. These examples can then become your test cases, and you can test them against multiple supported Terraform version to maximize compatibility.</p> <p>While running each test case, Spacelift will - as usual, initialize, plan and apply the resource, but also destroy everything in the end, checking for errors. In the meantime, it will also validate that some resources have actually been created by the tests - though as for now it does not care what these are.</p> <p>A test case can be marked as <code>negative</code>, which means that it is expected to fail. In an example above one of the test cases is expected to fail if one of the required Terraform variables is not set. Negative test cases are as useful as positive ones because they can prove that the module will not work under certain - unexpected or erroneous - circumstances.</p> <p>Test cases will be executed in parallel (as much as worker count permits) for each of the test cases version you have specified in the module configuration.</p> <p>Tests run both on proposed and tracked changes. When a tracked change occurs, we create a Version. Versions are described in more detail in the Versions section.</p> <p>Info</p> <p>Each test case will have its own commit status in GitHub / GitLab.</p>"},{"location":"vendors/terraform/module-registry.html#test-case-ordering","title":"Test case ordering","text":"<p>You can specify the order in which test cases should be executed by setting the <code>depends_on</code> property on a test case. This property accepts a list of test case <code>id</code>s that must be executed before the current test case. For example:</p> <pre><code>version: 1\nmodule_version: 1.0.0\n\ntests:\n# This one is executed first.\n- name: Test the module with 0.12.7\nid: test-0.12.7\nterraform_version: 0.12.7\n\n# This is executed second, because it depends on the first test case.\n- name: Test the submodule with 0.13.0\nid: test-0.13.0\nproject_root: submodule\nterraform_version: 0.13.0\ndepends_on: [\"test-0.12.7\"]\n\n# This is executed third, because it depends on the second test case.\n- name: Ensure that the submodule can fail\ndepends_on: [\"test-0.13.0\"]\nnegative: true\nproject_root: submodule\n</code></pre> <p>Note that in order to refer to a test case, you need to set a unique <code>id</code> to it.</p>"},{"location":"vendors/terraform/module-registry.html#versions","title":"Versions","text":"<p>Tip</p> <p>If you would like to import old module versions, you can use a bash script that lists the tags in the repo and then use spacectl: <code>spacectl module create-version --id &lt;MODULE ID&gt; --sha &lt;COMMIT SHA&gt; --version &lt;MODULE VERSION&gt;</code></p> <p>Whenever tests succeed on a tracked change, a new Version is created based on the <code>module_version</code> in the configuration. Important thing to note is that Spacelift will not let you reuse the number of a successful version, and will require you to strictly follow semantic versioning - ie. you can't go to from <code>0.2.0</code> to <code>0.4.0</code>, skipping <code>0.3.0</code> entirely.</p> <p></p> <p>Two proposed git flow are as follows:</p> <p>The first one would be to have a main branch and create feature branches for changes. Whenever you merge to the main branch you bump the version and release it.</p> <p>If you'd like to ensure that the version is bumped before merging, we recommend adding something similar to your CI/CD pipeline:</p> <pre><code>git diff --exit-code --quiet HEAD^ HEAD -- .spacelift/config.yml\n\nif [ $? -ne 0 ]; then\necho \"Make sure to bump the module version in .spacelift/config.yml\"\nexit 1\nfi\n</code></pre> <p>If you want more control over release schedules, you could go with the following:</p> <ul> <li>A release branch</li> <li>A main branch</li> <li>Feature branches</li> </ul> <p>Whenever you add a new functionality, you may want to create a feature branch and open Pull Request from it to the main branch. Whenever you want to release a new version, you merge the main branch into the release branch.</p> <p>You can also use Git push policies to further customize this.</p> <p>Tip</p> <p>If you would like to manage your Terraform Module versions using git tags, and would like git tag events to push your module to the Spacelift module registry. Please review our Tag-driven Terraform Module Release Flow.</p> <p>Info</p> <p>If no test cases are present, the version is immediately marked green.</p>"},{"location":"vendors/terraform/module-registry.html#marking-versions-as-bad","title":"Marking versions as bad","text":"<p>If you don't want people to use a specific version of your module, you can mark it as bad. Currently, this feature doesn't have any technical implications - it is still downloadable and usable, but it's a good way to communicate to your users that a specific version is not recommended.</p> <p>You need to click on the version number and then click on the Mark as bad button on the right. Make sure to leave a note explaining why the version is bad.</p> <p></p>"},{"location":"vendors/terraform/module-registry.html#modules-in-practice","title":"Modules in practice","text":"<p>In order to use modules, you have to source them from the Spacelift module registry. You can generate the necessary snippet, by opening the page of the specific module version, and clicking show instructions.</p> <p></p> <p>Info</p> <p>Stacks that use private modules need access to the Space the modules reside in, which can be achieved via Space Inheritance.</p>"},{"location":"vendors/terraform/module-registry.html#sharing-modules","title":"Sharing modules","text":"<p>Unlike Stacks, modules can be shared between Spacelift accounts in a sense that while they're always managed by a single account, they can be made accessible to an arbitrary number of other accounts.</p> <p>In order to share the module with other accounts, please add their names in subdomain form (all lowercase) in the module settings Sharing section:</p> <p></p> <p>This can also be accomplished programmatically using our Terraform provider.</p>"},{"location":"vendors/terraform/module-registry.html#using-modules-outside-of-spacelift","title":"Using modules outside of Spacelift","text":"<p>Modules hosted in the private registry can be used outside of Spacelift.</p> <p>The easiest way is to have Terraform retrieve and store the credentials by running the following command in a terminal:</p> <pre><code>terraform login spacelift.io\n</code></pre> <p>After you confirm that you want to proceed, Terraform will open your default web browser and ask you to log in to your Spacelift account. Once this is done, Terraform will store the credentials in the <code>~/.terraform.d/credentials.tfrc.json</code> file for use by subsequent commands.</p> <p>Warning</p> <p>The method above requires a web browser which is not always practical, for example on remote server with no GUI. In that case, you can use credentials generated from API keys. The credentials file generated upon the creation of each API key contains a section explaining how a key can be used to set up credentials in the Terraform configuration file (<code>.terraformrc</code>). To learn more about this please refer directly to Terraform documentation.</p>"},{"location":"vendors/terraform/module-registry.html#dependabot","title":"Dependabot","text":"<p>If you want to use Dependabot to automatically update your module versions, you can use the following <code>dependabot.yml</code> configuration:</p> <pre><code>version: 2\nregistries:\nspacelift-private-registry:\ntype: terraform-registry\nurl: https://app.spacelift.io\ntoken: ${{ secrets.SPACELIFT_TOKEN }}\nupdates:\n- package-ecosystem: \"terraform\"\ndirectory: \"/\"\nregistries:\n- spacelift-private-registry\nschedule:\ninterval: \"daily\"\n</code></pre> <p>It is important for the <code>url</code> to be <code>https://app.spacelift.io</code> and for the <code>token</code> to be a Spacelift API key. Although Admin access is not required, if you are using our login policies, non-admin keys must be defined within this policy.</p>"},{"location":"vendors/terraform/provider-registry.html","title":"Provider registry","text":"<p>Warning</p> <p>This feature is currently in open public beta and is free to use regardless of your pricing plan. Once GA, it will be available as part of our Enterprise plan.</p>"},{"location":"vendors/terraform/provider-registry.html#intro","title":"Intro","text":"<p>While the Terraform ecosystem is vast and growing, sometimes there is no official provider for your use case, especially if you need to interface with internal or niche tooling. This is where the Spacelift provider registry comes in. It's a place where you can publish your own providers. These providers can be used both inside, and outside of Spacelift.</p>"},{"location":"vendors/terraform/provider-registry.html#publishing-a-provider","title":"Publishing a provider","text":""},{"location":"vendors/terraform/provider-registry.html#assumptions","title":"Assumptions","text":"<p>We will not be covering the process of writing a Terraform provider in this article. If you are interested in that, please refer to the official documentation. In this article, we will assume that you already have a provider that you want to publish.</p> <p>We will also focus on providing step-by-step instructions for GitHub Actions users. If you are using a different CI/CD tool, you will need to adapt the steps accordingly based on its documentation. Note that you don't need to use a CI/CD tool to publish a provider - you could do it from your laptop. However, we recommend using a CI/CD tool to automate the process and provide an audit trail of what's been done, when, and by whom.</p> <p>Last but not least, we assume you're going to use GoReleaser to build your provider. This is by far the most common way of managing Terraform providers which need to be available for different operating systems and architectures. If you're not familiar with GoReleaser, please refer to the official documentation. You can also check out Terraform's official <code>terraform-provider-scaffolding</code> template repository for an example of using GoReleaser with Terraform providers.</p>"},{"location":"vendors/terraform/provider-registry.html#creating-a-provider","title":"Creating a provider","text":"<p>To create a provider in Spacelift, you have three options:</p>"},{"location":"vendors/terraform/provider-registry.html#use-our-terraform-provider-preferable","title":"Use our Terraform provider (preferable)","text":"<p>This is the easiest way to do it, as it will let you manage your provider declaratively in the future:</p> provider.tf<pre><code>resource \"spacelift_terraform_provider\" \"provider\" {\n  # This is the type of your provider.\ntype        = \"myinternaltool\"\nspace_id    = \"root\"\ndescription = \"Explain what this is for\"\n}\n</code></pre> <p>Warning</p> <p>The <code>type</code> attribute in the <code>spacelift_terraform_provider</code> resource refers to the unique name of the provider within Spacelift account. It must consist of lowercase letters only.</p> <p>It is possible to mark the provider as public, which will make it available to everyone. This is generally not recommended, as it will make it easy for others to use your provider without your knowledge. At the same time, this is the only way of sharing a provider between Spacelift accounts. If you're doing that, make sure there is nothing sensitive in your provider. In order to mark the provider as public, you need to set its <code>public</code> attribute to <code>true</code>.</p>"},{"location":"vendors/terraform/provider-registry.html#use-the-api","title":"Use the API","text":"<p>To create a Terraform Provider using the GraphQL API, you can use the <code>terraformProviderCreate</code> mutation. This mutation allows you to create a new provider with the specified inputs.</p> <p>After successfully creating the Terraform Provider, you will receive an output of type <code>TerraformProvider</code>, which contains various fields providing information about the provider. These fields include the ID, creation timestamp, description, labels, latest version number, public accessibility, associated space details, update timestamp, specific version details, and a list of all versions of the provider.</p> <p>For more detailed information about the GraphQL API and its integration, please refer to the API documentation.</p>"},{"location":"vendors/terraform/provider-registry.html#create-the-provider-manually-in-the-ui","title":"Create the provider manually in the UI","text":"<p>In order to create a provider in the UI, navigate to the Terraform registry section of the account view, switch to the Providers tab and click the Create provider button:</p> <p></p> <p>In the Create Terraform provider drawer, you will need to provide the type, space and optionally a description and/or labels:</p> <p></p> <p>Info</p> <p>Note that we've put the provider in the <code>root</code> space. This is because we want to give everyone access to it. If you want to make it available only to a specific team, you can put it in a team-specific different space. In general, unless providers are made public, they can be accessed by all users and stacks belonging to the same space and its children (assuming they're set to inherit resources).</p>"},{"location":"vendors/terraform/provider-registry.html#register-a-gpg-key","title":"Register a GPG key","text":"<p>Warning</p> <p>Only Spacelift root admins can manage account GPG keys. If you're not a root admin, you will need to ask one to do it for you.</p> <p>Terraform uses GPG keys to verify the authenticity of providers. Before you can publish a provider version, you need to register a GPG key with Spacelift. Similarly to creating a provider, you have three options to register a GPG key:</p>"},{"location":"vendors/terraform/provider-registry.html#use-our-cli-tool-called-spacectl","title":"Use our CLI tool called <code>spacectl</code>","text":"<p>One reason we do not want to do it declaratively through the Terraform provider is that it would inevitably lead to the private key being stored in some Terraform state, which is not ideal. <code>spacectl</code> will let you register a GPG key without storing the private key anywhere outside of your system.</p> <p>If you have an existing GPG key that you want to use, you can use <code>spacectl</code> to register it:</p> <pre><code>spacectl provider add-gpg-key \\\n--import \\\n--name=\"My first GPG key\" \\\n--path=\"Path to the ASCII-armored private key\"\n</code></pre> <p>Alternatively, <code>spacectl</code> can generate a new key for you. Note that <code>spacectl</code> generates GPG keys without a passphrase:</p> <pre><code>spacectl provider add-gpg-key \\\n--generate \\\n--name=\"My first GPG key\" \\\n--email=\"Your email address\" \\\n--path=\"Path to save the ASCII-armored private key to\"\n</code></pre> <p>You can list all your GPG keys using <code>spacectl</code>:</p> <pre><code>spacectl provider list-gpg-keys\n</code></pre>"},{"location":"vendors/terraform/provider-registry.html#use-the-api_1","title":"Use the API","text":"<p>To register a GPG Key using the GraphQL API, you can utilize the <code>gpgKeyCreate</code> mutation. This mutation allows you to register a GPG key with the specified inputs.</p> <p>After successfully registering the GPG Key, you will receive an output of type <code>GpgKey</code>, which contains various fields providing information about the key. These fields include the creation timestamp, the user who created the key, the optional description, the ID, the name of the key, the revocation timestamp (if the key has been revoked), the user who revoked the key (if applicable), and the timestamp of the last update to the key.</p> <p>For more detailed information about the GraphQL API and its integration, please refer to the API documentation.</p>"},{"location":"vendors/terraform/provider-registry.html#register-the-gpg-key-manually-in-the-ui","title":"Register the GPG key manually in the UI","text":"<p>In order to register a GPG key in the UI, navigate to the Terraform registry section of the account view, switch to the GPG keys tab and click the Register GPG key button:</p> <p></p> <p>In the Register GPG key drawer, you will need to provide the name, the ASCII-armored private key and optionally a description:</p> <p></p>"},{"location":"vendors/terraform/provider-registry.html#cicd-setup","title":"CI/CD setup","text":"<p>Now that you have a provider and a GPG key, you can set up your CI/CD tool to publish provider versions. First, let's set up the GoReleaser config file in your provider repository:</p> .goreleaser.yml<pre><code>builds:\n- env: ['CGO_ENABLED=0']\nflags: ['-trimpath']\nldflags: ['-s -w -X main.version={{ .Version }} -X main.commit={{ .Commit }}']\ngoos: [darwin, linux]\ngoarch: [amd64, arm64]\nbinary: '{{ .ProjectName }}_v{{ .Version }}'\n\narchives:\n- format: zip\nname_template: '{{ .ProjectName }}_{{ .Version }}_{{ .Os }}_{{ .Arch }}'\n\nchecksum:\nname_template: '{{ .ProjectName }}_{{ .Version }}_SHA256SUMS'\nalgorithm: sha256\n\nsigns:\n- artifacts: checksum\nargs:\n- \"--batch\"\n- \"--local-user\"\n- \"{{ .Env.GPG_FINGERPRINT }}\"\n- \"--output\"\n- \"${signature}\"\n- \"--detach-sign\"\n- \"${artifact}\"\n\nrelease:\ndisable: true\n</code></pre> <p>This setup assumes that the name of your project (repository) is <code>terraform-provider-$name</code>. If it is not (maybe you're using a monorepo?) then you will need to change the config accordingly, presumably by hardcoding the project name.</p> <p>Next, let's make sure you have an API key for the Spacelift account you want to publish the provider to. You can refer to the API key management section of the API documentation for more information on how to do that. Note that the key you're generating must have admin access to the space that the provider lives in. If the provider is in the <code>root</code> space, then the key must have root admin access.</p> <p>We can now add a GitHub Actions workflow definition to our repository:</p> .github/workflows/release.yml<pre><code>name: release\non:\npush:\n\npermissions:\ncontents: write\n\njobs:\ngoreleaser:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v4\n\n- name: Unshallow\nrun: git fetch --prune --unshallow\n\n- name: Set up Go\nuses: actions/setup-go@v4\nwith:\ngo-version-file: 'go.mod'\ncache: true\n\n- name: Import GPG key\nuses: crazy-max/ghaction-import-gpg@v5\nid: import_gpg\nwith:\n# The private key must be stored in an environment variable registered\n# with GitHub. The expected format is ASCII-armored.\n#\n# If you need to use a passphrase, you can populate it in this\n# section, too.\ngpg_private_key: ${{ secrets.GPG_PRIVATE_KEY }}\n\n- name: Install spacectl\nuses: spacelift-io/setup-spacectl@main\nenv:\nGITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n- name: Run GoReleaser\n# We will only run GoReleaser when a tag is pushed. Semantic versioning\n# is required, but build metadata is not supported.\nif: startsWith(github.ref, 'refs/tags/')\nuses: goreleaser/goreleaser-action@v5\nwith:\nversion: latest\nargs: release --clean\nenv:\nGPG_FINGERPRINT: ${{ steps.import_gpg.outputs.fingerprint }}\n\n- name: Release new version\nif: startsWith(github.ref, 'refs/tags/')\nenv:\nGPG_KEY_ID: ${{ steps.import_gpg.outputs.keyid }}\n\n# This is the URL of the Spacelift account hosting the provider.\nSPACELIFT_API_KEY_ENDPOINT: https://youraccount.app.spacelift.io\n\n# This is the ID of the API key you generated earlier.\nSPACELIFT_API_KEY_ID: ${{ secrets.SPACELIFT_API_KEY_ID }}\n\n# This is the secret of the API key you generated earlier.\nSPACELIFT_API_KEY_SECRET: ${{ secrets.SPACELIFT_API_KEY_SECRET }}\nrun: # Don't forget to change the provider type!\nspacectl provider create-version --type=TYPE-change-me!!!\n</code></pre> <p>If everything is fine, pushing a tag like <code>v0.1.0</code> should create a new draft version of the provider in Spacelift. You can list the versions of your provider using <code>spacectl</code>:</p> <pre><code>spacectl provider list-versions --type=$YOUR_PROVIDER_TYPE\n</code></pre> <p>Warning</p> <p>The <code>type</code> parameter in the <code>spacectl provider list-versions</code> command refers to the unique name of the provider within Spacelift account. It must consist of lowercase letters only.</p> <p>Note the version status. Versions start their life as drafts, and you can publish them by grabbing their ID (first column) and using <code>spacectl</code>:</p> <pre><code>spacectl provider publish-version --version=$YOUR_VERSION_ID\n</code></pre> <p>Warning</p> <p>The <code>version</code> parameter in the <code>spacectl provider publish-versions</code> command refers to the unique ID of the provider version within Spacelift account.</p> <p>Once published, your version is ready to use. See the next section for more information.</p>"},{"location":"vendors/terraform/provider-registry.html#using-providers","title":"Using providers","text":"<p>Terraform providers hosted by Spacelift can be used the same way as providers hosted by the Terraform Registry. The only difference is that you need to specify the Spacelift registry URL in your Terraform configuration.</p> main.tf<pre><code>terraform {\nrequired_providers {\nyourprovider = {\nsource  = \"spacelift.io/your-org/yourprovider\"\n}\n}\n}\n</code></pre> <p>The above example does not refer to a specific version, meaning that you are going to always use the latest available (published) version of your provider. That said, you can use any versioning syntax supported by the Terraform Registry - learn more about it here.</p>"},{"location":"vendors/terraform/provider-registry.html#using-providers-inside-spacelift","title":"Using providers inside Spacelift","text":"<p>All runs in any of the stacks belonging to the provider's space or one of its children will be able to read the provider from the Spacelift registry, same as with modules This is because the runs are automatically authenticated to the spacelift.io registry while the run is in progress.</p>"},{"location":"vendors/terraform/provider-registry.html#using-providers-outside-of-spacelift","title":"Using providers outside of Spacelift","text":"<p>If you need to use the provider outside of Spacelift, you will need to authenticate to the registry first, either interactively (eg. on your machine) or using an API key (eg. in automation). This process is the same as with modules, so please refer to that section for more information.</p> <p>Your access to the required provider will be determined by your access to the space that the provider lives in. If you have read access to the space, you will be able to use the provider.</p>"},{"location":"vendors/terraform/provider-registry.html#other-management-tasks","title":"Other management tasks","text":"<p>Beyond creating and publishing new versions, there are a few other tasks you can perform on your provider. In this section we will cover the most common ones.</p>"},{"location":"vendors/terraform/provider-registry.html#revoking-gpg-keys","title":"Revoking GPG keys","text":"<p>If you lose control over your GPG key, you will want to revoke it. Revoking a key has no automatic impact on provider versions already published, but it will prevent you from publishing new versions signed with that key. You can revoke a key using <code>spacectl</code>:</p> <pre><code>spacectl provider revoke-gpg-key --key=$ID_OF_YOUR_KEY\n</code></pre> <p>... or by using the UI. In order to do that, click the Revoke button inside the three dots menu next to the key you want to revoke:</p> <p></p> <p>If you want to also revoke any of the provider versions signed with this key, refer to the section on revoking provider versions.</p>"},{"location":"vendors/terraform/provider-registry.html#deleting-provider-versions","title":"Deleting provider versions","text":"<p>You can permanently delete any draft provider version using <code>spacectl</code>:</p> <pre><code>spacectl provider delete-version --version=$ID_OF_YOUR_VERSION\n</code></pre> <p>You can also do that in the UI, just navigate to the provider page, find the version you want to delete, and click the Delete button inside the three dots menu next to the version you want to delete:</p> <p></p> <p>A deleted version disappears from the list of versions, and you can reuse its number in the future.</p> <p>You cannot delete published versions. If you want to disable a published version, you will need to revoke it instead.</p>"},{"location":"vendors/terraform/provider-registry.html#revoking-provider-versions","title":"Revoking provider versions","text":"<p>You can revoke any published provider version using <code>spacectl</code>:</p> <pre><code>spacectl provider revoke-version --version=$ID_OF_YOUR_VERSION\n</code></pre> <p>Similarly to deleting a draft provider version, you can delete a published one in the UI by clicking the Revoke button inside the three dots menu next to the version you want to revoke:</p> <p></p> <p>This will prevent anyone from using the version in the future, but it will not delete it. The version will remain on the list of versions, and will never be available again. You will not be able to reuse the version number of a revoked version.</p>"},{"location":"vendors/terraform/resource-sanitization.html","title":"Resource Sanitization","text":"<p>The Terraform state can contain very sensitive data. Sometimes this is unavoidable because of the design of certain Terraform providers or because the definition of what is sensitive isn't always simple and may vary between individuals and organizations.</p> <p>Spacelift provides two different approaches for sanitizing values when resources are stored or passed to Plan policies:</p> <ul> <li>Default Sanitization: All string values are sanitized.</li> <li>Smart Sanitization: Only the values marked as sensitive are sanitized.</li> </ul> <p>For example, if we take the following definition for an AWS RDS instance:</p> <pre><code>resource \"aws_db_instance\" \"example\" {\nallocated_storage    = 10\ndb_name              = \"exampledb\"\nengine               = \"mysql\"\nengine_version       = \"5.7\"\ninstance_class       = var.instance_class\nusername             = var.username\npassword             = var.password\nparameter_group_name = \"default.mysql5.7\"\nskip_final_snapshot  = true\n}\n\nvariable \"instance_class\" {\ndefault     = \"db.t3.micro\"\ndescription = \"Instance type\"\ntype        = string\n}\n\nvariable \"username\" {\ndescription = \"Admin username\"\nsensitive   = true\ntype        = string\n}\n\nvariable \"password\" {\ndescription = \"Admin password\"\nsensitive   = true\ntype        = string\n}\n</code></pre> <p>Spacelift will supply something similar to the following to any plan policies:</p> Default SanitizationSmart Sanitization <pre><code>{\n\u2026\n\"terraform\": {\n\"resource_changes\": [\n{\n\"address\": \"aws_db_instance.example\",\n\"change\": {\n\"actions\": [\"create\"],\n\"after\": {\n\"allocated_storage\": 10,\n\"db_name\": \"59832d41\",\n\"engine\": \"eae35047\",\n\"engine_version\": \"fc40d152\",\n\"instance_class\": \"4f8189cd\",\n\"parameter_group_name\": \"bead1390\",\n\"password\": \"c1707d9d\",\n\"skip_final_snapshot\": true,\n\"username\": \"6266e7ae\",\n\u2026\n},\n\u2026\n},\n\u2026\n}\n]\n}\n\u2026\n}\n</code></pre> <pre><code>{\n\u2026\n\"terraform\": {\n\"resource_changes\": [\n{\n\"address\": \"aws_db_instance.example\",\n\"change\": {\n\"actions\": [\"create\"],\n\"after\": {\n\"allocated_storage\": 10,\n\"db_name\": \"exampledb\",\n\"engine\": \"mysql\",\n\"engine_version\": \"5.7\",\n\"instance_class\": \"db.t3.micro\",\n\"parameter_group_name\": \"default.mysql5.7\",\n\"password\": \"c1707d9d\",\n\"skip_final_snapshot\": true,\n\"username\": \"6266e7ae\",\n\u2026\n},\n\u2026\n},\n\u2026\n}\n]\n}\n\u2026\n}\n</code></pre> <p>As you can see in the example above, with the Default Sanitization all string values are hashed which makes it difficult to comprehend logs, work with outputs/created resources, and write policies against the changes in your stacks. With Smart Sanitization, only sensitive values are hashed.</p> <p>Smart Sanitization allows you to author Plan policies against non-sensitive string values without the need for a call to the <code>sanitized()</code> function.</p> Default SanitizationSmart Sanitization <pre><code>package spacelift\nimport future.keywords\n\nallowed_instance_classes := [\"db.t3.micro\", \"db.t3.small\", \"db.t3.medium\"]\n\n# The values have to be sanitized before they can be compared with the resource change value\nsanitized_allowed_instance_classes := {c | sanitized(allowed_instance_classes[_], c)}\n\ndeny[\"Instance class is not allowed\"] {\n  resource := input.terraform.resource_changes[_]\n  not resource.change.after.instance_class in sanitized_allowed_instance_classes\n}\n\ndeny[\"Username cannot be 'admin'\"] {\n  resource := input.terraform.resource_changes[_]\n\n  # The value has to be sanitized before they can be compared with the resource change value\n  resource.change.after.username = sanitized(\"admin\")\n}\n\n\n# Enable sampling to help us debug the policy\nsample { true }\n</code></pre> <pre><code>package spacelift\nimport future.keywords\n\n# No need to sanitize the values because the argument is not marked as sensitive\nallowed_instance_classes := [\"db.t3.micro\", \"db.t3.small\", \"db.t3.medium\"]\n\ndeny[\"Instance class is not allowed\"] {\n  resource := input.terraform.resource_changes[_]\n  not resource.change.after.instance_class in allowed_instance_classes\n}\n\ndeny[\"Username cannot be 'admin'\"] {\n  resource := input.terraform.resource_changes[_]\n\n  # The value has to be sanitized before it can be compared because the argument is marked as sensitive\n  resource.change.after.username = sanitized(\"admin\")\n}\n\n\n# Enable sampling to help us debug the policy\nsample { true }\n</code></pre> <p>Info</p> <p>The same sanitization is also applied to resources shown in the resources views.</p>"},{"location":"vendors/terraform/resource-sanitization.html#default-sanitization","title":"Default Sanitization","text":"<p>Unless you enable Smart Sanitization, or disable Sanitization altogether, Default Sanitization will be used.</p>"},{"location":"vendors/terraform/resource-sanitization.html#smart-sanitization","title":"Smart Sanitization","text":"<p>Info</p> <p>Due to limitations in the data output by Terraform, Smart Sanitization can only be used on stacks that use Terraform versions 1.0.1 or above. Using an unsupported version will fail.</p> <p>As we rely on the sensitive argument in Terraform to determine which of your values are sensitive we recommend that you ensure your variables, outputs, and resources have their <code>sensitive</code> arguments set properly.</p>"},{"location":"vendors/terraform/resource-sanitization.html#how-to-enable-smart-sanitization-in-your-stacks","title":"How to enable Smart Sanitization in your stacks","text":""},{"location":"vendors/terraform/resource-sanitization.html#when-using-the-spacelift-user-interface","title":"When using the Spacelift User Interface","text":"<p>If you're using the Spacelift user interface to create your stacks, you can enable Smart Sanitization in your existing stack settings page under the \"Backend\" section. If you're creating new stacks it will be an option when you select your Terraform version in our wizard.</p> <p></p>"},{"location":"vendors/terraform/resource-sanitization.html#when-creating-stacks-with-the-spacelift-terraform-provider","title":"When creating stacks with the Spacelift Terraform Provider","text":"<p>If you\u2019re using the Spacelift Terraform provider for creating Spacelift stacks, you are able to set the property <code>terraform_smart_sanitization</code>. For example, to create a simple stack you can use the following code:</p> <pre><code>resource \"spacelift_stack\" \"user_dashboard_internal\" {\nadministrative               = true\nautodeploy                   = true\nbranch                       = \"main\"\ndescription                  = \u201cCreates and manages the user management internal dashboard\"\nname                         = \"User Dashboard Internal\"\nrepository                   = \"management\"\nterraform_smart_sanitization = true\nterraform_version            = \"1.3.1\"\n}\n</code></pre>"},{"location":"vendors/terraform/resource-sanitization.html#disabling-sanitization","title":"Disabling Sanitization","text":"<p>If you have a situation where the <code>sanitized()</code> helper function doesn't provide you with enough flexibility to create a particular policy, you can disable sanitization completely for a stack. To do this, add the <code>feature:disable_resource_sanitization</code> label to your stack. This will disable sanitization for any future runs.</p>"},{"location":"vendors/terraform/state-management.html","title":"State management","text":"<p>For those of you who don't want to manage Terraform state, Spacelift offers an optional sophisticated state backend synchronized with the rest of the application to maximize security and convenience. The ability to have Spacelift manage the state for you is only available during stack creation.</p> <p>As you can see, it's also possible to import an existing Terraform state at this point, which is useful for users who want to upgrade their previous Terraform workflow.</p> <p>Info</p> <p>If you're using Spacelift to manage your stack, do not specify any Terraform backend whatsoever. The one-off config will be dynamically injected into every run and task.</p>"},{"location":"vendors/terraform/state-management.html#do-or-do-not-there-is-no-try","title":"Do. Or do not. There is no try.","text":"<p>In this section we'd like to give you a few reasons why it could be useful to trust Spacelift to take care of your Terraform state. To keep things level, we'll also give you a reason not to.</p>"},{"location":"vendors/terraform/state-management.html#do","title":"Do","text":"<ol> <li> <p>It's super simple - just two clicks during stack setup. Otherwise there's nothing to set up on your end, so one fewer sensitive thing to worry about. Feel free to refer to how it works on our end, but overall we believe it to be a rather sensible and secure setup, at least on par with anything you could set up on your end.</p> </li> <li> <p>It's protected against accidental or malicious access. Again, you can refer to the more technical section on the inner workings of the state server, but the gist is that we're able to map state access and state changes to legitimate Spacelift runs, thus automatically blocking all other unauthorized traffic. As far as we know, no other backend is capable of that, which is one more reason to give us a go.</p> </li> </ol>"},{"location":"vendors/terraform/state-management.html#dont","title":"Don't","text":"<ol> <li>We'll let you in on a little secret now - behind the pixie dust it's still Amazon S3 all the way down, and at this stage we store all our data in Ireland. If you're not OK with that, you're better off managing the state on your end.</li> </ol>"},{"location":"vendors/terraform/state-management.html#how-it-works","title":"How it works","text":"<p>S3, like half of the Internet. The pixie dust we're adding on top of it involves generating one-off credentials for every run and task and injecting them directly into the root of your Terraform project as a <code>.tf</code> file.</p> <p>Warning</p> <p>If you have some Terraform state backend already specified in your code, the initialization phase will keep failing until you remove it.</p> <p>The state server is an HTTP endpoint implementing the Terraform standard state management protocol. Our backend always ensures that the credentials belong to one of the runs or tasks that are currently marked as active on our end, and their state indicates that they should be accessing or modifying the state. Once this is established, we just pass the request to S3 with the right parameters.</p>"},{"location":"vendors/terraform/state-management.html#state-history","title":"State history","text":"<p>If your state is managed by spacelift, you can list all the changes on your state, and eventually rollback to an old version if needed.</p> <p></p> <p>Info</p> <p>Not all runs or tasks will trigger a new state version, so you should not expect to see an exhaustive list of your runs and tasks in this list. For example runs that produce no Terraform changes do not result in a new state version being created.</p> <p>Non-current state versions are kept for 30 days.</p>"},{"location":"vendors/terraform/state-management.html#state-rollback","title":"State rollback","text":"<p>In certain unusual scenarios you can end up with a broken or corrupted state being created. This could happen for example if there was a bug during a Terraform provider upgrade.</p> <p>State rollback allows you to recover from this by rolling back your state to a previous version.</p> <p>Rolling back your state will not apply any changes to your current infrastructure. It just reverts your state to an older version. It's up to you to trigger the proper tasks or runs to fix the state and re-apply the desired Terraform configuration.</p> <p>Warning</p> <p>You should really understand what you are doing when performing a rollback.</p> <p>State rollback should be used as a break-glass operation just after a corrupted state has been created.</p> <p>If a stack is currently using a rolled-back state, a warning will be shown in the stack header.</p> <p></p> <p>To be able to roll back a state, the 3 conditions below must be satisfied:</p> <ul> <li>You must be a stack admin</li> <li>The stack must be locked</li> <li>The stack must not have any pending runs or tasks</li> </ul> <p>If those three conditions are met, you will be able to rollback your stack to a previous version of your state file.</p> <p></p> <p>After rollback completes succesfully, a new version of your state will appear above the other state versions and will be marked as a rollback.</p> <p></p>"},{"location":"vendors/terraform/state-management.html#importing-resources-into-your-terraform-state","title":"Importing resources into your Terraform State","text":"<p>So you have an existing resource that was created by other means and would like that resource to be reflected in your terraform state. This is an excellent use case for the terraform import command. When you're managing your own terraform state, you would typically run this command locally to import said resource(s) to your state file, but what do I do when I'm using Spacelift-managed state you might ask? Spacelift Task to the rescue!</p> <p>To do this, use the following steps:</p> <ul> <li>Select the Spacelift Stack to which you would like to import state for.</li> <li>Within the navigation, select \"Tasks\"</li> </ul> <p></p> <ul> <li>Run the <code>terraform import</code> command needed to import your state file to the Spacelift-managed state by typing the command into the text input and clicking the perform button. Note: If you are using Terragrunt on Spacelift, you will need to run <code>terragrunt import</code></li> </ul> <p></p> <ul> <li>Follow the status of your task's execution to ensure it was executed successfully. When completed, you should see an output similar to the following within the \"Performing\" step of your task.</li> </ul> <p></p>"},{"location":"vendors/terraform/state-management.html#importing-existing-state-file-into-your-terraform-stacks","title":"Importing existing state file into your Terraform Stacks","text":"<p>When creating a stack, you can optionally import an existing Terraform state file so that Spacelift can manage it going forward.</p> <p></p> <p>You can also import an existing Terraform state file when using Spacelift Terraform provider.</p> stack.tf<pre><code>resource \"spacelift_stack\" \"example-stack\" {\nname = \"Example Stack in Spacelift\"\n\n  # Source code.\nrepository        = \"&lt;Repository Name&gt;\"\nbranch            = \"main\"\n\n  # State file information\nimport_state      = \"&lt;State File to Upload&gt;\"\nimport_state_file = \"&lt;Path to the State file&gt;\"\n}\n</code></pre>"},{"location":"vendors/terraform/state-management.html#exporting-spacelift-managed-terraform-state-file","title":"Exporting Spacelift-managed Terraform state file","text":"<p>Info</p> <p>If you enable external state access, you can export the stack's state from outside of Spacelift.</p> <p>If a Terraform stack's state is managed by Spacelift and you need to export it you can do so by running the following command in a Task:</p> <pre><code>terraform state pull &gt; terraform.tfstate\n</code></pre> <p>The local workspace is discarded after the Task has finished so you most likely want to combine this command with another one that pushes the <code>terraform.tfstate</code> file to some remote location.</p> <p>Here is an example of pushing the state file to an AWS S3 bucket (without using an intermediary file):</p> <pre><code>terraform state pull | aws s3 cp - s3://example-bucket/folder/sub-folder/terraform.tfstate\n</code></pre>"},{"location":"vendors/terraform/storing-complex-variables.html","title":"Storing Complex Variables","text":"<p>Terraform supports a variety of variable types such as <code>string</code>, <code>number</code>, <code>list</code>, <code>bool</code>, and <code>map</code>. The full list of Terraform's variable types can be found in the Terraform documentation here.</p> <p>When using \"complex\" variable types with Spacelift such as <code>map</code> and <code>list</code> you'll need to utilize Terraform's jsonencode function when storing these variables as an environment variable in your Spacelift Stack environment or context.</p>"},{"location":"vendors/terraform/storing-complex-variables.html#usage-example","title":"Usage Example","text":"<pre><code>locals {\nmap = {\nfoo = \"bar\"\n}\nlist = [\"this\", \"is\", \"a\", \"list\"]\n}\n\nresource \"spacelift_context\" \"example\" {\ndescription = \"Example of storing complex variable types\"\nname        = \"Terraform Complex Variable Types Example\"\n}\n\nresource \"spacelift_environment_variable\" \"map_example\" {\ncontext_id = spacelift_context.example.id\nname       = \"map_example\"\nvalue      = jsonencode(local.map)\nwrite_only = false\n}\n\nresource \"spacelift_environment_variable\" \"list_example\" {\ncontext_id = spacelift_context.example.id\nname       = \"list_example\"\nvalue      = jsonencode(local.list)\nwrite_only = false\n}\n</code></pre> <p>Notice the use of the <code>jsonencode</code> function when storing these complex variable types. This will allow you to successfully store these variable types within Spacelift.</p> <p></p>"},{"location":"vendors/terraform/storing-complex-variables.html#consuming-stored-variables","title":"Consuming Stored Variables","text":"<p>When consuming complex variable types in your environment, there is no need to use the <code>jsondecode()</code> function.</p>"},{"location":"vendors/terraform/terraform-provider.html","title":"Provider","text":"<p>What would you say if you could manage Spacelift resources - that is stacks, contexts, integrations, and configuration - using Spacelift? We hate ClickOps as much as anyone, so we designed everything from the ground up to be easily managed using a Terraform provider. We hope that advanced users will define most of their resources programmatically.</p>"},{"location":"vendors/terraform/terraform-provider.html#self-hosted-version-compatibility","title":"Self-Hosted Version Compatibility","text":"<p>The Terraform provider uses our GraphQL API to manage Spacelift, and relies on certain features being available in the API in order to work. What this can sometimes mean is that a new feature is added to the Terraform provider which hasn't yet been made available in the GraphQL API for Self-Hosted versions of Spacelift.</p> <p>Because of this, it's not always possible to use the latest version of the Terraform provider with Self-Hosted, and we recommend that you pin to a known-compatible version. You can do this using a <code>required_providers</code> block like in the following example:</p> <pre><code>terraform {\nrequired_providers {\nspacelift = {\nsource = \"spacelift-io/spacelift\"\nversion = \"1.8.0\"\n}\n}\n}\n</code></pre> <p>The following table shows the latest version of the Terraform provider known to work with our Self-Hosted versions:</p> Self-Hosted Version Max Provider Version 0.0.12 1.8.0 0.0.11 1.8.0 0.0.10 1.4.0 0.0.9 1.3.1 0.0.8-hotfix.1 1.3.1 0.0.8 1.3.1"},{"location":"vendors/terraform/terraform-provider.html#taking-it-for-a-spin","title":"Taking it for a spin","text":"<p>Our Terraform provider is open source and its README always contains the latest available documentation. It's also distributed as part of our Docker runner image and available through our own provider registry. The purpose of this article isn't as much to document the provider itself but to show how it can be used to incorporate Spacelift resources into your infra-as-code.</p> <p>So, without further ado, let's define a stack:</p> stack.tf<pre><code>resource \"spacelift_stack\" \"managed-stack\" {\nname = \"Stack managed by Spacelift\"\n\n  # Source code.\nrepository = \"testing-spacelift\"\nbranch     = \"master\"\n}\n</code></pre> <p>That's awesome. But can we put Terraform to good use and integrate it with resources from a completely different provider? Sure we can, and we have a good excuse, too. Stacks accessibility can be managed by GitHub teams, so why don't we define some?</p> stack-and-teams.tf<pre><code>resource \"github_team\" \"stack-readers\" {\nname = \"managed-stack-readers\"\n}\n\nresource \"github_team\" \"stack-writers\" {\nname = \"managed-stack-writers\"\n}\n\nresource \"spacelift_stack\" \"managed-stack\" {\nname = \"Stack managed by Spacelift\"\n\n  # Source code.\nrepository = \"testing-spacelift\"\nbranch     = \"master\"\n\n}\n</code></pre> <p>Now that we programmatically combine Spacelift and GitHub resources, let's add AWS to the mix and give our new stack a dedicated IAM role:</p> stack-teams-and-iam.tf<pre><code>resource \"github_team\" \"stack-readers\" {\nname = \"managed-stack-readers\"\n}\n\nresource \"github_team\" \"stack-writers\" {\nname = \"managed-stack-writers\"\n}\n\nresource \"spacelift_stack\" \"managed-stack\" {\nname = \"Stack managed by Spacelift\"\n\n  # Source code.\nrepository = \"testing-spacelift\"\nbranch     = \"master\"\n\n}\n\n# IAM role.\nresource \"aws_iam_role\" \"managed-stack\" {\nname = \"spacelift-managed-stack\"\n\nassume_role_policy = jsonencode({\nVersion   = \"2012-10-17\"\nStatement = [\njsondecode(\nspacelift_stack.managed-stack.aws_assume_role_policy_statement\n)\n]\n})\n}\n\n# Attaching a nice, powerful policy to it.\nresource \"aws_iam_role_policy_attachment\" \"managed-stack\" {\nrole       = aws_iam_role.managed-stack.name\npolicy_arn = \"arn:aws:iam::aws:policy/PowerUserAccess\"\n}\n\n# Telling Spacelift stack to assume it.\nresource \"spacelift_stack_aws_role\" \"managed-stack\" {\nstack_id = spacelift_stack.managed-stack.id\nrole_arn = aws_iam_role.managed-stack.arn\n}\n</code></pre> <p>Success</p> <p>OK, so who wants to go back to clicking on things in the web GUI? Because you will likely need to do some clicking, too, at least with your first stack.</p>"},{"location":"vendors/terraform/terraform-provider.html#how-it-works","title":"How it works","text":"<p>Depending on whether you're using Terraform 0.12.x or higher, the Spacelift provider is distributed slightly differently. For 0.12.x users, the provider is distributed as a binary available in the runner Docker image in the same folder we put the Terraform binary. If you're using Terraform 0.13 and above, you can benefit from pulling our provider directly from our own provider registry. In order to do that, just point Terraform to the right location:</p> <pre><code>provider \"spacelift\" {}\n\nterraform {\nrequired_providers {\nspacelift = {\nsource = \"spacelift-io/spacelift\"\n}\n}\n}\n</code></pre>"},{"location":"vendors/terraform/terraform-provider.html#using-inside-spacelift","title":"Using inside Spacelift","text":"<p>Within Spacelift, the provider is configured by an environment variable <code>SPACELIFT_API_TOKEN</code> injected into each run and task belonging to stacks marked as administrative. This value is a bearer token that contains all the details necessary for the provider to work, including the full address of the API endpoint to talk to. It's technically valid for 3 hours but only when the run responsible for generating it is in Planning, Applying or Performing (for tasks) state and throughout that time it provides full administrative access to Spacelift entities that can be managed by Terraform within the same Spacelift account.</p>"},{"location":"vendors/terraform/terraform-provider.html#using-outside-of-spacelift","title":"Using outside of Spacelift","text":"<p>If you want to run the Spacelift provider outside of Spacelift, or you need to manage resources across multiple Spacelift accounts from the same Terraform project, the preferred method is to generate and use dedicated API keys. Note that unless you're just accessing whitelisted data resources, the Terraform use case will normally require marking the API key as administrative.</p> <p>In order to set up the provider to use an API key, you will need the key ID, secret, and the API key endpoint:</p> <pre><code>variable \"spacelift_key_id\" {}\nvariable \"spacelift_key_secret\" {}\n\nprovider \"spacelift\" {\napi_key_endpoint = \"https://your-account.app.spacelift.io\"\napi_key_id       = var.spacelift_key_id\napi_key_secret   = var.spacelift_key_secret\n}\n</code></pre> <p>These values can also be passed using environment variables, though this will only work to set up the provider for a single Spacelift account:</p> <ul> <li><code>SPACELIFT_API_KEY_ENDPOINT</code> for <code>api_key_endpoint</code>;</li> <li><code>SPACELIFT_API_KEY_ID</code> for <code>api_key_id</code>;</li> <li><code>SPACELIFT_API_KEY_SECRET</code> for <code>api_key_secret</code>;</li> </ul> <p>If you want to talk to multiple Spacelift accounts, you just need to set up provider aliases like this:</p> <pre><code>variable \"spacelift_first_key_id\" {}\nvariable \"spacelift_first_key_secret\" {}\n\nvariable \"spacelift_second_key_id\" {}\nvariable \"spacelift_second_key_secret\" {}\n\nprovider \"spacelift\" {\nalias = \"first\"\n\napi_key_endpoint = \"https://first.app.spacelift.io\"\napi_key_id       = var.spacelift_first_key_id\napi_key_secret   = var.spacelift_first_key_secret\n}\n\nprovider \"spacelift\" {\nalias = \"second\"\n\napi_key_endpoint = \"https://second.app.spacelift.io\"\napi_key_id       = var.spacelift_second_key_id\napi_key_secret   = var.spacelift_second_key_secret\n}\n</code></pre> <p>If you're running from inside Spacelift, you can still use the default, zero-setup provider for the current account with providers for accounts set up through API keys:</p> <pre><code>variable \"spacelift_that_key_id\" {}\nvariable \"spacelift_that_key_secret\" {}\n\nprovider \"spacelift\" {\nalias = \"this\"\n}\n\nprovider \"spacelift\" {\nalias = \"that\"\n\napi_key_endpoint = \"https://that.app.spacelift.io\"\napi_key_id       = var.spacelift_that_key_id\napi_key_secret   = var.spacelift_that_key_secret\n}\n</code></pre>"},{"location":"vendors/terraform/terraform-provider.html#proposed-workflow","title":"Proposed workflow","text":"<p>We suggest to first manually create a single administrative stack, and then use it to programmatically define other stacks as necessary. If you're using an integration like AWS, you should probably give the role associated with this stack full IAM access too, allowing it to create separate roles and policies for individual stacks.</p> <p>If you want to share data or outputs between stacks, please consider programmatically creating Stack Dependencies.</p> <p>Info</p> <p>Programmatically generated stacks can still be manually augmented, for example by setting extra elements of the environment. Thanks to the magic of Terraform, these will simply be invisible to (and thus not disturbed by) your resource definitions.</p>"},{"location":"vendors/terraform/terraform-provider.html#boundaries-of-programmatic-management","title":"Boundaries of programmatic management","text":"<p>Spacelift administrative tokens are not like user tokens. Specifically, they allow access to a much smaller subset of the API. They allow managing the lifecycles of stacks, contexts, integrations, and configuration, but they won't allow you to create or even access Terraform state, runs or tasks, or their associated logs.</p> <p>Administrative tokens have no superpowers either. They can't read write-only configuration elements any more than you can as a user. Unlike human users with user tokens, administrative tokens won't allow you to run <code>env</code> in a task and read back the logs.</p> <p>In general, we believe that things like runs or tasks do not fit the (relatively static) Terraform resource lifecycle model and that hiding those parts of the API from Terraform helps us ensure the integrity of potentially sensitive data - just see the example above.</p>"},{"location":"vendors/terraform/terragrunt.html","title":"Terragrunt","text":"<p>Info</p> <p>We have recently released a new Terragrunt native platform in Spacelift and it is currently in beta. You can find documentation on this here.</p>"},{"location":"vendors/terraform/terragrunt.html#using-terragrunt","title":"Using Terragrunt","text":"<p>Whether a Terraform stack is using Terragrunt or not is controlled by the presence of <code>terragrunt</code> label on the stack:</p> <p></p> <p>If present, all workloads will use <code>terragrunt</code> instead of <code>terraform</code> as the main command. Since the Terragrunt API is a superset of Terraform's, this is completely transparent to the end user.</p> <p>Terragrunt is installed on our standard runner image. If you're not using our runner image, you can install Terragrunt separately.</p> <p>During the Initialization phase we're showing you the exact binary that will process your job, along with its location:</p> <p></p>"},{"location":"vendors/terraform/terragrunt.html#versioning-with-terragrunt","title":"Versioning with Terragrunt","text":"<p>When working with Terragrunt, you will still specify the Terraform version to be used to process your job. We don't do it for Terragrunt, which is way more relaxed in terms of how it interacts with Terraform versions, especially since we're only using a very stable subset of its API.</p> <p>On our runner image, we install a version of Terragrunt that will work with the latest version of Terraform that we support. If you need a specific version of Terragrunt, feel free to create a custom runner image and install the Terragrunt version of your choosing.</p>"},{"location":"vendors/terraform/terragrunt.html#scope-of-support","title":"Scope of support","text":"<p>We're currently using Terragrunt the same way we're using Terraform, running <code>init</code>, <code>plan</code>, and <code>apply</code> commands. This means we're not supporting executing Terraform commands on multiple modules at once (<code>run-all</code>). This functionality was designed to operate in a very different mode and environment, and is strictly outside our scope. However, run-all is supported in our new Terragrunt native platform in Spacelift which is currently in beta. You can find documentation on this here.</p> <p>We also support authentication with Spacelift modules by automatically filling the <code>TG_TF_REGISTRY_TOKEN</code> environment variable for each run. Terragrunt uses this variable to authenticate with private module registries.</p>"},{"location":"vendors/terraform/terragrunt.html#debugging-terragrunt","title":"Debugging Terragrunt","text":"<p>Similar to Terraform, Terragrunt provides an advanced logging mode, and as of the writing of this documentation, there are currently two ways it can be enabled:</p> <ol> <li> <p>Using the <code>--terragrunt-log-level debug</code> CLI flag (You'll need to set this flag using the <code>TF_CLI_ARGS</code> environment variable. For example, <code>TF_CLI_ARGS=\"--terragrunt-log-level debug\"</code></p> </li> <li> <p>Using the <code>TERRAGRUNT_LOG_LEVEL</code> environment variable. Logging levels supported: <code>info</code> (default), <code>panic</code> <code>fatal</code> <code>error</code> <code>warn</code> <code>debug</code> <code>trace</code></p> </li> </ol> <p>Please refer to the Setting Environment Variables section within our Terraform Debugging Guide for more information on how to set these variables on your Spacelift Stack(s).</p> <p>For more information on logging with Terragrunt, please refer to the Terragrunt documentation.</p>"},{"location":"vendors/terraform/version-management.html","title":"Version management","text":""},{"location":"vendors/terraform/version-management.html#intro-to-terraform-versioning","title":"Intro to Terraform versioning","text":"<p>Terraform is an actively developed open-source product with a mighty sponsor. This means frequent releases - in fact, over the last few months (since minor version <code>0.12</code>) we've been seeing nearly weekly releases. While that's all great news for us as Terraform users, we need to be aware of how version management works in order not to be caught off-guard.</p> <p>Historically (until 0.15.x), once the state was written to (applied) with a higher version of Terraform, there was no way back. Hence, you had to be very careful when updating your current Terraform versions. If you're still using an older version of Terraform with Spacelift, you will likely want to use the runtime configuration to preview the intended changes before you make the jump.</p> <p>Currently (since version 0.15.x) the state format is stable so this is no longer a problem and you don't need to be that extra careful with Terraform versions.</p>"},{"location":"vendors/terraform/version-management.html#terraform-versions-in-spacelift","title":"Terraform versions in Spacelift","text":"<p>Terraform binaries are neither distributed with Spacelift nor with its runner Docker image. Instead, Spacelift dynamically detects the right version for each workflow (run or task), downloads the right binary on demand, verifies it, and mounts it as read-only on the runner Docker container as <code>/bin/terraform</code> to be directly available in the runner's <code>$PATH</code>:</p> <p></p> <p>There are two ways to tell Spacelift which Terraform version to use. The main one is to set the version directly on the stack. The version can be set in the Backend section of the stack configuration:</p> <p></p> <p>Note that you can either point to a specific version or define a version range, which is particularly useful if you don't want to update your code every time the Terraform version changes. The exact supported syntax options can be found here.</p> <p>The other way of specifying the Terraform version is to set it through runtime configuration. The runtime configuration is useful if you want to validate your Terraform code with a newer version of the binary before committing to it - which is especially important in older versions where the state format was not yet stable.</p> <p>If you're creating stacks programmatically but intend to make independent changes to the Terraform version (eg. using runtime configuration), we advise you to ignore any subsequent changes.</p> <p>In order to determine the version of the Terraform binary to mount, Spacelift will first look at the runtime configuration. If it does not contain the version setting for the current stack, the stack setting is then considered. If there is no version set on the current stack, the newest supported Terraform version is used. We always advise creating stacks with the newest available version of Terraform, though we realize it may not be the best option if the project is imported from elsewhere or incompatible providers are used.</p> <p>Warning</p> <p>The newest Terraform version supported by Spacelift may lag a bit behind the latest available Terraform release. We err on the side of caution and thus separately verify each version to ensure that it works as expected and that our code is compatible with its output and general behavior. We're trying to catch up roughly within a week but may temporarily blacklist a faulty version. If you need a compatibility check and a bump sooner than that, please get in touch with our support.</p> <p>Once we apply a run with a particular version of Terraform, we set it on the stack to make sure that we don't implicitly attempt to update it using a lower version.</p>"},{"location":"vendors/terraform/version-management.html#migrating-to-newer-versions","title":"Migrating to newer versions","text":"<p>In order to migrate a stack to a newer version of Terraform, we suggest opening a feature branch bumping the version through runtime configuration. Open a Pull Request in GitHub from the feature branch to the tracked branch to easily get a link to your proposed run and see if everything looks good. If it does, merge your Pull Request and enjoy working with the latest and greatest version of Terraform. Otherwise, try making necessary changes until your code is working or postpone the migration until you have the bandwidth to do so.</p> <p>Info</p> <p>In general, we suggest to try and keep up with the latest Terraform releases. The longer you wait, the more serious is the migration work going to be. Terraform evolves, providers evolve, external APIs evolve and so should your code.</p>"},{"location":"vendors/terraform/workflow-tool.html","title":"Workflow Tool","text":"<p>The Workflow Tool stack setting allows you to choose between three options:</p> <ul> <li>OpenTofu.</li> <li>Terraform (FOSS).</li> <li>Custom.</li> </ul> <p>The OpenTofu and Terraform (FOSS) options give you out of the box support for using OpenTofu and for open source versions of Terraform respectively. When you use either of those options, all you need to do is choose the version you want to use and you're good to go.</p> <p>The rest of this page explains the Custom option. This option allows you to customize the commands that are executed as part of Spacelift's Terraform workflow. This can be useful if you want to run a custom binary instead of one of the OpenTofu or Terraform versions supported out the box by Spacelift.</p> <p>Info</p> <p>Note that any custom binary is considered third-party software and you need to make sure you have the necessary rights (e.g. license) to use it.</p>"},{"location":"vendors/terraform/workflow-tool.html#how-does-it-work","title":"How does it work?","text":"<p>Each stage of the Terraform workflow uses certain commands to perform tasks such as generating a Terraform plan, or getting the current state. We provide a built-in set of commands to use for Terraform, but you can also specify your own custom commands. You do this via a file called <code>.spacelift/workflow.yml</code>.</p> <p>The following is an example of what the workflow commands look like for Terraform:</p> <pre><code># Used to initialize your root module.\ninit: terraform init -input=false\n\n# Used to select the correct workspace. Only used for Stacks that are using a custom state\n# backend, and which have the Workspace setting configured.\n#\n# Available template parameters:\n# - .WorkspaceName - contains the name of the workspace to select.\nworkspaceSelect: terraform workspace select \"{{ .WorkspaceName }}\"\n\n# Used to create a new workspace if none with the required name exists. Only used for Stacks\n# that are using a custom state backend, and which have the Workspace setting configured.\n#\n# Available template parameters:\n# - .WorkspaceName - contains the name of the workspace to select.\nworkspaceNew: terraform workspace new \"{{ .WorkspaceName }}\"\n\n# Used to generate a plan of the infrastructure changes that will be applied.\n#\n# Available template parameters:\n# - .Lock         - whether the state should be locked.\n# - .Refresh      - whether state resources should be refreshed.\n# - .PlanFileName - the name of the file to store the plan in.\n# - .Targets      - the list of targets to plan. Used during targeted replans.\nplan: terraform plan -input=false -lock={{ .Lock }} {{ if not .Refresh }}-refresh=false {{ end }}-out={{ .PlanFileName }} {{ range .Targets }}-target='{{ . }}' {{ end }}\n\n# Outputs the current state information as JSON.\nshowState: terraform show -json\n\n# Used to convert a plan file to its JSON representation.\n#\n# Available template parameters:\n# - .PlanFileName - the name of the file containing the plan.\nshowPlan: terraform show -json \"{{ .PlanFileName }}\"\n\n# Used to get the current outputs from the state.\ngetOutputs: terraform output -json\n\n# Used to apply any changes contained in the specified plan.\n#\n# Available template parameters:\n# - .PlanFileName - the name of the file containing the plan.\napply: terraform apply -auto-approve -input=false \"{{ .PlanFileName }}\"\n\n# Used to tear down any resources as part of deleting a stack.\ndestroy: terraform destroy -auto-approve -input=false\n</code></pre>"},{"location":"vendors/terraform/workflow-tool.html#how-to-configure-a-custom-tool","title":"How to configure a custom tool","text":"<p>To use a custom tool, three things are required:</p> <ol> <li>A way of providing the tool to your runs.</li> <li>A way of specifying the commands that should be executed.</li> <li>Indicating that you want to use a custom tool on your stack/module.</li> </ol>"},{"location":"vendors/terraform/workflow-tool.html#providing-a-tool","title":"Providing a tool","text":"<p>There are two main ways of providing a custom tool:</p> <ul> <li>Using a custom runner image.</li> <li>Using a before_init hook to download your custom tool.</li> </ul>"},{"location":"vendors/terraform/workflow-tool.html#specifying-the-commands","title":"Specifying the commands","text":"<p>Your custom workflow commands need to be provided in a <code>workflow.yml</code> file stored in the <code>.spacelift</code> folder at the root of your workspace. There are three main ways of providing this:</p> <ol> <li>Via a mounted file in the stack's environment.</li> <li>Via a mounted file stored in a context</li> <li>Directly via your Git repo.</li> </ol> <p>The option you choose will depend on your exact use-case, for example using the stack's environment allows you to quickly test out a new custom tool, using a context allows you to easily share the same configuration across multiple stacks, and storing the configuration in your Git repo allows you to track your settings along with the rest of your code.</p> <p>Here is an example configuration to use a fictional tool called <code>super-iac</code>:</p> <pre><code>init: super-iac init -input=false\nworkspaceSelect: super-iac workspace select \"{{ .WorkspaceName }}\"\nworkspaceNew: super-iac workspace new \"{{ .WorkspaceName }}\"\nplan: super-iac plan -input=false -lock={{ .Lock }} {{ if not .Refresh }}-refresh=false {{ end }}-out={{ .PlanFileName }} {{ range .Targets }}-target='{{ . }}' {{ end }}\nshowState: super-iac show -json\nshowPlan: super-iac show -json \"{{ .PlanFileName }}\"\ngetOutputs: super-iac output -json\napply: super-iac apply -auto-approve -input=false \"{{ .PlanFileName }}\"\ndestroy: super-iac destroy -auto-approve -input=false\n</code></pre>"},{"location":"vendors/terraform/workflow-tool.html#using-a-custom-tool-on-a-stackmodule","title":"Using a custom tool on a Stack/Module","text":"<p>To update your Stack/Module to use a custom workflow tool, edit the Workflow tool setting in the Backend settings:</p> <p></p> <p>When you choose the Custom option, the version selector will disappear:</p> <p></p> <p>This is because you are responsible for ensuring that the tool is available to the Run, and Spacelift will not automatically download the tool for you.</p>"},{"location":"vendors/terraform/workflow-tool.html#runtime-configuration","title":"Runtime Configuration","text":"<p>You can also use runtime configuration to configure the workflow tool for a stack or module. This allows you to try out a different configuration as part of a PR without adjusting your stack/module settings.</p> <p>The following example shows how to configure a particular stack to use a custom tool:</p> <pre><code>version: \"1\"\n\nstacks:\ncustom-workflow-stack:\nterraform_workflow_tool: \"CUSTOM\"\n</code></pre> <p>The following example shows how to do the same for a module test case:</p> <pre><code>version: 1\nmodule_version: 0.0.1\n\ntests:\n- name: Create a pet\nproject_root: examples/create-a-pet\nterraform_workflow_tool: \"CUSTOM\"\n</code></pre>"},{"location":"vendors/terraform/workflow-tool.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"vendors/terraform/workflow-tool.html#workflow-file-not-found","title":"Workflow file not found","text":"<p>If no workflow.yml file has been created but your Stack has been configured to use a custom tool, you may get an error message like the following:</p> <p></p> <p>If this happens, please ensure you have added a <code>.spacelift/workflow.yml</code> file to your Git repository, or attached it to your Stack's environment via a mounted file.</p>"},{"location":"vendors/terraform/workflow-tool.html#commands-missing","title":"Commands missing","text":"<p>If your <code>.spacelift/workflow.yml</code> does not contain all the required command definitions, or if any commands are empty, you will get an error message like the following:</p> <p></p> <p>This check is designed as a protection mechanism in case new commands are added but your workflow hasn't been updated. In this case, please provide an implementation for the specified commands (in the example the <code>init</code> and <code>workspaceSelect</code> commands),</p>"},{"location":"vendors/terraform/workflow-tool.html#tool-not-found","title":"Tool not found","text":"<p>If your custom tool binary cannot be found you will get an error message like the following:</p> <p></p> <p>In this situation, please ensure that you are providing a custom workflow tool via a custom runner image or workflow hook.</p>"},{"location":"vendors/terragrunt/index.html","title":"Terragrunt","text":"<p>Warning</p> <p>Terragrunt support is currently in beta and has some important limitations to take into consideration. Please see our documentation here for more information.</p>"},{"location":"vendors/terragrunt/index.html#why-use-terragrunt","title":"Why use Terragrunt?","text":"<p>Terragrunt serves as a valuable companion to Terraform, functioning as a thin wrapper that offers a suite of additional tools, ultimately enhancing the management and deployment of your infrastructure configurations.</p>"},{"location":"vendors/terragrunt/index.html#improved-management-of-configurations","title":"Improved Management of Configurations","text":"<p>Working with complex Terraform configurations often becomes challenging. Terragrunt intervenes here by offering a structured approach to managing these configurations. It also provides you with a mechanism to manage dependencies between various infrastructure components effectively.</p>"},{"location":"vendors/terragrunt/index.html#efficient-handling-of-remote-state","title":"Efficient Handling of Remote State","text":"<p>In an infrastructure setup, managing remote state and its associated configurations is critical. Terragrunt brings in utilities that can handle remote state locking and configuration adeptly, effectively reducing the chances of race conditions and configuration errors.</p>"},{"location":"vendors/terragrunt/index.html#enhanced-reusability-and-maintainability","title":"Enhanced Reusability and Maintainability","text":"<p>If you're working with Infrastructure-as-Code (IaC) across multiple environments, Terragrunt's capability to reuse configurations can be a game-changer. This not only simplifies the management process but also makes the infrastructure deployments more maintainable.</p> <p>All these features combine to offer enhanced efficiency and reliability in your infrastructure deployments when using Terragrunt.</p>"},{"location":"vendors/terragrunt/index.html#why-use-spacelift-with-terragrunt","title":"Why use Spacelift with Terragrunt?","text":"<p>Integrating Spacelift with Terragrunt yields a robust and streamlined solution for Infrastructure-as-Code (IaC) management and deployment. Spacelift's platform is designed for IaC and therefore greatly complements Terragrunt's capabilities.</p> <p>Firstly, Spacelift's automation capabilities can bring significant benefits when paired with Terragrunt. You can automate and streamline the deployment of your complex Terraform configurations managed by Terragrunt, making your infrastructure management efficient and seamless.</p> <p>Spacelift provides a centralized platform for managing your IaC setup. Combined with Terragrunt's ability to effectively handle complex Terraform configurations and dependencies, you end up with a well-organized and optimized IaC environment that is easier to navigate and manage.</p> <p>Lastly, Spacelift is designed to integrate smoothly with your existing tech stack. Its compatibility with Terragrunt means that it can fit into your existing workflow without requiring substantial changes, ensuring a smooth transition and consistent operations.</p>"},{"location":"vendors/terragrunt/index.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Getting Started</li> <li>Run-all</li> <li>Limitations</li> <li>Reference</li> </ul>"},{"location":"vendors/terragrunt/getting-started.html","title":"Getting Started","text":"<p>Warning</p> <p>Terragrunt support is currently in beta and has some important limitations to take into consideration. Please see our documentation here for more information.</p> <p>This documentation will be using an example repository Spacelift provides here. This repository contains 2 different examples, but for getting started we will be using the single-stack example.</p>"},{"location":"vendors/terragrunt/getting-started.html#creating-a-new-stack","title":"Creating a new stack","text":"<p>In Spacelift, go ahead and click the Add Stack button to create a new Stack.</p>"},{"location":"vendors/terragrunt/getting-started.html#naming-the-stack","title":"Naming the stack","text":"<p>Once in the stack creation wizard, Give your stack a name such as terragrunt-example then press Continue to configure our VCS settings for the stack.</p> <p></p>"},{"location":"vendors/terragrunt/getting-started.html#linking-to-the-terragrunt-code","title":"Linking to the Terragrunt code","text":"<p>On this VCS configuration screen, Select Raw Git as your Provider, and provide the following URL: https://github.com/spacelift-io-examples/terragrunt-examples.git and a Project Root of single-project.</p> <p></p> <p>Pressing next on this page will take you through to the backend configuration page.</p> <p>This page has quite a few options but the ones that we will be using for this example are as follows:</p> <p></p> <p>Press Continue to finish the backend configuration and move through to defining stack behavior. For this example nothing needs to be edited here so be sure to hit Save Stack to complete the process.</p>"},{"location":"vendors/terragrunt/getting-started.html#deploying-the-stack","title":"Deploying the stack","text":"<p>Spacelift will take you to the Runs view for the Stack you've just created. Once on this page, press the Trigger button to trigger a new Run.</p> <p></p> <p>You should now be taken through the process of deploying your stack. This takes you through multiple stages of a run, moving through initialization, planning and ending up in an unconfirmed state.</p>"},{"location":"vendors/terragrunt/getting-started.html#examining-the-planned-changes","title":"Examining the planned changes","text":"<p>In this unconfirmed state we have time to review what is going to change if we were to continue and confirm and apply.  By pressing the <code>changes +2</code> button at the top of the page, we are taken to an overview of the planned changes.</p> <p></p> <p>For this stack we should see that 2 resources are being created and we can see what values these resources and outputs are expected to have. This is a very useful view to see at a glance, what is going to happen in as a result of your deployment.</p>"},{"location":"vendors/terragrunt/getting-started.html#confirming-and-applying-the-changes","title":"Confirming and applying the changes","text":"<p>Info</p> <p>Due to security reasons and limitations of Terragrunt and its mocked outputs, Spacelift does not use the planfile in the applying as there is a possibility to deploy mock values.</p> <p>Once you're happy with the changes, press the Confirm button to allow the run to continue and begin the applying phase of the run.</p>"},{"location":"vendors/terragrunt/getting-started.html#conclusion","title":"Conclusion","text":"<p>You have now successfully deployed a Terragrunt stack using Spacelift! Congratulations.</p> <p>For further reading, we recommend looking into using the <code>Run all</code> setting to enable you to deploy multiple projects using <code>terragrunt run-all</code> by reading our documentation here. We also recommend that if you are looking into using Terragrunt with Spacelift that you read our page on the Limitations of Terragrunt.</p>"},{"location":"vendors/terragrunt/limitations.html","title":"Limitations","text":""},{"location":"vendors/terragrunt/limitations.html#state-management","title":"State management","text":"<p>Terragrunt is a great tool for organizing your state management configuration and allows you to easily define how you manage your state across multiple projects. However, It is not currently possible when using Terragrunt's run-all functionality to relate state files to projects in a consistent manner. For this reason Spacelift does not support storing state for Terragrunt based stacks, and you will need to maintain your own remote state backend configuration.</p>"},{"location":"vendors/terragrunt/limitations.html#terragrunt-mocked-outputs","title":"Terragrunt mocked outputs","text":"<p>Mocked outputs in Terragrunt are placeholder values used during the development or planning phases of Terragrunt deployments.</p> <p>For example, you may have a module that provides the output of a connection string to a database, but during the planning phase that database does not yet exist. In this case you would use a mocked output in Terragrunt to ensure that any dependencies that rely on this output in their planning phase have access to at least some data.</p> <p>This mocked data is only used if the output does not already exist in the state. Therefore in situations such as the initial run of your stack, or the introduction of new outputs with mocked values, these mocked values will be used.</p>"},{"location":"vendors/terragrunt/limitations.html#mocked-outputs-and-plan-policies","title":"Mocked outputs and Plan policies","text":"<p>Due to the nature of the mocked outputs and the way that Spacelift uses the plan data to provide the input to plan policies, it is possible that these mocked output values could be used as input values for your plan policies and you should take precaution when writing policies that check against values that could be mocked.</p>"},{"location":"vendors/terragrunt/limitations.html#mocked-outputs-and-the-apply-phase","title":"Mocked outputs and the Apply phase","text":"<p>Terragrunt consumes the mocked outputs and places those values within the plan file that is stored on disk as part of the planning phase. Because the plan file has the possibility of containing mocked outputs Spacelift does not use the plan files in the apply phase. This does mean there is a possibility of changes happening between the planning and applying phase, but Spacelift has taken the stance that it is more important from a security standpoint to not allow any mocked outputs to be deployed here. Nobody wants to deploy something with a mocked, hardcoded password!</p>"},{"location":"vendors/terragrunt/reference.html","title":"Reference","text":""},{"location":"vendors/terragrunt/reference.html#stack-settings","title":"Stack Settings","text":"<ul> <li>Use Run All - If Spacelift should use Terragrunt's run-all functionality in this stack.</li> <li>Smart Sanitization - If Spacelift should use Smart Sanitization for the resources and outputs generated by this stack.</li> <li>Terragrunt Version - Which version or range of versions of Terragrunt should be used</li> <li>Terraform Version - Which version or range of versions of Terraform should be used</li> </ul> <p>Info</p> <p>Spacelift makes use of the Terraform Version Compatibility Table to determine which version combinations of Terraform and Terragrunt are compatible with each other.</p>"},{"location":"vendors/terragrunt/reference.html#environment-variables","title":"Environment Variables","text":"<p>In Terragrunt stacks you can use environment variables to add options to the command line arguments sent to Terragrunt. To use these environment variables, you can see the documentation here.</p> <p>There is an environment variable for each phase of the run:</p> <ul> <li><code>TG_CLI_ARGS_init</code> - Allows you to add options sent to the Terragrunt application during the initalization phase</li> <li><code>TG_CLI_ARGS_plan</code> - Allows you to add options sent to the Terragrunt application during the planning phase</li> <li><code>TG_CLI_ARGS_apply</code> - Allows you to add options sent to the Terragrunt application during the applying phase</li> </ul>"},{"location":"vendors/terragrunt/run-all.html","title":"Using Run-All","text":""},{"location":"vendors/terragrunt/run-all.html#introduction","title":"Introduction","text":"<p>Terragrunt's run-all feature allows you to deploy multiple projects and modules and this very powerful feature allows you to develop large amounts of infrastructure as code with dependencies, mocked values, and keeping things DRY.</p>"},{"location":"vendors/terragrunt/run-all.html#how-to-use-run-all","title":"How to use run-all","text":"<p>When creating a stack in Spacelift, you can enable the Use Run All property to start using this functionality natively in your own stacks!</p> <p></p> <p>When this option is enabled, Spacelift will run all Terragrunt commands with the <code>run-all</code> option, taking into account any dependencies between your projects!</p>"},{"location":"vendors/terragrunt/run-all.html#why-are-my-resources-named-strangely","title":"Why are my resources named strangely?","text":"<p> Due to the way Terragrunt works, you may find that modules and projects create outputs or resources with the same names, and we want to make sure that you can determine the origin of all of your resources without getting confused about \"Which project created <code>output.password</code>\"?</p> <p>For this reason, we prepend the addresses of your outputs and resources in the Spacelift user interface with the project they originated from. This allows you to keep on top of your resources, understand at a glance how things were created, and identify issues faster, Giving you more control and visibility over the resources you create!</p>"}]}