{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"\ud83d\udc4b Hello, Spacelift! \u00bb Spacelift is a sophisticated, continuous integration and deployment (CI/CD) platform for infrastructure-as-code, currently supporting Terraform , Pulumi , AWS CloudFormation , Kubernetes , and Ansible . It's designed and implemented by long-time DevOps practitioners based on previous experience with large-scale installations - dozens of teams, hundreds of engineers, and tens of thousands of cloud resources. At the same time, Spacelift is super easy to get started with - you can go from zero to fully managing your cloud resources within less than a minute, with no pre-requisites. It integrates nicely with the large players in the field - notably GitHub and AWS . If you're new to Spacelift, please spend some time browsing through the articles in the same order as they appear in the menu - start with the main concepts and follow with integrations. If you're more advanced, you can navigate directly to the article you need, or use the search feature to find a specific piece of information. If you still have questions, feel free to reach out to us. Do I need another CI/CD for my infrastructure? \u00bb Yes, we believe it's a good idea. While in an ideal world one CI system would be enough to cover all use cases, we don't live in an ideal world. Regular CI tools can get you started easily, but Terraform has a rather unusual execution model and a highly stateful nature. Also, mind the massive blast radius when things go wrong. We believe Spacelift offers a perfect blend of regular CI's versatility and methodological rigor of a specialized, security-conscious infrastructure tool - enough to give it a shot even if you're currently happy with your infra-as-code CI/CD setup. In the following sections, we'll try to present the main challenges of running Terraform in a general purpose CI system, as well as show how Spacelift addresses those. At the end of the day, it's mostly about two things - collaboration and security. Collaboration \u00bb Wait, aren't CIs built for collaboration? Yes, assuming stateless tools and processes. Running stateless builds and tests is what regular CIs are exceptionally good at. But many of us have noticed that deployments are actually trickier to get right. And that's hardly a surprise. They're more stateful, they may depend on what's already running. Terraform and your infrastructure, in general, is an extreme example of a stateful system . It's so stateful that it actually has something called state (see what we just did there?) as one of its core concepts. CIs generally struggle with that. They don't really understand the workflows they run, so they can't for example serialize certain types of jobs. Like terraform apply , which introduces actual changes to your infrastructure. As far as your CI system is concerned, running those in parallel is fair game. But what it does to Terraform is nothing short of a disaster - your state is confused and no longer represents any kind of reality. Untangling this mess can take forever. But you can add manual approval steps Yes, you can. But the whole point of your CI/CD system is to automate your work. First of all, becoming a human semaphore for a software tool isn't the best use of a highly skilled and motivated professional. Also, over-reliance on humans to oversee software processes will inevitably lead to costly mistakes because we, humans, are infinitely more fallible than well-programmed machines. It's ultimately much cheaper to use the right tool for the job than turn yourself into a part of a tool. But you can do state locking ! Yup, we hear you. In theory, it's a great feature. In practice, it has its limitations. First, it's a massive pain when working as a team. Your CI won't serialize jobs that can write state, and state locking means that all but one of the parallel jobs will simply fail. It's a safe default, that's for sure, but not a great developer experience. And the more people work on your infrastructure, the more frustrating the process will become. And that's just applying changes. By default, running terraform plan locks the state, too. So you can't really run multiple CI jobs in parallel, even if they're only meant to preview changes, because each of them will attempt to lock the state. Yes, you can work around this by explicitly not locking state in CI jobs that you know won't make any state changes, but at this point, you've already put so much work into creating a pipeline that's fragile at best and requires you to manually synchronize it. And we haven't even discussed security yet. Security \u00bb Terraform is used to manage infrastructure, which normally requires credentials. Usually, very powerful credentials. Administrative credentials, sometimes. And these can do a lot of damage . The thing with CIs is that you need to provide those credentials statically, and once you do, there's no way you can control how they're used. And that's what makes CIs powerful - after all, they let you run arbitrary code, normally based on some configuration file that you have checked in with your Terraform code. So, what's exactly stopping a prankster from adding terraform destroy -auto-approve as an extra CI step? Or printing out those credentials and using them to mine their crypto of choice? There are better ways to get fired. ...you'll say and we hear you. Those jobs are audited after all. No, if we were disgruntled employees we'd never do something as stupid. We'd get an SSH session and leak those precious credentials this way. Since it's unlikely you rotate them every day, we'd take our sweet time before using them for our nefarious purposes. Which wouldn't be possible with Spacelift BTW, which generates one-off temporary credentials for major cloud providers. But nobody does that! Yes, you don't hear many of those stories. Most mistakes happen to well-meaning people. But in the world of infrastructure, even the tiniest of mistakes can cause major outages - like that typo we once made in our DNS config. That's why Spacelift adds an extra layer of policy that allows you to control - separately from your infrastructure project! - what code can be executed , what changes can be made , when and by whom . This isn't only useful to protect yourself from the baddies, but allows you to implement an automated code review pipeline .","title":"\ud83d\udc4b Hello, Spacelift!"},{"location":"index.html#hello-spacelift","text":"Spacelift is a sophisticated, continuous integration and deployment (CI/CD) platform for infrastructure-as-code, currently supporting Terraform , Pulumi , AWS CloudFormation , Kubernetes , and Ansible . It's designed and implemented by long-time DevOps practitioners based on previous experience with large-scale installations - dozens of teams, hundreds of engineers, and tens of thousands of cloud resources. At the same time, Spacelift is super easy to get started with - you can go from zero to fully managing your cloud resources within less than a minute, with no pre-requisites. It integrates nicely with the large players in the field - notably GitHub and AWS . If you're new to Spacelift, please spend some time browsing through the articles in the same order as they appear in the menu - start with the main concepts and follow with integrations. If you're more advanced, you can navigate directly to the article you need, or use the search feature to find a specific piece of information. If you still have questions, feel free to reach out to us.","title":"\ud83d\udc4b Hello, Spacelift!"},{"location":"index.html#do-i-need-another-cicd-for-my-infrastructure","text":"Yes, we believe it's a good idea. While in an ideal world one CI system would be enough to cover all use cases, we don't live in an ideal world. Regular CI tools can get you started easily, but Terraform has a rather unusual execution model and a highly stateful nature. Also, mind the massive blast radius when things go wrong. We believe Spacelift offers a perfect blend of regular CI's versatility and methodological rigor of a specialized, security-conscious infrastructure tool - enough to give it a shot even if you're currently happy with your infra-as-code CI/CD setup. In the following sections, we'll try to present the main challenges of running Terraform in a general purpose CI system, as well as show how Spacelift addresses those. At the end of the day, it's mostly about two things - collaboration and security.","title":"Do I need another CI/CD for my infrastructure?"},{"location":"index.html#collaboration","text":"Wait, aren't CIs built for collaboration? Yes, assuming stateless tools and processes. Running stateless builds and tests is what regular CIs are exceptionally good at. But many of us have noticed that deployments are actually trickier to get right. And that's hardly a surprise. They're more stateful, they may depend on what's already running. Terraform and your infrastructure, in general, is an extreme example of a stateful system . It's so stateful that it actually has something called state (see what we just did there?) as one of its core concepts. CIs generally struggle with that. They don't really understand the workflows they run, so they can't for example serialize certain types of jobs. Like terraform apply , which introduces actual changes to your infrastructure. As far as your CI system is concerned, running those in parallel is fair game. But what it does to Terraform is nothing short of a disaster - your state is confused and no longer represents any kind of reality. Untangling this mess can take forever. But you can add manual approval steps Yes, you can. But the whole point of your CI/CD system is to automate your work. First of all, becoming a human semaphore for a software tool isn't the best use of a highly skilled and motivated professional. Also, over-reliance on humans to oversee software processes will inevitably lead to costly mistakes because we, humans, are infinitely more fallible than well-programmed machines. It's ultimately much cheaper to use the right tool for the job than turn yourself into a part of a tool. But you can do state locking ! Yup, we hear you. In theory, it's a great feature. In practice, it has its limitations. First, it's a massive pain when working as a team. Your CI won't serialize jobs that can write state, and state locking means that all but one of the parallel jobs will simply fail. It's a safe default, that's for sure, but not a great developer experience. And the more people work on your infrastructure, the more frustrating the process will become. And that's just applying changes. By default, running terraform plan locks the state, too. So you can't really run multiple CI jobs in parallel, even if they're only meant to preview changes, because each of them will attempt to lock the state. Yes, you can work around this by explicitly not locking state in CI jobs that you know won't make any state changes, but at this point, you've already put so much work into creating a pipeline that's fragile at best and requires you to manually synchronize it. And we haven't even discussed security yet.","title":"Collaboration"},{"location":"index.html#security","text":"Terraform is used to manage infrastructure, which normally requires credentials. Usually, very powerful credentials. Administrative credentials, sometimes. And these can do a lot of damage . The thing with CIs is that you need to provide those credentials statically, and once you do, there's no way you can control how they're used. And that's what makes CIs powerful - after all, they let you run arbitrary code, normally based on some configuration file that you have checked in with your Terraform code. So, what's exactly stopping a prankster from adding terraform destroy -auto-approve as an extra CI step? Or printing out those credentials and using them to mine their crypto of choice? There are better ways to get fired. ...you'll say and we hear you. Those jobs are audited after all. No, if we were disgruntled employees we'd never do something as stupid. We'd get an SSH session and leak those precious credentials this way. Since it's unlikely you rotate them every day, we'd take our sweet time before using them for our nefarious purposes. Which wouldn't be possible with Spacelift BTW, which generates one-off temporary credentials for major cloud providers. But nobody does that! Yes, you don't hear many of those stories. Most mistakes happen to well-meaning people. But in the world of infrastructure, even the tiniest of mistakes can cause major outages - like that typo we once made in our DNS config. That's why Spacelift adds an extra layer of policy that allows you to control - separately from your infrastructure project! - what code can be executed , what changes can be made , when and by whom . This isn't only useful to protect yourself from the baddies, but allows you to implement an automated code review pipeline .","title":"Security"},{"location":"getting-started.html","text":"\ud83d\ude80 Getting Started \u00bb Hello and welcome to Spacelift! In this guide we will briefly introduce some key concepts that you need to know to work with Spacelift. These concepts will be followed by detailed instructions to help you create and configure your first run with Spacelift. Introduction to Main Concepts \u00bb Stacks \u00bb A stack is a central entity in Spacelift. It connects with your source control repository and manages the state of infrastructure. It facilitates integration with cloud providers (AWS, Azure, Google Cloud) and other important Spacelift components. You can learn more about Stacks in Spacelift detailed documentation . State Management \u00bb State can be managed by your backend, or for Terraform projects - can be imported into Spacelift. It is not required to let Spacelift manage your infrastructure state. Worker Pools \u00bb The underlying compute used by Spacelift is called a worker, and workers are managed in groups known as worker pools. In order for Spacelift to operate correctly, you will need to provision at least one worker pool. You can learn more about worker pools here . Policies \u00bb Spacelift policies provide a way to express rules as code, rules that manage your Infrastructure as Code (IaC) environment, and help make common decisions such as login, access, and execution. Policies are based on the Open Policy Agent project and can be defined using its rule language Rego . You can learn more about policies here . Cloud Integration \u00bb Spacelift provides native integration with AWS. Integration with other cloud providers is also possible via OIDC Federation or programmatic connection with their identity services. You can learn more about cloud provider integration in Spacelift detailed documentation. Change Workflow \u00bb Spacelift deeply integrates with your Version Control System (VCS). Pull requests are evaluated by Spacelift to provide a preview of the changes being made to infrastructure; these changes are deployed automatically when PRs are merged. You can learn more about VCS integration here. Step by Step \u00bb This section provides step-by-step instructions to help you set up and get the most out of Spacelift. If you want to learn about core concepts, please have a look at the main concepts section. First Stack Run \u00bb You can get started with either forking our Terraform Starter repository and testing all Spacelift capabilities in under 15 minutes or you can explore Spacelift on your own by adding your own repository and going from zero to fully managing your cloud resources. Step 1: Install Spacelift \u00bb Follow the install guide to get Spacelift up and running. Step 2: Connect your Version Control System (VCS) \u00bb In this section we will be connecting GitHub as our VCS. You can find more information about other supported VCS providers here . To connect GitHub as your VCS, follow the guide for setting up the GitHub integration . Please select any of your GitHub repositories that create local resources (we will not be integrating with any cloud providers to keep this guide simple and quick). If you do not have a GitHub repository of this kind, you can fork our terraform-starter repository (Make sure to allow the installed GitHub app access to the forked repository). Please refer to the Source Control section of the documentation to connect a different VCS. Step 3: Create Your First Spacelift Stack \u00bb Click on the Add Stack button. In the Integrate VCS tab, choose your VCS provider, select the repository that you gave access to Spacelift in the first step and select a branch that you want to be attached with your Stack. Click Continue. Click on Continue to configure the backend. Choose Terraform as your backend with a supported version. Leave the default option to let Spacelift manage state for this stack. Leave the default options checked for Define Behavior and click Continue . Give your stack a name and click Save Stack . Step 4: Trigger your First Run \u00bb Click on Trigger to kick start a Spacelift run that will check out the source code, run terraform commands on it and then present you with an option to apply (confirm) these changes. After clicking Trigger, you will be taken directly into the run. Click on Confirm and your changes will be applied. Your output will look different based on your code repository and the resources it creates. Congratulation! \ud83d\ude80 You've just created your first Spacelift stack and completed your first deployment! Now it is time to add other users to your Spacelift account. Adding Users \u00bb Now comes the moment when you want to show Spacelift to your colleagues. There are a few different ways to grant users access to your Spacelift account but in the beginning, we are going to add them as single users. Go to the \"Policies\" page that can be found on the left sidebar and click the \"Add policy\" button in the top-right corner. Name the policy and select \"Login policy\" as the type. Then copy/paste and edit the example below that matches the identity provider you used to sign up for the Spacelift account. GitHub GitLab, Google, Microsoft This example uses GitHub usernames to grant access to Spacelift. 1 2 3 4 5 6 7 8 9 package spacelift admins : = { \"alice\" } allowed : = { \"bob\" , \"charlie\" , \"danny\" } login : = input . session . login admin { admins [ login ] } allow { allowed [ login ] } deny { not admin ; not allow } Tip GitHub organization admins are automatically Spacelift admins. There is no need to grant them permissions in the Login policy. This example uses email addresses to grant access to Spacelift. 1 2 3 4 5 6 7 8 9 10 package spacelift admins := { \"alice@example.com\" } allowed := { \"bob@example.com\" } login := input.session.login admin { admins[login] } allow { allowed[login] } # allow { endswith(input.session.login, \"@example.com\") } Alternatively, grant access to every user with an @example.com email address deny { not admin; not allow } Now your colleagues should be able to access your Spacelift account as well! Note While the approach above is fine to get started and try Spacelift, granting access to individuals is less safe than granting access to teams and restricting access to account members. In the latter case, when they lose access to your organization, they automatically lose access to Spacelift while when whitelisting individuals and not restricting access to members only, you'll need to remember to explicitly remove them from your Spacelift Login policy, too. Before you go live in production with Spacelift, we recommend that you switch to using teams in Login policies and consider configuring the Single Sign-On (SSO) integration , if applicable. Additional Reading \u00bb Learn how to integrate with AWS as a cloud provider for your infrastructure Try creating and attaching policies with stacks for common use cases Spacelift workflow with GitHub change requests (PR) Setting up Spacelift workers Using environment variables and contexts with stacks Configuring stack behavior with common settings","title":"\ud83d\ude80 Getting Started"},{"location":"getting-started.html#getting-started","text":"Hello and welcome to Spacelift! In this guide we will briefly introduce some key concepts that you need to know to work with Spacelift. These concepts will be followed by detailed instructions to help you create and configure your first run with Spacelift.","title":"\ud83d\ude80 Getting Started"},{"location":"getting-started.html#introduction-to-main-concepts","text":"","title":"Introduction to Main Concepts"},{"location":"getting-started.html#stacks","text":"A stack is a central entity in Spacelift. It connects with your source control repository and manages the state of infrastructure. It facilitates integration with cloud providers (AWS, Azure, Google Cloud) and other important Spacelift components. You can learn more about Stacks in Spacelift detailed documentation .","title":"Stacks"},{"location":"getting-started.html#state-management","text":"State can be managed by your backend, or for Terraform projects - can be imported into Spacelift. It is not required to let Spacelift manage your infrastructure state.","title":"State Management"},{"location":"getting-started.html#worker-pools","text":"The underlying compute used by Spacelift is called a worker, and workers are managed in groups known as worker pools. In order for Spacelift to operate correctly, you will need to provision at least one worker pool. You can learn more about worker pools here .","title":"Worker Pools"},{"location":"getting-started.html#policies","text":"Spacelift policies provide a way to express rules as code, rules that manage your Infrastructure as Code (IaC) environment, and help make common decisions such as login, access, and execution. Policies are based on the Open Policy Agent project and can be defined using its rule language Rego . You can learn more about policies here .","title":"Policies"},{"location":"getting-started.html#cloud-integration","text":"Spacelift provides native integration with AWS. Integration with other cloud providers is also possible via OIDC Federation or programmatic connection with their identity services. You can learn more about cloud provider integration in Spacelift detailed documentation.","title":"Cloud Integration"},{"location":"getting-started.html#change-workflow","text":"Spacelift deeply integrates with your Version Control System (VCS). Pull requests are evaluated by Spacelift to provide a preview of the changes being made to infrastructure; these changes are deployed automatically when PRs are merged. You can learn more about VCS integration here.","title":"Change Workflow"},{"location":"getting-started.html#step-by-step","text":"This section provides step-by-step instructions to help you set up and get the most out of Spacelift. If you want to learn about core concepts, please have a look at the main concepts section.","title":"Step by Step"},{"location":"getting-started.html#first-stack-run","text":"You can get started with either forking our Terraform Starter repository and testing all Spacelift capabilities in under 15 minutes or you can explore Spacelift on your own by adding your own repository and going from zero to fully managing your cloud resources.","title":"First Stack Run"},{"location":"getting-started.html#step-1-install-spacelift","text":"Follow the install guide to get Spacelift up and running.","title":"Step 1: Install Spacelift"},{"location":"getting-started.html#step-2-connect-your-version-control-system-vcs","text":"In this section we will be connecting GitHub as our VCS. You can find more information about other supported VCS providers here . To connect GitHub as your VCS, follow the guide for setting up the GitHub integration . Please select any of your GitHub repositories that create local resources (we will not be integrating with any cloud providers to keep this guide simple and quick). If you do not have a GitHub repository of this kind, you can fork our terraform-starter repository (Make sure to allow the installed GitHub app access to the forked repository). Please refer to the Source Control section of the documentation to connect a different VCS.","title":"Step 2: Connect your Version Control System (VCS)"},{"location":"getting-started.html#step-3-create-your-first-spacelift-stack","text":"Click on the Add Stack button. In the Integrate VCS tab, choose your VCS provider, select the repository that you gave access to Spacelift in the first step and select a branch that you want to be attached with your Stack. Click Continue. Click on Continue to configure the backend. Choose Terraform as your backend with a supported version. Leave the default option to let Spacelift manage state for this stack. Leave the default options checked for Define Behavior and click Continue . Give your stack a name and click Save Stack .","title":"Step 3: Create Your First Spacelift Stack"},{"location":"getting-started.html#step-4-trigger-your-first-run","text":"Click on Trigger to kick start a Spacelift run that will check out the source code, run terraform commands on it and then present you with an option to apply (confirm) these changes. After clicking Trigger, you will be taken directly into the run. Click on Confirm and your changes will be applied. Your output will look different based on your code repository and the resources it creates. Congratulation! \ud83d\ude80 You've just created your first Spacelift stack and completed your first deployment! Now it is time to add other users to your Spacelift account.","title":"Step 4: Trigger your First Run"},{"location":"getting-started.html#adding-users","text":"Now comes the moment when you want to show Spacelift to your colleagues. There are a few different ways to grant users access to your Spacelift account but in the beginning, we are going to add them as single users. Go to the \"Policies\" page that can be found on the left sidebar and click the \"Add policy\" button in the top-right corner. Name the policy and select \"Login policy\" as the type. Then copy/paste and edit the example below that matches the identity provider you used to sign up for the Spacelift account. GitHub GitLab, Google, Microsoft This example uses GitHub usernames to grant access to Spacelift. 1 2 3 4 5 6 7 8 9 package spacelift admins : = { \"alice\" } allowed : = { \"bob\" , \"charlie\" , \"danny\" } login : = input . session . login admin { admins [ login ] } allow { allowed [ login ] } deny { not admin ; not allow } Tip GitHub organization admins are automatically Spacelift admins. There is no need to grant them permissions in the Login policy. This example uses email addresses to grant access to Spacelift. 1 2 3 4 5 6 7 8 9 10 package spacelift admins := { \"alice@example.com\" } allowed := { \"bob@example.com\" } login := input.session.login admin { admins[login] } allow { allowed[login] } # allow { endswith(input.session.login, \"@example.com\") } Alternatively, grant access to every user with an @example.com email address deny { not admin; not allow } Now your colleagues should be able to access your Spacelift account as well! Note While the approach above is fine to get started and try Spacelift, granting access to individuals is less safe than granting access to teams and restricting access to account members. In the latter case, when they lose access to your organization, they automatically lose access to Spacelift while when whitelisting individuals and not restricting access to members only, you'll need to remember to explicitly remove them from your Spacelift Login policy, too. Before you go live in production with Spacelift, we recommend that you switch to using teams in Login policies and consider configuring the Single Sign-On (SSO) integration , if applicable.","title":"Adding Users"},{"location":"getting-started.html#additional-reading","text":"Learn how to integrate with AWS as a cloud provider for your infrastructure Try creating and attaching policies with stacks for common use cases Spacelift workflow with GitHub change requests (PR) Setting up Spacelift workers Using environment variables and contexts with stacks Configuring stack behavior with common settings","title":"Additional Reading"},{"location":"concepts/resources.html","text":"Resources \u00bb One major benefit of specialized tools like Spacelift - as opposed to general-purpose CI/CD platforms - is that they intimately understand the material they're working with. With regards to infra-as-code, the most important part of this story is understanding your managed resources in-depth. Both from the current perspective, but also being able to put each resource in its historical context. The Resources view is the result of multiple months of meticulous work understanding and documenting the lifecycle of each resource managed by Spacelift, regardless of the technology used - Terraform, Terragrunt, Pulumi or AWS CloudFormation. Stack-level resources \u00bb This screen shows you the stack-level resources view. By default, resources are shown grouped in a hierarchical manner, grouped by parent. This allows you to see the structure of each of your infrastructure projects: Depending on the architecture as well as technology used, your tree may look slightly different - for example, Pulumi trees are generally deeper: Apart from being grouped by parent, resources can be grouped provider and type. Let's group one of our small stacks by provider: We can see lots of AWS resources, a few from Stripe, and a lonely one from Datadog. Let's now filter just the Datadog ones, and group them by type: Let's now take a look at one of the resources, for example a single stripe_price : The panel that is now showing on the right hand side of the Resources view shows the details of a single resource, which is now highlighted using a light blue overlay. On this panel, we see two noteworthy things. Starting with the lower right hand corner, we have the vendor-specific representation of the resource. Note how for security purposes all string values are sanitized. In fact, we never get to see them directly - we only see first 7 characters of their checksum. If you know the possible value, you can easily do the comparison. If you don't, then the secret is safe. As a side note, we do not need to sanitize anything with Pulumi because the team there did an exceptional job with regards to secret management: More importantly, though, you can drill down to see the runs that either created, or last updated each of the managed resources. Let's now go back to our stripe_price, and click on the ID of the run shown in the Last updated by section. You will notice a little menu pop up: Clicking on this menu item will take you to the run in question: One extra click on the commit SHA will take you to the GitHub commit. Depending on your Git flow, the commit may be linked to a Pull Request, giving you the ultimate visibility into the infrastructure change management process: Navigating the resource tree \u00bb When grouping by resources parent - that is, seeing them as a tree, you can easily click into each subtree, like if you were changing your working directory. You can do this by clicking on the dot representing each node in the tree: Once you click into the subtree, you will be able to click out by clicking on the new virtual root, as if you were running cd .. in your terminal: One important aspect about navigating the resource tree is that once you click into the subtree, you are effectively filtering by the ancestor. Which means that grouping and filtering options will now operate on a subset of resources within that subtree: Account-level resources \u00bb Warning Account-level resource view is still a work in progress, so some of the UX may change frequently. A view similar to stack-level resources is available for the entire account, too. For this presentation we will use a very symmetrical account - our automated testing instance repeatedly creating and updating the same types of resources in different ways to quickly detect potential regressions: By default we group by parent, giving you the same hierarchical view as for stack-level resources. But you will notice that there's a new common account root serving as a \"virtual\" parent for stacks. In this view you can also filter and group by different properties, with one extra property being the Stack:","title":"Resources"},{"location":"concepts/resources.html#resources","text":"One major benefit of specialized tools like Spacelift - as opposed to general-purpose CI/CD platforms - is that they intimately understand the material they're working with. With regards to infra-as-code, the most important part of this story is understanding your managed resources in-depth. Both from the current perspective, but also being able to put each resource in its historical context. The Resources view is the result of multiple months of meticulous work understanding and documenting the lifecycle of each resource managed by Spacelift, regardless of the technology used - Terraform, Terragrunt, Pulumi or AWS CloudFormation.","title":"Resources"},{"location":"concepts/resources.html#stack-level-resources","text":"This screen shows you the stack-level resources view. By default, resources are shown grouped in a hierarchical manner, grouped by parent. This allows you to see the structure of each of your infrastructure projects: Depending on the architecture as well as technology used, your tree may look slightly different - for example, Pulumi trees are generally deeper: Apart from being grouped by parent, resources can be grouped provider and type. Let's group one of our small stacks by provider: We can see lots of AWS resources, a few from Stripe, and a lonely one from Datadog. Let's now filter just the Datadog ones, and group them by type: Let's now take a look at one of the resources, for example a single stripe_price : The panel that is now showing on the right hand side of the Resources view shows the details of a single resource, which is now highlighted using a light blue overlay. On this panel, we see two noteworthy things. Starting with the lower right hand corner, we have the vendor-specific representation of the resource. Note how for security purposes all string values are sanitized. In fact, we never get to see them directly - we only see first 7 characters of their checksum. If you know the possible value, you can easily do the comparison. If you don't, then the secret is safe. As a side note, we do not need to sanitize anything with Pulumi because the team there did an exceptional job with regards to secret management: More importantly, though, you can drill down to see the runs that either created, or last updated each of the managed resources. Let's now go back to our stripe_price, and click on the ID of the run shown in the Last updated by section. You will notice a little menu pop up: Clicking on this menu item will take you to the run in question: One extra click on the commit SHA will take you to the GitHub commit. Depending on your Git flow, the commit may be linked to a Pull Request, giving you the ultimate visibility into the infrastructure change management process:","title":"Stack-level resources"},{"location":"concepts/resources.html#navigating-the-resource-tree","text":"When grouping by resources parent - that is, seeing them as a tree, you can easily click into each subtree, like if you were changing your working directory. You can do this by clicking on the dot representing each node in the tree: Once you click into the subtree, you will be able to click out by clicking on the new virtual root, as if you were running cd .. in your terminal: One important aspect about navigating the resource tree is that once you click into the subtree, you are effectively filtering by the ancestor. Which means that grouping and filtering options will now operate on a subset of resources within that subtree:","title":"Navigating the resource tree"},{"location":"concepts/resources.html#account-level-resources","text":"Warning Account-level resource view is still a work in progress, so some of the UX may change frequently. A view similar to stack-level resources is available for the entire account, too. For this presentation we will use a very symmetrical account - our automated testing instance repeatedly creating and updating the same types of resources in different ways to quickly detect potential regressions: By default we group by parent, giving you the same hierarchical view as for stack-level resources. But you will notice that there's a new common account root serving as a \"virtual\" parent for stacks. In this view you can also filter and group by different properties, with one extra property being the Stack:","title":"Account-level resources"},{"location":"concepts/worker-pools.html","text":"Worker pools \u00bb Tip A worker is a logical entity that processes a single run at a time. As a result, your number of workers is equal to your maximum concurrency. Typically, a virtual server (AWS EC2 or Azure/GCP VM) hosts a single worker to keep things simple and avoid coordination and resource management overhead. Containerized workers can share the same virtual server because the management is handled by the orchestrator. Setting up \u00bb Generate Worker Private Key \u00bb We use asymmetric encryption to ensure that any temporary run state can only be accessed by workers in a given worker pool. To support this, you need to generate a private key that can be used for this purpose, and use it to create a certificate signing request to give to Spacelift. We'll generate a certificate for you, so that workers can use it to authenticate with the Spacelift backend. The following command will generate the key and CSR: 1 openssl req -new -newkey rsa:4096 -nodes -keyout spacelift.key -out spacelift.csr Warning Don't forget to store the spacelift.key file (private key) in a secure location. You\u2019ll need it later, when launching workers in your worker pool. You can set up your worker pool from the Spacelift UI by navigating to Worker Pools section of your account, or you can also create it programmatically using the spacelift_worker_pool resource type within the Spacelift Terraform provider . Navigate to Worker Pools \u00bb Add Worker Pool Entity \u00bb Give your worker pool a name, and submit the spacelift.csr file in the worker pool creation form. After creation of the worker pool, you\u2019ll receive a Spacelift token . This token contains configuration for your worker pool launchers, as well as the certificate we generated for you based on the certificate signing request. Warning After clicking create, you will receive a token for the worker pool. Don't forget to save your Spacelift token in a secure location as you'll need this later when launching the worker pool. Launch Worker Pool \u00bb The Self-Hosted release archive contains a copy of the Spacelift launcher binary built specifically for your version of Self-Hosted. You can find this at bin/spacelift-launcher . This binary is also uploaded to the downloads S3 bucket during the Spacelift installation process. For more information on how to find your bucket name see here . In order to work, the launcher expects to be able to write to the local Docker socket. Unless you're using a Docker-based container scheduler like Kubernetes or ECS, please make sure that Docker is installed and running. Finally, you can run the launcher binary by setting two environment variables: SPACELIFT_TOKEN - the token you\u2019ve received from Spacelift on worker pool creation SPACELIFT_POOL_PRIVATE_KEY - the contents of the private key file you generated, in base64. Info You need to encode the entire private key using base-64, making it a single line of text. The simplest approach is to just run cat spacelift.key | base64 -w 0 in your command line. For Mac users, the command is cat spacelift.key | base64 -b 0 . Congrats! Your launcher should now connect to the Spacelift backend and start handling runs. CloudFormation Template \u00bb The easiest way to deploy workers for self-hosting is to deploy the CloudFormation template found in cloudformation/workerpool.yaml . PseudoRandomSuffix \u00bb The CloudFormation stack uses a parameter called PseudoRandomSuffix in order to ensure that certain resources are unique within an AWS account. The value of this parameter does not matter, other than that it is unique per worker pool stack you deploy. You should choose a value that is 6 characters long and made up of letters and numbers, for example ab12cd . Create a secret \u00bb First, create a new secret in SecretsManager, and add your token and the base64-encoded value of your private key. Use the key SPACELIFT_TOKEN for your token and SPACELIFT_POOL_PRIVATE_KEY for the private key. It should look something like this: Give your secret a name and create it. It doesn't matter what this name is, but you'll need it when deploying the CloudFormation stack. Get the downloads bucket name \u00bb The downloads bucket name is output at the end of the installation process. If you don't have a note of it, you can also get it from the resources of the spacelift-infra-s3 stack in CloudFormation: AMI \u00bb You can use your own custom-built AMI for your workers, or you can use one of the pre-built images we provide. For a list of the correct AMI to use for the region you want to deploy your worker to, see the spacelift-worker-image releases page . Note: please make sure to choose the x86_64 version of the AMI. Subnets and Security Group \u00bb You will need to have an existing VPC to deploy your pool into, and will need to provide a list of subnet IDs and security groups to match your requirements. Using a custom IAM role \u00bb By default we will create the instance role for the EC2 ASG as part of the worker pool stack, but you can also provide your own custom role via the InstanceRoleName parameter. This allows you to grant permissions to additional AWS resources that your workers need access to. A great example of this is allowing access to a private ECR in order to use a custom runner image. At a minimum, your role must fulfil the following requirements: It must have a trust relationship that allows role assumption by EC2. It needs to have the following managed policies attached: AutoScalingReadOnlyAccess . CloudWatchAgentServerPolicy . AmazonSSMManagedInstanceCore . Injecting custom commands during instance startup \u00bb You have the option to inject custom commands into the EC2 user data. This can be useful if you want to install additional software on your workers, or if you want to run a custom script during instance startup, or just add some additional environment variables. The script must be a valid shell script and should be put into Secrets Manager. Then you can provide the name of the secret as CustomUserDataSecretName when deploying the stack. Example: In the example above, we used spacelift/userdata as a secret name so the parameter will look like this: 1 2 3 4 [ ... ] --parameter-overrides \\ CustomUserDataSecretName = \"spacelift/userdata\" \\ [ ... ] Granting access to a private ECR \u00bb To allow your worker role to access a private ECR, you can attach a policy similar to the following to your instance role (replacing <repository-arn> with the ARN of your ECR repository): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:BatchCheckLayerAvailability\" ], \"Resource\" : \"<repository-arn>\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetAuthorizationToken\" ], \"Resource\" : \"*\" } ] } NOTE: repository ARNs are in the format arn:<partition>:ecr:<region>:<account-id>:repository/<repository-name> . Proxy Configuration \u00bb If you need to use an HTTP proxy for internet access, you can provide the proxy configuration using the following CloudFormation parameters: HttpProxyConfig . HttpsProxyConfig . NoProxyConfig . For example, you could use the following command to deploy a worker with a proxy configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"eu-west-1\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"012345678901-spacelift-infra-spacelift-downloads\" \\ SecretName = \"spacelift/default-worker-pool-credentials\" \\ SecurityGroups = \"sg-0d1e157a19ba2106f\" \\ Subnets = \"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\ ImageId = \"ami-0ead0234bef4f51b0\" \\ HttpProxyConfig = \"http://proxy.example.com:1234\" \\ HttpsProxyConfig = \"https://proxy.example.com:4321\" \\ NoProxyConfig = \"some.domain,another.domain\" \\ --capabilities \"CAPABILITY_NAMED_IAM\" Using custom CA certificates \u00bb If you use a custom certificate authority to issue TLS certs for components that Spacelift will communicate with, for example your VCS system, you need to provide your custom CA certificates to the worker. You do this by creating a secret in SecretsManager containing a base64 encoded JSON string. The format of the JSON object is as follows: 1 { \"caCertificates\" : [ \"<base64-encoded-cert-1>\" , \"<base64-encoded-cert-2>\" , \"<base64-encoded-cert-N>\" ]} For example, if you had a file called ca-certs.json containing the following content: 1 2 3 4 5 { \"caCertificates\" : [ \"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\" ] } You could then encode it to base64 using base64 -w0 < ca-certs.json (or base64 -b 0 < ca-certs.json on a Mac), resulting in the following string: 1 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 You would then create a secret in SecretsManager, and deploy the worker pool using the following command (replacing <ca-cert-secret-name> with the name of your secret): 1 2 3 4 5 6 7 8 9 10 11 12 13 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"eu-west-1\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"012345678901-spacelift-infra-spacelift-downloads\" \\ SecretName = \"spacelift/default-worker-pool-credentials\" \\ SecurityGroups = \"sg-0d1e157a19ba2106f\" \\ Subnets = \"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\ ImageId = \"ami-0ead0234bef4f51b0\" \\ AdditionalRootCAsSecretName = \"<ca-cert-secret-name>\" \\ --capabilities \"CAPABILITY_NAMED_IAM\" Deploying the Template \u00bb To deploy your worker pool stack, you can use the following command: 1 2 3 4 5 6 7 8 9 10 11 12 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"<region>\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"<binaries-bucket>\" \\ SecretName = \"<secret-name>\" \\ SecurityGroups = \"<security-groups>\" \\ Subnets = \"<subnets>\" \\ ImageId = \"<ami-id>\" \\ --capabilities \"CAPABILITY_NAMED_IAM\" For example, to deploy to eu-west-1 you might use something like this: 1 2 3 4 5 6 7 8 9 10 11 12 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"eu-west-1\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"012345678901-spacelift-infra-spacelift-downloads\" \\ SecretName = \"spacelift/default-worker-pool-credentials\" \\ SecurityGroups = \"sg-0d1e157a19ba2106f\" \\ Subnets = \"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\ ImageId = \"ami-0ead0234bef4f51b0\" \\ --capabilities \"CAPABILITY_NAMED_IAM\" To use a custom instance role, you might use something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"eu-west-1\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"012345678901-spacelift-infra-spacelift-downloads\" \\ SecretName = \"spacelift/default-worker-pool-credentials\" \\ SecurityGroups = \"sg-0d1e157a19ba2106f\" \\ Subnets = \"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\ ImageId = \"ami-0ead0234bef4f51b0\" \\ InstanceRoleName = \"default-worker-role\" \\ --capabilities \"CAPABILITY_NAMED_IAM\" Terraform Modules \u00bb Our public AWS , Azure and GCP Terraform modules are not currently compatible with self-hosting. Running Workers in Kubernetes \u00bb You can run Spacelift workers for your self-hosted instance in Kubernetes, for example using our Helm chart . The main thing to be aware of is that the launcher is designed to work with a specific version of Spacelift, so it's important to use the correct container image for your Spacelift install. Finding the Launcher Image \u00bb During the installation process for your self-hosted image, an ECR repository is created for storing launcher images named spacelift-launcher . At the end of the installation the launcher image URI and tag are output. If you didn't take a note of it at the time, you can find the ECR repository URI via the AWS console, or by running the following command: 1 aws ecr describe-repositories --region <aws-region> --repository-names \"spacelift-launcher\" --output json | jq -r '.repositories[0].repositoryUri' The repository URI will be in the format <account-id>.dkr.ecr.<region>.amazonaws.com/spacelift-launcher . To calculate the correct image to use, add the version of your self-hosted installation onto the end, for example: 1 012345678901 .dkr.ecr.eu-west-2.amazonaws.com/spacelift-launcher:v0.0.6 Note: the cluster that you run the Launcher in must be able to pull the launcher image from your ECR repository, so you will need to ensure that it has the correct permissions to do so. Helm Chart \u00bb By default our Helm chart is configured to use public.ecr.aws/spacelift/launcher . The latest tag of that image is guaranteed to always work with the SaaS version of Spacelift. For self-hosted instances, you should configure the chart to use the correct launcher image URI and tag. For example, for the image specified in the Finding the Launcher Image section, you would use the following Helm values: 1 2 3 4 launcher : image : repository : \"012345678901.dkr.ecr.eu-west-2.amazonaws.com/spacelift-launcher\" tag : \"v0.0.6\" Configuration options \u00bb A number of configuration variables is available to customize how your launcher behaves: SPACELIFT_DOCKER_CONFIG_DIR - if set, the value of this variable will point to the directory containing Docker configuration, which includes credentials for private Docker registries. Private workers can populate this directory for example by executing docker login before the launcher process is started; SPACELIFT_MASK_ENVS - comma-delimited list of whitelisted environment variables that are passed to the workers but should never appear in the logs; SPACELIFT_WORKER_NETWORK - network ID/name to connect the launched worker containers, defaults to bridge ; SPACELIFT_WORKER_EXTRA_MOUNTS - additional files or directories to be mounted to the launched worker docker containers during either read or write runs , as a comma-separated list of mounts in the form of /host/path:/container/path ; SPACELIFT_WORKER_WO_EXTRA_MOUNTS - Additional directories to be mounted to the worker docker container during write only runs , as a comma separated list of mounts in the form of /host/path:/container/path ; SPACELIFT_WORKER_RO_EXTRA_MOUNTS - Additional directories to be mounted to the worker docker container during read only runs , as a comma separated list of mounts in the form of /host/path:/container/path ; SPACELIFT_WORKER_RUNTIME - runtime to use for worker container; SPACELIFT_WHITELIST_ENVS - comma-delimited list of environment variables to pass from the launcher's own environment to the workers' environment. They can be prefixed with ro_ to only be included in read only runs or wo_ to only be included in write only runs; SPACELIFT_LAUNCHER_LOGS_TIMEOUT - custom timeout (the default is 7 minutes ) for killing jobs not producing any logs. This is a duration flag, expecting a duration-formatted value, eg 1000s ; SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY - file that contains the run initialization policy that will be parsed/used; If the run initialized policy can not be validated at the startup the worker pool will exit with an appropriate error; SPACELIFT_LAUNCHER_RUN_TIMEOUT - custom maximum run time - the default is 70 minutes . This is a duration flag, expecting a duration-formatted value, eg. 120m ; Passing metadata tags \u00bb When the launcher from a worker pool is registering with the mothership, you can send along some tags that will allow you to uniquely identify the process/machine for the purpose of draining or debugging. Any environment variables using SPACELIFT_METADATA_ prefix will be passed on. As an example, if you're running Spacelift workers in EC2, you can do the following just before you execute the launcher binary: 1 export SPACELIFT_METADATA_instance_id = $( ec2-metadata --instance-id | cut -d ' ' -f2 ) Doing so will set your EC2 instance ID as instance_id tag in your worker. Please see injecting custom commands during instance startup for information about how to do this when using our CloudFormation template. Network Security \u00bb Private workers need to be able to make outbound connections in order to communicate with Spacelift, as well as to access any resources required by your runs. If you have policies in place that require you to limit the outbound traffic allowed from your workers, you can use the following lists as a guide. AWS Services \u00bb Your worker needs access to the following AWS services in order to function correctly. You can refer to the AWS documentation for their IP address ranges. Access to the public Elastic Container Registry if using our default runner image. Access to your Self-Hosted server, for example https://spacelift.myorg.com . Access to the AWS IoT Core endpoints in your installation region for worker communication via MQTT. Access to Amazon S3 in your installation region for uploading run logs. Other \u00bb In addition, you will also need to allow access to the following: Your VCS provider. Access to any custom container registries you use if using custom runner images. Access to any other infrastructure required as part of your runs. Using worker pools \u00bb Worker pools must be explicitly attached to stacks and/or modules in order to start processing their workloads. This can be done in the Behavior section of stack and module settings: Worker Pool Management Views \u00bb You can view the activity and status of every aspect of your worker pool in the worker pool detail view. You can navigate to the worker pool of your choosing by clicking on the appropriate entry in the worker pools list view. Private Worker Pool \u00bb A private worker pool is a worker pool for which you are responsible for managing the workers. Workers \u00bb The workers tab lists all workers for this worker pool and their status. Status \u00bb A worker can have three possible statuses: DRAINED which indicates that the workers is not accepting new work. BUSY which indicates that the worker is currently processing or about to process a run. IDLE which indicates that the worker is available to start processing new runs. Queued \u00bb Queued lists all the run that can be scheduled and are currently in progress. In progress runs will be the first entries in the list when using the view without any filtering. Info Reasons a run might not be shown in this list: a tracked run is waiting on a tracked run , the run has is dependent on other runs. Used by \u00bb Stacks and/or Modules that are using the private worker pool.","title":"Worker pools"},{"location":"concepts/worker-pools.html#worker-pools","text":"Tip A worker is a logical entity that processes a single run at a time. As a result, your number of workers is equal to your maximum concurrency. Typically, a virtual server (AWS EC2 or Azure/GCP VM) hosts a single worker to keep things simple and avoid coordination and resource management overhead. Containerized workers can share the same virtual server because the management is handled by the orchestrator.","title":"Worker pools"},{"location":"concepts/worker-pools.html#setting-up","text":"","title":"Setting up"},{"location":"concepts/worker-pools.html#generate-worker-private-key","text":"We use asymmetric encryption to ensure that any temporary run state can only be accessed by workers in a given worker pool. To support this, you need to generate a private key that can be used for this purpose, and use it to create a certificate signing request to give to Spacelift. We'll generate a certificate for you, so that workers can use it to authenticate with the Spacelift backend. The following command will generate the key and CSR: 1 openssl req -new -newkey rsa:4096 -nodes -keyout spacelift.key -out spacelift.csr Warning Don't forget to store the spacelift.key file (private key) in a secure location. You\u2019ll need it later, when launching workers in your worker pool. You can set up your worker pool from the Spacelift UI by navigating to Worker Pools section of your account, or you can also create it programmatically using the spacelift_worker_pool resource type within the Spacelift Terraform provider .","title":"Generate Worker Private Key"},{"location":"concepts/worker-pools.html#navigate-to-worker-pools","text":"","title":"Navigate to Worker Pools"},{"location":"concepts/worker-pools.html#add-worker-pool-entity","text":"Give your worker pool a name, and submit the spacelift.csr file in the worker pool creation form. After creation of the worker pool, you\u2019ll receive a Spacelift token . This token contains configuration for your worker pool launchers, as well as the certificate we generated for you based on the certificate signing request. Warning After clicking create, you will receive a token for the worker pool. Don't forget to save your Spacelift token in a secure location as you'll need this later when launching the worker pool.","title":"Add Worker Pool Entity"},{"location":"concepts/worker-pools.html#launch-worker-pool","text":"The Self-Hosted release archive contains a copy of the Spacelift launcher binary built specifically for your version of Self-Hosted. You can find this at bin/spacelift-launcher . This binary is also uploaded to the downloads S3 bucket during the Spacelift installation process. For more information on how to find your bucket name see here . In order to work, the launcher expects to be able to write to the local Docker socket. Unless you're using a Docker-based container scheduler like Kubernetes or ECS, please make sure that Docker is installed and running. Finally, you can run the launcher binary by setting two environment variables: SPACELIFT_TOKEN - the token you\u2019ve received from Spacelift on worker pool creation SPACELIFT_POOL_PRIVATE_KEY - the contents of the private key file you generated, in base64. Info You need to encode the entire private key using base-64, making it a single line of text. The simplest approach is to just run cat spacelift.key | base64 -w 0 in your command line. For Mac users, the command is cat spacelift.key | base64 -b 0 . Congrats! Your launcher should now connect to the Spacelift backend and start handling runs.","title":"Launch Worker Pool"},{"location":"concepts/worker-pools.html#cloudformation-template","text":"The easiest way to deploy workers for self-hosting is to deploy the CloudFormation template found in cloudformation/workerpool.yaml .","title":"CloudFormation Template"},{"location":"concepts/worker-pools.html#pseudorandomsuffix","text":"The CloudFormation stack uses a parameter called PseudoRandomSuffix in order to ensure that certain resources are unique within an AWS account. The value of this parameter does not matter, other than that it is unique per worker pool stack you deploy. You should choose a value that is 6 characters long and made up of letters and numbers, for example ab12cd .","title":"PseudoRandomSuffix"},{"location":"concepts/worker-pools.html#create-a-secret","text":"First, create a new secret in SecretsManager, and add your token and the base64-encoded value of your private key. Use the key SPACELIFT_TOKEN for your token and SPACELIFT_POOL_PRIVATE_KEY for the private key. It should look something like this: Give your secret a name and create it. It doesn't matter what this name is, but you'll need it when deploying the CloudFormation stack.","title":"Create a secret"},{"location":"concepts/worker-pools.html#get-the-downloads-bucket-name","text":"The downloads bucket name is output at the end of the installation process. If you don't have a note of it, you can also get it from the resources of the spacelift-infra-s3 stack in CloudFormation:","title":"Get the downloads bucket name"},{"location":"concepts/worker-pools.html#ami","text":"You can use your own custom-built AMI for your workers, or you can use one of the pre-built images we provide. For a list of the correct AMI to use for the region you want to deploy your worker to, see the spacelift-worker-image releases page . Note: please make sure to choose the x86_64 version of the AMI.","title":"AMI"},{"location":"concepts/worker-pools.html#subnets-and-security-group","text":"You will need to have an existing VPC to deploy your pool into, and will need to provide a list of subnet IDs and security groups to match your requirements.","title":"Subnets and Security Group"},{"location":"concepts/worker-pools.html#using-a-custom-iam-role","text":"By default we will create the instance role for the EC2 ASG as part of the worker pool stack, but you can also provide your own custom role via the InstanceRoleName parameter. This allows you to grant permissions to additional AWS resources that your workers need access to. A great example of this is allowing access to a private ECR in order to use a custom runner image. At a minimum, your role must fulfil the following requirements: It must have a trust relationship that allows role assumption by EC2. It needs to have the following managed policies attached: AutoScalingReadOnlyAccess . CloudWatchAgentServerPolicy . AmazonSSMManagedInstanceCore .","title":"Using a custom IAM role"},{"location":"concepts/worker-pools.html#injecting-custom-commands-during-instance-startup","text":"You have the option to inject custom commands into the EC2 user data. This can be useful if you want to install additional software on your workers, or if you want to run a custom script during instance startup, or just add some additional environment variables. The script must be a valid shell script and should be put into Secrets Manager. Then you can provide the name of the secret as CustomUserDataSecretName when deploying the stack. Example: In the example above, we used spacelift/userdata as a secret name so the parameter will look like this: 1 2 3 4 [ ... ] --parameter-overrides \\ CustomUserDataSecretName = \"spacelift/userdata\" \\ [ ... ]","title":"Injecting custom commands during instance startup"},{"location":"concepts/worker-pools.html#granting-access-to-a-private-ecr","text":"To allow your worker role to access a private ECR, you can attach a policy similar to the following to your instance role (replacing <repository-arn> with the ARN of your ECR repository): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:BatchCheckLayerAvailability\" ], \"Resource\" : \"<repository-arn>\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetAuthorizationToken\" ], \"Resource\" : \"*\" } ] } NOTE: repository ARNs are in the format arn:<partition>:ecr:<region>:<account-id>:repository/<repository-name> .","title":"Granting access to a private ECR"},{"location":"concepts/worker-pools.html#proxy-configuration","text":"If you need to use an HTTP proxy for internet access, you can provide the proxy configuration using the following CloudFormation parameters: HttpProxyConfig . HttpsProxyConfig . NoProxyConfig . For example, you could use the following command to deploy a worker with a proxy configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"eu-west-1\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"012345678901-spacelift-infra-spacelift-downloads\" \\ SecretName = \"spacelift/default-worker-pool-credentials\" \\ SecurityGroups = \"sg-0d1e157a19ba2106f\" \\ Subnets = \"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\ ImageId = \"ami-0ead0234bef4f51b0\" \\ HttpProxyConfig = \"http://proxy.example.com:1234\" \\ HttpsProxyConfig = \"https://proxy.example.com:4321\" \\ NoProxyConfig = \"some.domain,another.domain\" \\ --capabilities \"CAPABILITY_NAMED_IAM\"","title":"Proxy Configuration"},{"location":"concepts/worker-pools.html#using-custom-ca-certificates","text":"If you use a custom certificate authority to issue TLS certs for components that Spacelift will communicate with, for example your VCS system, you need to provide your custom CA certificates to the worker. You do this by creating a secret in SecretsManager containing a base64 encoded JSON string. The format of the JSON object is as follows: 1 { \"caCertificates\" : [ \"<base64-encoded-cert-1>\" , \"<base64-encoded-cert-2>\" , \"<base64-encoded-cert-N>\" ]} For example, if you had a file called ca-certs.json containing the following content: 1 2 3 4 5 { \"caCertificates\" : [ \"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\" ] } You could then encode it to base64 using base64 -w0 < ca-certs.json (or base64 -b 0 < ca-certs.json on a Mac), resulting in the following string: 1 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 You would then create a secret in SecretsManager, and deploy the worker pool using the following command (replacing <ca-cert-secret-name> with the name of your secret): 1 2 3 4 5 6 7 8 9 10 11 12 13 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"eu-west-1\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"012345678901-spacelift-infra-spacelift-downloads\" \\ SecretName = \"spacelift/default-worker-pool-credentials\" \\ SecurityGroups = \"sg-0d1e157a19ba2106f\" \\ Subnets = \"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\ ImageId = \"ami-0ead0234bef4f51b0\" \\ AdditionalRootCAsSecretName = \"<ca-cert-secret-name>\" \\ --capabilities \"CAPABILITY_NAMED_IAM\"","title":"Using custom CA certificates"},{"location":"concepts/worker-pools.html#deploying-the-template","text":"To deploy your worker pool stack, you can use the following command: 1 2 3 4 5 6 7 8 9 10 11 12 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"<region>\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"<binaries-bucket>\" \\ SecretName = \"<secret-name>\" \\ SecurityGroups = \"<security-groups>\" \\ Subnets = \"<subnets>\" \\ ImageId = \"<ami-id>\" \\ --capabilities \"CAPABILITY_NAMED_IAM\" For example, to deploy to eu-west-1 you might use something like this: 1 2 3 4 5 6 7 8 9 10 11 12 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"eu-west-1\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"012345678901-spacelift-infra-spacelift-downloads\" \\ SecretName = \"spacelift/default-worker-pool-credentials\" \\ SecurityGroups = \"sg-0d1e157a19ba2106f\" \\ Subnets = \"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\ ImageId = \"ami-0ead0234bef4f51b0\" \\ --capabilities \"CAPABILITY_NAMED_IAM\" To use a custom instance role, you might use something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 aws cloudformation deploy --no-cli-pager \\ --stack-name spacelift-default-worker-pool \\ --template-file \"cloudformation/workerpool.yaml\" \\ --region \"eu-west-1\" \\ --parameter-overrides \\ PseudoRandomSuffix = \"ab12cd\" \\ BinariesBucket = \"012345678901-spacelift-infra-spacelift-downloads\" \\ SecretName = \"spacelift/default-worker-pool-credentials\" \\ SecurityGroups = \"sg-0d1e157a19ba2106f\" \\ Subnets = \"subnet-44ca1b771ca7bcc1a,subnet-6b61ec08772f47ba2\" \\ ImageId = \"ami-0ead0234bef4f51b0\" \\ InstanceRoleName = \"default-worker-role\" \\ --capabilities \"CAPABILITY_NAMED_IAM\"","title":"Deploying the Template"},{"location":"concepts/worker-pools.html#terraform-modules","text":"Our public AWS , Azure and GCP Terraform modules are not currently compatible with self-hosting.","title":"Terraform Modules"},{"location":"concepts/worker-pools.html#running-workers-in-kubernetes","text":"You can run Spacelift workers for your self-hosted instance in Kubernetes, for example using our Helm chart . The main thing to be aware of is that the launcher is designed to work with a specific version of Spacelift, so it's important to use the correct container image for your Spacelift install.","title":"Running Workers in Kubernetes"},{"location":"concepts/worker-pools.html#finding-the-launcher-image","text":"During the installation process for your self-hosted image, an ECR repository is created for storing launcher images named spacelift-launcher . At the end of the installation the launcher image URI and tag are output. If you didn't take a note of it at the time, you can find the ECR repository URI via the AWS console, or by running the following command: 1 aws ecr describe-repositories --region <aws-region> --repository-names \"spacelift-launcher\" --output json | jq -r '.repositories[0].repositoryUri' The repository URI will be in the format <account-id>.dkr.ecr.<region>.amazonaws.com/spacelift-launcher . To calculate the correct image to use, add the version of your self-hosted installation onto the end, for example: 1 012345678901 .dkr.ecr.eu-west-2.amazonaws.com/spacelift-launcher:v0.0.6 Note: the cluster that you run the Launcher in must be able to pull the launcher image from your ECR repository, so you will need to ensure that it has the correct permissions to do so.","title":"Finding the Launcher Image"},{"location":"concepts/worker-pools.html#helm-chart","text":"By default our Helm chart is configured to use public.ecr.aws/spacelift/launcher . The latest tag of that image is guaranteed to always work with the SaaS version of Spacelift. For self-hosted instances, you should configure the chart to use the correct launcher image URI and tag. For example, for the image specified in the Finding the Launcher Image section, you would use the following Helm values: 1 2 3 4 launcher : image : repository : \"012345678901.dkr.ecr.eu-west-2.amazonaws.com/spacelift-launcher\" tag : \"v0.0.6\"","title":"Helm Chart"},{"location":"concepts/worker-pools.html#configuration-options","text":"A number of configuration variables is available to customize how your launcher behaves: SPACELIFT_DOCKER_CONFIG_DIR - if set, the value of this variable will point to the directory containing Docker configuration, which includes credentials for private Docker registries. Private workers can populate this directory for example by executing docker login before the launcher process is started; SPACELIFT_MASK_ENVS - comma-delimited list of whitelisted environment variables that are passed to the workers but should never appear in the logs; SPACELIFT_WORKER_NETWORK - network ID/name to connect the launched worker containers, defaults to bridge ; SPACELIFT_WORKER_EXTRA_MOUNTS - additional files or directories to be mounted to the launched worker docker containers during either read or write runs , as a comma-separated list of mounts in the form of /host/path:/container/path ; SPACELIFT_WORKER_WO_EXTRA_MOUNTS - Additional directories to be mounted to the worker docker container during write only runs , as a comma separated list of mounts in the form of /host/path:/container/path ; SPACELIFT_WORKER_RO_EXTRA_MOUNTS - Additional directories to be mounted to the worker docker container during read only runs , as a comma separated list of mounts in the form of /host/path:/container/path ; SPACELIFT_WORKER_RUNTIME - runtime to use for worker container; SPACELIFT_WHITELIST_ENVS - comma-delimited list of environment variables to pass from the launcher's own environment to the workers' environment. They can be prefixed with ro_ to only be included in read only runs or wo_ to only be included in write only runs; SPACELIFT_LAUNCHER_LOGS_TIMEOUT - custom timeout (the default is 7 minutes ) for killing jobs not producing any logs. This is a duration flag, expecting a duration-formatted value, eg 1000s ; SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY - file that contains the run initialization policy that will be parsed/used; If the run initialized policy can not be validated at the startup the worker pool will exit with an appropriate error; SPACELIFT_LAUNCHER_RUN_TIMEOUT - custom maximum run time - the default is 70 minutes . This is a duration flag, expecting a duration-formatted value, eg. 120m ;","title":"Configuration options"},{"location":"concepts/worker-pools.html#passing-metadata-tags","text":"When the launcher from a worker pool is registering with the mothership, you can send along some tags that will allow you to uniquely identify the process/machine for the purpose of draining or debugging. Any environment variables using SPACELIFT_METADATA_ prefix will be passed on. As an example, if you're running Spacelift workers in EC2, you can do the following just before you execute the launcher binary: 1 export SPACELIFT_METADATA_instance_id = $( ec2-metadata --instance-id | cut -d ' ' -f2 ) Doing so will set your EC2 instance ID as instance_id tag in your worker. Please see injecting custom commands during instance startup for information about how to do this when using our CloudFormation template.","title":"Passing metadata tags"},{"location":"concepts/worker-pools.html#network-security","text":"Private workers need to be able to make outbound connections in order to communicate with Spacelift, as well as to access any resources required by your runs. If you have policies in place that require you to limit the outbound traffic allowed from your workers, you can use the following lists as a guide.","title":"Network Security"},{"location":"concepts/worker-pools.html#aws-services","text":"Your worker needs access to the following AWS services in order to function correctly. You can refer to the AWS documentation for their IP address ranges. Access to the public Elastic Container Registry if using our default runner image. Access to your Self-Hosted server, for example https://spacelift.myorg.com . Access to the AWS IoT Core endpoints in your installation region for worker communication via MQTT. Access to Amazon S3 in your installation region for uploading run logs.","title":"AWS Services"},{"location":"concepts/worker-pools.html#other","text":"In addition, you will also need to allow access to the following: Your VCS provider. Access to any custom container registries you use if using custom runner images. Access to any other infrastructure required as part of your runs.","title":"Other"},{"location":"concepts/worker-pools.html#using-worker-pools","text":"Worker pools must be explicitly attached to stacks and/or modules in order to start processing their workloads. This can be done in the Behavior section of stack and module settings:","title":"Using worker pools"},{"location":"concepts/worker-pools.html#worker-pool-management-views","text":"You can view the activity and status of every aspect of your worker pool in the worker pool detail view. You can navigate to the worker pool of your choosing by clicking on the appropriate entry in the worker pools list view.","title":"Worker Pool Management Views"},{"location":"concepts/worker-pools.html#private-worker-pool","text":"A private worker pool is a worker pool for which you are responsible for managing the workers.","title":"Private Worker Pool"},{"location":"concepts/worker-pools.html#workers","text":"The workers tab lists all workers for this worker pool and their status.","title":"Workers"},{"location":"concepts/worker-pools.html#status","text":"A worker can have three possible statuses: DRAINED which indicates that the workers is not accepting new work. BUSY which indicates that the worker is currently processing or about to process a run. IDLE which indicates that the worker is available to start processing new runs.","title":"Status"},{"location":"concepts/worker-pools.html#queued","text":"Queued lists all the run that can be scheduled and are currently in progress. In progress runs will be the first entries in the list when using the view without any filtering. Info Reasons a run might not be shown in this list: a tracked run is waiting on a tracked run , the run has is dependent on other runs.","title":"Queued"},{"location":"concepts/worker-pools.html#used-by","text":"Stacks and/or Modules that are using the private worker pool.","title":"Used by"},{"location":"concepts/blueprint/index.html","text":"Blueprint \u00bb There are multiple ways to create stacks in Spacelift. Our recommended way is to use our Terraform provider and programmatically create stacks using an administrative stack. However, sometimes you might want to create a stack manually, or you might want to create a stack temporarily (like pulling up a test environment, then destroying it). In these cases managing a stack from a VCS system and Terraform could be clumsy, this is where Blueprints come in handy. What is a Blueprint? \u00bb A Blueprint is a template for a stack and its configuration. The template can contain variables that can be filled in by providing inputs when creating a stack from the Blueprint. The template can also contain a list of other resources that will be created when the stack is created. You can configure the following resources in a Blueprint: All stack settings including: Name, description, labels, Space Behavioral settings: administrative, auto-apply, auto-destroy, hooks, runner image etc. VCS configuration Vendor configuration for your IaaC provider Environment variables , both non-sensitive and sensitive Mounted files Attaching Contexts Attaching Policies Attaching AWS integrations Schedules: Drift detection Task Delete Blueprint states \u00bb There are two states: draft and published. Draft is the default state, it means that the blueprint \"development\" is in progress and not meant to be used. You cannot create a stack from a draft blueprint. Published means that the blueprint is ready to be used. You can publish a blueprint by clicking the Publish button in the UI. A published blueprint cannot be moved back to draft state. You need to clone the blueprint, edit it and publish it. Permissions \u00bb Blueprints permissions are managed by Spaces . You can only create, update and delete a blueprint in a Space you have admin access to but can be read by anyone with read access to the Space. Once the blueprint is published and you want to create a stack from it, the read access will be enough as long as you have admin access to the Space where the stack will be created. How to create a Blueprint \u00bb Choose Blueprints on the left menu and click on Create blueprint . As of now, we only support YAML format. The template engine will be familiar for those who used GitHub Actions before. The absolute minimum you'll need to provide is name , space , vcs and vendor ; all others are optional. Here's a small working example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 inputs : - id : stack_name name : Stack name stack : name : ${{ inputs.stack_name }} space : root vcs : branch : main repository : my-repository provider : GITHUB vendor : terraform : manage_state : true version : \"1.3.0\" Preview of a Blueprint The Create a stack button is inactive because the blueprint is in draft state. You can publish it by clicking the Publish button. After that, you can create a stack from the blueprint. Now, let's look at a massive example that covers all the available configuration options: Click to expand 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 inputs : - id : environment name : Environment to deploy to # type is not mandatory, defaults to short_text - id : app name : App name (used for naming convention) type : short_text - id : description name : Description of the stack type : long_text # long_text means you'll have a bigger text area in the UI - id : connstring name : Connection string to the database type : secret # secret means the input will be masked in the UI - id : tf_version name : Terraform version of the stack type : select options : - \"1.3.0\" - \"1.4.6\" - \"1.5.0\" - id : manage_state name : Should Spacelift manage the state of Terraform default : true type : boolean - id : destroy_task_epoch name : Epoch timestamp of when to destroy the resources type : number options : # If true, a tracked run will be triggered right after the stack is created trigger_run : true stack : name : ${{ inputs.app }}-{{ inputs.environment }}-stack space : root # The single-quote is needed to avoid YAML parsing errors since the question mark # and the colon is a reserved character in YAML. description : '${{ inputs.environment == \"prod\" ? \"Production stack\" : \"Non-production stack\" }}. Stack created at ${{ string(context.time) }}.' is_disabled : ${{ inputs.environment != 'prod' }} labels : - Environment/${{ inputs.environment }} - Vendor/Terraform - Owner/${{ context.user.login }} - Blueprint/${{ context.blueprint.name }} - Space/${{ context.blueprint.space }} administrative : false allow_promotion : false auto_deploy : false auto_retry : false local_preview_enabled : true protect_from_deletion : false runner_image : public.ecr.aws/mycorp/spacelift-runner:latest worker_pool : 01GQ29K8SYXKZVHPZ4HG00BK2E attachments : contexts : - id : my-first-context-vnfq2 priority : 1 clouds : aws : id : 01GQ29K8SYXKZVHPZ4HG00BK2E read : true write : true azure : id : 01GQ29K8SYXKZVHPZ4HG00BK2E read : true write : true subscription_id : 12345678-1234-1234-1234-123456789012 policies : - my-push-policy-1 - my-approval-policy-1 environment : variables : - name : MY_ENV_VAR value : my-env-var-value description : This is my non-encrypted env var - name : TF_VAR_CONNECTION_STRING value : ${{ inputs.connstring }} description : The connection string to the database secret : true mounted_files : - path : a.json content : | { \"a\": \"b\" } description : This is the configuration of x feature hooks : apply : before : [ \"sh\" , \"-c\" , \"echo 'before apply'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after apply'\" ] init : before : [ \"sh\" , \"-c\" , \"echo 'before init'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after init'\" ] plan : before : [ \"sh\" , \"-c\" , \"echo 'before plan'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after plan'\" ] perform : before : [ \"sh\" , \"-c\" , \"echo 'before perform'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after perform'\" ] destroy : before : [ \"sh\" , \"-c\" , \"echo 'before destroy'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after destroy'\" ] run : # There is no before hook for run after : [ \"sh\" , \"-c\" , \"echo 'after run'\" ] schedules : drift : cron : - \"0 0 * * *\" - \"5 5 * * 0\" reconcile : true ignore_state : true # If true, the schedule will run even if the stack is in a failed state timezone : UTC tasks : # You need to provide either a cron or a timestamp_unix - command : \"terraform apply -auto-approve\" cron : - \"0 0 * * *\" - command : \"terraform apply -auto-approve\" timestamp_unix : ${{ int(timestamp('2024-01-01T10:00:20.021-05:00')) }} delete : delete_resources : ${{ inputs.environment == 'prod' }} timestamp_unix : ${{ inputs.destroy_task_epoch - 86400 }} vcs : branch : master project_root : modules/apps/${{ inputs.app }} namespace : \"my-namespace\" # Note that this is just the name of the repository, not the full URL repository : my-repository provider : GITHUB # Possible values: GITHUB, GITLAB, BITBUCKET_DATACENTER, BITBUCKET_CLOUD, GITHUB_ENTERPRISE, AZURE_DEVOPS vendor : terraform : manage_state : ${{ inputs.manage_state }} version : ${{ inputs.tf_version }} workspace : workspace-${{ inputs.environment }} use_smart_sanitization : ${{ inputs.environment != 'prod' }} ansible : playbook : playbook.yml cloudformation : entry_template_file : cf/main.yml template_bucket : template_bucket stack_name : ${{ inputs.app }}-${{ inputs.environment }} region : '${{ inputs.environment.contains(\"prod\") ? \"us-east-1\" : \"us-east-2\" }}' kubernetes : namespace : ${{ inputs.app }} pulumi : stack_name : ${{ inputs.app }}-${{ inputs.environment }} login_url : https://app.pulumi.com As you noticed if we attach an existing resource to the stack (such as Worker Pool, Cloud integration, Policy or Context) we use the unique identifier of the resource. Typically, there is a button for it in the UI but you can also find it in the URL of the resource. Example of resource IDs Template engine \u00bb We built our own variable substitution engine based on Google CEL . The library is available on GitHub . Functions, objects \u00bb In the giant example above, you might have noticed something interesting: inline functions! CEL supports a couple of functions, such as: contains , startsWith , endsWith , matches , size and a bunch of others. You can find the full list in the language definition . It also supports some basic operators, such as: * , / , - , + , relations ( == , != , < , <= , > , >= ), && , || , ! , ?: (yes, it supports the ternary operator \ud83c\udf89) and in . Hint It could be useful to look into the unit tests of the library. Look for the invocations of interpret function. There is one caveat to keep in mind: keep the YAML syntax valid. YAML syntax validity \u00bb There are reserved characters in YAML, such as > (multiline string) | (multiline string), : (key-value pair marker), ? (mapping key) etc . If you use these characters as part of a CEL expression, you'll need to use quotes around the expression to escape it. For example: Invalid template: 1 2 stack : name : ${{ 2 > 1 ? \"yes\" : \"no\" }} -my-stack See how the syntax highlighter is confused? Valid template: 1 2 stack : name : '${{ 2 > 1 ? \"yes\" : \"no\" }}-my-stack' Results in: 1 2 stack : name : 'yes-my-stack' Variables \u00bb Since you probably don't want to create stacks with the exact same name and configuration, you'll use variables. Inputs \u00bb Inputs are defined in the inputs section of the template. You can use them in the template by prefixing them with ${{ inputs. and suffixing them with }} . For example, ${{ inputs.environment }} will be replaced with the value of the environment input. You can use these variables in CEL functions as well. For example, trigger_run: ${{ inputs.environment == 'prod' }} will be replaced with trigger_run: true or trigger_run: false depending on the value of the environment input. The input object has id , name , description , type , default and options fields. The mandatory fields are id and name . The id is used to refer to the input in the template. The name and the description are just helper fields for the user in the Stack creation tab. The type is the type of the input . The default is an optional default value of the input. The options is a list of options for the select input type. Example: 1 2 3 4 5 inputs : - id : app_name name : The name of the app stack : name : ${{ inputs.app_name }}-my-stack Input types \u00bb If the input type is not provided, it defaults to short_text . Other options are: Type Description short_text A short text input. long_text A long text input. Typically used for multiline strings. secret A secret input. The value of the input will be masked in the UI. number An integer input. boolean A boolean input. select A multi option input. In case of select , it is mandatory to provide options . float A float input. An example including all the types: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 inputs : - id : app_name name : The name of the stack # No type provided, defaults to short_text - id : description name : The description of the stack type : long_text - id : connstring name : Connection string to the database type : secret - id : number_of_instances name : The number of instances type : number - id : delete_protection name : Is delete protection enabled? type : boolean - id : environment name : The environment to deploy to type : select options : - prod - staging - dev - id : scale_factor name : The scale factor of the app type : float # You can optionally provide a default value default : 1.5 Context \u00bb We also provide an input object called context . It contains the following properties: Property Type Description time google.protobuf.Timestamp UTC time of the evaluation of the template. random_string string A random string of 6 characters (numbers and letters, no special characters). random_number int A random number between 0 and 1000000. random_uuid string A random UUID. user.login string The login of the person who triggered the blueprint creation; as provided by the SSO provider. user.name string The full name of the person who triggered the blueprint creation; as provided by the SSO provider. user.account string The account subdomain of the user who triggered the blueprint creation. blueprint.name string The name of the blueprint that was used to create the stack. blueprint.space string The space ID of the blueprint that was used to create the stack. blueprint.created_at google.protobuf.Timestamp The time when the blueprint was created. blueprint.updated_at google.protobuf.Timestamp The time when the blueprint was last updated. blueprint.published_at google.protobuf.Timestamp The time when the blueprint was published. blueprint.labels list(string) The labels of the blueprint. Here is an example of using a few of them: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 stack : name : integration-tests-${{ inputs.app }}-${{ context.random_string }} description : | Temporary integration test stack for ${{ inputs.app }}. Deployed in ${{ context.time.getFullYear() }}. The base blueprint was created at ${{ string(context.blueprint.created_at) }}. labels : - owner/${{ context.user.login }} - blueprints/${{ context.blueprint.name }} environment : variables : - name : DEPLOYMENT_ID value : ${{ context.random_uuid }} schedules : delete : delete_resources : ${{ context.random_number % 2 == 0 }} # Russian roulette timestamp_unix : ${{ int(context.time) + duration(30m).getSeconds() }} # Delete the stack in 30 minutes Results in: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 stack : name : integration-tests-my-app-vG3j3a description : | Temporary integration test stack for my-app. Deployed in 2023. The base blueprint was created at 2020-01-01T10:00:20.021-05:00. labels : - owner/johndoe - blueprints/my-blueprint environment : variables : - name : DEPLOYMENT_ID value : 6c9c4e3e-6b5d-4b3a-9c9c-4e3e6b5d4b3a schedules : delete : delete_resources : true # Russian roulette timestamp_unix : 1674139424 # Delete the stack in 30 minutes Note that this is not a working example as it misses a few things ( inputs section, vcs etc.), but it should give you an idea of what you can do. Tip What can you do with google.protobuf.Timestamp and google.protobuf.Duration ? Check out the language definition , it contains all the methods and type conversions available. Validation \u00bb We do not validate drafted blueprints, you can do whatever you want with them. However, if you publish your blueprint, we'll make sure it includes the required fields and you'll get an error if it doesn't. One caveat : we cannot validate fields that have variables because we don't know the value of the variable. On the other hand, if you try to create a stack from the blueprint and supply the inputs to the template, we'll be able to do the full validation. Let's say: 1 2 3 4 5 6 7 8 inputs : - id : timestamp name : Delete timestamp of the stack type : number stack : schedules : delete : timestamp_unix : ${{ inputs.timestamp }} We cannot make sure that the input variable is indeed a proper 10 digit epoch timestamp, we will only find out once you supply the actual input. Schema \u00bb The up-to-date schema of a Blueprint is available through a GraphQL query for authenticated users: 1 2 3 { blueprintSchema } Tip Remember that there are multiple ways to interract with Spacelift. You can use the GraphQL API , the CLI , the Terraform Provider or the web UI itself if you're feeling fancy. For simplicity, here is the current schema, but it might change in the future: Click to expand 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , \"title\" : \"Blueprint\" , \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"stack\" ], \"properties\" : { \"inputs\" : { \"$ref\" : \"#/definitions/inputs\" }, \"stack\" : { \"$ref\" : \"#/definitions/stack\" }, \"options\" : { \"$ref\" : \"#/definitions/options\" } }, \"definitions\" : { \"inputs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/input\" } }, \"input\" : { \"type\" : \"object\" , \"oneOf\" : [ { \"additionalProperties\" : false , \"required\" : [ \"id\" , \"name\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"default\" : { \"oneOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"number\" }, { \"type\" : \"boolean\" } ] }, \"type\" : { \"type\" : \"string\" , \"enum\" : [ \"short_text\" , \"long_text\" , \"secret\" , \"boolean\" , \"number\" , \"float\" ] } } }, { \"additionalProperties\" : false , \"required\" : [ \"id\" , \"name\" , \"type\" , \"options\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"default\" : { \"oneOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"number\" }, { \"type\" : \"boolean\" } ] }, \"type\" : { \"type\" : \"string\" , \"enum\" : [ \"select\" ] }, \"options\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"type\" : \"string\" } } } } ] }, \"stack\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" , \"space\" , \"vcs\" , \"vendor\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"description\" : { \"type\" : \"string\" }, \"labels\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"administrative\" : { \"type\" : \"boolean\" }, \"allow_promotion\" : { \"type\" : \"boolean\" }, \"auto_deploy\" : { \"type\" : \"boolean\" }, \"auto_retry\" : { \"type\" : \"boolean\" }, \"is_disabled\" : { \"type\" : \"boolean\" }, \"local_preview_enabled\" : { \"type\" : \"boolean\" }, \"protect_from_deletion\" : { \"type\" : \"boolean\" }, \"runner_image\" : { \"type\" : \"string\" }, \"space\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"worker_pool\" : { \"type\" : \"string\" }, \"attachments\" : { \"$ref\" : \"#/definitions/attachment\" }, \"environment\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"mounted_files\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/mounted_file\" } }, \"variables\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/variable\" } } } }, \"hooks\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"apply\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"init\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"plan\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"perform\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"destroy\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"run\" : { \"$ref\" : \"#/definitions/after_hook\" } } }, \"schedules\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"drift\" : { \"$ref\" : \"#/definitions/drift_detection_schedule\" }, \"tasks\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/task_schedule\" } }, \"delete\" : { \"$ref\" : \"#/definitions/delete_schedule\" } } }, \"vcs\" : { \"type\" : \"object\" , \"oneOf\" : [ { \"additionalProperties\" : false , \"required\" : [ \"branch\" , \"provider\" , \"repository\" ], \"properties\" : { \"branch\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"project_root\" : { \"type\" : \"string\" }, \"provider\" : { \"type\" : \"string\" , \"enum\" : [ \"GITHUB\" , \"GITLAB\" , \"BITBUCKET_DATACENTER\" , \"BITBUCKET_CLOUD\" , \"GITHUB_ENTERPRISE\" , \"SHOWCASE\" , \"AZURE_DEVOPS\" ] }, \"namespace\" : { \"type\" : \"string\" }, \"repository\" : { \"type\" : \"string\" , \"minLength\" : 1 , \"description\" : \"The name of the repository.\" } } }, { \"additionalProperties\" : false , \"required\" : [ \"branch\" , \"provider\" , \"repository_url\" ], \"properties\" : { \"branch\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"project_root\" : { \"type\" : \"string\" }, \"provider\" : { \"type\" : \"string\" , \"enum\" : [ \"RAW_GIT\" ] }, \"repository\" : { \"type\" : \"string\" , \"description\" : \"The name of the repository. If not provided, it'll be extracted from the repository_url.\" }, \"namespace\" : { \"type\" : \"string\" , \"description\" : \"The namespace of the repository. If not provided, it'll be extracted from the repository_url.\" }, \"repository_url\" : { \"type\" : \"string\" , \"minLength\" : 1 , \"description\" : \"The URL of the repository. This is only used for the 'GIT' provider.\" } } } ] }, \"vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"ansible\" : { \"$ref\" : \"#/definitions/ansible_vendor\" }, \"cloudformation\" : { \"$ref\" : \"#/definitions/cloudformation_vendor\" }, \"kubernetes\" : { \"$ref\" : \"#/definitions/kubernetes_vendor\" }, \"pulumi\" : { \"$ref\" : \"#/definitions/pulumi_vendor\" }, \"terraform\" : { \"$ref\" : \"#/definitions/terraform_vendor\" } } } } }, \"attachment\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"contexts\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/context\" } }, \"clouds\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"aws\" : { \"$ref\" : \"#/definitions/aws_attachment\" }, \"azure\" : { \"$ref\" : \"#/definitions/azure_attachment\" } } }, \"policies\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } } }, \"aws_attachment\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"id\" , \"read\" , \"write\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"read\" : { \"type\" : \"boolean\" }, \"write\" : { \"type\" : \"boolean\" } } }, \"azure_attachment\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"id\" , \"read\" , \"write\" , \"subscription_id\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"read\" : { \"type\" : \"boolean\" }, \"write\" : { \"type\" : \"boolean\" }, \"subscription_id\" : { \"type\" : \"string\" } } }, \"context\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"id\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"priority\" : { \"type\" : \"integer\" , \"minimum\" : 0 } } }, \"mounted_file\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"path\" , \"content\" ], \"properties\" : { \"path\" : { \"type\" : \"string\" }, \"content\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"secret\" : { \"type\" : \"boolean\" } } }, \"variable\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" , \"value\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"value\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"secret\" : { \"type\" : \"boolean\" } } }, \"after_hook\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"after\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"minLength\" : 1 } } }, \"before_after_hook\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"before\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"minLength\" : 1 }, \"after\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"minLength\" : 1 } } }, \"drift_detection_schedule\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"cron\" , \"reconcile\" ], \"properties\" : { \"cron\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/cron_schedule\" , \"maxLength\" : 1 } }, \"reconcile\" : { \"type\" : \"boolean\" }, \"ignore_state\" : { \"type\" : \"boolean\" }, \"timezone\" : { \"type\" : \"string\" } } }, \"task_schedule\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"command\" ], \"oneOf\" : [ { \"required\" : [ \"command\" , \"cron\" ] }, { \"required\" : [ \"command\" , \"timestamp_unix\" ] } ], \"properties\" : { \"command\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"cron\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/cron_schedule\" , \"minLength\" : 1 } }, \"timestamp_unix\" : { \"type\" : \"number\" , \"minimum\" : 1600000000 }, \"timezone\" : { \"type\" : \"string\" } } }, \"cron_schedule\" : { \"type\" : \"string\" , \"pattern\" : \"^(\\\\*|\\\\d+|\\\\d+-\\\\d+|\\\\d+\\\\/\\\\d+)(\\\\s+(\\\\*|\\\\d+|\\\\d+-\\\\d+|\\\\d+\\\\/\\\\d+)){4}$\" }, \"delete_schedule\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"timestamp_unix\" ], \"properties\" : { \"delete_resources\" : { \"type\" : \"boolean\" }, \"timestamp_unix\" : { \"type\" : \"number\" , \"minimum\" : 1600000000 } } }, \"ansible_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"playbook\" ], \"properties\" : { \"playbook\" : { \"type\" : \"string\" , \"minLength\" : 1 } } }, \"cloudformation_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"entry_template_file\" , \"template_bucket\" , \"stack_name\" , \"region\" ], \"properties\" : { \"entry_template_file\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"template_bucket\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"stack_name\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"region\" : { \"type\" : \"string\" , \"minLength\" : 1 } } }, \"kubernetes_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"namespace\" ], \"properties\" : { \"namespace\" : { \"type\" : \"string\" , \"minLength\" : 1 } } }, \"pulumi_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"stack_name\" , \"login_url\" ], \"properties\" : { \"stack_name\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"login_url\" : { \"type\" : \"string\" , \"minLength\" : 1 } } }, \"terraform_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"manage_state\" ], \"properties\" : { \"version\" : { \"type\" : \"string\" }, \"workspace\" : { \"type\" : \"string\" }, \"use_smart_sanitization\" : { \"type\" : \"boolean\" }, \"manage_state\" : { \"type\" : \"boolean\" } } }, \"options\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"trigger_run\" : { \"type\" : \"boolean\" } } } } }","title":"Blueprint"},{"location":"concepts/blueprint/index.html#blueprint","text":"There are multiple ways to create stacks in Spacelift. Our recommended way is to use our Terraform provider and programmatically create stacks using an administrative stack. However, sometimes you might want to create a stack manually, or you might want to create a stack temporarily (like pulling up a test environment, then destroying it). In these cases managing a stack from a VCS system and Terraform could be clumsy, this is where Blueprints come in handy.","title":"Blueprint"},{"location":"concepts/blueprint/index.html#what-is-a-blueprint","text":"A Blueprint is a template for a stack and its configuration. The template can contain variables that can be filled in by providing inputs when creating a stack from the Blueprint. The template can also contain a list of other resources that will be created when the stack is created. You can configure the following resources in a Blueprint: All stack settings including: Name, description, labels, Space Behavioral settings: administrative, auto-apply, auto-destroy, hooks, runner image etc. VCS configuration Vendor configuration for your IaaC provider Environment variables , both non-sensitive and sensitive Mounted files Attaching Contexts Attaching Policies Attaching AWS integrations Schedules: Drift detection Task Delete","title":"What is a Blueprint?"},{"location":"concepts/blueprint/index.html#blueprint-states","text":"There are two states: draft and published. Draft is the default state, it means that the blueprint \"development\" is in progress and not meant to be used. You cannot create a stack from a draft blueprint. Published means that the blueprint is ready to be used. You can publish a blueprint by clicking the Publish button in the UI. A published blueprint cannot be moved back to draft state. You need to clone the blueprint, edit it and publish it.","title":"Blueprint states"},{"location":"concepts/blueprint/index.html#permissions","text":"Blueprints permissions are managed by Spaces . You can only create, update and delete a blueprint in a Space you have admin access to but can be read by anyone with read access to the Space. Once the blueprint is published and you want to create a stack from it, the read access will be enough as long as you have admin access to the Space where the stack will be created.","title":"Permissions"},{"location":"concepts/blueprint/index.html#how-to-create-a-blueprint","text":"Choose Blueprints on the left menu and click on Create blueprint . As of now, we only support YAML format. The template engine will be familiar for those who used GitHub Actions before. The absolute minimum you'll need to provide is name , space , vcs and vendor ; all others are optional. Here's a small working example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 inputs : - id : stack_name name : Stack name stack : name : ${{ inputs.stack_name }} space : root vcs : branch : main repository : my-repository provider : GITHUB vendor : terraform : manage_state : true version : \"1.3.0\" Preview of a Blueprint The Create a stack button is inactive because the blueprint is in draft state. You can publish it by clicking the Publish button. After that, you can create a stack from the blueprint. Now, let's look at a massive example that covers all the available configuration options: Click to expand 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 inputs : - id : environment name : Environment to deploy to # type is not mandatory, defaults to short_text - id : app name : App name (used for naming convention) type : short_text - id : description name : Description of the stack type : long_text # long_text means you'll have a bigger text area in the UI - id : connstring name : Connection string to the database type : secret # secret means the input will be masked in the UI - id : tf_version name : Terraform version of the stack type : select options : - \"1.3.0\" - \"1.4.6\" - \"1.5.0\" - id : manage_state name : Should Spacelift manage the state of Terraform default : true type : boolean - id : destroy_task_epoch name : Epoch timestamp of when to destroy the resources type : number options : # If true, a tracked run will be triggered right after the stack is created trigger_run : true stack : name : ${{ inputs.app }}-{{ inputs.environment }}-stack space : root # The single-quote is needed to avoid YAML parsing errors since the question mark # and the colon is a reserved character in YAML. description : '${{ inputs.environment == \"prod\" ? \"Production stack\" : \"Non-production stack\" }}. Stack created at ${{ string(context.time) }}.' is_disabled : ${{ inputs.environment != 'prod' }} labels : - Environment/${{ inputs.environment }} - Vendor/Terraform - Owner/${{ context.user.login }} - Blueprint/${{ context.blueprint.name }} - Space/${{ context.blueprint.space }} administrative : false allow_promotion : false auto_deploy : false auto_retry : false local_preview_enabled : true protect_from_deletion : false runner_image : public.ecr.aws/mycorp/spacelift-runner:latest worker_pool : 01GQ29K8SYXKZVHPZ4HG00BK2E attachments : contexts : - id : my-first-context-vnfq2 priority : 1 clouds : aws : id : 01GQ29K8SYXKZVHPZ4HG00BK2E read : true write : true azure : id : 01GQ29K8SYXKZVHPZ4HG00BK2E read : true write : true subscription_id : 12345678-1234-1234-1234-123456789012 policies : - my-push-policy-1 - my-approval-policy-1 environment : variables : - name : MY_ENV_VAR value : my-env-var-value description : This is my non-encrypted env var - name : TF_VAR_CONNECTION_STRING value : ${{ inputs.connstring }} description : The connection string to the database secret : true mounted_files : - path : a.json content : | { \"a\": \"b\" } description : This is the configuration of x feature hooks : apply : before : [ \"sh\" , \"-c\" , \"echo 'before apply'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after apply'\" ] init : before : [ \"sh\" , \"-c\" , \"echo 'before init'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after init'\" ] plan : before : [ \"sh\" , \"-c\" , \"echo 'before plan'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after plan'\" ] perform : before : [ \"sh\" , \"-c\" , \"echo 'before perform'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after perform'\" ] destroy : before : [ \"sh\" , \"-c\" , \"echo 'before destroy'\" ] after : [ \"sh\" , \"-c\" , \"echo 'after destroy'\" ] run : # There is no before hook for run after : [ \"sh\" , \"-c\" , \"echo 'after run'\" ] schedules : drift : cron : - \"0 0 * * *\" - \"5 5 * * 0\" reconcile : true ignore_state : true # If true, the schedule will run even if the stack is in a failed state timezone : UTC tasks : # You need to provide either a cron or a timestamp_unix - command : \"terraform apply -auto-approve\" cron : - \"0 0 * * *\" - command : \"terraform apply -auto-approve\" timestamp_unix : ${{ int(timestamp('2024-01-01T10:00:20.021-05:00')) }} delete : delete_resources : ${{ inputs.environment == 'prod' }} timestamp_unix : ${{ inputs.destroy_task_epoch - 86400 }} vcs : branch : master project_root : modules/apps/${{ inputs.app }} namespace : \"my-namespace\" # Note that this is just the name of the repository, not the full URL repository : my-repository provider : GITHUB # Possible values: GITHUB, GITLAB, BITBUCKET_DATACENTER, BITBUCKET_CLOUD, GITHUB_ENTERPRISE, AZURE_DEVOPS vendor : terraform : manage_state : ${{ inputs.manage_state }} version : ${{ inputs.tf_version }} workspace : workspace-${{ inputs.environment }} use_smart_sanitization : ${{ inputs.environment != 'prod' }} ansible : playbook : playbook.yml cloudformation : entry_template_file : cf/main.yml template_bucket : template_bucket stack_name : ${{ inputs.app }}-${{ inputs.environment }} region : '${{ inputs.environment.contains(\"prod\") ? \"us-east-1\" : \"us-east-2\" }}' kubernetes : namespace : ${{ inputs.app }} pulumi : stack_name : ${{ inputs.app }}-${{ inputs.environment }} login_url : https://app.pulumi.com As you noticed if we attach an existing resource to the stack (such as Worker Pool, Cloud integration, Policy or Context) we use the unique identifier of the resource. Typically, there is a button for it in the UI but you can also find it in the URL of the resource. Example of resource IDs","title":"How to create a Blueprint"},{"location":"concepts/blueprint/index.html#template-engine","text":"We built our own variable substitution engine based on Google CEL . The library is available on GitHub .","title":"Template engine"},{"location":"concepts/blueprint/index.html#functions-objects","text":"In the giant example above, you might have noticed something interesting: inline functions! CEL supports a couple of functions, such as: contains , startsWith , endsWith , matches , size and a bunch of others. You can find the full list in the language definition . It also supports some basic operators, such as: * , / , - , + , relations ( == , != , < , <= , > , >= ), && , || , ! , ?: (yes, it supports the ternary operator \ud83c\udf89) and in . Hint It could be useful to look into the unit tests of the library. Look for the invocations of interpret function. There is one caveat to keep in mind: keep the YAML syntax valid.","title":"Functions, objects"},{"location":"concepts/blueprint/index.html#yaml-syntax-validity","text":"There are reserved characters in YAML, such as > (multiline string) | (multiline string), : (key-value pair marker), ? (mapping key) etc . If you use these characters as part of a CEL expression, you'll need to use quotes around the expression to escape it. For example: Invalid template: 1 2 stack : name : ${{ 2 > 1 ? \"yes\" : \"no\" }} -my-stack See how the syntax highlighter is confused? Valid template: 1 2 stack : name : '${{ 2 > 1 ? \"yes\" : \"no\" }}-my-stack' Results in: 1 2 stack : name : 'yes-my-stack'","title":"YAML syntax validity"},{"location":"concepts/blueprint/index.html#variables","text":"Since you probably don't want to create stacks with the exact same name and configuration, you'll use variables.","title":"Variables"},{"location":"concepts/blueprint/index.html#inputs","text":"Inputs are defined in the inputs section of the template. You can use them in the template by prefixing them with ${{ inputs. and suffixing them with }} . For example, ${{ inputs.environment }} will be replaced with the value of the environment input. You can use these variables in CEL functions as well. For example, trigger_run: ${{ inputs.environment == 'prod' }} will be replaced with trigger_run: true or trigger_run: false depending on the value of the environment input. The input object has id , name , description , type , default and options fields. The mandatory fields are id and name . The id is used to refer to the input in the template. The name and the description are just helper fields for the user in the Stack creation tab. The type is the type of the input . The default is an optional default value of the input. The options is a list of options for the select input type. Example: 1 2 3 4 5 inputs : - id : app_name name : The name of the app stack : name : ${{ inputs.app_name }}-my-stack","title":"Inputs"},{"location":"concepts/blueprint/index.html#input-types","text":"If the input type is not provided, it defaults to short_text . Other options are: Type Description short_text A short text input. long_text A long text input. Typically used for multiline strings. secret A secret input. The value of the input will be masked in the UI. number An integer input. boolean A boolean input. select A multi option input. In case of select , it is mandatory to provide options . float A float input. An example including all the types: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 inputs : - id : app_name name : The name of the stack # No type provided, defaults to short_text - id : description name : The description of the stack type : long_text - id : connstring name : Connection string to the database type : secret - id : number_of_instances name : The number of instances type : number - id : delete_protection name : Is delete protection enabled? type : boolean - id : environment name : The environment to deploy to type : select options : - prod - staging - dev - id : scale_factor name : The scale factor of the app type : float # You can optionally provide a default value default : 1.5","title":"Input types"},{"location":"concepts/blueprint/index.html#context","text":"We also provide an input object called context . It contains the following properties: Property Type Description time google.protobuf.Timestamp UTC time of the evaluation of the template. random_string string A random string of 6 characters (numbers and letters, no special characters). random_number int A random number between 0 and 1000000. random_uuid string A random UUID. user.login string The login of the person who triggered the blueprint creation; as provided by the SSO provider. user.name string The full name of the person who triggered the blueprint creation; as provided by the SSO provider. user.account string The account subdomain of the user who triggered the blueprint creation. blueprint.name string The name of the blueprint that was used to create the stack. blueprint.space string The space ID of the blueprint that was used to create the stack. blueprint.created_at google.protobuf.Timestamp The time when the blueprint was created. blueprint.updated_at google.protobuf.Timestamp The time when the blueprint was last updated. blueprint.published_at google.protobuf.Timestamp The time when the blueprint was published. blueprint.labels list(string) The labels of the blueprint. Here is an example of using a few of them: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 stack : name : integration-tests-${{ inputs.app }}-${{ context.random_string }} description : | Temporary integration test stack for ${{ inputs.app }}. Deployed in ${{ context.time.getFullYear() }}. The base blueprint was created at ${{ string(context.blueprint.created_at) }}. labels : - owner/${{ context.user.login }} - blueprints/${{ context.blueprint.name }} environment : variables : - name : DEPLOYMENT_ID value : ${{ context.random_uuid }} schedules : delete : delete_resources : ${{ context.random_number % 2 == 0 }} # Russian roulette timestamp_unix : ${{ int(context.time) + duration(30m).getSeconds() }} # Delete the stack in 30 minutes Results in: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 stack : name : integration-tests-my-app-vG3j3a description : | Temporary integration test stack for my-app. Deployed in 2023. The base blueprint was created at 2020-01-01T10:00:20.021-05:00. labels : - owner/johndoe - blueprints/my-blueprint environment : variables : - name : DEPLOYMENT_ID value : 6c9c4e3e-6b5d-4b3a-9c9c-4e3e6b5d4b3a schedules : delete : delete_resources : true # Russian roulette timestamp_unix : 1674139424 # Delete the stack in 30 minutes Note that this is not a working example as it misses a few things ( inputs section, vcs etc.), but it should give you an idea of what you can do. Tip What can you do with google.protobuf.Timestamp and google.protobuf.Duration ? Check out the language definition , it contains all the methods and type conversions available.","title":"Context"},{"location":"concepts/blueprint/index.html#validation","text":"We do not validate drafted blueprints, you can do whatever you want with them. However, if you publish your blueprint, we'll make sure it includes the required fields and you'll get an error if it doesn't. One caveat : we cannot validate fields that have variables because we don't know the value of the variable. On the other hand, if you try to create a stack from the blueprint and supply the inputs to the template, we'll be able to do the full validation. Let's say: 1 2 3 4 5 6 7 8 inputs : - id : timestamp name : Delete timestamp of the stack type : number stack : schedules : delete : timestamp_unix : ${{ inputs.timestamp }} We cannot make sure that the input variable is indeed a proper 10 digit epoch timestamp, we will only find out once you supply the actual input.","title":"Validation"},{"location":"concepts/blueprint/index.html#schema","text":"The up-to-date schema of a Blueprint is available through a GraphQL query for authenticated users: 1 2 3 { blueprintSchema } Tip Remember that there are multiple ways to interract with Spacelift. You can use the GraphQL API , the CLI , the Terraform Provider or the web UI itself if you're feeling fancy. For simplicity, here is the current schema, but it might change in the future: Click to expand 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , \"title\" : \"Blueprint\" , \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"stack\" ], \"properties\" : { \"inputs\" : { \"$ref\" : \"#/definitions/inputs\" }, \"stack\" : { \"$ref\" : \"#/definitions/stack\" }, \"options\" : { \"$ref\" : \"#/definitions/options\" } }, \"definitions\" : { \"inputs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/input\" } }, \"input\" : { \"type\" : \"object\" , \"oneOf\" : [ { \"additionalProperties\" : false , \"required\" : [ \"id\" , \"name\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"default\" : { \"oneOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"number\" }, { \"type\" : \"boolean\" } ] }, \"type\" : { \"type\" : \"string\" , \"enum\" : [ \"short_text\" , \"long_text\" , \"secret\" , \"boolean\" , \"number\" , \"float\" ] } } }, { \"additionalProperties\" : false , \"required\" : [ \"id\" , \"name\" , \"type\" , \"options\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"default\" : { \"oneOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"number\" }, { \"type\" : \"boolean\" } ] }, \"type\" : { \"type\" : \"string\" , \"enum\" : [ \"select\" ] }, \"options\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"type\" : \"string\" } } } } ] }, \"stack\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" , \"space\" , \"vcs\" , \"vendor\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"description\" : { \"type\" : \"string\" }, \"labels\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"administrative\" : { \"type\" : \"boolean\" }, \"allow_promotion\" : { \"type\" : \"boolean\" }, \"auto_deploy\" : { \"type\" : \"boolean\" }, \"auto_retry\" : { \"type\" : \"boolean\" }, \"is_disabled\" : { \"type\" : \"boolean\" }, \"local_preview_enabled\" : { \"type\" : \"boolean\" }, \"protect_from_deletion\" : { \"type\" : \"boolean\" }, \"runner_image\" : { \"type\" : \"string\" }, \"space\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"worker_pool\" : { \"type\" : \"string\" }, \"attachments\" : { \"$ref\" : \"#/definitions/attachment\" }, \"environment\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"mounted_files\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/mounted_file\" } }, \"variables\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/variable\" } } } }, \"hooks\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"apply\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"init\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"plan\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"perform\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"destroy\" : { \"$ref\" : \"#/definitions/before_after_hook\" }, \"run\" : { \"$ref\" : \"#/definitions/after_hook\" } } }, \"schedules\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"drift\" : { \"$ref\" : \"#/definitions/drift_detection_schedule\" }, \"tasks\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/task_schedule\" } }, \"delete\" : { \"$ref\" : \"#/definitions/delete_schedule\" } } }, \"vcs\" : { \"type\" : \"object\" , \"oneOf\" : [ { \"additionalProperties\" : false , \"required\" : [ \"branch\" , \"provider\" , \"repository\" ], \"properties\" : { \"branch\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"project_root\" : { \"type\" : \"string\" }, \"provider\" : { \"type\" : \"string\" , \"enum\" : [ \"GITHUB\" , \"GITLAB\" , \"BITBUCKET_DATACENTER\" , \"BITBUCKET_CLOUD\" , \"GITHUB_ENTERPRISE\" , \"SHOWCASE\" , \"AZURE_DEVOPS\" ] }, \"namespace\" : { \"type\" : \"string\" }, \"repository\" : { \"type\" : \"string\" , \"minLength\" : 1 , \"description\" : \"The name of the repository.\" } } }, { \"additionalProperties\" : false , \"required\" : [ \"branch\" , \"provider\" , \"repository_url\" ], \"properties\" : { \"branch\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"project_root\" : { \"type\" : \"string\" }, \"provider\" : { \"type\" : \"string\" , \"enum\" : [ \"RAW_GIT\" ] }, \"repository\" : { \"type\" : \"string\" , \"description\" : \"The name of the repository. If not provided, it'll be extracted from the repository_url.\" }, \"namespace\" : { \"type\" : \"string\" , \"description\" : \"The namespace of the repository. If not provided, it'll be extracted from the repository_url.\" }, \"repository_url\" : { \"type\" : \"string\" , \"minLength\" : 1 , \"description\" : \"The URL of the repository. This is only used for the 'GIT' provider.\" } } } ] }, \"vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"ansible\" : { \"$ref\" : \"#/definitions/ansible_vendor\" }, \"cloudformation\" : { \"$ref\" : \"#/definitions/cloudformation_vendor\" }, \"kubernetes\" : { \"$ref\" : \"#/definitions/kubernetes_vendor\" }, \"pulumi\" : { \"$ref\" : \"#/definitions/pulumi_vendor\" }, \"terraform\" : { \"$ref\" : \"#/definitions/terraform_vendor\" } } } } }, \"attachment\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"contexts\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/context\" } }, \"clouds\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"aws\" : { \"$ref\" : \"#/definitions/aws_attachment\" }, \"azure\" : { \"$ref\" : \"#/definitions/azure_attachment\" } } }, \"policies\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } } }, \"aws_attachment\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"id\" , \"read\" , \"write\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"read\" : { \"type\" : \"boolean\" }, \"write\" : { \"type\" : \"boolean\" } } }, \"azure_attachment\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"id\" , \"read\" , \"write\" , \"subscription_id\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"read\" : { \"type\" : \"boolean\" }, \"write\" : { \"type\" : \"boolean\" }, \"subscription_id\" : { \"type\" : \"string\" } } }, \"context\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"id\" ], \"properties\" : { \"id\" : { \"type\" : \"string\" }, \"priority\" : { \"type\" : \"integer\" , \"minimum\" : 0 } } }, \"mounted_file\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"path\" , \"content\" ], \"properties\" : { \"path\" : { \"type\" : \"string\" }, \"content\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"secret\" : { \"type\" : \"boolean\" } } }, \"variable\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" , \"value\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"value\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"secret\" : { \"type\" : \"boolean\" } } }, \"after_hook\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"after\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"minLength\" : 1 } } }, \"before_after_hook\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"before\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"minLength\" : 1 }, \"after\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"minLength\" : 1 } } }, \"drift_detection_schedule\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"cron\" , \"reconcile\" ], \"properties\" : { \"cron\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/cron_schedule\" , \"maxLength\" : 1 } }, \"reconcile\" : { \"type\" : \"boolean\" }, \"ignore_state\" : { \"type\" : \"boolean\" }, \"timezone\" : { \"type\" : \"string\" } } }, \"task_schedule\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"command\" ], \"oneOf\" : [ { \"required\" : [ \"command\" , \"cron\" ] }, { \"required\" : [ \"command\" , \"timestamp_unix\" ] } ], \"properties\" : { \"command\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"cron\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/cron_schedule\" , \"minLength\" : 1 } }, \"timestamp_unix\" : { \"type\" : \"number\" , \"minimum\" : 1600000000 }, \"timezone\" : { \"type\" : \"string\" } } }, \"cron_schedule\" : { \"type\" : \"string\" , \"pattern\" : \"^(\\\\*|\\\\d+|\\\\d+-\\\\d+|\\\\d+\\\\/\\\\d+)(\\\\s+(\\\\*|\\\\d+|\\\\d+-\\\\d+|\\\\d+\\\\/\\\\d+)){4}$\" }, \"delete_schedule\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"timestamp_unix\" ], \"properties\" : { \"delete_resources\" : { \"type\" : \"boolean\" }, \"timestamp_unix\" : { \"type\" : \"number\" , \"minimum\" : 1600000000 } } }, \"ansible_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"playbook\" ], \"properties\" : { \"playbook\" : { \"type\" : \"string\" , \"minLength\" : 1 } } }, \"cloudformation_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"entry_template_file\" , \"template_bucket\" , \"stack_name\" , \"region\" ], \"properties\" : { \"entry_template_file\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"template_bucket\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"stack_name\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"region\" : { \"type\" : \"string\" , \"minLength\" : 1 } } }, \"kubernetes_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"namespace\" ], \"properties\" : { \"namespace\" : { \"type\" : \"string\" , \"minLength\" : 1 } } }, \"pulumi_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"stack_name\" , \"login_url\" ], \"properties\" : { \"stack_name\" : { \"type\" : \"string\" , \"minLength\" : 1 }, \"login_url\" : { \"type\" : \"string\" , \"minLength\" : 1 } } }, \"terraform_vendor\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"manage_state\" ], \"properties\" : { \"version\" : { \"type\" : \"string\" }, \"workspace\" : { \"type\" : \"string\" }, \"use_smart_sanitization\" : { \"type\" : \"boolean\" }, \"manage_state\" : { \"type\" : \"boolean\" } } }, \"options\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"trigger_run\" : { \"type\" : \"boolean\" } } } } }","title":"Schema"},{"location":"concepts/configuration/index.html","text":"Configuration \u00bb While Spacelift stacks typically link source code with infrastructure resources, it is often the broadly defined configuration that serves as the glue that's keeping everything together. These can be that access credentials, backend definitions or user-defined variables affecting the behavior of resource definitions found in the \"raw\" source code. This section focuses on three aspects of configuration, each of which warrants its own help article: Direct stack environment , that is environment variables and mounted files; Contexts , that is environments (often partially defined) shared between stacks and/or Terraform modules ; Runtime configuration as defined in the .spacelift/config.yml file; A general note on precedence \u00bb Some configuration settings can be defined on multiple levels. If they're over-defined (the same setting is defined multiple times), the end result will depend on generic rules of precedence. These rules will be the same for all applicable settings: stack-specific runtime configuration will take the highest precedence; configuration defined directly (either through the environment , or settings ) on the stack will go second; common runtime configuration will go next; anything defined at the context level will take the lowest precedence - furthermore, contexts can be attached with a priority level further defining the exact precedence;","title":"Configuration"},{"location":"concepts/configuration/index.html#configuration","text":"While Spacelift stacks typically link source code with infrastructure resources, it is often the broadly defined configuration that serves as the glue that's keeping everything together. These can be that access credentials, backend definitions or user-defined variables affecting the behavior of resource definitions found in the \"raw\" source code. This section focuses on three aspects of configuration, each of which warrants its own help article: Direct stack environment , that is environment variables and mounted files; Contexts , that is environments (often partially defined) shared between stacks and/or Terraform modules ; Runtime configuration as defined in the .spacelift/config.yml file;","title":"Configuration"},{"location":"concepts/configuration/index.html#a-general-note-on-precedence","text":"Some configuration settings can be defined on multiple levels. If they're over-defined (the same setting is defined multiple times), the end result will depend on generic rules of precedence. These rules will be the same for all applicable settings: stack-specific runtime configuration will take the highest precedence; configuration defined directly (either through the environment , or settings ) on the stack will go second; common runtime configuration will go next; anything defined at the context level will take the lowest precedence - furthermore, contexts can be attached with a priority level further defining the exact precedence;","title":"A general note on precedence"},{"location":"concepts/configuration/context.html","text":"Context \u00bb Introduction \u00bb On a high level, context is a bundle of configuration elements ( environment variables and mounted files ) independent of any stack that can be managed separately and attached to as many or as few stacks as necessary. Contexts are only directly accessible to administrators from the account view: The list of contexts merely shows the name and description of the context. Clicking on the name allows you to edit it . Management \u00bb Managing a context is quite straightforward - you can create , edit , attach, detach and delete it. The below paragraphs will focus on doing that through the web GUI but doing it programmatically using our Terraform provider is an attractive alternative. Creating \u00bb As an account administrator you can create a new context from the Contexts screen as seen on the above screenshot by pressing the Add context button: This takes you to a simple form where the only inputs are name and description: The required name is what you'll see in the context list and in the dropdown when attaching the context. Make sure that it's informative enough to be able to immediately communicate the purpose of the context, but short enough so that it fits nicely in the dropdown, and no important information is cut off. The optional description is completely free-form and it supports Markdown . This is a good place perhaps for a thorough explanation of the purpose of the stack, perhaps a link or two, and/or a funny GIF. In the web GUI this description will only show on the Contexts screen so it's not a big deal anyway. Warning Based on the original name , Spacelift generates an immutable slug that serves as a unique identifier of this context. If the name and the slug diverge significantly, things may become confusing. So even though you can change the context name at any point, we strongly discourage all non-trivial changes. Editing \u00bb Editing the context is only a little more exciting. You can edit the context from its dedicated view by pressing the Edit button: This switches the context into editing mode where you can change the name and description but also manage configuration elements the same way you'd do for the stack environment , only much simpler - without overrides and computed values : Attaching and detaching \u00bb Attaching and detaching contexts actually happens from the stack management view. To attach a context, select the Contexts tab. This should show you a dropdown with all the contexts available for attaching, and a slider to set the priority of the attachment : Info A context can only be attached once to a given stack, so if it's already attached, it will not be visible in the dropdown menu. OK, let's attach the context with priority 0 and see what gives: Now this attached context will also contribute to the stack environment ... ...and be visible on the list of attached contexts: In order to detach the context, you can just press the Detach button and the context will stop contributing to the stack's environment : A note on priority \u00bb You may be wondering what the priority slider is for. A priority is a property of context-stack relationship - in fact, the only property. All the contexts attached to a stack are sorted by priority (lowest first), though values don't need to be unique. This ordering establishes precedence rules between contexts should there be a conflict and multiple contexts define the same value. Deleting \u00bb Deleting a context is straightforward - by pressing the Delete button in the context view you can get rid of an unnecessary context: Warning Deleting a context will also automatically detach it from all the stacks it was attached to. Make sure you only delete contexts that are no longer useful. For security purposes we do not store historical stuff and actually remove the deleted data from all of our data storage systems. Use cases \u00bb We can see two main use cases for contexts, depending on whether the context data is supplied externally or produced by Spacelift . Shared setup \u00bb If the data is external to Spacelift, it's likely that this is a form of shared setup - that is, configuration elements that are common to multiple stacks, and grouped as a context for convenience. One example of this use case is cloud provider configuration, either for Terraform or Pulumi. Instead of attaching the same values - some of them probably secret and pretty sensitive - to individual stacks, contexts allow you to define those once and then have admins attach them to the stacks that need them. A variation of this use case is collections of Terraform input variables that may be shared by multiple stacks - for example things relating to a particular system environment (staging, production etc). In this case the collection of variables can specify things like environment name, DNS domain name or a reference to it (eg. zone ID), tags, references to provider accounts and similar settings. Again, instead of setting these on individual stacks, an admin can group them into a context, and attach to the eligible stacks. Remote state alternative (Terraform-specific) \u00bb If the data in the context is produced by one or more Spacelift stacks, contexts can be an attractive alternative to the Terraform remote state . In this use case, contexts can serve as outputs for stacks that can be consumed by (attached to) other stacks. So, instead of exposing the entire state, a stack can use Spacelift Terraform provider to define values on a context - either managed by the same stack , or managed externally. Managing a context externally can be particularly useful when multiple stacks contribute to a particular context. Info In order to use the Terraform provider to define contexts or its configuration elements the stack has to be marked as administrative . As an example of one such use case, let's imagine an organization where shared infrastructure (VPC, DNS, compute cluster etc.) is centrally managed by a DevOps team, which exposes it as a service to be used by individual product development teams. In order to be able to use the shared infrastructure, each team needs to address multiple entities that are generated by the central infra repo. In vanilla Terraform one would likely use remote state provider, but that might expose secrets and settings the DevOps team would rather keep it to themselves. Using a context on the other hand allows the team to decide (and hopefully document) what constitutes their \"external API\". The proposed setup for the above use case would involve two administrative stacks - one to manage all the stacks, and the other for the DevOps team. The management stack would programmatically define the DevOps one, and possibly also its context. The DevOps team would receive the context ID as an input variable, and use it to expose outputs as spacelift_environment_variable and/or spacelift_mounted_file resources. The management stack could then simply attach the context populated by the DevOps stack to other stacks it defines and manages. Extending Terraform CLI Configuration (Terraform-specific) \u00bb For some of our Terraform users, a convenient way to configure the Terraform CLI behavior is through customizing the ~/.terraformrc file. Spacelift allows you to extend terraform CLI configuration through the use of mounted files .","title":"Context"},{"location":"concepts/configuration/context.html#context","text":"","title":"Context"},{"location":"concepts/configuration/context.html#introduction","text":"On a high level, context is a bundle of configuration elements ( environment variables and mounted files ) independent of any stack that can be managed separately and attached to as many or as few stacks as necessary. Contexts are only directly accessible to administrators from the account view: The list of contexts merely shows the name and description of the context. Clicking on the name allows you to edit it .","title":"Introduction"},{"location":"concepts/configuration/context.html#management","text":"Managing a context is quite straightforward - you can create , edit , attach, detach and delete it. The below paragraphs will focus on doing that through the web GUI but doing it programmatically using our Terraform provider is an attractive alternative.","title":"Management"},{"location":"concepts/configuration/context.html#creating","text":"As an account administrator you can create a new context from the Contexts screen as seen on the above screenshot by pressing the Add context button: This takes you to a simple form where the only inputs are name and description: The required name is what you'll see in the context list and in the dropdown when attaching the context. Make sure that it's informative enough to be able to immediately communicate the purpose of the context, but short enough so that it fits nicely in the dropdown, and no important information is cut off. The optional description is completely free-form and it supports Markdown . This is a good place perhaps for a thorough explanation of the purpose of the stack, perhaps a link or two, and/or a funny GIF. In the web GUI this description will only show on the Contexts screen so it's not a big deal anyway. Warning Based on the original name , Spacelift generates an immutable slug that serves as a unique identifier of this context. If the name and the slug diverge significantly, things may become confusing. So even though you can change the context name at any point, we strongly discourage all non-trivial changes.","title":"Creating"},{"location":"concepts/configuration/context.html#editing","text":"Editing the context is only a little more exciting. You can edit the context from its dedicated view by pressing the Edit button: This switches the context into editing mode where you can change the name and description but also manage configuration elements the same way you'd do for the stack environment , only much simpler - without overrides and computed values :","title":"Editing"},{"location":"concepts/configuration/context.html#attaching-and-detaching","text":"Attaching and detaching contexts actually happens from the stack management view. To attach a context, select the Contexts tab. This should show you a dropdown with all the contexts available for attaching, and a slider to set the priority of the attachment : Info A context can only be attached once to a given stack, so if it's already attached, it will not be visible in the dropdown menu. OK, let's attach the context with priority 0 and see what gives: Now this attached context will also contribute to the stack environment ... ...and be visible on the list of attached contexts: In order to detach the context, you can just press the Detach button and the context will stop contributing to the stack's environment :","title":"Attaching and detaching"},{"location":"concepts/configuration/context.html#a-note-on-priority","text":"You may be wondering what the priority slider is for. A priority is a property of context-stack relationship - in fact, the only property. All the contexts attached to a stack are sorted by priority (lowest first), though values don't need to be unique. This ordering establishes precedence rules between contexts should there be a conflict and multiple contexts define the same value.","title":"A note on priority"},{"location":"concepts/configuration/context.html#deleting","text":"Deleting a context is straightforward - by pressing the Delete button in the context view you can get rid of an unnecessary context: Warning Deleting a context will also automatically detach it from all the stacks it was attached to. Make sure you only delete contexts that are no longer useful. For security purposes we do not store historical stuff and actually remove the deleted data from all of our data storage systems.","title":"Deleting"},{"location":"concepts/configuration/context.html#use-cases","text":"We can see two main use cases for contexts, depending on whether the context data is supplied externally or produced by Spacelift .","title":"Use cases"},{"location":"concepts/configuration/context.html#shared-setup","text":"If the data is external to Spacelift, it's likely that this is a form of shared setup - that is, configuration elements that are common to multiple stacks, and grouped as a context for convenience. One example of this use case is cloud provider configuration, either for Terraform or Pulumi. Instead of attaching the same values - some of them probably secret and pretty sensitive - to individual stacks, contexts allow you to define those once and then have admins attach them to the stacks that need them. A variation of this use case is collections of Terraform input variables that may be shared by multiple stacks - for example things relating to a particular system environment (staging, production etc). In this case the collection of variables can specify things like environment name, DNS domain name or a reference to it (eg. zone ID), tags, references to provider accounts and similar settings. Again, instead of setting these on individual stacks, an admin can group them into a context, and attach to the eligible stacks.","title":"Shared setup"},{"location":"concepts/configuration/context.html#remote-state-alternative-terraform-specific","text":"If the data in the context is produced by one or more Spacelift stacks, contexts can be an attractive alternative to the Terraform remote state . In this use case, contexts can serve as outputs for stacks that can be consumed by (attached to) other stacks. So, instead of exposing the entire state, a stack can use Spacelift Terraform provider to define values on a context - either managed by the same stack , or managed externally. Managing a context externally can be particularly useful when multiple stacks contribute to a particular context. Info In order to use the Terraform provider to define contexts or its configuration elements the stack has to be marked as administrative . As an example of one such use case, let's imagine an organization where shared infrastructure (VPC, DNS, compute cluster etc.) is centrally managed by a DevOps team, which exposes it as a service to be used by individual product development teams. In order to be able to use the shared infrastructure, each team needs to address multiple entities that are generated by the central infra repo. In vanilla Terraform one would likely use remote state provider, but that might expose secrets and settings the DevOps team would rather keep it to themselves. Using a context on the other hand allows the team to decide (and hopefully document) what constitutes their \"external API\". The proposed setup for the above use case would involve two administrative stacks - one to manage all the stacks, and the other for the DevOps team. The management stack would programmatically define the DevOps one, and possibly also its context. The DevOps team would receive the context ID as an input variable, and use it to expose outputs as spacelift_environment_variable and/or spacelift_mounted_file resources. The management stack could then simply attach the context populated by the DevOps stack to other stacks it defines and manages.","title":"Remote state alternative (Terraform-specific)"},{"location":"concepts/configuration/context.html#extending-terraform-cli-configuration-terraform-specific","text":"For some of our Terraform users, a convenient way to configure the Terraform CLI behavior is through customizing the ~/.terraformrc file. Spacelift allows you to extend terraform CLI configuration through the use of mounted files .","title":"Extending Terraform CLI Configuration (Terraform-specific)"},{"location":"concepts/configuration/environment.html","text":"Environment \u00bb If you take a look at the Environment screen of a stack you will notice it's pretty busy - in fact it's the second busiest view in Spacelift ( run being the undisputed winner). Ultimately though, all the records here are either environment variables or mounted files . The main part of the view represents the synthetic outcome determining what your run will \"see\" when executed. If this does not make sense yet, please hang on and read the remainder of this article. Environment variables \u00bb The concept of environment variables is instinctively understood by all programmers. It's represented as a key-value mapping available to all processes running in a given environment. Both with Pulumi and Terraform, environment variables are frequently used to configure providers. Additionally, when prefixed with TF_VAR_ they are used in Terraform to use environment variables as Terraform input variables . Info Spacelift does not provide a dedicated mechanism of defining Terraform input variables because the combination of TF_VAR_ environment variables and mounted files should cover all use cases without the need to introduce an extra entity. Adding an environment variable is rather straightforward - don't worry yet about the visibility (difference between plain and secret variables). This is described in a separate section : ...and so is editing: Environment variable interpolation \u00bb Note that environment variables can refer to other environment variables using simple interpolation. For example, if you have an environment variable FOO with a value of bar you can use it to define another environment variable BAZ as ${FOO}-baz which will result in bar-baz being set as the value of BAZ . This interpolation is lazily and dynamically evaluated on the worker, and will work between environment variables defined in different ways, including contexts . Computed values \u00bb You will possibly notice some environment variables being marked as <computed> , which means that their value is only computed at runtime. These are not directly set on the stack but come from various integrations - for example, AWS credentials ( AWS_ACCESS_KEY_ID and friends) are set by the AWS integration and SPACELIFT_API_TOKEN is injected into each run to serve a number of purposes. You cannot set a computed value but you can override it - that is, explicitly set an environment variable on a stack that has the same name as the variable that comes from integration. This is due to precedence rules that warrant its own dedicated section . Overriding a computed value is almost like editing a regular stack variable, although worth noticing is Override replacing Edit and the lack of Delete action: When you click Override , you can replace the value computed at runtime with a static one: Note how it becomes a regular write-only variable upon saving: If you delete this variable, it will again be replaced by the computed one. If you want to get rid of the computed variable entirely, you will need to disable the integration that originally led to its inclusion in this list. Spacelift environment \u00bb The Spacelift environment section lists a special subset of computed values that are injected into each run and that provide some Spacelift-specific metadata about the context of the job being executed. These are prefixed so that they can be used directly as input variables to Terraform configuration, and their names always clearly suggest the content: Info Unless you know exactly what you're doing, we generally discourage overriding these dynamic variables, to avoid confusion. Per-stage environment variables \u00bb The Spacelift flow can be broken down into a number of stages - most importantly: Initializing , where we prepare the workspace; Planning , which calculates the changes; Applying , which makes the actual changes; In this model, only the Applying phase makes any actual changes to your resources and your state and needs the credentials that support it. Yet frequently, the practice is to pass the same credentials to all stages. The reason for that is either the lack of awareness or - more often - the limitations in the tooling. Depending on your flow, this may be a potential security issue because even if you manually review every job before it reaches the Applying stage, the Planning phase can do a lot of damage . Spacelift supports a more security-conscious approach by allowing users to define variables that are passed to read (in practice, everything except for Applying ) and write stages. By default, we pass an environment variable to all stages, but prefixes can be used to change the default behavior. An environment variable whose name starts with the ro_ prefix is only passed to read stages but not to the write ( Applying ) stage. On the other hand, an environment variable whose name starts with the wo_ prefix is only passed to the write ( Applying ) stage but not to the read ones. Combining the two prefixes makes it easy to create flows that limit the exposure of admin credentials to the code that has been thoroughly reviewed. The example below uses a GITHUB_TOKEN environment variable used by the GitHub Terraform provider variable split into two separate environment variables: The first token will potentially be exposed to less-trusted code, so it makes sense to create it with read-only permissions. The second token on the other hand will only be exposed to the reviewed code and can be given write or admin permissions. A similar approach can be used for AWS, GCP, Azure, or any other cloud provider credentials. Mounted files \u00bb Every now and then an environment variable is not what you need - you need a file instead. Terraform Kubernetes provider is a great example - one of the common ways of configuring it involves setting a KUBECONFIG variable pointing to the actual config file which needs to be present in your workspace as well. It's almost like creating an environment variable, though instead of typing (or pasting) the value you'll be uploading a file: Info Notice how you can give your file a name that's different to the name of the uploaded entity. In fact, you can use / characters in the file path to nest it deeper in directory tree - for example a/b/c/d/e.json is a perfectly valid file path. Similar to environment variables, mounted files can have different visibility settings - you can learn more about it here . One thing to note here is that plaintext files can be downloaded back straight from the UI or API while secret ones will only be visible to the run executed for the stack . Info Mounted files are limited to 2 MB in size. If you need to inject larger files into your workspace, we suggest that you make them part of the Docker runner image , or retrieve them dynamically using something like wget or curl . Project structure \u00bb When discussing mounted files, it is important to understand the structure of the Spacelift workspace. Every Spacelift workload gets a dedicated directory /mnt/workspace/ , which also serves as a root for all the mounted files. Your Git repository is cloned into /mnt/workspace/source/ , which also serves as the working directory for your project, unless explicitly overridden by the project root configuration setting (either on the stack level or on in the runtime configuration ). Warning Mounted files may be put into /mnt/workspace/source/ as well and it's a legitimate use case, for example, to dynamically inject backend settings or even add extra infra definitions. Just beware of path clashes as mounted files will override your project source code in case of conflict. Sometimes this is what you want, sometimes not. Attached contexts \u00bb While contexts are important enough to warrant their own dedicated article , it's also crucial to understand how they interact with environment variables and mounted files set directly on the stack , as well as with computed values . Perhaps you've noticed the blue labels on one of the earlier screenshots. If you haven't, here they are again, with a proper highlight: The highlighted label is the name of the attached context that supplies those values. The sorted list of attached contexts is located below the calculated environment view, and each entry can be unfurled to see its exact content. Similar to computed values , those coming from contexts can also be overridden. Here's an example: Note how we can now Delete the variable - this would revert it to the value defined by the context. Contexts can both provide environment variables as well as mounted files , and both can be overridden directly on the stack. Info If you want to get rid of the context-provided variable or file entirely, you will need to detach the context itself . A note on visibility \u00bb Perhaps you may have noticed how environment variables and mounted files come in two flavors - plain and secret . Here they are in the form for the new environment variable: ...and here they are in the form for the new mounted file: Functionally, the difference between the two is pretty simple - plain values are accessible in the web GUI and through the API , and secret ones aren't - they're only made available to Runs and Tasks . Here's an example of two environment variables in the GUI - one plain, and one secret (also referred to as write-only ): Mounted files are similar - plain can be downloaded from the web GUI or through the API , and secret can't. Here's the difference in the GUI: While the content of secret (write-only) environment variables and mounted files is not accessible through the GUI or API , the checksums are always available so if you have the value handy and just want to check if that's the same value as the one set in Spacelift, you can compare its checksum with the one reported by us - check out the most recent GraphQL API schema for more details. Info Though all of our data is encrypted both at rest and in transit , secret (write-only) values enjoy two extra layers of protection.","title":"Environment"},{"location":"concepts/configuration/environment.html#environment","text":"If you take a look at the Environment screen of a stack you will notice it's pretty busy - in fact it's the second busiest view in Spacelift ( run being the undisputed winner). Ultimately though, all the records here are either environment variables or mounted files . The main part of the view represents the synthetic outcome determining what your run will \"see\" when executed. If this does not make sense yet, please hang on and read the remainder of this article.","title":"Environment"},{"location":"concepts/configuration/environment.html#environment-variables","text":"The concept of environment variables is instinctively understood by all programmers. It's represented as a key-value mapping available to all processes running in a given environment. Both with Pulumi and Terraform, environment variables are frequently used to configure providers. Additionally, when prefixed with TF_VAR_ they are used in Terraform to use environment variables as Terraform input variables . Info Spacelift does not provide a dedicated mechanism of defining Terraform input variables because the combination of TF_VAR_ environment variables and mounted files should cover all use cases without the need to introduce an extra entity. Adding an environment variable is rather straightforward - don't worry yet about the visibility (difference between plain and secret variables). This is described in a separate section : ...and so is editing:","title":"Environment variables"},{"location":"concepts/configuration/environment.html#environment-variable-interpolation","text":"Note that environment variables can refer to other environment variables using simple interpolation. For example, if you have an environment variable FOO with a value of bar you can use it to define another environment variable BAZ as ${FOO}-baz which will result in bar-baz being set as the value of BAZ . This interpolation is lazily and dynamically evaluated on the worker, and will work between environment variables defined in different ways, including contexts .","title":"Environment variable interpolation"},{"location":"concepts/configuration/environment.html#computed-values","text":"You will possibly notice some environment variables being marked as <computed> , which means that their value is only computed at runtime. These are not directly set on the stack but come from various integrations - for example, AWS credentials ( AWS_ACCESS_KEY_ID and friends) are set by the AWS integration and SPACELIFT_API_TOKEN is injected into each run to serve a number of purposes. You cannot set a computed value but you can override it - that is, explicitly set an environment variable on a stack that has the same name as the variable that comes from integration. This is due to precedence rules that warrant its own dedicated section . Overriding a computed value is almost like editing a regular stack variable, although worth noticing is Override replacing Edit and the lack of Delete action: When you click Override , you can replace the value computed at runtime with a static one: Note how it becomes a regular write-only variable upon saving: If you delete this variable, it will again be replaced by the computed one. If you want to get rid of the computed variable entirely, you will need to disable the integration that originally led to its inclusion in this list.","title":"Computed values"},{"location":"concepts/configuration/environment.html#spacelift-environment","text":"The Spacelift environment section lists a special subset of computed values that are injected into each run and that provide some Spacelift-specific metadata about the context of the job being executed. These are prefixed so that they can be used directly as input variables to Terraform configuration, and their names always clearly suggest the content: Info Unless you know exactly what you're doing, we generally discourage overriding these dynamic variables, to avoid confusion.","title":"Spacelift environment"},{"location":"concepts/configuration/environment.html#per-stage-environment-variables","text":"The Spacelift flow can be broken down into a number of stages - most importantly: Initializing , where we prepare the workspace; Planning , which calculates the changes; Applying , which makes the actual changes; In this model, only the Applying phase makes any actual changes to your resources and your state and needs the credentials that support it. Yet frequently, the practice is to pass the same credentials to all stages. The reason for that is either the lack of awareness or - more often - the limitations in the tooling. Depending on your flow, this may be a potential security issue because even if you manually review every job before it reaches the Applying stage, the Planning phase can do a lot of damage . Spacelift supports a more security-conscious approach by allowing users to define variables that are passed to read (in practice, everything except for Applying ) and write stages. By default, we pass an environment variable to all stages, but prefixes can be used to change the default behavior. An environment variable whose name starts with the ro_ prefix is only passed to read stages but not to the write ( Applying ) stage. On the other hand, an environment variable whose name starts with the wo_ prefix is only passed to the write ( Applying ) stage but not to the read ones. Combining the two prefixes makes it easy to create flows that limit the exposure of admin credentials to the code that has been thoroughly reviewed. The example below uses a GITHUB_TOKEN environment variable used by the GitHub Terraform provider variable split into two separate environment variables: The first token will potentially be exposed to less-trusted code, so it makes sense to create it with read-only permissions. The second token on the other hand will only be exposed to the reviewed code and can be given write or admin permissions. A similar approach can be used for AWS, GCP, Azure, or any other cloud provider credentials.","title":"Per-stage environment variables"},{"location":"concepts/configuration/environment.html#mounted-files","text":"Every now and then an environment variable is not what you need - you need a file instead. Terraform Kubernetes provider is a great example - one of the common ways of configuring it involves setting a KUBECONFIG variable pointing to the actual config file which needs to be present in your workspace as well. It's almost like creating an environment variable, though instead of typing (or pasting) the value you'll be uploading a file: Info Notice how you can give your file a name that's different to the name of the uploaded entity. In fact, you can use / characters in the file path to nest it deeper in directory tree - for example a/b/c/d/e.json is a perfectly valid file path. Similar to environment variables, mounted files can have different visibility settings - you can learn more about it here . One thing to note here is that plaintext files can be downloaded back straight from the UI or API while secret ones will only be visible to the run executed for the stack . Info Mounted files are limited to 2 MB in size. If you need to inject larger files into your workspace, we suggest that you make them part of the Docker runner image , or retrieve them dynamically using something like wget or curl .","title":"Mounted files"},{"location":"concepts/configuration/environment.html#project-structure","text":"When discussing mounted files, it is important to understand the structure of the Spacelift workspace. Every Spacelift workload gets a dedicated directory /mnt/workspace/ , which also serves as a root for all the mounted files. Your Git repository is cloned into /mnt/workspace/source/ , which also serves as the working directory for your project, unless explicitly overridden by the project root configuration setting (either on the stack level or on in the runtime configuration ). Warning Mounted files may be put into /mnt/workspace/source/ as well and it's a legitimate use case, for example, to dynamically inject backend settings or even add extra infra definitions. Just beware of path clashes as mounted files will override your project source code in case of conflict. Sometimes this is what you want, sometimes not.","title":"Project structure"},{"location":"concepts/configuration/environment.html#attached-contexts","text":"While contexts are important enough to warrant their own dedicated article , it's also crucial to understand how they interact with environment variables and mounted files set directly on the stack , as well as with computed values . Perhaps you've noticed the blue labels on one of the earlier screenshots. If you haven't, here they are again, with a proper highlight: The highlighted label is the name of the attached context that supplies those values. The sorted list of attached contexts is located below the calculated environment view, and each entry can be unfurled to see its exact content. Similar to computed values , those coming from contexts can also be overridden. Here's an example: Note how we can now Delete the variable - this would revert it to the value defined by the context. Contexts can both provide environment variables as well as mounted files , and both can be overridden directly on the stack. Info If you want to get rid of the context-provided variable or file entirely, you will need to detach the context itself .","title":"Attached contexts"},{"location":"concepts/configuration/environment.html#a-note-on-visibility","text":"Perhaps you may have noticed how environment variables and mounted files come in two flavors - plain and secret . Here they are in the form for the new environment variable: ...and here they are in the form for the new mounted file: Functionally, the difference between the two is pretty simple - plain values are accessible in the web GUI and through the API , and secret ones aren't - they're only made available to Runs and Tasks . Here's an example of two environment variables in the GUI - one plain, and one secret (also referred to as write-only ): Mounted files are similar - plain can be downloaded from the web GUI or through the API , and secret can't. Here's the difference in the GUI: While the content of secret (write-only) environment variables and mounted files is not accessible through the GUI or API , the checksums are always available so if you have the value handy and just want to check if that's the same value as the one set in Spacelift, you can compare its checksum with the one reported by us - check out the most recent GraphQL API schema for more details. Info Though all of our data is encrypted both at rest and in transit , secret (write-only) values enjoy two extra layers of protection.","title":"A note on visibility"},{"location":"concepts/configuration/runtime-configuration/index.html","text":"Runtime configuration \u00bb The runtime configuration is an optional setup applied to individual runs instead of being global to the stack. It's defined in .spacelift/config.yml YAML file at the root of your repository. A single file is used to define settings for all stacks associated with its host Git repository, so the file structure looks like this: .spacelift/config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 version : \"1\" stack_defaults : runner_image : your/first:runner # Note that tflint is not installed by # default - this example assumes that your # runner image has this available. before_init : - terraform fmt -check - tflint # Note that every field in the configuration is # optional, and has a reasonable default. This file # allows you to override those defaults, and you can # merely override individual fields. stacks : # The key of is the immutable slug of your stack # which you will find in the URL. babys-first-stack : &shared before_apply : - hostname project_root : infra terraform_version : 0.12.4 babys-second-stack : << : *shared terraform_version : 0.13.0 environment : AWS_REGION : eu-west-1 The top level of the file contains three keys - version which in practice is currently ignored but may be useful in the future, stacks containing a mapping of immutable stack id to the stack configuration block and stack_defaults , containing the defaults common to all stacks using this source code repository. Note that corresponding stack-specific settings will override any stack defaults. Considering the precedence of settings, below is the order that will be followed, starting from the most important to the least important: The configuration for a specified stack defined in config.yml The stack configuration set in the Spacelift UI. The stack defaults defined config.yml In cases where there is no stack slug defined in the config, only the first two sources are considered: The stack configuration set in the Spacelift UI The stack defaults defined in config.yml Info Since we adopted everyone's favorite data serialization format, you can use all the YAML shenanigans you can think of - things like anchors and inline JSON can keep your config DRY and neat. Purpose of runtime configuration \u00bb The whole concept of runtime configuration may initially sound unnecessary, but it ultimately allows flexibility that would otherwise be hard to achieve. In general, its purpose is to preview effects of changes not related to the source code (eg. Terraform or Pulumi version upgrades, variable changes etc.), before they become an established part of your infra. While stack environment applies both to tracked and non-tracked branches, a runtime configuration change can be pushed to a feature branch, which triggers proposed runs allowing you to preview the changes before they have a chance to affect your state. Info If the runtime configuration file is not present or does not contain your stack, default values are used - refer to each setting for its respective default. Stacks configuration block \u00bb before_ and after_ hooks \u00bb Info Each collection defaults to an empty array . These scripts allow customizing the Spacelift workflow - see the relevant documentation here . The following are available: before_init after_init before_plan after_plan before_apply after_apply before_perform after_perform before_destroy after_destroy after_run environment map \u00bb Info Defaults to an empty map . The environment allows you to declaratively pass some environment variables to the runtime configuration of the Stack. In case of a conflict, these variables will override both the ones passed via attached Contexts and those directly set in Stack's environment . project_root setting \u00bb Info Defaults to an empty string , pointing to the working directory for the run. Project root is the path of your project directory inside the Hub repository. You can use this setting to point Spacelift to the right place if the repo contains source code for multiple stacks in various folders or serves multiple purposes like those increasingly popular monorepos combining infrastructure definitions with source code, potentially even for multiple applications. runner_image setting \u00bb Info Defaults to public.ecr.aws/spacelift/runner-terraform:latest The runner image is the Docker image used to run your workloads. By making it a runtime setting, Spacelift allows testing the image before it modifies your infrastructure. terraform_version setting \u00bb Info Defaults to the latest known supported Terraform version. This setting is only valid on Terraform stacks and specifies the Terraform version that the run will use. The main use case is testing a newer version of Terraform before you use it to change the state since the way back is very hard. This version can only be equal to or higher than the one already used to apply state changes. For more details on Terraform version management, please refer to its dedicated help section . Pulumi version management is based on Docker images .","title":"Runtime configuration"},{"location":"concepts/configuration/runtime-configuration/index.html#runtime-configuration","text":"The runtime configuration is an optional setup applied to individual runs instead of being global to the stack. It's defined in .spacelift/config.yml YAML file at the root of your repository. A single file is used to define settings for all stacks associated with its host Git repository, so the file structure looks like this: .spacelift/config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 version : \"1\" stack_defaults : runner_image : your/first:runner # Note that tflint is not installed by # default - this example assumes that your # runner image has this available. before_init : - terraform fmt -check - tflint # Note that every field in the configuration is # optional, and has a reasonable default. This file # allows you to override those defaults, and you can # merely override individual fields. stacks : # The key of is the immutable slug of your stack # which you will find in the URL. babys-first-stack : &shared before_apply : - hostname project_root : infra terraform_version : 0.12.4 babys-second-stack : << : *shared terraform_version : 0.13.0 environment : AWS_REGION : eu-west-1 The top level of the file contains three keys - version which in practice is currently ignored but may be useful in the future, stacks containing a mapping of immutable stack id to the stack configuration block and stack_defaults , containing the defaults common to all stacks using this source code repository. Note that corresponding stack-specific settings will override any stack defaults. Considering the precedence of settings, below is the order that will be followed, starting from the most important to the least important: The configuration for a specified stack defined in config.yml The stack configuration set in the Spacelift UI. The stack defaults defined config.yml In cases where there is no stack slug defined in the config, only the first two sources are considered: The stack configuration set in the Spacelift UI The stack defaults defined in config.yml Info Since we adopted everyone's favorite data serialization format, you can use all the YAML shenanigans you can think of - things like anchors and inline JSON can keep your config DRY and neat.","title":"Runtime configuration"},{"location":"concepts/configuration/runtime-configuration/index.html#purpose-of-runtime-configuration","text":"The whole concept of runtime configuration may initially sound unnecessary, but it ultimately allows flexibility that would otherwise be hard to achieve. In general, its purpose is to preview effects of changes not related to the source code (eg. Terraform or Pulumi version upgrades, variable changes etc.), before they become an established part of your infra. While stack environment applies both to tracked and non-tracked branches, a runtime configuration change can be pushed to a feature branch, which triggers proposed runs allowing you to preview the changes before they have a chance to affect your state. Info If the runtime configuration file is not present or does not contain your stack, default values are used - refer to each setting for its respective default.","title":"Purpose of runtime configuration"},{"location":"concepts/configuration/runtime-configuration/index.html#stacks-configuration-block","text":"","title":"Stacks configuration block"},{"location":"concepts/configuration/runtime-configuration/index.html#before_-and-after_-hooks","text":"Info Each collection defaults to an empty array . These scripts allow customizing the Spacelift workflow - see the relevant documentation here . The following are available: before_init after_init before_plan after_plan before_apply after_apply before_perform after_perform before_destroy after_destroy after_run","title":"before_ and after_ hooks"},{"location":"concepts/configuration/runtime-configuration/index.html#environment-map","text":"Info Defaults to an empty map . The environment allows you to declaratively pass some environment variables to the runtime configuration of the Stack. In case of a conflict, these variables will override both the ones passed via attached Contexts and those directly set in Stack's environment .","title":"environment map"},{"location":"concepts/configuration/runtime-configuration/index.html#project_root-setting","text":"Info Defaults to an empty string , pointing to the working directory for the run. Project root is the path of your project directory inside the Hub repository. You can use this setting to point Spacelift to the right place if the repo contains source code for multiple stacks in various folders or serves multiple purposes like those increasingly popular monorepos combining infrastructure definitions with source code, potentially even for multiple applications.","title":"project_root setting"},{"location":"concepts/configuration/runtime-configuration/index.html#runner_image-setting","text":"Info Defaults to public.ecr.aws/spacelift/runner-terraform:latest The runner image is the Docker image used to run your workloads. By making it a runtime setting, Spacelift allows testing the image before it modifies your infrastructure.","title":"runner_image setting"},{"location":"concepts/configuration/runtime-configuration/index.html#terraform_version-setting","text":"Info Defaults to the latest known supported Terraform version. This setting is only valid on Terraform stacks and specifies the Terraform version that the run will use. The main use case is testing a newer version of Terraform before you use it to change the state since the way back is very hard. This version can only be equal to or higher than the one already used to apply state changes. For more details on Terraform version management, please refer to its dedicated help section . Pulumi version management is based on Docker images .","title":"terraform_version setting"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html","text":"YAML reference \u00bb This document is a reference for the Spacelift configuration keys that are used in the .spacelift/config.yml file to configure one or more Stacks . Warning The .spacelift/config.yml file must be located at the root of your repository, not at the project root . Stack settings \u00bb version stack_defaults stacks Module settings \u00bb version module_version test_defaults tests version \u00bb The version property is optional and currently ignored but for the sake of completeness you may want to set the value to \"1\". stack_defaults \u00bb stack_defaults represent default settings that will apply to every stack defined in the id-settings map. Any default setting is overridden by an explicitly set stack-specific value. Key Required Type Description after_apply N list<string> List of commands executed after applying changes . after_destroy N list<string> List of commands executed after destroying managed resources. after_init N list<string> List of commands executed after first interacting with the backend (eg. terraform init). after_perform N list<string> List of commands executed after performing a custom task after_plan N list<string> List of commands executed after planning changes after_run N list<string> List of commands executed after every run, regardless of its outcome before_apply N list<string> List of commands executed before applying changes . before_destroy N list<string> List of commands executed before destroying managed resources. before_init N list<string> List of commands executed before first interacting with the backend (eg. terraform init ). before_perform N list<string> List of commands executed before performing a custom task before_plan N list<string> List of commands executed before planning changes environment N map<string, string> Map of extra environment variables and their values passed to the job project_root N string Optional folder inside the repository serving as the root of your stack runner_image N string Name of the custom runner image , if any terraform_version N string For Terraform stacks, Terraform version number to be used stacks \u00bb The stacks section is a map using stack public ID (slug) as keys and stack settings - described in this section - as values. If you're using this mapping together with stack defaults, note that any default setting is overridden by an explicitly set stack-specific value. This is particularly important for list and map fields where one may assume that these are merged. In practice, they're not merged - they're replaced. If you want merging semantics, YAML provides native methods to merge arrays and maps . module_version \u00bb Module version is a required string value that must conform to the semantic versioning scheme . Note that pre-releases and builds/nightlies are not supported - only the standard $major.$minor.$patch format will work. test_defaults \u00bb Test defaults are runtime settings following this scheme that will apply to all test cases for a module . tests \u00bb The tests section represents a list of test cases for a module, each containing the standard runtime settings in addition to the test-specific settings: Key Required Type Description name Y string Unique name of the test case negative N bool Indicates whether the test is negative (expected to fail) id N string Unique identifier of the test case which can be used to refer to the test case depends_on N list<string> List of test case id s this test depends on","title":"YAML reference"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#yaml-reference","text":"This document is a reference for the Spacelift configuration keys that are used in the .spacelift/config.yml file to configure one or more Stacks . Warning The .spacelift/config.yml file must be located at the root of your repository, not at the project root .","title":"YAML reference"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#stack-settings","text":"version stack_defaults stacks","title":"Stack settings"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#module-settings","text":"version module_version test_defaults tests","title":"Module settings"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#version","text":"The version property is optional and currently ignored but for the sake of completeness you may want to set the value to \"1\".","title":"version"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#stack_defaults","text":"stack_defaults represent default settings that will apply to every stack defined in the id-settings map. Any default setting is overridden by an explicitly set stack-specific value. Key Required Type Description after_apply N list<string> List of commands executed after applying changes . after_destroy N list<string> List of commands executed after destroying managed resources. after_init N list<string> List of commands executed after first interacting with the backend (eg. terraform init). after_perform N list<string> List of commands executed after performing a custom task after_plan N list<string> List of commands executed after planning changes after_run N list<string> List of commands executed after every run, regardless of its outcome before_apply N list<string> List of commands executed before applying changes . before_destroy N list<string> List of commands executed before destroying managed resources. before_init N list<string> List of commands executed before first interacting with the backend (eg. terraform init ). before_perform N list<string> List of commands executed before performing a custom task before_plan N list<string> List of commands executed before planning changes environment N map<string, string> Map of extra environment variables and their values passed to the job project_root N string Optional folder inside the repository serving as the root of your stack runner_image N string Name of the custom runner image , if any terraform_version N string For Terraform stacks, Terraform version number to be used","title":"stack_defaults"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#stacks","text":"The stacks section is a map using stack public ID (slug) as keys and stack settings - described in this section - as values. If you're using this mapping together with stack defaults, note that any default setting is overridden by an explicitly set stack-specific value. This is particularly important for list and map fields where one may assume that these are merged. In practice, they're not merged - they're replaced. If you want merging semantics, YAML provides native methods to merge arrays and maps .","title":"stacks"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#module_version","text":"Module version is a required string value that must conform to the semantic versioning scheme . Note that pre-releases and builds/nightlies are not supported - only the standard $major.$minor.$patch format will work.","title":"module_version"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#test_defaults","text":"Test defaults are runtime settings following this scheme that will apply to all test cases for a module .","title":"test_defaults"},{"location":"concepts/configuration/runtime-configuration/runtime-yaml-reference.html#tests","text":"The tests section represents a list of test cases for a module, each containing the standard runtime settings in addition to the test-specific settings: Key Required Type Description name Y string Unique name of the test case negative N bool Indicates whether the test is negative (expected to fail) id N string Unique identifier of the test case which can be used to refer to the test case depends_on N list<string> List of test case id s this test depends on","title":"tests"},{"location":"concepts/policy/index.html","text":"Policy \u00bb Introduction \u00bb Policy-as-code is the idea of expressing rules using a high-level programming language and treating them as you normally treat code, which includes version control as well as continuous integration and deployment. This approach extends the infrastructure-as-code approach to also cover the rules governing this infrastructure, and the platform that manages it. Spacelift as a development platform is built around this concept and allows defining policies that involve various decision points in the application. User-defined policies can decide: Login: who gets to log in to your Spacelift account and with what level of access; Access: who gets to access individual Stacks and with what level of access; Approval: who can approve or reject a run and how a run can be approved; Initialization: which Runs and Tasks can be started ; Notification: routing and filtering notifications ; Plan: which changes can be applied ; Push: how Git push events are interpreted ; Task: which one-off commands can be executed ; Trigger: what happens when blocking runs terminate ; Please refer to the following table for information on what each policy types returns, and the rules available within each policy. Type Purpose Types Returns Rules Login Allow or deny login, grant admin access Positive and negative boolean allow , admin , deny , deny_admin Access Grant or deny appropriate level of stack access Positive and negative boolean read , write , deny , deny_write Approval Who can approve or reject a run and how a run can be approved Positive and negative boolean approve, reject Initialization Blocks suspicious runs before they start Negative set<string> deny Notification Routes and filters notifications Positive map<string, any> inbox , slack , webhook Plan Gives feedback on runs after planning phase Negative set<string> deny , warn Push Determines how a Git push event is interpreted Positive and negative boolean track , propose , ignore , ignore_track , notrigger Task Blocks suspicious tasks from running Negative set<string> deny Trigger Selects stacks for which to trigger a tracked run Positive set<string> trigger Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for, please reach out to our support and we will craft a policy to do exactly what you need. How it works \u00bb Spacelift uses an open-source project called Open Policy Agent and its rule language, Rego , to execute user-defined pieces of code we call Policies at various decision points. Policies come in different flavors that we call types , with each type being executed at a different decision point. You can think of policies as snippets of code that receive some JSON-formatted input and are allowed to produce some output in a predefined form. This input normally represents the data that should be enough to make some decision in its context. Each policy type exposes slightly different data, so please refer to their respective schemas for more information. Except for login policies that are global, all other policy types operate on the stack level, and they can be attached to multiple stacks, just as contexts are, which both facilitates code reuse and allows flexibility. Policies only affect stacks they're attached to. Please refer to the relevant section of this article for more information about attaching policies. Multiple policies of the same type can be attached to a single stack, in which case they are evaluated separately to avoid having their code (like local variables and helper rules) affect one another. However, once these policies are evaluated against the same input, their results are combined. So if you allow user login from one policy but deny it from another, the result will still be a denial. Policy language \u00bb Rego - the language that we're using to execute policies - is a very elegant, Turing incomplete data query language. It takes a few hours (tops) to get your head around all of its quirks but if you can handle SQL and the likes of jq , you'll find Rego pretty familiar. For each policy, we also give you plenty of examples that you can tweak to achieve your goals, and each of those examples comes with a link allowing you to execute it in the Rego playground . Constraints \u00bb To keep policies functionally pure and relatively snappy, we disabled some Rego built-ins that can query external or runtime data. These are: http.send opa.runtime rego.parse_module time.now_ns trace Disabling time.now_ns may seem surprising at first - after all, what's wrong with getting the current timestamp? Alas, depending on the current timestamp will make your policies impure and thus tricky to test - and we encourage you to test your policies thoroughly ! You will notice though that the current timestamp in Rego-compatible form (Unix nanoseconds) is available as request.timestamp_ns in every policy payload, so please use it instead. Policies must be self-contained and cannot refer to external resources (e.g., files in a VCS repository). Return Types \u00bb There are currently nine types of supported policies and while each of them is different, they have a lot in common. In particular, they can fall into a few groups based on what rules are expected to return. Boolean \u00bb Login and access policies expect rules to return a boolean value ( true or false ). Each type of policy defines its own set of rules corresponding to different access levels. In these cases, various types of rules can be positive or negative - that is, they can explicitly allow or deny access. Set of Strings \u00bb The second group of policies ( initialization , plan , and task ) is expected to generate a set of strings that serve as direct feedback to the user. Those rules are generally negative in that they can only block certain actions - it's only their lack that counts as an implicit success. Here's a practical difference between the two types: boolean.rego 1 2 3 4 5 6 7 package spacelift # This is a simple deny rule . # When it matches , no feedback is provided . deny { true } string.rego 1 2 3 4 5 6 7 package spacelift # This is a deny rule with string value . # When it matches , that value is reported to the user . deny [ \"the user will see this\" ] { true } For the policies that generate a set of strings, you want these strings to be both informative and relevant, so you'll see this pattern a lot in the examples: 1 2 3 4 5 6 7 8 9 10 11 package spacelift we_dont_create : = { \"scary\" , \"resource\" , \"types\" } # This is an example of a plan policy . deny [ sprintf ( \"some rule violated (%s)\" , [ resource . address ])] { some resource created_resources [ resource ] we_dont_create [ resource . type ] } Complex objects \u00bb Final group of policies ( notification ) will generate and return more complex objects. These are typically JSON objects. In terms of syntax they are still very similar to other policies which return sets of strings, but they provide additional information inside the returned decision. For example here is a rule which will return a JSON object to be used when creating a custom notification: 1 2 3 4 5 6 7 8 9 10 11 package spacelift inbox [{ \"title\" : \"Tracked run finished!\" , \"body\" : sprintf(\"Run ID: %s\", [run.id]) , \"severity\" : \"INFO\" , }] { run : = input . run_updated . run run . type == \"TRACKED\" run . state == \"FINISHED\" } Helper Functions \u00bb The following helper functions can be used in Spacelift policies: Name Description output := sanitized(x) output is the string x sanitized using the same algorithm we use to sanitize secrets. result := exec(x) Executes the command x . result is an object containing status , stdout and stderr . Only applicable for run initialization policies for private workers . Creating policies \u00bb There are two ways of creating policies - through the web UI and through the Terraform provider . We generally suggest the latter as it's much easier to manage down the line and allows proper unit testing . Here's how you'd define a plan policy in Terraform and attach it to a stack (also created here with minimal configuration for completeness): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 resource \"spacelift_stack\" \"example-stack\" { name = \"Example stack\" repository = \"example-stack\" branch = \"master\" } # This example assumes that you have Rego policies in a separate # folder called \"policies\" . resource \"spacelift_policy\" \"example-policy\" { name = \"Example policy\" body = file ( \"$ { path . module } /policies/example-policy.rego\" ) type = \"TERRAFORM_PLAN\" } resource \"spacelift_policy_attachment\" \"example-attachment\" { stack_id = spacelift_stack . example - stack . id policy_id = spacelift_policy . example - policy . id } On the other hand, if you want to create a policy in the UI, here's how you could go about that. Note that you must be a Spacelift admin to manage policies . First, go to the Policies screen in your account view, and click the Add policy button: This takes you to the policy creation screen where you can choose the type of policy you want to create, and edit its body. For each type of policy you're also given an explanation and a few examples. We'll be creating an access policy that gives members of the Engineering GitHub team read access to a stack: Once you're done, click on the Create policy button to save it. Don't worry, policy body is mutable so you'll always be able to edit it if need be. Attaching policies \u00bb Automatically \u00bb Policies, with the exception of Login policies , can be automatically attached to stacks using the autoattach:label special label where label is the name of a label attached to stacks and/or modules in your Spacelift account you wish the policy to be attached to. Policy Attachment Example \u00bb In the example below, the policy will be automatically attached to all stacks/modules with the label production . Wildcard Policy Attachments \u00bb In addition to being able to automatically attach policies using a specific label, you can also choose to attach a policy to stacks/modules in the account using a wildcard, for example using autoattach:* as a label on a policy, will attach the policy to all stacks/modules. Manually \u00bb In the web UI attaching policies is done in the stack management view, in the Policies tab: Policy workbench \u00bb One thing we've noticed while working with policies in practice is that it takes a while to get them right. This is not only because the concept or the underlying language introduce a learning curve, but also because the feedback cycle can be slow: write a plan policy, make a code change, trigger a run, verify policy behavior... rinse and repeat. This can easily take hours. Enter policy workbench . Policy workbench allows you to capture policy evaluation events so that you can adjust the policy independently and therefore shorten the entire cycle. In order to make use of the workbench, you will first need to sample policy inputs . Sampling policy inputs \u00bb Each of Spacelift's policies supports an additional boolean rule called sample . Returning true from this rule means that the input to the policy evaluation is captured, along with the policy body at the time and the exact result of the policy evaluation. You can for example just capture every evaluation with a simple: 1 sample { true } If that feels a bit simplistic and spammy, you can adjust this rule to capture only certain types of inputs. For example, in this case we will only want to capture evaluations that returned in an empty least for deny reasons (eg. with a plan or task policy): 1 sample { count ( deny ) == 0 } You can also sample a certain percentage of policy evaluations. Given that we don't generally allow nondeterministic evaluations, you'd need to depend on a source of randomness internal to the input. In this example we will use the timestamp - note that since it's originally expressed in nanoseconds, we will turn it into milliseconds to get a better spread. We'll also want to sample every 10th evaluation: 1 2 3 4 sample { millis : = round ( input . request . timestamp_ns / 1e6 ) millis % 100 <= 10 } Why sample? \u00bb Capturing all evaluations sounds tempting but it will also be extremely messy. We're only showing 100 most recent evaluations from the past 7 days , so if you capture everything then the most valuable samples can be drowned by irrelevant or uninteresting ones. Also, sampling adds a small performance penalty to your operations. Policy workbench in practice \u00bb In order to show you how to work with the policy workbench, we are going to use a task policy that whitelists just two tasks - an innocent ls , and tainting a particular resource. It also only samples successful evaluations, where the list of deny reasons is empty: Info This example comes from our test repo , which gives you hands-in experience with most Spacelift functionalities within 10-15 minutes, depending on whether you like to RTFM or not. We strongly recommend you give it a go. In order to get to the policy workbench, first click on the Edit button in the upper right hand corner of the policy screen: Then, click on the Show simulation panel link on the right hand side of the screen: If your policy has been used evaluated and sampled, your screen should look something like this: On the left hand side you have the policy body. On the right hand side there's a dropdown with timestamped evaluations (inputs) of this policy, color-coded for their ultimate outcome. Selecting one of the inputs allows you to simulate the evaluation: While running simulations, you can edit both the input and the policy body. If you edit the policy body, or choose an input that has been evaluated with a different policy body, you will get a warning like this: Clicking on the Show changes link within that warning shows you the exact difference between the policy body in the editor panel, and the one used for evaluating the selected input: Once you're happy with your new policy body, you can click on the Save changes button to make sure that the new body is used for future evaluations. Is it safe? \u00bb Yes, policy sampling is perfectly safe. Session data may contain some personal information like username, name and IP, but that data is only persisted for 7 days. Most importantly, in plan policies the inputs hash all the string attributes of resources, ensuring that no sensitive data leaks through this means. Last but not least, the policy workbench - including access to previous inputs - is only available to Spacelift account administrators . Testing policies \u00bb Info In the examples for each type of policy we invite you to play around with the policy and its input in the Rego playground . While certainly useful, we won't consider it proper unit testing. The whole point of policy-as-code is being able to handle it as code, which involves everyone's favorite bit - testing. Testing policies is crucial because you don't want them accidentally allow the wrong crowd to do the wrong things. Luckily, Spacelift uses a well-documented and well-supported open source language called Rego, which has built-in support for testing. Testing Rego is extensively covered in their documentation so in this section we'll only look at things specific to Spacelift. Let's define a simple login policy that denies access to non-members , and write a test for it: deny-non-members.rego 1 2 3 package spacelift deny { not input . session . member } You'll see that we simply mock out the input received by the policy: deny-non-members_test.rego 1 2 3 4 5 6 7 8 9 package spacelift test_non_member { deny with input as { \"session\" : { \"member\": false } } } test_member_not_denied { not deny with input as { \"session\" : { \"member\": true } } } We can then test it in the console using opa test command (note the glob, which captures both the source and its associated test): 1 2 \u276f opa test deny-non-members* PASS: 2 /2 Testing policies that provide feedback to the users is only slightly more complex. Instead of checking for boolean values, you'll be testing for set equality. Let's define a simple run initialization policy that denies commits to a particular branch (because why not): deny-sandbox.rego 1 2 3 4 5 6 package spacelift deny [ sprintf ( \"don't push to %s\" , [ branch ])] { branch : = input . commit . branch branch == \"sandbox\" } In the respective test, we will check that the set return by the deny rule either has the expected element for the matching input, or is empty for non-matching one: deny-sandbox_test.rego 1 2 3 4 5 6 7 8 9 10 11 12 13 package spacelift test_sandbox_denied { expected : = { \"don't push to sandbox\" } deny == expected with input as { \"commit\" : { \"branch\": \"sandbox\" } } } test_master_not_denied { expected : = set () deny == expected with input as { \"commit\" : { \"branch\": \"master\" } } } Again, we can then test it in the console using opa test command (note the glob, which captures both the source and its associated test): 1 2 \u276f opa test deny-sandbox* PASS: 2 /2 Success We suggest you always unit test your policies and apply the same continuous integration principles as with your application code. You can set up a CI project using the vendor of your choice for the same repository that's linked to the Spacelift project that's defining those policies, to get an external validation. Policy flags \u00bb By default, each policy is completely self-contained and does not depend on the result of previous policies. There are at times situations where you want to introduce a chain of policies passing some data to one another. Different types of policies have access to different types of data required to make a decision, and you can use policy flags to pass that data (or more likely, a useful digest of that data) between them. Let's take a look at a simple example. Let's say you have a push policy with access to the list of files affected by a push or a PR event. You want to introduce a form of ownership control where changes to different files need approval from different users. For example, a change in the network directory may require approval from the network team, while a change in the database directory needs an approval from the DBAs. Approvals are handled by an approval policy but the problem is that it no longer retains access to the list of affected files. This is a great use case to use flags. Let's have the push policy set arbitrary review flags on the run. This can be a separate push policy as in this example, or part of one of your pre-existing push policies. For the sake of simplicity, the example below will only focus on the network bit. flag_for_review.rego 1 2 3 4 5 6 7 8 9 10 11 package spacelift network_review_flag = \"review:network\" flag[network_review_flag] { startswith(input.push.affected_files[_], \"network/\") } flag[network_review_flag] { startswith(input.pull_request.diff[_], \"network/*\") } Now, we can introduce a network approval policies using this flag. network-review.rego 1 2 3 4 5 6 7 8 9 10 package spacelift network_review_required { input.run.flags[_] == \"review:network\" } approve { not network_review_required } approve { input.reviews.current.approvals[_].session.teams[_] == \"DBA\" } There are a few things worth knowing about flags: They are arbitrary strings and Spacelift makes no assumptions about their format or content. They are immutable . Once set, they cannot be changed or unset; They are passed between policy types . If you have multiple policies of the same type, they will not be able to see each other's flags; They can be set by any policies that explicitly touch a run : push , approval , plan and trigger ; They are always accessible through run 's flags property whenever the run resource is present in the input document; Also worth noting is the fact that flags are shown in the GUI, so even if you're not using them to explicitly pass the data between different types of policies, they can still be useful for debugging purposes. Below is an example of an approval policy exposing decision-making details:","title":"Policy"},{"location":"concepts/policy/index.html#policy","text":"","title":"Policy"},{"location":"concepts/policy/index.html#introduction","text":"Policy-as-code is the idea of expressing rules using a high-level programming language and treating them as you normally treat code, which includes version control as well as continuous integration and deployment. This approach extends the infrastructure-as-code approach to also cover the rules governing this infrastructure, and the platform that manages it. Spacelift as a development platform is built around this concept and allows defining policies that involve various decision points in the application. User-defined policies can decide: Login: who gets to log in to your Spacelift account and with what level of access; Access: who gets to access individual Stacks and with what level of access; Approval: who can approve or reject a run and how a run can be approved; Initialization: which Runs and Tasks can be started ; Notification: routing and filtering notifications ; Plan: which changes can be applied ; Push: how Git push events are interpreted ; Task: which one-off commands can be executed ; Trigger: what happens when blocking runs terminate ; Please refer to the following table for information on what each policy types returns, and the rules available within each policy. Type Purpose Types Returns Rules Login Allow or deny login, grant admin access Positive and negative boolean allow , admin , deny , deny_admin Access Grant or deny appropriate level of stack access Positive and negative boolean read , write , deny , deny_write Approval Who can approve or reject a run and how a run can be approved Positive and negative boolean approve, reject Initialization Blocks suspicious runs before they start Negative set<string> deny Notification Routes and filters notifications Positive map<string, any> inbox , slack , webhook Plan Gives feedback on runs after planning phase Negative set<string> deny , warn Push Determines how a Git push event is interpreted Positive and negative boolean track , propose , ignore , ignore_track , notrigger Task Blocks suspicious tasks from running Negative set<string> deny Trigger Selects stacks for which to trigger a tracked run Positive set<string> trigger Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for, please reach out to our support and we will craft a policy to do exactly what you need.","title":"Introduction"},{"location":"concepts/policy/index.html#how-it-works","text":"Spacelift uses an open-source project called Open Policy Agent and its rule language, Rego , to execute user-defined pieces of code we call Policies at various decision points. Policies come in different flavors that we call types , with each type being executed at a different decision point. You can think of policies as snippets of code that receive some JSON-formatted input and are allowed to produce some output in a predefined form. This input normally represents the data that should be enough to make some decision in its context. Each policy type exposes slightly different data, so please refer to their respective schemas for more information. Except for login policies that are global, all other policy types operate on the stack level, and they can be attached to multiple stacks, just as contexts are, which both facilitates code reuse and allows flexibility. Policies only affect stacks they're attached to. Please refer to the relevant section of this article for more information about attaching policies. Multiple policies of the same type can be attached to a single stack, in which case they are evaluated separately to avoid having their code (like local variables and helper rules) affect one another. However, once these policies are evaluated against the same input, their results are combined. So if you allow user login from one policy but deny it from another, the result will still be a denial.","title":"How it works"},{"location":"concepts/policy/index.html#policy-language","text":"Rego - the language that we're using to execute policies - is a very elegant, Turing incomplete data query language. It takes a few hours (tops) to get your head around all of its quirks but if you can handle SQL and the likes of jq , you'll find Rego pretty familiar. For each policy, we also give you plenty of examples that you can tweak to achieve your goals, and each of those examples comes with a link allowing you to execute it in the Rego playground .","title":"Policy language"},{"location":"concepts/policy/index.html#constraints","text":"To keep policies functionally pure and relatively snappy, we disabled some Rego built-ins that can query external or runtime data. These are: http.send opa.runtime rego.parse_module time.now_ns trace Disabling time.now_ns may seem surprising at first - after all, what's wrong with getting the current timestamp? Alas, depending on the current timestamp will make your policies impure and thus tricky to test - and we encourage you to test your policies thoroughly ! You will notice though that the current timestamp in Rego-compatible form (Unix nanoseconds) is available as request.timestamp_ns in every policy payload, so please use it instead. Policies must be self-contained and cannot refer to external resources (e.g., files in a VCS repository).","title":"Constraints"},{"location":"concepts/policy/index.html#return-types","text":"There are currently nine types of supported policies and while each of them is different, they have a lot in common. In particular, they can fall into a few groups based on what rules are expected to return.","title":"Return Types"},{"location":"concepts/policy/index.html#boolean","text":"Login and access policies expect rules to return a boolean value ( true or false ). Each type of policy defines its own set of rules corresponding to different access levels. In these cases, various types of rules can be positive or negative - that is, they can explicitly allow or deny access.","title":"Boolean"},{"location":"concepts/policy/index.html#set-of-strings","text":"The second group of policies ( initialization , plan , and task ) is expected to generate a set of strings that serve as direct feedback to the user. Those rules are generally negative in that they can only block certain actions - it's only their lack that counts as an implicit success. Here's a practical difference between the two types: boolean.rego 1 2 3 4 5 6 7 package spacelift # This is a simple deny rule . # When it matches , no feedback is provided . deny { true } string.rego 1 2 3 4 5 6 7 package spacelift # This is a deny rule with string value . # When it matches , that value is reported to the user . deny [ \"the user will see this\" ] { true } For the policies that generate a set of strings, you want these strings to be both informative and relevant, so you'll see this pattern a lot in the examples: 1 2 3 4 5 6 7 8 9 10 11 package spacelift we_dont_create : = { \"scary\" , \"resource\" , \"types\" } # This is an example of a plan policy . deny [ sprintf ( \"some rule violated (%s)\" , [ resource . address ])] { some resource created_resources [ resource ] we_dont_create [ resource . type ] }","title":"Set of Strings"},{"location":"concepts/policy/index.html#complex-objects","text":"Final group of policies ( notification ) will generate and return more complex objects. These are typically JSON objects. In terms of syntax they are still very similar to other policies which return sets of strings, but they provide additional information inside the returned decision. For example here is a rule which will return a JSON object to be used when creating a custom notification: 1 2 3 4 5 6 7 8 9 10 11 package spacelift inbox [{ \"title\" : \"Tracked run finished!\" , \"body\" : sprintf(\"Run ID: %s\", [run.id]) , \"severity\" : \"INFO\" , }] { run : = input . run_updated . run run . type == \"TRACKED\" run . state == \"FINISHED\" }","title":"Complex objects"},{"location":"concepts/policy/index.html#helper-functions","text":"The following helper functions can be used in Spacelift policies: Name Description output := sanitized(x) output is the string x sanitized using the same algorithm we use to sanitize secrets. result := exec(x) Executes the command x . result is an object containing status , stdout and stderr . Only applicable for run initialization policies for private workers .","title":"Helper Functions"},{"location":"concepts/policy/index.html#creating-policies","text":"There are two ways of creating policies - through the web UI and through the Terraform provider . We generally suggest the latter as it's much easier to manage down the line and allows proper unit testing . Here's how you'd define a plan policy in Terraform and attach it to a stack (also created here with minimal configuration for completeness): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 resource \"spacelift_stack\" \"example-stack\" { name = \"Example stack\" repository = \"example-stack\" branch = \"master\" } # This example assumes that you have Rego policies in a separate # folder called \"policies\" . resource \"spacelift_policy\" \"example-policy\" { name = \"Example policy\" body = file ( \"$ { path . module } /policies/example-policy.rego\" ) type = \"TERRAFORM_PLAN\" } resource \"spacelift_policy_attachment\" \"example-attachment\" { stack_id = spacelift_stack . example - stack . id policy_id = spacelift_policy . example - policy . id } On the other hand, if you want to create a policy in the UI, here's how you could go about that. Note that you must be a Spacelift admin to manage policies . First, go to the Policies screen in your account view, and click the Add policy button: This takes you to the policy creation screen where you can choose the type of policy you want to create, and edit its body. For each type of policy you're also given an explanation and a few examples. We'll be creating an access policy that gives members of the Engineering GitHub team read access to a stack: Once you're done, click on the Create policy button to save it. Don't worry, policy body is mutable so you'll always be able to edit it if need be.","title":"Creating policies"},{"location":"concepts/policy/index.html#attaching-policies","text":"","title":"Attaching policies"},{"location":"concepts/policy/index.html#automatically","text":"Policies, with the exception of Login policies , can be automatically attached to stacks using the autoattach:label special label where label is the name of a label attached to stacks and/or modules in your Spacelift account you wish the policy to be attached to.","title":"Automatically"},{"location":"concepts/policy/index.html#policy-attachment-example","text":"In the example below, the policy will be automatically attached to all stacks/modules with the label production .","title":"Policy Attachment Example"},{"location":"concepts/policy/index.html#wildcard-policy-attachments","text":"In addition to being able to automatically attach policies using a specific label, you can also choose to attach a policy to stacks/modules in the account using a wildcard, for example using autoattach:* as a label on a policy, will attach the policy to all stacks/modules.","title":"Wildcard Policy Attachments"},{"location":"concepts/policy/index.html#manually","text":"In the web UI attaching policies is done in the stack management view, in the Policies tab:","title":"Manually"},{"location":"concepts/policy/index.html#policy-workbench","text":"One thing we've noticed while working with policies in practice is that it takes a while to get them right. This is not only because the concept or the underlying language introduce a learning curve, but also because the feedback cycle can be slow: write a plan policy, make a code change, trigger a run, verify policy behavior... rinse and repeat. This can easily take hours. Enter policy workbench . Policy workbench allows you to capture policy evaluation events so that you can adjust the policy independently and therefore shorten the entire cycle. In order to make use of the workbench, you will first need to sample policy inputs .","title":"Policy workbench"},{"location":"concepts/policy/index.html#sampling-policy-inputs","text":"Each of Spacelift's policies supports an additional boolean rule called sample . Returning true from this rule means that the input to the policy evaluation is captured, along with the policy body at the time and the exact result of the policy evaluation. You can for example just capture every evaluation with a simple: 1 sample { true } If that feels a bit simplistic and spammy, you can adjust this rule to capture only certain types of inputs. For example, in this case we will only want to capture evaluations that returned in an empty least for deny reasons (eg. with a plan or task policy): 1 sample { count ( deny ) == 0 } You can also sample a certain percentage of policy evaluations. Given that we don't generally allow nondeterministic evaluations, you'd need to depend on a source of randomness internal to the input. In this example we will use the timestamp - note that since it's originally expressed in nanoseconds, we will turn it into milliseconds to get a better spread. We'll also want to sample every 10th evaluation: 1 2 3 4 sample { millis : = round ( input . request . timestamp_ns / 1e6 ) millis % 100 <= 10 }","title":"Sampling policy inputs"},{"location":"concepts/policy/index.html#why-sample","text":"Capturing all evaluations sounds tempting but it will also be extremely messy. We're only showing 100 most recent evaluations from the past 7 days , so if you capture everything then the most valuable samples can be drowned by irrelevant or uninteresting ones. Also, sampling adds a small performance penalty to your operations.","title":"Why sample?"},{"location":"concepts/policy/index.html#policy-workbench-in-practice","text":"In order to show you how to work with the policy workbench, we are going to use a task policy that whitelists just two tasks - an innocent ls , and tainting a particular resource. It also only samples successful evaluations, where the list of deny reasons is empty: Info This example comes from our test repo , which gives you hands-in experience with most Spacelift functionalities within 10-15 minutes, depending on whether you like to RTFM or not. We strongly recommend you give it a go. In order to get to the policy workbench, first click on the Edit button in the upper right hand corner of the policy screen: Then, click on the Show simulation panel link on the right hand side of the screen: If your policy has been used evaluated and sampled, your screen should look something like this: On the left hand side you have the policy body. On the right hand side there's a dropdown with timestamped evaluations (inputs) of this policy, color-coded for their ultimate outcome. Selecting one of the inputs allows you to simulate the evaluation: While running simulations, you can edit both the input and the policy body. If you edit the policy body, or choose an input that has been evaluated with a different policy body, you will get a warning like this: Clicking on the Show changes link within that warning shows you the exact difference between the policy body in the editor panel, and the one used for evaluating the selected input: Once you're happy with your new policy body, you can click on the Save changes button to make sure that the new body is used for future evaluations.","title":"Policy workbench in practice"},{"location":"concepts/policy/index.html#is-it-safe","text":"Yes, policy sampling is perfectly safe. Session data may contain some personal information like username, name and IP, but that data is only persisted for 7 days. Most importantly, in plan policies the inputs hash all the string attributes of resources, ensuring that no sensitive data leaks through this means. Last but not least, the policy workbench - including access to previous inputs - is only available to Spacelift account administrators .","title":"Is it safe?"},{"location":"concepts/policy/index.html#testing-policies","text":"Info In the examples for each type of policy we invite you to play around with the policy and its input in the Rego playground . While certainly useful, we won't consider it proper unit testing. The whole point of policy-as-code is being able to handle it as code, which involves everyone's favorite bit - testing. Testing policies is crucial because you don't want them accidentally allow the wrong crowd to do the wrong things. Luckily, Spacelift uses a well-documented and well-supported open source language called Rego, which has built-in support for testing. Testing Rego is extensively covered in their documentation so in this section we'll only look at things specific to Spacelift. Let's define a simple login policy that denies access to non-members , and write a test for it: deny-non-members.rego 1 2 3 package spacelift deny { not input . session . member } You'll see that we simply mock out the input received by the policy: deny-non-members_test.rego 1 2 3 4 5 6 7 8 9 package spacelift test_non_member { deny with input as { \"session\" : { \"member\": false } } } test_member_not_denied { not deny with input as { \"session\" : { \"member\": true } } } We can then test it in the console using opa test command (note the glob, which captures both the source and its associated test): 1 2 \u276f opa test deny-non-members* PASS: 2 /2 Testing policies that provide feedback to the users is only slightly more complex. Instead of checking for boolean values, you'll be testing for set equality. Let's define a simple run initialization policy that denies commits to a particular branch (because why not): deny-sandbox.rego 1 2 3 4 5 6 package spacelift deny [ sprintf ( \"don't push to %s\" , [ branch ])] { branch : = input . commit . branch branch == \"sandbox\" } In the respective test, we will check that the set return by the deny rule either has the expected element for the matching input, or is empty for non-matching one: deny-sandbox_test.rego 1 2 3 4 5 6 7 8 9 10 11 12 13 package spacelift test_sandbox_denied { expected : = { \"don't push to sandbox\" } deny == expected with input as { \"commit\" : { \"branch\": \"sandbox\" } } } test_master_not_denied { expected : = set () deny == expected with input as { \"commit\" : { \"branch\": \"master\" } } } Again, we can then test it in the console using opa test command (note the glob, which captures both the source and its associated test): 1 2 \u276f opa test deny-sandbox* PASS: 2 /2 Success We suggest you always unit test your policies and apply the same continuous integration principles as with your application code. You can set up a CI project using the vendor of your choice for the same repository that's linked to the Spacelift project that's defining those policies, to get an external validation.","title":"Testing policies"},{"location":"concepts/policy/index.html#policy-flags","text":"By default, each policy is completely self-contained and does not depend on the result of previous policies. There are at times situations where you want to introduce a chain of policies passing some data to one another. Different types of policies have access to different types of data required to make a decision, and you can use policy flags to pass that data (or more likely, a useful digest of that data) between them. Let's take a look at a simple example. Let's say you have a push policy with access to the list of files affected by a push or a PR event. You want to introduce a form of ownership control where changes to different files need approval from different users. For example, a change in the network directory may require approval from the network team, while a change in the database directory needs an approval from the DBAs. Approvals are handled by an approval policy but the problem is that it no longer retains access to the list of affected files. This is a great use case to use flags. Let's have the push policy set arbitrary review flags on the run. This can be a separate push policy as in this example, or part of one of your pre-existing push policies. For the sake of simplicity, the example below will only focus on the network bit. flag_for_review.rego 1 2 3 4 5 6 7 8 9 10 11 package spacelift network_review_flag = \"review:network\" flag[network_review_flag] { startswith(input.push.affected_files[_], \"network/\") } flag[network_review_flag] { startswith(input.pull_request.diff[_], \"network/*\") } Now, we can introduce a network approval policies using this flag. network-review.rego 1 2 3 4 5 6 7 8 9 10 package spacelift network_review_required { input.run.flags[_] == \"review:network\" } approve { not network_review_required } approve { input.reviews.current.approvals[_].session.teams[_] == \"DBA\" } There are a few things worth knowing about flags: They are arbitrary strings and Spacelift makes no assumptions about their format or content. They are immutable . Once set, they cannot be changed or unset; They are passed between policy types . If you have multiple policies of the same type, they will not be able to see each other's flags; They can be set by any policies that explicitly touch a run : push , approval , plan and trigger ; They are always accessible through run 's flags property whenever the run resource is present in the input document; Also worth noting is the fact that flags are shown in the GUI, so even if you're not using them to explicitly pass the data between different types of policies, they can still be useful for debugging purposes. Below is an example of an approval policy exposing decision-making details:","title":"Policy flags"},{"location":"concepts/policy/approval-policy.html","text":"Approval policy \u00bb The approval policy allows organizations to create sophisticated run review and approval flows that reflect their preferred workflow, security goals, and business objectives. Without an explicit approval policy, anyone with write access to a stack can create a run (or a task ). An approval policy can make this way more granular and contextual. Runs can be reviewed when they enter one of the two states - queued or unconfirmed . When a queued run needs approval, it will not be scheduled before that approval is received, and if it is of a blocking type, it will block newer runs from scheduling, too. A queued run that's pending approval can be canceled at any point. Here's an example of a queued run waiting for a human review - note how the last approval policy evaluation returned an Undecided decision. There's also a review button next to the cancelation one: Review can be positive (approve) or negative (reject): With a positive review, the approval policy could evaluate to Approve thus unblocking the run: When an unconfirmed run needs approval, you will not be able to confirm it until that approval is received. The run can however be discarded at any point: In principle, the run review and approval process are very similar to GitHub's Pull Request review, the only exception being that it's the Rego policy (rather than a set of checkboxes and dropdowns) that defines the exact conditions to approve the run. Tip If separate run approval and confirmation steps sound confusing, don't worry. Just think about how GitHub's Pull Requests work - you can approve a PR before merging it in a separate step. A PR approval means \"I'm OK with this being merged\". A run approval means \"I'm OK with that action being executed\". Rules \u00bb Your approval policy can define the following boolean rules: approve : the run is approved and no longer requires (or allows) review; reject : the run fails immediately; While the 'approve' rule must be defined in order for the run to be able to progress, it's perfectly valid to not define the 'reject' rule. In that case, runs that look invalid can be cleaned up ( canceled or discarded ) manually. It's also perfectly acceptable for any given policy evaluation to return 'false' on both 'approve' and 'reject' rules. This only means that the result is yet 'undecided' and more reviews will be necessary to reach the conclusion. A perfect example would be a policy that requires 2 approvals for a given job - the first review is not yet supposed to set the 'approve' value to 'true'. Info Users must have write or admin access to the stack to be able to approve changes. How it works \u00bb When a user reviews the run, Spacelift persists their review and passes it to the approval policy, along with other reviews, plus some information about the run and its stack. The same user can review the same run as many times as they want, but only their newest review will be presented to the approval policy. This mechanism allows you to change your mind, very similar to Pull Request reviews. Data input \u00bb This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 { \"reviews\" : { // run reviews \"current\" : { // reviews for the current state \"approvals\" : [{ // positive reviews \"author\" : \"string - reviewer username\" , \"request\" : { // request data of the review \"remote_ip\" : \"string - user IP\" , \"timestamp_ns\" : \"number - review creation Unix timestamp in nanoseconds\" , }, \"session\" : { // session data of the review \"login\" : \"string - username of the reviewer\" , \"name\" : \"string - full name of the reviewer\" , \"teams\" : [ \"string - names of teams the reviewer was a member of\" ] }, \"state\" : \"string - the state of the run at the time of the approval\" , }], \"rejections\" : [ /* negative reviews, see \"approvals\" for schema */ ] }, \"older\" : [ /* reviews for previous state(s), see \"current\" for schema */ ] }, \"run\" : { // the run metadata \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch the run was triggered from\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" }, \"phase\" : \"string enum - plan | apply\" } ], \"command\" : \"string or null, set when the run type is TASK\" , \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"creator_session\" : { \"admin\" : \"boolean - is the current user a Spacelift admin\" , \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - username of the creator\" , \"name\" : \"string - full name of the creator\" , \"teams\" : [ \"string - names of teams the creator was a member of\" ], \"machine\" : \"boolean - whether the run was initiated by a human or a machine\" }, \"drift_detection\" : \"boolean - is this a drift detection run\" , \"flags\" : [ \"string - list of flags set on the run by other policies\" ], \"id\" : \"string - the run ID\" , \"runtime_config\" : { \"before_init\" : [ \"string - command to run before run initialization\" ], \"project_root\" : \"string - root of the Terraform project\" , \"runner_image\" : \"string - Docker image used to execute the run\" , \"terraform_version\" : \"string - Terraform version used to for the run\" }, \"state\" : \"string - the current run state\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - type of the run\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }, \"stack\" : { // the stack metadata \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } } } Examples \u00bb Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need. Two approvals and no rejections to approve an Unconfirmed run \u00bb In this example, each Unconfirmed run will require two approvals - including proposed runs triggered by Git events. Additionally, the run should have no rejections. Anyone who rejects the run will need to change their mind in order for the run to go through. Info We suggest requiring more than one review because one approval should come from the run/commit author to indicate that they're aware of what they're doing, especially if their VCS handle is different than their IdP handle. This is something we practice internally at Spacelift . 1 2 3 4 5 6 7 8 package spacelift approve { input . run . state != \"UNCONFIRMED\" } approve { count ( input . reviews . current . approvals ) > 1 count ( input . reviews . current . rejections ) == 0 } Here's a minimal example to play with . Two to approve, two to reject \u00bb This is a variation of the above policy, but one that will automatically fail any run that receives more than one rejection. 1 2 3 4 5 package spacelift approve { input . run . state != \"UNCONFIRMED\" } approve { count ( input . reviews . current . approvals ) > 1 } reject { count ( input . reviews . current . rejections ) > 1 } Here's a minimal example to play with . Require approval for a task command not on the allowlist \u00bb 1 2 3 4 5 6 7 8 9 10 11 12 package spacelift allowlist : = [ \"ps\" , \"ls\" , \"rm -rf /\" ] # Approve when not a task . approve { input . run . type != \"TASK\" } # Approve when allowlisted . approve { input . run . command == allowlist [ _ ] } # Approve with two or more approvals . approve { count ( input . reviews . current . approvals ) > 1 } Here's a minimal example to play with . Combining multiple rules \u00bb Usually, you will want to apply different rules to different types of jobs. Since approval policies are attached to stacks, you will want to be smart about how you combine different rules. Here's how you can do that in a readable way, combining two of the above approval flows as an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package spacelift # First , let 's define all conditions that require explicit # user approval . requires_approval { input . run . state == \"UNCONFIRMED\" } requires_approval { input . run . type == \"TASK\" } # Then , let 's automatically approve all other jobs . approve { not requires_approval } # Autoapprove some task commands . Note how we don 't check for run type # because only tasks will the have \"command\" field set . task_allowlist : = [ \"ps\" , \"ls\" , \"rm -rf /\" ] approve { input . run . command == task_allowlist [ _ ] } # Two approvals and no rejections to approve . approve { count ( input . reviews . current . approvals ) > 1 count ( input . reviews . current . rejections ) == 0 } Here's a minimal example to play with . Role-based approval \u00bb Sometimes you want to give certain roles but not others the power to approve certain workloads. The policy below approves an unconfirmed run or a task when either a Director approves it, or both DevOps and Security roles approve it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package spacelift # First , let 's define all conditions that require explicit # user approval . requires_approval { input . run . state == \"UNCONFIRMED\" } requires_approval { input . run . type == \"TASK\" } approve { not requires_approval } approvals : = input . reviews . current . approvals # Let 's define what it means to be approved by a director , DevOps and Security . director_approval { approvals [ _ ]. session . teams [ _ ] == \"Director\" } devops_approval { approvals [ _ ]. session . teams [ _ ] == \"DevOps\" } security_approval { approvals [ _ ]. session . teams [ _ ] == \"Security\" } # Approve when a single director approves : approve { director_approval } # Approve when both DevOps and Security approve : approve { devops_approval ; security_approval } Here's a minimal example to play with . Require private worker pool \u00bb You might want to ensure that your runs always get scheduled on a private worker pool, and do not fall back to the public worker pool. You could use an Approval policy similar to this one to achieve this: 1 2 3 4 5 6 7 package spacelift # Approve any runs on private workers approve { not input . stack . worker_pool . public } # Reject any runs on public workers reject { input . stack . worker_pool . public } Here's a minimal example to play with . You probably want to auto-attach this policy to some, if not all, of your stacks.","title":"Approval policy"},{"location":"concepts/policy/approval-policy.html#approval-policy","text":"The approval policy allows organizations to create sophisticated run review and approval flows that reflect their preferred workflow, security goals, and business objectives. Without an explicit approval policy, anyone with write access to a stack can create a run (or a task ). An approval policy can make this way more granular and contextual. Runs can be reviewed when they enter one of the two states - queued or unconfirmed . When a queued run needs approval, it will not be scheduled before that approval is received, and if it is of a blocking type, it will block newer runs from scheduling, too. A queued run that's pending approval can be canceled at any point. Here's an example of a queued run waiting for a human review - note how the last approval policy evaluation returned an Undecided decision. There's also a review button next to the cancelation one: Review can be positive (approve) or negative (reject): With a positive review, the approval policy could evaluate to Approve thus unblocking the run: When an unconfirmed run needs approval, you will not be able to confirm it until that approval is received. The run can however be discarded at any point: In principle, the run review and approval process are very similar to GitHub's Pull Request review, the only exception being that it's the Rego policy (rather than a set of checkboxes and dropdowns) that defines the exact conditions to approve the run. Tip If separate run approval and confirmation steps sound confusing, don't worry. Just think about how GitHub's Pull Requests work - you can approve a PR before merging it in a separate step. A PR approval means \"I'm OK with this being merged\". A run approval means \"I'm OK with that action being executed\".","title":"Approval policy"},{"location":"concepts/policy/approval-policy.html#rules","text":"Your approval policy can define the following boolean rules: approve : the run is approved and no longer requires (or allows) review; reject : the run fails immediately; While the 'approve' rule must be defined in order for the run to be able to progress, it's perfectly valid to not define the 'reject' rule. In that case, runs that look invalid can be cleaned up ( canceled or discarded ) manually. It's also perfectly acceptable for any given policy evaluation to return 'false' on both 'approve' and 'reject' rules. This only means that the result is yet 'undecided' and more reviews will be necessary to reach the conclusion. A perfect example would be a policy that requires 2 approvals for a given job - the first review is not yet supposed to set the 'approve' value to 'true'. Info Users must have write or admin access to the stack to be able to approve changes.","title":"Rules"},{"location":"concepts/policy/approval-policy.html#how-it-works","text":"When a user reviews the run, Spacelift persists their review and passes it to the approval policy, along with other reviews, plus some information about the run and its stack. The same user can review the same run as many times as they want, but only their newest review will be presented to the approval policy. This mechanism allows you to change your mind, very similar to Pull Request reviews.","title":"How it works"},{"location":"concepts/policy/approval-policy.html#data-input","text":"This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 { \"reviews\" : { // run reviews \"current\" : { // reviews for the current state \"approvals\" : [{ // positive reviews \"author\" : \"string - reviewer username\" , \"request\" : { // request data of the review \"remote_ip\" : \"string - user IP\" , \"timestamp_ns\" : \"number - review creation Unix timestamp in nanoseconds\" , }, \"session\" : { // session data of the review \"login\" : \"string - username of the reviewer\" , \"name\" : \"string - full name of the reviewer\" , \"teams\" : [ \"string - names of teams the reviewer was a member of\" ] }, \"state\" : \"string - the state of the run at the time of the approval\" , }], \"rejections\" : [ /* negative reviews, see \"approvals\" for schema */ ] }, \"older\" : [ /* reviews for previous state(s), see \"current\" for schema */ ] }, \"run\" : { // the run metadata \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch the run was triggered from\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" }, \"phase\" : \"string enum - plan | apply\" } ], \"command\" : \"string or null, set when the run type is TASK\" , \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"creator_session\" : { \"admin\" : \"boolean - is the current user a Spacelift admin\" , \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - username of the creator\" , \"name\" : \"string - full name of the creator\" , \"teams\" : [ \"string - names of teams the creator was a member of\" ], \"machine\" : \"boolean - whether the run was initiated by a human or a machine\" }, \"drift_detection\" : \"boolean - is this a drift detection run\" , \"flags\" : [ \"string - list of flags set on the run by other policies\" ], \"id\" : \"string - the run ID\" , \"runtime_config\" : { \"before_init\" : [ \"string - command to run before run initialization\" ], \"project_root\" : \"string - root of the Terraform project\" , \"runner_image\" : \"string - Docker image used to execute the run\" , \"terraform_version\" : \"string - Terraform version used to for the run\" }, \"state\" : \"string - the current run state\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - type of the run\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }, \"stack\" : { // the stack metadata \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } } }","title":"Data input"},{"location":"concepts/policy/approval-policy.html#examples","text":"Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.","title":"Examples"},{"location":"concepts/policy/approval-policy.html#two-approvals-and-no-rejections-to-approve-an-unconfirmed-run","text":"In this example, each Unconfirmed run will require two approvals - including proposed runs triggered by Git events. Additionally, the run should have no rejections. Anyone who rejects the run will need to change their mind in order for the run to go through. Info We suggest requiring more than one review because one approval should come from the run/commit author to indicate that they're aware of what they're doing, especially if their VCS handle is different than their IdP handle. This is something we practice internally at Spacelift . 1 2 3 4 5 6 7 8 package spacelift approve { input . run . state != \"UNCONFIRMED\" } approve { count ( input . reviews . current . approvals ) > 1 count ( input . reviews . current . rejections ) == 0 } Here's a minimal example to play with .","title":"Two approvals and no rejections to approve an Unconfirmed run"},{"location":"concepts/policy/approval-policy.html#two-to-approve-two-to-reject","text":"This is a variation of the above policy, but one that will automatically fail any run that receives more than one rejection. 1 2 3 4 5 package spacelift approve { input . run . state != \"UNCONFIRMED\" } approve { count ( input . reviews . current . approvals ) > 1 } reject { count ( input . reviews . current . rejections ) > 1 } Here's a minimal example to play with .","title":"Two to approve, two to reject"},{"location":"concepts/policy/approval-policy.html#require-approval-for-a-task-command-not-on-the-allowlist","text":"1 2 3 4 5 6 7 8 9 10 11 12 package spacelift allowlist : = [ \"ps\" , \"ls\" , \"rm -rf /\" ] # Approve when not a task . approve { input . run . type != \"TASK\" } # Approve when allowlisted . approve { input . run . command == allowlist [ _ ] } # Approve with two or more approvals . approve { count ( input . reviews . current . approvals ) > 1 } Here's a minimal example to play with .","title":"Require approval for a task command not on the allowlist"},{"location":"concepts/policy/approval-policy.html#combining-multiple-rules","text":"Usually, you will want to apply different rules to different types of jobs. Since approval policies are attached to stacks, you will want to be smart about how you combine different rules. Here's how you can do that in a readable way, combining two of the above approval flows as an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package spacelift # First , let 's define all conditions that require explicit # user approval . requires_approval { input . run . state == \"UNCONFIRMED\" } requires_approval { input . run . type == \"TASK\" } # Then , let 's automatically approve all other jobs . approve { not requires_approval } # Autoapprove some task commands . Note how we don 't check for run type # because only tasks will the have \"command\" field set . task_allowlist : = [ \"ps\" , \"ls\" , \"rm -rf /\" ] approve { input . run . command == task_allowlist [ _ ] } # Two approvals and no rejections to approve . approve { count ( input . reviews . current . approvals ) > 1 count ( input . reviews . current . rejections ) == 0 } Here's a minimal example to play with .","title":"Combining multiple rules"},{"location":"concepts/policy/approval-policy.html#role-based-approval","text":"Sometimes you want to give certain roles but not others the power to approve certain workloads. The policy below approves an unconfirmed run or a task when either a Director approves it, or both DevOps and Security roles approve it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package spacelift # First , let 's define all conditions that require explicit # user approval . requires_approval { input . run . state == \"UNCONFIRMED\" } requires_approval { input . run . type == \"TASK\" } approve { not requires_approval } approvals : = input . reviews . current . approvals # Let 's define what it means to be approved by a director , DevOps and Security . director_approval { approvals [ _ ]. session . teams [ _ ] == \"Director\" } devops_approval { approvals [ _ ]. session . teams [ _ ] == \"DevOps\" } security_approval { approvals [ _ ]. session . teams [ _ ] == \"Security\" } # Approve when a single director approves : approve { director_approval } # Approve when both DevOps and Security approve : approve { devops_approval ; security_approval } Here's a minimal example to play with .","title":"Role-based approval"},{"location":"concepts/policy/approval-policy.html#require-private-worker-pool","text":"You might want to ensure that your runs always get scheduled on a private worker pool, and do not fall back to the public worker pool. You could use an Approval policy similar to this one to achieve this: 1 2 3 4 5 6 7 package spacelift # Approve any runs on private workers approve { not input . stack . worker_pool . public } # Reject any runs on public workers reject { input . stack . worker_pool . public } Here's a minimal example to play with . You probably want to auto-attach this policy to some, if not all, of your stacks.","title":"Require private worker pool"},{"location":"concepts/policy/login-policy.html","text":"Login policy \u00bb Purpose \u00bb Login policies can allow users to log in to the account, and optionally give them admin privileges, too. Unlike all other policy types, login policies are global and can't be attached to individual stacks. They take effect immediately once they're created and affect all future login attempts. Info Login policies are only evaluated for the Cloud or Enterprise plan . Warning Login policies don't affect GitHub organization or SSO admins and private account owners who always get admin access to their respective Spacelift accounts. This is to avoid a situation where a bad login policy locks out everyone from the account. Danger Any change made (create, update or delete) to a login policy will invalidate all active sessions, except the session making the change. A login policy can define the following types of boolean rules: allow - allows the user to log in as a non-admin ; admin - allows the user to log in as an account-wide admin - note that you don't need to explicitly allow admin users; deny - denies login attempt, no matter the result of other ( allow and admin ) rules; deny_admin - denies the current user admin access to the stack, no matter the outcome of other rules; space_admin/space_write/space_read - manages access levels to spaces. More on that in Spaces Access Control ; If no rules match, the default action will be to deny a login attempt. Note that giving folks admin access is a big thing. Admins can do pretty much everything in Spacelift - create and delete stacks, trigger runs or tasks, create, delete and attach contexts and policies, etc. Instead, you can give users limited admin access using the space_admin rule. Danger In practice, any time you define an allow or admin rule, you should probably think of restricting access using a deny rule, too. Please see the examples below to get a better feeling for it. Data input \u00bb This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"request\" : { \"remote_ip\" : \"string - IP of the user trying to log in\" , \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"session\" : { \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - username of the user trying to log in\" , \"member\" : \"boolean - is the user a member of the account\" , \"name\" : \"string - full name of the user trying to log in - may be empty\" , \"teams\" : [ \"string - names of teams the user is a member of\" ] }, \"spaces\" : [ { \"id\" : \"string - ID of the space\" , \"name\" : \"string - name of the space\" , \"labels\" : [ \"string - label of the space\" ] } ] } Tip OPA string comparisons are case-sensitive so make sure to use the proper case, as defined in your Identity Provider when comparing values. It might be helpful to enable sampling on the policy to see the exact values passed by the Identity Provider. Two fields in the session object may require further explanation: member and teams . Account membership \u00bb When you first log in to Spacelift, we use GitHub as the identity provider and thus we're able to get some of your details from there with username (login) being the most important one. However, each Spacelift account is linked to one and only one GitHub account. Thus, when you log in to a Spacelift account, we're checking if you're a member of that GitHub account. When that GitHub account is an organization, we can explicitly query for your organization membership. If you're a member, you get the member field set to true . If you're not - it's false . For private accounts it's different - they can only have one member, so the check is even simpler - if your login is the same as the name of the linked GitHub account, you get the member field set to true . If it isn't - it's false . When using Single Sign-On with SAML, every successful login attempt will necessarily require that the member field is set to true - if the linked IdP could verify you, you must be a member. Warning Watch this field very closely - it may be very useful for your deny rules. Teams \u00bb When using the default identity provider (GitHub), Teams are only queried for organization accounts - if you're a member of the GitHub organization linked to a Spacelift account, Spacelift will query GitHub API for the full list of teams you're a member of. This list will be available in the session.teams field. For private accounts and non-members, this list will be empty. Note that Spacelift treats GitHub team membership as transitive - for example let's assume Charlie is a member of the Badass team, which is a child of team Awesome . Charlie's list of teams includes both Awesome and Badass , even though he's not a direct member of the team Awesome . For Single Sign-On, the list of teams is pretty much arbitrary and depends on how the SAML assertion attribute is mapped to your user record on the IdP end. Please see the relevant article for more details. Warning Watch this field very closely - it may be very useful for your allow and admin rules. Examples \u00bb Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need. There are three possible use cases for login policies - granting access to folks in your org who would otherwise not have it, managing access for external contributors or restricting access to specific circumstances. Let's look into these use cases one by one. Managing access levels within an organization \u00bb In high-security environments where the principle of least access is applied, it's quite possible that nobody on the infra team gets admin access to GitHub . Still, it would be pretty useful for those people to be in charge of your Spacelift account. Let's create a login policy that will allow every member of the DevOps team to get admin access, and everyone in Engineering to get regular access - we'll give them more granular access to individual stacks later using stack access policies . While at it, let's also explicitly deny access to all non-members just to be on the safe side. 1 2 3 4 5 6 7 8 9 package spacelift teams : = input . session . teams # Make sure to use the GitHub team names , not IDs ( e . g ., \"Example Team\" not \"example-team\" ) # and to omit the GitHub organization name admin { teams [ _ ] == \"DevOps\" } allow { teams [ _ ] == \"Engineering\" } deny { not input . session . member } Here's a minimal example to play with . This is also important for Single Sign-On integrations: only the integration creator gets administrative permissions by default, so all other administrators must be granted their access using a login policy. Granting access to external contributors \u00bb Danger This feature is not available when using Single Sign-On - your identity provider must be able to successfully validate each user trying to log in to Spacelift. Sometimes you have folks (short-term consultants, most likely) who are not members of your organization but need access to your Spacelift account - either as regular members or perhaps even as admins. There's also the situation where a bunch of friends is working on a hobby project in a personal GitHub account and they could use access to Spacelift. Here are examples of a policy that allows a bunch of whitelisted folks to get regular access and one to get admin privileges: GitHub Google This example uses GitHub usernames to grant access to Spacelift. 1 2 3 4 5 6 7 8 9 package spacelift admins : = { \"alice\" } allowed : = { \"bob\" , \"charlie\" , \"danny\" } login : = input . session . login admin { admins [ login ] } allow { allowed [ login ] } deny { not admins [ login ]; not allowed [ login ] } Here's a minimal example to play with . This example uses email addresses managed by Google to grant access to Spacelift. 1 2 3 4 5 6 7 8 package spacelift admins := { \"alice@example.com\" } login := input.session.login admin { admins[login] } allow { endswith(input.session.login, \"@example.com\") } deny { not admins[login]; not allow } Warning Note that granting access to individuals is less safe than granting access to teams and restricting access to account members. In the latter case, when they lose access to your GitHub org, they automatically lose access to Spacelift. But when whitelisting individuals and not restricting access to members only, you'll need to remember to explicitly remove them from your Spacelift login policy, too. Restricting access to specific circumstances \u00bb Stable and secure infrastructure is crucial to your business continuity. And all changes to your infrastructure carry some risk, so you may want to somehow restrict access to it. The example below is pretty extreme but it shows a very comprehensive policy where you restrict Spacelift access to users logging in from the office IP during business hours. You may want to use elements of this policy to create your own - less draconian - version, or keep it this way to support everyone's work-life balance. Note that this example only defines deny rules so you'll likely want to add some allow and admin rules, too - either in this policy or in a separate one. 1 2 3 4 5 6 7 8 9 10 11 12 package spacelift now : = input . request . timestamp_ns clock : = time . clock ([ now , \"America/Los_Angeles\" ]) weekend : = { \"Saturday\" , \"Sunday\" } weekday : = time . weekday ( now ) ip : = input . request . remote_ip deny { weekend [ weekday ] } deny { clock [ 0 ] < 9 } deny { clock [ 0 ] > 17 } deny { not net . cidr_contains ( \"12.34.56.0/24\" , ip ) } There's a lot to digest here, so a playground example may be helpful. Granting limited admin access \u00bb Very often you'd like to give a user admin access limited to a certain set of resources, so that they can manage them without having access to all other resources in that account. You can find more on that use case in Spaces . Rewriting teams \u00bb In addition to boolean rules regulating access to your Spacelift account, the login policy exposes the team rule, which allows one to dynamically rewrite the list of teams received from the identity provider. This operation allows one to define Spacelift roles independent of the identity provider. To illustrate this use case, let's imagine you want to define a Superwriter role for someone who's: logging in from an office VPN; is a member of the DevOps team, as defined by your IdP; is not a member of the Contractors team, as defined by your IdP; 1 2 3 4 5 6 7 8 9 10 11 package spacelift team [ \"Superwriter\" ] { office_vpn devops not contractor } contractor { input . session . teams [ _ ] == \"Contractors\" } devops { input . session . teams [ _ ] == \"DevOps\" } office_vpn { net . cidr_contains ( \"12.34.56.0/24\" , input . request . remote_ip ) } What's important here is that the team rule overwrites the original list of teams, meaning that if it evaluates to a non-empty collection, it will replace the original list of teams in the session. In the above example, the Superwriter role will become the only team for the evaluated user session. If the above is not what you want, and you still would like to retain the original list of teams, you can modify the above example the following way: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package spacelift # This rule will copy each of the existing teams to the # new modified list . team [ name ] { name : = input . session . teams [ _ ] } team [ \"Superwriter\" ] { office_vpn devops not contractor } contractor { input . session . teams [ _ ] == \"Contractors\" } devops { input . session . teams [ _ ] == \"DevOps\" } office_vpn { net . cidr_contains ( \"12.34.56.0/24\" , input . request . remote_ip ) } A playground example of the above is available here . Hint Because the user session is updated, the rewritten teams are available in the data input provided to the policy types that receive user information. For example, the rewritten teams can be used in Access policies . Default login policy \u00bb If no login policies are defined on the account, Spacelift behaves as if it had this policy: 1 2 3 package spacelift allow { input . session . member }","title":"Login policy"},{"location":"concepts/policy/login-policy.html#login-policy","text":"","title":"Login policy"},{"location":"concepts/policy/login-policy.html#purpose","text":"Login policies can allow users to log in to the account, and optionally give them admin privileges, too. Unlike all other policy types, login policies are global and can't be attached to individual stacks. They take effect immediately once they're created and affect all future login attempts. Info Login policies are only evaluated for the Cloud or Enterprise plan . Warning Login policies don't affect GitHub organization or SSO admins and private account owners who always get admin access to their respective Spacelift accounts. This is to avoid a situation where a bad login policy locks out everyone from the account. Danger Any change made (create, update or delete) to a login policy will invalidate all active sessions, except the session making the change. A login policy can define the following types of boolean rules: allow - allows the user to log in as a non-admin ; admin - allows the user to log in as an account-wide admin - note that you don't need to explicitly allow admin users; deny - denies login attempt, no matter the result of other ( allow and admin ) rules; deny_admin - denies the current user admin access to the stack, no matter the outcome of other rules; space_admin/space_write/space_read - manages access levels to spaces. More on that in Spaces Access Control ; If no rules match, the default action will be to deny a login attempt. Note that giving folks admin access is a big thing. Admins can do pretty much everything in Spacelift - create and delete stacks, trigger runs or tasks, create, delete and attach contexts and policies, etc. Instead, you can give users limited admin access using the space_admin rule. Danger In practice, any time you define an allow or admin rule, you should probably think of restricting access using a deny rule, too. Please see the examples below to get a better feeling for it.","title":"Purpose"},{"location":"concepts/policy/login-policy.html#data-input","text":"This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"request\" : { \"remote_ip\" : \"string - IP of the user trying to log in\" , \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"session\" : { \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - username of the user trying to log in\" , \"member\" : \"boolean - is the user a member of the account\" , \"name\" : \"string - full name of the user trying to log in - may be empty\" , \"teams\" : [ \"string - names of teams the user is a member of\" ] }, \"spaces\" : [ { \"id\" : \"string - ID of the space\" , \"name\" : \"string - name of the space\" , \"labels\" : [ \"string - label of the space\" ] } ] } Tip OPA string comparisons are case-sensitive so make sure to use the proper case, as defined in your Identity Provider when comparing values. It might be helpful to enable sampling on the policy to see the exact values passed by the Identity Provider. Two fields in the session object may require further explanation: member and teams .","title":"Data input"},{"location":"concepts/policy/login-policy.html#account-membership","text":"When you first log in to Spacelift, we use GitHub as the identity provider and thus we're able to get some of your details from there with username (login) being the most important one. However, each Spacelift account is linked to one and only one GitHub account. Thus, when you log in to a Spacelift account, we're checking if you're a member of that GitHub account. When that GitHub account is an organization, we can explicitly query for your organization membership. If you're a member, you get the member field set to true . If you're not - it's false . For private accounts it's different - they can only have one member, so the check is even simpler - if your login is the same as the name of the linked GitHub account, you get the member field set to true . If it isn't - it's false . When using Single Sign-On with SAML, every successful login attempt will necessarily require that the member field is set to true - if the linked IdP could verify you, you must be a member. Warning Watch this field very closely - it may be very useful for your deny rules.","title":"Account membership"},{"location":"concepts/policy/login-policy.html#teams","text":"When using the default identity provider (GitHub), Teams are only queried for organization accounts - if you're a member of the GitHub organization linked to a Spacelift account, Spacelift will query GitHub API for the full list of teams you're a member of. This list will be available in the session.teams field. For private accounts and non-members, this list will be empty. Note that Spacelift treats GitHub team membership as transitive - for example let's assume Charlie is a member of the Badass team, which is a child of team Awesome . Charlie's list of teams includes both Awesome and Badass , even though he's not a direct member of the team Awesome . For Single Sign-On, the list of teams is pretty much arbitrary and depends on how the SAML assertion attribute is mapped to your user record on the IdP end. Please see the relevant article for more details. Warning Watch this field very closely - it may be very useful for your allow and admin rules.","title":"Teams"},{"location":"concepts/policy/login-policy.html#examples","text":"Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need. There are three possible use cases for login policies - granting access to folks in your org who would otherwise not have it, managing access for external contributors or restricting access to specific circumstances. Let's look into these use cases one by one.","title":"Examples"},{"location":"concepts/policy/login-policy.html#managing-access-levels-within-an-organization","text":"In high-security environments where the principle of least access is applied, it's quite possible that nobody on the infra team gets admin access to GitHub . Still, it would be pretty useful for those people to be in charge of your Spacelift account. Let's create a login policy that will allow every member of the DevOps team to get admin access, and everyone in Engineering to get regular access - we'll give them more granular access to individual stacks later using stack access policies . While at it, let's also explicitly deny access to all non-members just to be on the safe side. 1 2 3 4 5 6 7 8 9 package spacelift teams : = input . session . teams # Make sure to use the GitHub team names , not IDs ( e . g ., \"Example Team\" not \"example-team\" ) # and to omit the GitHub organization name admin { teams [ _ ] == \"DevOps\" } allow { teams [ _ ] == \"Engineering\" } deny { not input . session . member } Here's a minimal example to play with . This is also important for Single Sign-On integrations: only the integration creator gets administrative permissions by default, so all other administrators must be granted their access using a login policy.","title":"Managing access levels within an organization"},{"location":"concepts/policy/login-policy.html#granting-access-to-external-contributors","text":"Danger This feature is not available when using Single Sign-On - your identity provider must be able to successfully validate each user trying to log in to Spacelift. Sometimes you have folks (short-term consultants, most likely) who are not members of your organization but need access to your Spacelift account - either as regular members or perhaps even as admins. There's also the situation where a bunch of friends is working on a hobby project in a personal GitHub account and they could use access to Spacelift. Here are examples of a policy that allows a bunch of whitelisted folks to get regular access and one to get admin privileges: GitHub Google This example uses GitHub usernames to grant access to Spacelift. 1 2 3 4 5 6 7 8 9 package spacelift admins : = { \"alice\" } allowed : = { \"bob\" , \"charlie\" , \"danny\" } login : = input . session . login admin { admins [ login ] } allow { allowed [ login ] } deny { not admins [ login ]; not allowed [ login ] } Here's a minimal example to play with . This example uses email addresses managed by Google to grant access to Spacelift. 1 2 3 4 5 6 7 8 package spacelift admins := { \"alice@example.com\" } login := input.session.login admin { admins[login] } allow { endswith(input.session.login, \"@example.com\") } deny { not admins[login]; not allow } Warning Note that granting access to individuals is less safe than granting access to teams and restricting access to account members. In the latter case, when they lose access to your GitHub org, they automatically lose access to Spacelift. But when whitelisting individuals and not restricting access to members only, you'll need to remember to explicitly remove them from your Spacelift login policy, too.","title":"Granting access to external contributors"},{"location":"concepts/policy/login-policy.html#restricting-access-to-specific-circumstances","text":"Stable and secure infrastructure is crucial to your business continuity. And all changes to your infrastructure carry some risk, so you may want to somehow restrict access to it. The example below is pretty extreme but it shows a very comprehensive policy where you restrict Spacelift access to users logging in from the office IP during business hours. You may want to use elements of this policy to create your own - less draconian - version, or keep it this way to support everyone's work-life balance. Note that this example only defines deny rules so you'll likely want to add some allow and admin rules, too - either in this policy or in a separate one. 1 2 3 4 5 6 7 8 9 10 11 12 package spacelift now : = input . request . timestamp_ns clock : = time . clock ([ now , \"America/Los_Angeles\" ]) weekend : = { \"Saturday\" , \"Sunday\" } weekday : = time . weekday ( now ) ip : = input . request . remote_ip deny { weekend [ weekday ] } deny { clock [ 0 ] < 9 } deny { clock [ 0 ] > 17 } deny { not net . cidr_contains ( \"12.34.56.0/24\" , ip ) } There's a lot to digest here, so a playground example may be helpful.","title":"Restricting access to specific circumstances"},{"location":"concepts/policy/login-policy.html#granting-limited-admin-access","text":"Very often you'd like to give a user admin access limited to a certain set of resources, so that they can manage them without having access to all other resources in that account. You can find more on that use case in Spaces .","title":"Granting limited admin access"},{"location":"concepts/policy/login-policy.html#rewriting-teams","text":"In addition to boolean rules regulating access to your Spacelift account, the login policy exposes the team rule, which allows one to dynamically rewrite the list of teams received from the identity provider. This operation allows one to define Spacelift roles independent of the identity provider. To illustrate this use case, let's imagine you want to define a Superwriter role for someone who's: logging in from an office VPN; is a member of the DevOps team, as defined by your IdP; is not a member of the Contractors team, as defined by your IdP; 1 2 3 4 5 6 7 8 9 10 11 package spacelift team [ \"Superwriter\" ] { office_vpn devops not contractor } contractor { input . session . teams [ _ ] == \"Contractors\" } devops { input . session . teams [ _ ] == \"DevOps\" } office_vpn { net . cidr_contains ( \"12.34.56.0/24\" , input . request . remote_ip ) } What's important here is that the team rule overwrites the original list of teams, meaning that if it evaluates to a non-empty collection, it will replace the original list of teams in the session. In the above example, the Superwriter role will become the only team for the evaluated user session. If the above is not what you want, and you still would like to retain the original list of teams, you can modify the above example the following way: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package spacelift # This rule will copy each of the existing teams to the # new modified list . team [ name ] { name : = input . session . teams [ _ ] } team [ \"Superwriter\" ] { office_vpn devops not contractor } contractor { input . session . teams [ _ ] == \"Contractors\" } devops { input . session . teams [ _ ] == \"DevOps\" } office_vpn { net . cidr_contains ( \"12.34.56.0/24\" , input . request . remote_ip ) } A playground example of the above is available here . Hint Because the user session is updated, the rewritten teams are available in the data input provided to the policy types that receive user information. For example, the rewritten teams can be used in Access policies .","title":"Rewriting teams"},{"location":"concepts/policy/login-policy.html#default-login-policy","text":"If no login policies are defined on the account, Spacelift behaves as if it had this policy: 1 2 3 package spacelift allow { input . session . member }","title":"Default login policy"},{"location":"concepts/policy/notification-policy.html","text":"Notification policy \u00bb Purpose \u00bb Notification policies can be used to filter, route and adjust the body of notification messages sent by Spacelift. The policy works at the Space level meaning that it does not need to be attached to a specific stack , but rather is always evaluated if the Space it's in can be accessed by whatever action is being evaluated. It's also important to note that all notifications go through the policy evaluation. This means any of them can be redirected to the routes defined in the policy. A notification policy can define the following rules: inbox - allows messages to be routed to the Spacelift notification inbox; slack - allows messages to be routed to a given slack channel; webhook - allows messages to be routed to a given webhook; If no rules match no action is taken. Data input \u00bb This is the schema of the data input that each policy request can receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 { \"account\" : { \"name\" : \"string\" }, \"module_version\" : { \"module\" : { \"id\" : \"string - unique ID of the module\" , \"administrative\" : \"boolean - is the module administrative\" , \"branch\" : \"string - tracked branch of the module\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"name\" : \"string - name of the module\" , \"project_root\" : \"optional string - project root as set on the Module, if any\" , \"repository\" : \"string - name of the source repository\" , \"terraform_provider\" : \"string - name of the main Terraform provider used by the module\" , \"space\" : { \"id\" : \"string - id of a space\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"name\" : \"string - name of a the space\" }, \"worker_pool\" : { \"public\" : \"boolean - worker pool information\" , \"id\" : \"string - unique ID of the worker pool\" , \"name\" : \"string - name of the worker pool\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ] } }, \"version\" : { \"commit\" : { \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"hash\" : \"string\" , \"message\" : \"string\" , \"url\" : \"string\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"id\" : \"string - id of the version being created\" , \"latest\" : \"boolean - is the module version latest\" , \"number\" : \"string - semver version number\" , \"state\" : \"string - current module state: ACTIVE, FAILED\" , \"test_runs\" : [{ \"created_at\" : \"number (timestamp in nanoseconds)\" , \"id\" : \"string - id of the test\" , \"state\" : \"string - state of the test\" , \"title\" : \"string - title of the test\" , \"updated_at\" : \"number (timestamp in nanoseconds)\" , }] } }, \"run_updated\" : { \"state\" : \"string\" , \"username\" : \"string\" , \"note\" : \"string\" , \"run\" :{ \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch the run was triggered from\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" }, \"phase\" : \"string enum - plan | apply\" } ], \"command\" : \"string\" , \"commit\" : { \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"hash\" : \"string\" , \"message\" : \"string\" , \"url\" : \"string\" }, \"created_at\" : \"number (timestamp in nanoseconds)\" , \"creator_session\" : { \"admin\" : \"boolean\" , \"creator_ip\" : \"string\" , \"login\" : \"string\" , \"name\" : \"string\" , \"teams\" : [ \"string\" ], \"machine\" : \"boolean - whether the run was kicked off by a human or a machine\" }, \"drift_detection\" : \"boolean\" , \"flags\" : [ \"string\" ], \"id\" : \"string\" , \"runtime_config\" : { \"after_apply\" : [ \"string\" ], \"after_destroy\" : [ \"string\" ], \"after_init\" : [ \"string\" ], \"after_perform\" : [ \"string\" ], \"after_plan\" : [ \"string\" ], \"after_run\" : [ \"string\" ], \"before_apply\" : [ \"string\" ], \"before_destroy\" : [ \"string\" ], \"before_init\" : [ \"string\" ], \"before_perform\" : [ \"string\" ], \"before_plan\" : [ \"string\" ], \"environment\" : \"map[string]string\" , \"project_root\" : \"string\" , \"runner_image\" : \"string\" , \"terraform_version\" : \"string\" }, \"policy_receipts\" : [{ \"flags\" : [ \"string - flag assigned to the policy\" ], \"name\" : \"string - name of the policy\" , \"outcome\" : \"string - outcode of the policy\" , \"type\" : \"string - type of the policy\" }], \"state\" : \"string\" , \"triggered_by\" : \"string or null\" , \"type\" : \"string - PROPOSED or TRACKED\" , \"updated_at\" : \"number (timestamp in nanoseconds)\" , \"user_provided_metadata\" : [ \"string\" ] }, \"stack\" : { \"administrative\" : \"boolean\" , \"autodeploy\" : \"boolean\" , \"autoretry\" : \"boolean\" , \"branch\" : \"string\" , \"id\" : \"string\" , \"labels\" : [ \"string\" ], \"locked_by\" : \"string or null\" , \"name\" : \"string\" , \"namespace\" : \"string or null\" , \"project_root\" : \"string or null\" , \"repository\" : \"string\" , \"space\" : { \"id\" : \"string\" , \"labels\" : [ \"string\" ], \"name\" : \"string\" }, \"state\" : \"string\" , \"terraform_version\" : \"string or null\" , \"tracked_commit\" : { \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"hash\" : \"string\" , \"message\" : \"string\" , \"url\" : \"string\" } } }, \"state\" : \"string\" , \"timing\" : [ { \"duration\" : \"number (in nanoseconds)\" , \"state\" : \"string\" } ], \"username\" : \"string\" , \"webhook_endpoints\" : [ { \"id\" : \"custom-hook2\" , \"labels\" : [ \"example-label1\" , \"example-label2\" ] } ], \"internal_error\" : { \"error\" : \"string\" , \"message\" : \"string\" , \"severity\" : \"string - INFO, WARNING, ERROR\" } } The final JSON object received as input will depend on the type of notification being sent. Event depedant objects will only be present when those events happen. The best way to see what input your Notification policy received is to enable sampling and check the Policy Workbench , but you can also use the table below as a reference: Object Received Event account Any event webhook_endpoints Any event run_updated Run Updated internal_error Internal error occured module_version Module version updated Policy in practice \u00bb Using the notification policy, you can completely re-write notifications or control where and when they are sent. Let's look into how the policy works for each of the defined routes. Choosing a Space for your policy \u00bb When creating notification policies you should take into account the Space in which you're creating them. Generally the policy follows the same conventions as any other Spacelift component, with a few small caveats. Determining Space for run update notifications \u00bb Run update messages will rely on the Space that the run is happening in. It will check any policies in that Space including policies inherited from other Spaces. Determining Space for internal errors \u00bb Most internal errors will check for notification policies inside of the root Space. However if the policy is reporting about a component that belongs to a certain Space and it can determine to which one it is, then it will check for policies in that or any inherited Space. Here is a list of components it will check in order: Stack Worker pool AWS integration Policy Info If you are new to spaces, consider further exploring our documentation about them here Inbox notifications \u00bb Inbox notifications are what you receive in your Spacelift notification inbox . By default, these are errors that happened during some kind of action execution inside Spacelift and are always sent even if you do not have a policy created. However using the policy allows you to alter the body of those errors to add additional context, or even more importantly it allows you to create your own unique notifications. The inbox rule accepts multiple configurable parameters: title - a custom title for the message ( Optional ) body - a custom message body ( Optional ) severity - the severity level for the message ( Optional ) Creating new inbox notifications \u00bb For example here is a inbox rule which will send INFO level notification messages to your inbox when a tracked run has finished: 1 2 3 4 5 6 7 8 9 10 11 12 package spacelift inbox [{ \"title\" : \"Tracked run finished!\" , \"body\" : sprintf(\"http: //example.app.spacelift.io/stack/%s/run/%s has finished\", [stack.id, run.id]), \"severity\": \"INFO\", }] { stack := input.run_updated.stack run := input.run_updated.run run.type == \"TRACKED\" run.state == \"FINISHED\" } View the example in the rego playground . Slack messages \u00bb Slack messages can also be controlled using the notification policy, but before creating any policies that interact with Slack you will need to add the slack integration to your Spacelift account . Info The documentation section about Slack includes additional information like: available actions, slack access policies and more. Consider exploring that part of documentation first. Another important point to mention is that the rules for Slack require a channel_id to be defined. This can be found at the bottom of a channel's About section in Slack: Now you should be ready to define rules for routing Slack messages. Slack rules allow you to make the same filtering decisions as any other rule in the policy. They also allow you to edit the message bodies themselves in order to create custom messages. The Slack rules accept multiple configurable parameters: channel_id - the Slack channel to which the message will be delivered ( Required ) message - a custom message to be sent ( Optional ) Filtering and routing messages \u00bb For example if you wanted to receive only finished runs on a specific Slack channel you would define a rule like this: 1 2 3 4 5 6 7 8 package spacelift slack [{ \"channel_id\" : \"C0000000000\" }] { input . run_updated != null run : = input . run_updated . run run . state == \"FINISHED\" } View the example in the rego playground . Changing the message body \u00bb Together with filtering and routing messages you can also alter the message body itself, here is an example for sending a custom message where a run which tries to attach a policy requires confirmation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 package spacelift slack [{ \"channel_id\" : \"C0000000000\" , \"message\" : sprintf(\"http: //example.app.spacelift.io/stack/%s/run/%s is trying to attach a policy!\", [stack.id, run.id]), }] { stack := input.run_updated.stack run := input.run_updated.run run.type == \"TRACKED\" run.state == \"UNCONFIRMED\" change := run.changes[_] change.phase == \"plan\" change.entity.type == \"spacelift_policy_attachment\" } View the example in the rego playground . Webhook requests \u00bb Info This section of documentation requires you have configured at least one Named Webhook . Consider exploring that part of documentation first. Webhook notifications are a very powerful part of the notification policy. Using them, one is able to not only receive webhooks on specific events that happen in Spacelift, but also craft unique requests to be consumed by some third-party. The notification policy relies on named webhooks which can be created and managed in the Webhooks section of Spacelift . Any policy evaluation will always receive a list of possible webhooks together with their labels as input. The data received in the policy input should be used to determine which webhook will be used when sending the request. The webhook policy accepts multiple configurable parameters: endpoint_id - endpoint id (slug) to which the webhook will be delivered ( Required ) headers - a key value map which will be appended to request headers ( Optional ) payload - a custom valid JSON object to be sent as request body ( Optional ) method - a HTTP method to use when sending the request ( Optional ) Filtering webhook requests \u00bb Filtering and selecting webhooks can be done by using the received input data. Rules can be created where only specific actions should trigger a webhook being sent. For example we could define a rule which would allow a webhook to be sent about any drift detection run: 1 2 3 4 5 6 7 8 package spacelift webhook [{ \"endpoint_id\" : endpoint . id }] { endpoint : = input . webhook_endpoints [ _ ] endpoint . id == \"drift-hook\" input . run_updated . run . drift_detection == true input . run_updated . run . type == \"PROPOSED\" } View the example in the rego playground . Creating a custom webhook request \u00bb All requests sent will always include the default headers for verification, a payload which is appropriate for the message type and the method set as POST . However, by using the webhook rule we can modify the body of the request, change the method or add additional headers. For example, if we wanted to define a completely custom request for a tracked run we would define a rule like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package spacelift webhook [ wbdata ] { endpoint : = input . webhook_endpoints [ _ ] endpoint . id == \"testing-notifications\" wbdata : = { \"endpoint_id\" : endpoint . id , \"payload\" : { \"custom_field\": \"This is a custom message\", \"run_type\": input.run_updated.run.type, \"run_state\": input.run_updated.run.state, \"updated_at\": input.run_updated.run.updated_at, } , \"method\" : \"PUT\" , \"headers\" : { \"custom-header\": \"custom\", } , } input . run_updated . run . type == \"TRACKED\" } View the example in the rego playground . Using custom webhook requests also makes it quite easy to integrate Spacelift with any third-party webhook consumer. Custom webhook requests in action \u00bb Discord integration \u00bb Discord can be integrated to receive updates about Spacelift by simply creating a new webhook endpoint in your Discord server's integrations section and providing that as the endpoint when creating a new named webhook . Info For more information about making Discord webhooks follow their official webhook guide . After creating the webhook on both Discord and Spacelift you will need to define a new webhook rule like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Send updates about tracked runs to discord . webhook [ wbdata ] { endpoint : = input . webhook_endpoints [ _ ] endpoint . id == \"YOUR_WEBHOOK_ID_HERE\" stack : = input . run_updated . stack run : = input . run_updated . run wbdata : = { \"endpoint_id\" : endpoint . id , \"payload\" : { \"embeds\": [{ \"title\": \"Tracked run triggered!\", \"description\": sprintf(\"Stack: [%s](http: //example.app.spacelift.io/stack/%s)\\nRun ID: [%s](http://example.app.spacelift.io/stack/%s/run/%s)\\nRun state: %s\", [stack.name,stack.id,run.id,stack.id, run.id,run.state]), }] } } input . run_updated . run . type == \"TRACKED\" } And that's it! You should now be receiving updates about tracked runs to your Discord server:","title":"Notification policy"},{"location":"concepts/policy/notification-policy.html#notification-policy","text":"","title":"Notification policy"},{"location":"concepts/policy/notification-policy.html#purpose","text":"Notification policies can be used to filter, route and adjust the body of notification messages sent by Spacelift. The policy works at the Space level meaning that it does not need to be attached to a specific stack , but rather is always evaluated if the Space it's in can be accessed by whatever action is being evaluated. It's also important to note that all notifications go through the policy evaluation. This means any of them can be redirected to the routes defined in the policy. A notification policy can define the following rules: inbox - allows messages to be routed to the Spacelift notification inbox; slack - allows messages to be routed to a given slack channel; webhook - allows messages to be routed to a given webhook; If no rules match no action is taken.","title":"Purpose"},{"location":"concepts/policy/notification-policy.html#data-input","text":"This is the schema of the data input that each policy request can receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 { \"account\" : { \"name\" : \"string\" }, \"module_version\" : { \"module\" : { \"id\" : \"string - unique ID of the module\" , \"administrative\" : \"boolean - is the module administrative\" , \"branch\" : \"string - tracked branch of the module\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"name\" : \"string - name of the module\" , \"project_root\" : \"optional string - project root as set on the Module, if any\" , \"repository\" : \"string - name of the source repository\" , \"terraform_provider\" : \"string - name of the main Terraform provider used by the module\" , \"space\" : { \"id\" : \"string - id of a space\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"name\" : \"string - name of a the space\" }, \"worker_pool\" : { \"public\" : \"boolean - worker pool information\" , \"id\" : \"string - unique ID of the worker pool\" , \"name\" : \"string - name of the worker pool\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ] } }, \"version\" : { \"commit\" : { \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"hash\" : \"string\" , \"message\" : \"string\" , \"url\" : \"string\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"id\" : \"string - id of the version being created\" , \"latest\" : \"boolean - is the module version latest\" , \"number\" : \"string - semver version number\" , \"state\" : \"string - current module state: ACTIVE, FAILED\" , \"test_runs\" : [{ \"created_at\" : \"number (timestamp in nanoseconds)\" , \"id\" : \"string - id of the test\" , \"state\" : \"string - state of the test\" , \"title\" : \"string - title of the test\" , \"updated_at\" : \"number (timestamp in nanoseconds)\" , }] } }, \"run_updated\" : { \"state\" : \"string\" , \"username\" : \"string\" , \"note\" : \"string\" , \"run\" :{ \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch the run was triggered from\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" }, \"phase\" : \"string enum - plan | apply\" } ], \"command\" : \"string\" , \"commit\" : { \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"hash\" : \"string\" , \"message\" : \"string\" , \"url\" : \"string\" }, \"created_at\" : \"number (timestamp in nanoseconds)\" , \"creator_session\" : { \"admin\" : \"boolean\" , \"creator_ip\" : \"string\" , \"login\" : \"string\" , \"name\" : \"string\" , \"teams\" : [ \"string\" ], \"machine\" : \"boolean - whether the run was kicked off by a human or a machine\" }, \"drift_detection\" : \"boolean\" , \"flags\" : [ \"string\" ], \"id\" : \"string\" , \"runtime_config\" : { \"after_apply\" : [ \"string\" ], \"after_destroy\" : [ \"string\" ], \"after_init\" : [ \"string\" ], \"after_perform\" : [ \"string\" ], \"after_plan\" : [ \"string\" ], \"after_run\" : [ \"string\" ], \"before_apply\" : [ \"string\" ], \"before_destroy\" : [ \"string\" ], \"before_init\" : [ \"string\" ], \"before_perform\" : [ \"string\" ], \"before_plan\" : [ \"string\" ], \"environment\" : \"map[string]string\" , \"project_root\" : \"string\" , \"runner_image\" : \"string\" , \"terraform_version\" : \"string\" }, \"policy_receipts\" : [{ \"flags\" : [ \"string - flag assigned to the policy\" ], \"name\" : \"string - name of the policy\" , \"outcome\" : \"string - outcode of the policy\" , \"type\" : \"string - type of the policy\" }], \"state\" : \"string\" , \"triggered_by\" : \"string or null\" , \"type\" : \"string - PROPOSED or TRACKED\" , \"updated_at\" : \"number (timestamp in nanoseconds)\" , \"user_provided_metadata\" : [ \"string\" ] }, \"stack\" : { \"administrative\" : \"boolean\" , \"autodeploy\" : \"boolean\" , \"autoretry\" : \"boolean\" , \"branch\" : \"string\" , \"id\" : \"string\" , \"labels\" : [ \"string\" ], \"locked_by\" : \"string or null\" , \"name\" : \"string\" , \"namespace\" : \"string or null\" , \"project_root\" : \"string or null\" , \"repository\" : \"string\" , \"space\" : { \"id\" : \"string\" , \"labels\" : [ \"string\" ], \"name\" : \"string\" }, \"state\" : \"string\" , \"terraform_version\" : \"string or null\" , \"tracked_commit\" : { \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"hash\" : \"string\" , \"message\" : \"string\" , \"url\" : \"string\" } } }, \"state\" : \"string\" , \"timing\" : [ { \"duration\" : \"number (in nanoseconds)\" , \"state\" : \"string\" } ], \"username\" : \"string\" , \"webhook_endpoints\" : [ { \"id\" : \"custom-hook2\" , \"labels\" : [ \"example-label1\" , \"example-label2\" ] } ], \"internal_error\" : { \"error\" : \"string\" , \"message\" : \"string\" , \"severity\" : \"string - INFO, WARNING, ERROR\" } } The final JSON object received as input will depend on the type of notification being sent. Event depedant objects will only be present when those events happen. The best way to see what input your Notification policy received is to enable sampling and check the Policy Workbench , but you can also use the table below as a reference: Object Received Event account Any event webhook_endpoints Any event run_updated Run Updated internal_error Internal error occured module_version Module version updated","title":"Data input"},{"location":"concepts/policy/notification-policy.html#policy-in-practice","text":"Using the notification policy, you can completely re-write notifications or control where and when they are sent. Let's look into how the policy works for each of the defined routes.","title":"Policy in practice"},{"location":"concepts/policy/notification-policy.html#choosing-a-space-for-your-policy","text":"When creating notification policies you should take into account the Space in which you're creating them. Generally the policy follows the same conventions as any other Spacelift component, with a few small caveats.","title":"Choosing a Space for your policy"},{"location":"concepts/policy/notification-policy.html#determining-space-for-run-update-notifications","text":"Run update messages will rely on the Space that the run is happening in. It will check any policies in that Space including policies inherited from other Spaces.","title":"Determining Space for run update notifications"},{"location":"concepts/policy/notification-policy.html#determining-space-for-internal-errors","text":"Most internal errors will check for notification policies inside of the root Space. However if the policy is reporting about a component that belongs to a certain Space and it can determine to which one it is, then it will check for policies in that or any inherited Space. Here is a list of components it will check in order: Stack Worker pool AWS integration Policy Info If you are new to spaces, consider further exploring our documentation about them here","title":"Determining Space for internal errors"},{"location":"concepts/policy/notification-policy.html#inbox-notifications","text":"Inbox notifications are what you receive in your Spacelift notification inbox . By default, these are errors that happened during some kind of action execution inside Spacelift and are always sent even if you do not have a policy created. However using the policy allows you to alter the body of those errors to add additional context, or even more importantly it allows you to create your own unique notifications. The inbox rule accepts multiple configurable parameters: title - a custom title for the message ( Optional ) body - a custom message body ( Optional ) severity - the severity level for the message ( Optional )","title":"Inbox notifications"},{"location":"concepts/policy/notification-policy.html#creating-new-inbox-notifications","text":"For example here is a inbox rule which will send INFO level notification messages to your inbox when a tracked run has finished: 1 2 3 4 5 6 7 8 9 10 11 12 package spacelift inbox [{ \"title\" : \"Tracked run finished!\" , \"body\" : sprintf(\"http: //example.app.spacelift.io/stack/%s/run/%s has finished\", [stack.id, run.id]), \"severity\": \"INFO\", }] { stack := input.run_updated.stack run := input.run_updated.run run.type == \"TRACKED\" run.state == \"FINISHED\" } View the example in the rego playground .","title":"Creating new inbox notifications"},{"location":"concepts/policy/notification-policy.html#slack-messages","text":"Slack messages can also be controlled using the notification policy, but before creating any policies that interact with Slack you will need to add the slack integration to your Spacelift account . Info The documentation section about Slack includes additional information like: available actions, slack access policies and more. Consider exploring that part of documentation first. Another important point to mention is that the rules for Slack require a channel_id to be defined. This can be found at the bottom of a channel's About section in Slack: Now you should be ready to define rules for routing Slack messages. Slack rules allow you to make the same filtering decisions as any other rule in the policy. They also allow you to edit the message bodies themselves in order to create custom messages. The Slack rules accept multiple configurable parameters: channel_id - the Slack channel to which the message will be delivered ( Required ) message - a custom message to be sent ( Optional )","title":"Slack messages"},{"location":"concepts/policy/notification-policy.html#filtering-and-routing-messages","text":"For example if you wanted to receive only finished runs on a specific Slack channel you would define a rule like this: 1 2 3 4 5 6 7 8 package spacelift slack [{ \"channel_id\" : \"C0000000000\" }] { input . run_updated != null run : = input . run_updated . run run . state == \"FINISHED\" } View the example in the rego playground .","title":"Filtering and routing messages"},{"location":"concepts/policy/notification-policy.html#changing-the-message-body","text":"Together with filtering and routing messages you can also alter the message body itself, here is an example for sending a custom message where a run which tries to attach a policy requires confirmation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 package spacelift slack [{ \"channel_id\" : \"C0000000000\" , \"message\" : sprintf(\"http: //example.app.spacelift.io/stack/%s/run/%s is trying to attach a policy!\", [stack.id, run.id]), }] { stack := input.run_updated.stack run := input.run_updated.run run.type == \"TRACKED\" run.state == \"UNCONFIRMED\" change := run.changes[_] change.phase == \"plan\" change.entity.type == \"spacelift_policy_attachment\" } View the example in the rego playground .","title":"Changing the message body"},{"location":"concepts/policy/notification-policy.html#webhook-requests","text":"Info This section of documentation requires you have configured at least one Named Webhook . Consider exploring that part of documentation first. Webhook notifications are a very powerful part of the notification policy. Using them, one is able to not only receive webhooks on specific events that happen in Spacelift, but also craft unique requests to be consumed by some third-party. The notification policy relies on named webhooks which can be created and managed in the Webhooks section of Spacelift . Any policy evaluation will always receive a list of possible webhooks together with their labels as input. The data received in the policy input should be used to determine which webhook will be used when sending the request. The webhook policy accepts multiple configurable parameters: endpoint_id - endpoint id (slug) to which the webhook will be delivered ( Required ) headers - a key value map which will be appended to request headers ( Optional ) payload - a custom valid JSON object to be sent as request body ( Optional ) method - a HTTP method to use when sending the request ( Optional )","title":"Webhook requests"},{"location":"concepts/policy/notification-policy.html#filtering-webhook-requests","text":"Filtering and selecting webhooks can be done by using the received input data. Rules can be created where only specific actions should trigger a webhook being sent. For example we could define a rule which would allow a webhook to be sent about any drift detection run: 1 2 3 4 5 6 7 8 package spacelift webhook [{ \"endpoint_id\" : endpoint . id }] { endpoint : = input . webhook_endpoints [ _ ] endpoint . id == \"drift-hook\" input . run_updated . run . drift_detection == true input . run_updated . run . type == \"PROPOSED\" } View the example in the rego playground .","title":"Filtering webhook requests"},{"location":"concepts/policy/notification-policy.html#creating-a-custom-webhook-request","text":"All requests sent will always include the default headers for verification, a payload which is appropriate for the message type and the method set as POST . However, by using the webhook rule we can modify the body of the request, change the method or add additional headers. For example, if we wanted to define a completely custom request for a tracked run we would define a rule like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package spacelift webhook [ wbdata ] { endpoint : = input . webhook_endpoints [ _ ] endpoint . id == \"testing-notifications\" wbdata : = { \"endpoint_id\" : endpoint . id , \"payload\" : { \"custom_field\": \"This is a custom message\", \"run_type\": input.run_updated.run.type, \"run_state\": input.run_updated.run.state, \"updated_at\": input.run_updated.run.updated_at, } , \"method\" : \"PUT\" , \"headers\" : { \"custom-header\": \"custom\", } , } input . run_updated . run . type == \"TRACKED\" } View the example in the rego playground . Using custom webhook requests also makes it quite easy to integrate Spacelift with any third-party webhook consumer.","title":"Creating a custom webhook request"},{"location":"concepts/policy/notification-policy.html#custom-webhook-requests-in-action","text":"","title":"Custom webhook requests in action"},{"location":"concepts/policy/notification-policy.html#discord-integration","text":"Discord can be integrated to receive updates about Spacelift by simply creating a new webhook endpoint in your Discord server's integrations section and providing that as the endpoint when creating a new named webhook . Info For more information about making Discord webhooks follow their official webhook guide . After creating the webhook on both Discord and Spacelift you will need to define a new webhook rule like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Send updates about tracked runs to discord . webhook [ wbdata ] { endpoint : = input . webhook_endpoints [ _ ] endpoint . id == \"YOUR_WEBHOOK_ID_HERE\" stack : = input . run_updated . stack run : = input . run_updated . run wbdata : = { \"endpoint_id\" : endpoint . id , \"payload\" : { \"embeds\": [{ \"title\": \"Tracked run triggered!\", \"description\": sprintf(\"Stack: [%s](http: //example.app.spacelift.io/stack/%s)\\nRun ID: [%s](http://example.app.spacelift.io/stack/%s/run/%s)\\nRun state: %s\", [stack.name,stack.id,run.id,stack.id, run.id,run.state]), }] } } input . run_updated . run . type == \"TRACKED\" } And that's it! You should now be receiving updates about tracked runs to your Discord server:","title":"Discord integration"},{"location":"concepts/policy/run-initialization-policy.html","text":"Initialization policy \u00bb Warning This feature is deprecated. New users should not use this feature and existing users are encouraged to migrate to the approval policy , which offers a much more flexible and powerful way to control which runs are allowed to proceed. A migration guide is available here . Purpose \u00bb Initialization policy can prevent a Run or a Task from being initialized , thus blocking any custom code or commands from being executed. It superficially looks like a plan policy in that it affects an existing Run and prints feedback to logs, but it does not get access to the plan. Instead, it can be used to protect your stack from unwanted changes or enforce organizational rules concerning how and when runs are supposed to be triggered. Warning Server-side initialization policies are being deprecated. We will be replacing them with worker-side policies that can be set by using the launcher run initialization policy flag ( SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY ). For a limited time period we will be running both types of initialization policy checks but ultimately we're planning to move the pre-flight checks to the worker node, thus allowing customers to block suspicious looking jobs on their end. Let's create a simple initialization policy, attach it to the stack, and see what gives: 1 2 3 4 5 package spacelift deny [ \"you shall not pass\" ] { true } ...and boom: Rules \u00bb Initialization policies are simple in that they only use a single rule - deny - with a string message. A single result for that rule will fail the run before it has a chance to start - as we've just witnessed above. Data input \u00bb This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 { \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"message\" : \"string - commit message\" }, \"request\" : { \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"run\" : { \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" }, \"phase\" : \"string enum - plan | apply\" } ], \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"flags\" : [ \"string - list of flags set on the run by other policies\" ], \"id\" : \"string - the run ID\" , \"runtime_config\" : { \"before_init\" : [ \"string - command to run before run initialization\" ], \"project_root\" : \"string - root of the Terraform project\" , \"runner_image\" : \"string - Docker image used to execute the run\" , \"terraform_version\" : \"string - Terraform version used to for the run\" }, \"state\" : \"string - the current run state\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - PROPOSED or TRACKED\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }, \"stack\" : { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" } } Aliases \u00bb In addition to our helper functions , we provide aliases for commonly used parts of the input data: Alias Source commit input.commit run input.run runtime_config input.run.runtime_config stack input.stack Use cases \u00bb There are two main use cases for run initialization policies - protecting your stack from unwanted changes and enforcing organizational rules . Let's look at these one by one. Protect your stack from unwanted changes \u00bb While specialized, Spacelift is still a CI/CD platform and thus allows running custom code before Terraform initialization phase using before_init scripts. This is a very powerful feature, but as always, with great power comes great responsibility. Since those scripts get full access to your Terraform environment, how hard is it to create a commit on a feature branch that would run terraform destroy -auto-approve ? Sure, all Spacelift runs are tracked and this prank will sooner or later be tracked down to the individual who ran it, but at that point do you still have a business? That's where initialization policies can help. Let's explicitly blacklist all Terraform commands if they're running as before_init scripts. OK, let's maybe add a single exception for a formatting check. 1 2 3 4 5 6 7 8 package spacelift deny [ sprintf ( \"don't use Terraform please (%s)\" , [ command ])] { command : = input . run . runtime_config . before_init [ _ ] contains ( command , \"terraform\" ) command != \"terraform fmt -check\" } Feel free to play with this example in the Rego playground . OK, but what if someone gets clever and creates a Docker image that symlinks something very innocent-looking to terraform ? Well, you have two choices - you could replace a blacklist with a whitelist, but a clever attacker can be really clever. So the other choice is to make sure that a known good Docker is used to execute the run. Here's an example: 1 2 3 4 5 6 7 package spacelift deny [ sprintf ( \"unexpected runner image (%s)\" , [ image ])] { image : = input . run . runtime_config . runner_image image != \"spacelift/runner:latest\" } Here's the above example in the Rego playground . Danger Obviously, if you're using an image other than what we control, you still have to ensure that the attacker can't push bad code to your Docker repo. Alas, this is beyond our control. Enforce organizational rules \u00bb While the previous section was all about making sure that bad stuff does not get executed, this use case presents run initialization policies as a way to ensure best practices - ensuring that the right things get executed the right way and at the right time. One of the above examples explicitly whitelisted Terraform formatting check. Keeping your code formatted in a standard way is generally a good idea, so let's make sure that this command always gets executed first. Note that as per Anna Karenina principle this check is most elegantly defined as a negation of another rule matching the required state of affairs: 1 2 3 4 5 6 7 8 9 10 package spacelift deny [ \"please always run formatting check first\" ] { not formatting_first } formatting_first { input . run . runtime_config . before_init [ i ] == \"terraform fmt -check\" i == 0 } Here's this example in the Rego playground . This time we'll skip the mandatory \"don't deploy on weekends\" check because while it could also be implemented here, there are probably better places to do it. Instead, let's enforce a feature branch naming convention. We'll keep this example simple, requiring that feature branches start with either feature/ or fix/ , but you can go fancy and require references to Jira tickets or even look at commit messages: 1 2 3 4 5 6 7 8 package spacelift deny [ sprintf ( \"invalid feature branch name (%s)\" , [ branch ])] { branch : = input . commit . branch input . run . type == \"PROPOSED\" not re_match ( \"^(fix|feature) \\ /.*\" , branch ) } Here's this example in the Rego playground . Migration guide \u00bb A run initialization policy can be expressed as an approval policy if it defines a single reject rule, and an approve rule that is its negation. Below you will find equivalents of the examples above expressed as approval policies . Migration example: enforcing Terraform check \u00bb 1 2 3 4 5 6 7 8 9 10 package spacelift reject { not formatting_first } approve { not reject } formatting_first { input . run . runtime_config . before_init [ i ] == \"terraform fmt -check\" i == 0 } Migration example: disallowing before-init Terraform commands other than formatting \u00bb 1 2 3 4 5 6 7 8 package spacelift reject { command : = input . run . runtime_config . before_init [ _ ] contains ( command , \"terraform\" ); command != \"terraform fmt -check\" } approve { not reject } Migration example: enforcing runner image \u00bb 1 2 3 4 5 6 7 package spacelift reject { input . run . runtime_config . runner_image != \"spacelift/runner:latest\" } approve { not reject } Migration example: enforcing feature branch naming convention \u00bb 1 2 3 4 5 6 7 8 9 package spacelift reject { branch : = input . run . commit . branch input . run . type == \"PROPOSED\" not re_match ( \"^(fix|feature) \\ /.*\" , branch ) } approve { not reject }","title":"Initialization policy"},{"location":"concepts/policy/run-initialization-policy.html#initialization-policy","text":"Warning This feature is deprecated. New users should not use this feature and existing users are encouraged to migrate to the approval policy , which offers a much more flexible and powerful way to control which runs are allowed to proceed. A migration guide is available here .","title":"Initialization policy"},{"location":"concepts/policy/run-initialization-policy.html#purpose","text":"Initialization policy can prevent a Run or a Task from being initialized , thus blocking any custom code or commands from being executed. It superficially looks like a plan policy in that it affects an existing Run and prints feedback to logs, but it does not get access to the plan. Instead, it can be used to protect your stack from unwanted changes or enforce organizational rules concerning how and when runs are supposed to be triggered. Warning Server-side initialization policies are being deprecated. We will be replacing them with worker-side policies that can be set by using the launcher run initialization policy flag ( SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY ). For a limited time period we will be running both types of initialization policy checks but ultimately we're planning to move the pre-flight checks to the worker node, thus allowing customers to block suspicious looking jobs on their end. Let's create a simple initialization policy, attach it to the stack, and see what gives: 1 2 3 4 5 package spacelift deny [ \"you shall not pass\" ] { true } ...and boom:","title":"Purpose"},{"location":"concepts/policy/run-initialization-policy.html#rules","text":"Initialization policies are simple in that they only use a single rule - deny - with a string message. A single result for that rule will fail the run before it has a chance to start - as we've just witnessed above.","title":"Rules"},{"location":"concepts/policy/run-initialization-policy.html#data-input","text":"This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 { \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"message\" : \"string - commit message\" }, \"request\" : { \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"run\" : { \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" }, \"phase\" : \"string enum - plan | apply\" } ], \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"flags\" : [ \"string - list of flags set on the run by other policies\" ], \"id\" : \"string - the run ID\" , \"runtime_config\" : { \"before_init\" : [ \"string - command to run before run initialization\" ], \"project_root\" : \"string - root of the Terraform project\" , \"runner_image\" : \"string - Docker image used to execute the run\" , \"terraform_version\" : \"string - Terraform version used to for the run\" }, \"state\" : \"string - the current run state\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - PROPOSED or TRACKED\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }, \"stack\" : { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" } }","title":"Data input"},{"location":"concepts/policy/run-initialization-policy.html#aliases","text":"In addition to our helper functions , we provide aliases for commonly used parts of the input data: Alias Source commit input.commit run input.run runtime_config input.run.runtime_config stack input.stack","title":"Aliases"},{"location":"concepts/policy/run-initialization-policy.html#use-cases","text":"There are two main use cases for run initialization policies - protecting your stack from unwanted changes and enforcing organizational rules . Let's look at these one by one.","title":"Use cases"},{"location":"concepts/policy/run-initialization-policy.html#protect-your-stack-from-unwanted-changes","text":"While specialized, Spacelift is still a CI/CD platform and thus allows running custom code before Terraform initialization phase using before_init scripts. This is a very powerful feature, but as always, with great power comes great responsibility. Since those scripts get full access to your Terraform environment, how hard is it to create a commit on a feature branch that would run terraform destroy -auto-approve ? Sure, all Spacelift runs are tracked and this prank will sooner or later be tracked down to the individual who ran it, but at that point do you still have a business? That's where initialization policies can help. Let's explicitly blacklist all Terraform commands if they're running as before_init scripts. OK, let's maybe add a single exception for a formatting check. 1 2 3 4 5 6 7 8 package spacelift deny [ sprintf ( \"don't use Terraform please (%s)\" , [ command ])] { command : = input . run . runtime_config . before_init [ _ ] contains ( command , \"terraform\" ) command != \"terraform fmt -check\" } Feel free to play with this example in the Rego playground . OK, but what if someone gets clever and creates a Docker image that symlinks something very innocent-looking to terraform ? Well, you have two choices - you could replace a blacklist with a whitelist, but a clever attacker can be really clever. So the other choice is to make sure that a known good Docker is used to execute the run. Here's an example: 1 2 3 4 5 6 7 package spacelift deny [ sprintf ( \"unexpected runner image (%s)\" , [ image ])] { image : = input . run . runtime_config . runner_image image != \"spacelift/runner:latest\" } Here's the above example in the Rego playground . Danger Obviously, if you're using an image other than what we control, you still have to ensure that the attacker can't push bad code to your Docker repo. Alas, this is beyond our control.","title":"Protect your stack from unwanted changes"},{"location":"concepts/policy/run-initialization-policy.html#enforce-organizational-rules","text":"While the previous section was all about making sure that bad stuff does not get executed, this use case presents run initialization policies as a way to ensure best practices - ensuring that the right things get executed the right way and at the right time. One of the above examples explicitly whitelisted Terraform formatting check. Keeping your code formatted in a standard way is generally a good idea, so let's make sure that this command always gets executed first. Note that as per Anna Karenina principle this check is most elegantly defined as a negation of another rule matching the required state of affairs: 1 2 3 4 5 6 7 8 9 10 package spacelift deny [ \"please always run formatting check first\" ] { not formatting_first } formatting_first { input . run . runtime_config . before_init [ i ] == \"terraform fmt -check\" i == 0 } Here's this example in the Rego playground . This time we'll skip the mandatory \"don't deploy on weekends\" check because while it could also be implemented here, there are probably better places to do it. Instead, let's enforce a feature branch naming convention. We'll keep this example simple, requiring that feature branches start with either feature/ or fix/ , but you can go fancy and require references to Jira tickets or even look at commit messages: 1 2 3 4 5 6 7 8 package spacelift deny [ sprintf ( \"invalid feature branch name (%s)\" , [ branch ])] { branch : = input . commit . branch input . run . type == \"PROPOSED\" not re_match ( \"^(fix|feature) \\ /.*\" , branch ) } Here's this example in the Rego playground .","title":"Enforce organizational rules"},{"location":"concepts/policy/run-initialization-policy.html#migration-guide","text":"A run initialization policy can be expressed as an approval policy if it defines a single reject rule, and an approve rule that is its negation. Below you will find equivalents of the examples above expressed as approval policies .","title":"Migration guide"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-enforcing-terraform-check","text":"1 2 3 4 5 6 7 8 9 10 package spacelift reject { not formatting_first } approve { not reject } formatting_first { input . run . runtime_config . before_init [ i ] == \"terraform fmt -check\" i == 0 }","title":"Migration example: enforcing Terraform check"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-disallowing-before-init-terraform-commands-other-than-formatting","text":"1 2 3 4 5 6 7 8 package spacelift reject { command : = input . run . runtime_config . before_init [ _ ] contains ( command , \"terraform\" ); command != \"terraform fmt -check\" } approve { not reject }","title":"Migration example: disallowing before-init Terraform commands other than formatting"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-enforcing-runner-image","text":"1 2 3 4 5 6 7 package spacelift reject { input . run . runtime_config . runner_image != \"spacelift/runner:latest\" } approve { not reject }","title":"Migration example: enforcing runner image"},{"location":"concepts/policy/run-initialization-policy.html#migration-example-enforcing-feature-branch-naming-convention","text":"1 2 3 4 5 6 7 8 9 package spacelift reject { branch : = input . run . commit . branch input . run . type == \"PROPOSED\" not re_match ( \"^(fix|feature) \\ /.*\" , branch ) } approve { not reject }","title":"Migration example: enforcing feature branch naming convention"},{"location":"concepts/policy/stack-access-policy.html","text":"Access policy \u00bb Danger Access policies are deprecated in favour of Space access rules in the login policy. See Spaces for more details. Purpose \u00bb By default, non-admin users have no access to any Stacks or Modules and must be granted that explicitly. There are two levels of non-admin access - reader and writer, and the exact meaning of these roles is covered in a separate section . For now all we need to care about is that access policies are what we use to give appropriate level of access to individual stacks to non-admin users in your account. This type of access control is typically done either by building a separate user management system on your end or piggy-backing on one created by your identity provider. Both solutions have their limitations - a separate user management system makes it more difficult for organizations to onboard and offboard users, and the last thing we want is for a guy that was just fired to log in to Spacelift and have their revenge. User management systems are also pretty difficult to get right, too, especially if granular and sophisticated access controls are required. Piggy-backing on the identity provider is probably a safer bet and is used by many DevTools vendors. With this approach, having some access level to a GitHub repo would give you the same access level to all Spacelift stacks and/or modules associated with it. That's somewhat reasonable, but not as flexible as having your own fancy user management system. Imagine having two stacks linked to one repo, representing two environments - staging and production. It's quite possible that you'd appreciate separate access controls for these two. Access policies offer the best of both worlds - they give you a tool to build your own access management system using data obtained either from our identity provider (GitHub), or from your identity provider if using Single Sign-On integration . In subsequent sections we'll dive deeper into what data is exposed to your policies , how you can define access policies with different levels of access, and what those levels actually mean . Rules \u00bb Your access policy can define the following boolean rules: write : gives the current user write access to the stack or module; read : gives the current user read access to the stack or module; deny : denies the current user all access to the stack or module, no matter the outcome of other rules; deny_write : denies the current user write access to the stack or module, no matter the outcome of other rules; Note that write access automatically assumes read permissions, too, so there's no need to define separate read policies for writers. Another thing to keep in mind when defining access policies is that they are executed quickly. Internally, we expect that running all access policies on all the stacks in one request ( stacks in the GraphQL API ) will take less than 500 milliseconds - otherwise the request fails. That's actually plenty for modern computers, but think twice before creating fancy regex rules in your access policies. Readers and writers \u00bb There are two levels of non-admin access to a Spacelift stack or module - reader and writer. These are pretty intuitive for most developers, but this section will cover them in more detail to avoid any possible confusion. But first, let's try to understand the use case for different levels of access. In every non-trivial organization there will be different roles - folks who build and manage shared infrastructure, folks who build and manage their team or project-level infrastructure, and folks who use this infrastructure to build great things. The first group is probably the people who manage your Spacelift accounts - the admins . They need to be able to set up everything - create stacks , contexts and policies , and attach them accordingly. You'd normally use login policies to manage their access. The second group - folks who manage their team or project-level infrastructure - should have a reasonable level of access to their project. They should be able to define the environment , set up various integrations, trigger and confirm runs , execute tasks . This level of access is granted by the writer permission. However, writers should still operate within the boundaries defined by admins , who do that mainly by attaching contexts and policies to the stacks . Last but not least the third group - folks who build things on top of existing infra - don't necessarily need to define the infra, but they need to understand what's available and when things are changing. You'll probably want to allow them to contribute to infra definitions, too, and allow them to see feedback from proposed runs. They can't do anything, but they can see everything. These are the readers . Most modern organizations tend to provide this level of access to as many stakeholders as possible to maintain transparency and facilitate collaboration. Data input \u00bb This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \"request\" : { \"remote_ip\" : \"string - IP of the user making a request\" , \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"session\" : { \"admin\" : \"boolean - is the current user a Spacelift admin\" , \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - GitHub username of the logged in user\" , \"name\" : \"string - full name of the logged in GitHub user - may be empty\" , \"teams\" : [ \"string - names of org teams the user is a member of\" ], \"machine\" : \"boolean - whether the creator is a machine or a user\" }, \"stack\" : { // when access to a stack is being evaluated \"id\" : \"string - unique ID of the stack\" , \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" }, \"module\" : { // when access to a module is being evaluated \"id\" : \"string - unique ID of the module\" , \"administrative\" : \"boolean - is the stack administrative\" , \"branch\" : \"string - tracked branch of the module\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"repository\" : \"string - name of the source repository\" , \"terraform_provider\" : \"string - name of the main Terraform provider used by the module\" } } Examples \u00bb Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need. With all the above theory in mind, let's jump straight to the code and define some access policies. This section will cover some common exaxmples that can be copied more or less directly, and some contrived ones to serve as an inspiration. Info Remember that access policies must be attached to a stack or a module to take effect. Read access to everyone (in Engineering) \u00bb I get read access, you get read access, everyone gets read access. As long as they're members of the Engineering team: 1 2 3 package spacelift read { input . session . teams [ _ ] == \"Engineering\" } OK, that was simple. But let's also see it in the Rego playground . In case things go wrong, we want you to be there \u00bb You know when things go wrong it's usually because someone did something. Like an infra deployment. Let's try to make sure they're in the office when doing so and restrict write access to business hours and office IP range. This policy is best combined with one that gives read access. 1 2 3 4 5 6 7 8 9 10 11 12 13 package spacelift now : = input . request . timestamp_ns clock : = time . clock ([ now , \"America/Los_Angeles\" ]) weekend : = { \"Saturday\" , \"Sunday\" } weekday : = time . weekday ( now ) ip : = input . request . remote_ip write { input . session . teams [ _ ] == \"Product team\" } deny_write { weekend [ weekday ] } deny_write { clock [ 0 ] < 9 } deny_write { clock [ 0 ] > 17 } deny_write { not net . cidr_contains ( \"12.34.56.0/24\" , ip ) } Here is this example in Rego playground . Protect administrative stacks \u00bb Administrative stacks are powerful - getting write access to one is almost as good as being an admin - you can define and attach contexts and policies . So let's deny write access to them entirely. This works since access policies are not evaluated for admin users. 1 2 3 package spacelift deny_write { input . stack . administrative } And here's the necessary Rego playground example .","title":"Access policy"},{"location":"concepts/policy/stack-access-policy.html#access-policy","text":"Danger Access policies are deprecated in favour of Space access rules in the login policy. See Spaces for more details.","title":"Access policy"},{"location":"concepts/policy/stack-access-policy.html#purpose","text":"By default, non-admin users have no access to any Stacks or Modules and must be granted that explicitly. There are two levels of non-admin access - reader and writer, and the exact meaning of these roles is covered in a separate section . For now all we need to care about is that access policies are what we use to give appropriate level of access to individual stacks to non-admin users in your account. This type of access control is typically done either by building a separate user management system on your end or piggy-backing on one created by your identity provider. Both solutions have their limitations - a separate user management system makes it more difficult for organizations to onboard and offboard users, and the last thing we want is for a guy that was just fired to log in to Spacelift and have their revenge. User management systems are also pretty difficult to get right, too, especially if granular and sophisticated access controls are required. Piggy-backing on the identity provider is probably a safer bet and is used by many DevTools vendors. With this approach, having some access level to a GitHub repo would give you the same access level to all Spacelift stacks and/or modules associated with it. That's somewhat reasonable, but not as flexible as having your own fancy user management system. Imagine having two stacks linked to one repo, representing two environments - staging and production. It's quite possible that you'd appreciate separate access controls for these two. Access policies offer the best of both worlds - they give you a tool to build your own access management system using data obtained either from our identity provider (GitHub), or from your identity provider if using Single Sign-On integration . In subsequent sections we'll dive deeper into what data is exposed to your policies , how you can define access policies with different levels of access, and what those levels actually mean .","title":"Purpose"},{"location":"concepts/policy/stack-access-policy.html#rules","text":"Your access policy can define the following boolean rules: write : gives the current user write access to the stack or module; read : gives the current user read access to the stack or module; deny : denies the current user all access to the stack or module, no matter the outcome of other rules; deny_write : denies the current user write access to the stack or module, no matter the outcome of other rules; Note that write access automatically assumes read permissions, too, so there's no need to define separate read policies for writers. Another thing to keep in mind when defining access policies is that they are executed quickly. Internally, we expect that running all access policies on all the stacks in one request ( stacks in the GraphQL API ) will take less than 500 milliseconds - otherwise the request fails. That's actually plenty for modern computers, but think twice before creating fancy regex rules in your access policies.","title":"Rules"},{"location":"concepts/policy/stack-access-policy.html#readers-and-writers","text":"There are two levels of non-admin access to a Spacelift stack or module - reader and writer. These are pretty intuitive for most developers, but this section will cover them in more detail to avoid any possible confusion. But first, let's try to understand the use case for different levels of access. In every non-trivial organization there will be different roles - folks who build and manage shared infrastructure, folks who build and manage their team or project-level infrastructure, and folks who use this infrastructure to build great things. The first group is probably the people who manage your Spacelift accounts - the admins . They need to be able to set up everything - create stacks , contexts and policies , and attach them accordingly. You'd normally use login policies to manage their access. The second group - folks who manage their team or project-level infrastructure - should have a reasonable level of access to their project. They should be able to define the environment , set up various integrations, trigger and confirm runs , execute tasks . This level of access is granted by the writer permission. However, writers should still operate within the boundaries defined by admins , who do that mainly by attaching contexts and policies to the stacks . Last but not least the third group - folks who build things on top of existing infra - don't necessarily need to define the infra, but they need to understand what's available and when things are changing. You'll probably want to allow them to contribute to infra definitions, too, and allow them to see feedback from proposed runs. They can't do anything, but they can see everything. These are the readers . Most modern organizations tend to provide this level of access to as many stakeholders as possible to maintain transparency and facilitate collaboration.","title":"Readers and writers"},{"location":"concepts/policy/stack-access-policy.html#data-input","text":"This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \"request\" : { \"remote_ip\" : \"string - IP of the user making a request\" , \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"session\" : { \"admin\" : \"boolean - is the current user a Spacelift admin\" , \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - GitHub username of the logged in user\" , \"name\" : \"string - full name of the logged in GitHub user - may be empty\" , \"teams\" : [ \"string - names of org teams the user is a member of\" ], \"machine\" : \"boolean - whether the creator is a machine or a user\" }, \"stack\" : { // when access to a stack is being evaluated \"id\" : \"string - unique ID of the stack\" , \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" }, \"module\" : { // when access to a module is being evaluated \"id\" : \"string - unique ID of the module\" , \"administrative\" : \"boolean - is the stack administrative\" , \"branch\" : \"string - tracked branch of the module\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"repository\" : \"string - name of the source repository\" , \"terraform_provider\" : \"string - name of the main Terraform provider used by the module\" } }","title":"Data input"},{"location":"concepts/policy/stack-access-policy.html#examples","text":"Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need. With all the above theory in mind, let's jump straight to the code and define some access policies. This section will cover some common exaxmples that can be copied more or less directly, and some contrived ones to serve as an inspiration. Info Remember that access policies must be attached to a stack or a module to take effect.","title":"Examples"},{"location":"concepts/policy/stack-access-policy.html#read-access-to-everyone-in-engineering","text":"I get read access, you get read access, everyone gets read access. As long as they're members of the Engineering team: 1 2 3 package spacelift read { input . session . teams [ _ ] == \"Engineering\" } OK, that was simple. But let's also see it in the Rego playground .","title":"Read access to everyone (in Engineering)"},{"location":"concepts/policy/stack-access-policy.html#in-case-things-go-wrong-we-want-you-to-be-there","text":"You know when things go wrong it's usually because someone did something. Like an infra deployment. Let's try to make sure they're in the office when doing so and restrict write access to business hours and office IP range. This policy is best combined with one that gives read access. 1 2 3 4 5 6 7 8 9 10 11 12 13 package spacelift now : = input . request . timestamp_ns clock : = time . clock ([ now , \"America/Los_Angeles\" ]) weekend : = { \"Saturday\" , \"Sunday\" } weekday : = time . weekday ( now ) ip : = input . request . remote_ip write { input . session . teams [ _ ] == \"Product team\" } deny_write { weekend [ weekday ] } deny_write { clock [ 0 ] < 9 } deny_write { clock [ 0 ] > 17 } deny_write { not net . cidr_contains ( \"12.34.56.0/24\" , ip ) } Here is this example in Rego playground .","title":"In case things go wrong, we want you to be there"},{"location":"concepts/policy/stack-access-policy.html#protect-administrative-stacks","text":"Administrative stacks are powerful - getting write access to one is almost as good as being an admin - you can define and attach contexts and policies . So let's deny write access to them entirely. This works since access policies are not evaluated for admin users. 1 2 3 package spacelift deny_write { input . stack . administrative } And here's the necessary Rego playground example .","title":"Protect administrative stacks"},{"location":"concepts/policy/task-run-policy.html","text":"Task policy \u00bb Warning This feature is deprecated. New users should not use this feature and existing users are encouraged to migrate to the approval policy , which offers a much more flexible and powerful way to control which tasks are allowed to proceed. A migration guide is available here . Purpose \u00bb Spacelift tasks are a handy feature that allows an arbitrary command to be executed within the context of your fully initialized stack. This feature is designed to make running one-off administrative tasks (eg. resource tainting ) safer and more convenient. It can also be an attack vector allowing evil people to do bad things, or simply a footgun allowing well-meaning people to err in a spectacular way. Enter task policies. The sole purpose of task policies is to prevent certain commands from being executed, to prevent certain groups or individuals from executing any commands, or to prevent certain commands from being executed by certain groups or individuals. Info Preventing admins from running tasks using policies can only play an advisory role and should not be considered a safety measure. A bad actor with admin privileges can detach a policy from the stack and run whatever they want. Choose your admins wisely. Task policies are simple in that they only use a single rule - deny - with a string message. A single match for that rule will prevent a run from being created, with an appropriate API error. Let's see how that works in practice by defining a simple rule and attaching it to a stack: 1 2 3 package spacelift deny [ \"not in my town, you don't\" ] { true } And here's the outcome when trying to run a task: Data input \u00bb This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \"request\" : { \"command\" : \"string - command that the user is trying to execute as task\" , \"remote_ip\" : \"string - IP of the user trying to log in\" , \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"session\" : { \"admin\" : \"boolean - is the current user a Spacelift admin\" , \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - GitHub username of the current user\" , \"name\" : \"string - full name of the current user\" , \"teams\" : [ \"string - names of org teams the current user is a member of\" ], \"machine\" : \"boolean - whether the creator is a machine or a user\" }, \"stack\" : { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" } } Aliases \u00bb In addition to our helper functions , we provide aliases for commonly used parts of the input data: Alias Source request input.request session input.session stack input.stack Examples \u00bb Let's have a look at a few examples to see what you can accomplish with task policies. You've seen one example already - disabling tasks entirely. That's perhaps both heavy-handed and naive given that admins can detach the policy if needed. So let's only block non-admins from running tasks: 1 2 3 package spacelift deny [ \"only admins can run tasks\" ] { not input . session . admin } Let's look at an example of this simple policy in the Rego playground . That's still pretty harsh. We could possibly allow writers to run some commands we consider safe - like resource tainting and untainting . Let's try then, and please excuse the regex: 1 2 3 4 5 6 7 8 package spacelift deny [ sprintf ( \"command not allowed (%s)\" , [ command ])] { command : = input . request . command not input . session . admin not re_match ( \"^terraform \\\\ s(un)?taint \\\\ s[ \\\\ w \\\\ - \\\\ .]*$\" , command ) } Feel free to play with the above example in the Rego playground . If you want to keep whitelisting different commands, it may be more elegant to flip the rule logic, create a series of allowed rules, and define one deny rule as not allowed . Let's have a look at this approach, and while we're at it let's remind everyone not to run anything during the weekend: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package spacelift command : = input . request . command deny [ sprintf ( \"command not allowed (%s)\" , [ command ])] { not allowed } deny [ \"no tasks on weekends\" ] { today : = time . weekday ( input . request . timestamp_ns ) weekend : = { \"Saturday\" , \"Sunday\" } weekend [ today ] } allowed { input . session . admin } allowed { re_match ( \"^terraform \\\\ s(un)?taint \\\\ s[ \\\\ w \\\\ - \\\\ .]*$\" , command ) } allowed { re_match ( \"^terraform \\\\ simport \\\\ s[ \\\\ w \\\\ - \\\\ .]*$\" , command ) } As usual, this example is available to play around with . Migration guide \u00bb A task policy can be expressed as an approval policy if it defines a single reject rule, and an approve rule that is its negation. Below you will find equivalents of the examples above expressed as approval policies . Migration example: only allow terraform taint and untaint \u00bb 1 2 3 4 5 6 7 8 package spacelift reject { command : = input . run . command not re_match ( \"^terraform \\\\ s(un)?taint \\\\ s[ \\\\ w \\\\ - \\\\ .]*$\" , command ) } approve { not reject } Migration example: no tasks on weekends \u00bb 1 2 3 4 5 6 7 8 9 10 package spacelift reject { today : = time . weekday ( input . run . created_at ) weekend : = { \"Saturday\" , \"Sunday\" } weekend [ today ] } approve { not reject }","title":"Task policy"},{"location":"concepts/policy/task-run-policy.html#task-policy","text":"Warning This feature is deprecated. New users should not use this feature and existing users are encouraged to migrate to the approval policy , which offers a much more flexible and powerful way to control which tasks are allowed to proceed. A migration guide is available here .","title":"Task policy"},{"location":"concepts/policy/task-run-policy.html#purpose","text":"Spacelift tasks are a handy feature that allows an arbitrary command to be executed within the context of your fully initialized stack. This feature is designed to make running one-off administrative tasks (eg. resource tainting ) safer and more convenient. It can also be an attack vector allowing evil people to do bad things, or simply a footgun allowing well-meaning people to err in a spectacular way. Enter task policies. The sole purpose of task policies is to prevent certain commands from being executed, to prevent certain groups or individuals from executing any commands, or to prevent certain commands from being executed by certain groups or individuals. Info Preventing admins from running tasks using policies can only play an advisory role and should not be considered a safety measure. A bad actor with admin privileges can detach a policy from the stack and run whatever they want. Choose your admins wisely. Task policies are simple in that they only use a single rule - deny - with a string message. A single match for that rule will prevent a run from being created, with an appropriate API error. Let's see how that works in practice by defining a simple rule and attaching it to a stack: 1 2 3 package spacelift deny [ \"not in my town, you don't\" ] { true } And here's the outcome when trying to run a task:","title":"Purpose"},{"location":"concepts/policy/task-run-policy.html#data-input","text":"This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \"request\" : { \"command\" : \"string - command that the user is trying to execute as task\" , \"remote_ip\" : \"string - IP of the user trying to log in\" , \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"session\" : { \"admin\" : \"boolean - is the current user a Spacelift admin\" , \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - GitHub username of the current user\" , \"name\" : \"string - full name of the current user\" , \"teams\" : [ \"string - names of org teams the current user is a member of\" ], \"machine\" : \"boolean - whether the creator is a machine or a user\" }, \"stack\" : { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" } }","title":"Data input"},{"location":"concepts/policy/task-run-policy.html#aliases","text":"In addition to our helper functions , we provide aliases for commonly used parts of the input data: Alias Source request input.request session input.session stack input.stack","title":"Aliases"},{"location":"concepts/policy/task-run-policy.html#examples","text":"Let's have a look at a few examples to see what you can accomplish with task policies. You've seen one example already - disabling tasks entirely. That's perhaps both heavy-handed and naive given that admins can detach the policy if needed. So let's only block non-admins from running tasks: 1 2 3 package spacelift deny [ \"only admins can run tasks\" ] { not input . session . admin } Let's look at an example of this simple policy in the Rego playground . That's still pretty harsh. We could possibly allow writers to run some commands we consider safe - like resource tainting and untainting . Let's try then, and please excuse the regex: 1 2 3 4 5 6 7 8 package spacelift deny [ sprintf ( \"command not allowed (%s)\" , [ command ])] { command : = input . request . command not input . session . admin not re_match ( \"^terraform \\\\ s(un)?taint \\\\ s[ \\\\ w \\\\ - \\\\ .]*$\" , command ) } Feel free to play with the above example in the Rego playground . If you want to keep whitelisting different commands, it may be more elegant to flip the rule logic, create a series of allowed rules, and define one deny rule as not allowed . Let's have a look at this approach, and while we're at it let's remind everyone not to run anything during the weekend: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package spacelift command : = input . request . command deny [ sprintf ( \"command not allowed (%s)\" , [ command ])] { not allowed } deny [ \"no tasks on weekends\" ] { today : = time . weekday ( input . request . timestamp_ns ) weekend : = { \"Saturday\" , \"Sunday\" } weekend [ today ] } allowed { input . session . admin } allowed { re_match ( \"^terraform \\\\ s(un)?taint \\\\ s[ \\\\ w \\\\ - \\\\ .]*$\" , command ) } allowed { re_match ( \"^terraform \\\\ simport \\\\ s[ \\\\ w \\\\ - \\\\ .]*$\" , command ) } As usual, this example is available to play around with .","title":"Examples"},{"location":"concepts/policy/task-run-policy.html#migration-guide","text":"A task policy can be expressed as an approval policy if it defines a single reject rule, and an approve rule that is its negation. Below you will find equivalents of the examples above expressed as approval policies .","title":"Migration guide"},{"location":"concepts/policy/task-run-policy.html#migration-example-only-allow-terraform-taint-and-untaint","text":"1 2 3 4 5 6 7 8 package spacelift reject { command : = input . run . command not re_match ( \"^terraform \\\\ s(un)?taint \\\\ s[ \\\\ w \\\\ - \\\\ .]*$\" , command ) } approve { not reject }","title":"Migration example: only allow terraform taint and untaint"},{"location":"concepts/policy/task-run-policy.html#migration-example-no-tasks-on-weekends","text":"1 2 3 4 5 6 7 8 9 10 package spacelift reject { today : = time . weekday ( input . run . created_at ) weekend : = { \"Saturday\" , \"Sunday\" } weekend [ today ] } approve { not reject }","title":"Migration example: no tasks on weekends"},{"location":"concepts/policy/terraform-plan-policy.html","text":"Plan policy \u00bb Purpose \u00bb Plan policies are evaluated during a planning phase after vendor-specific change preview command (eg. terraform plan ) executes successfully. The body of the change is exported to JSON and parts of it are combined with Spacelift metadata to form the data input to the policy. Plan policies are the only ones that have access to the actual changes to the managed resources, so this is probably the best place to enforce organizational rules and best practices as well as do automated code review. There are two types of rules here that Spacelift will care about: deny and warn . Each of them must come with an appropriate message that will be shown in the logs. Any deny rules will print in red and will automatically fail the run, while warn rules will print in yellow and will at most mark the run for human review if the change affects the tracked branch and the Stack is set to autodeploy . Here is a super simple policy that will show both types of rules in action: 1 2 3 4 5 6 7 8 9 package spacelift deny [ \"you shall not pass\" ] { true # true means \"match everything\" } warn [ \"hey, you look suspicious\" ] { true } Let's create this policy, attach it to a Stack and take it for a spin by triggering a run : Yay, that works (and it fails our plan, too), but it's not terribly useful - unless of course you want to block all changes to your Stack in a really clumsy way. Let's look a bit deeper into the document that each plan policy receives, two possible use cases - rule enforcement and automated code review - and some cookbook examples . Data input \u00bb This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 { \"spacelift:\" : { \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"request\" : { \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"run\" : { \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch the run was triggered from\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" }, \"phase\" : \"string enum - plan | apply\" } ], \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"drift_detection\" : \"boolean - is this a drift detection run\" , \"flags\" : [ \"string - list of flags set on the run by other policies\" ], \"id\" : \"string - the run ID\" , \"runtime_config\" : { \"before_init\" : [ \"string - command to run before run initialization\" ], \"project_root\" : \"string - root of the Terraform project\" , \"runner_image\" : \"string - Docker image used to execute the run\" , \"terraform_version\" : \"string - Terraform version used to for the run\" }, \"state\" : \"string - the current run state\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - type of the run\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }, \"stack\" : { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"name\" : \"string - name of the stack\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"tracked_commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } } }, \"terraform\" : { \"resource_changes\" : [ { \"address\" : \"string - full address of the resource, including modules\" , \"type\" : \"string - type of the resource, eg. aws_iam_user\" , \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the resource, without type\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"provider_name\" : \"string - provider managing the resource, eg. aws\" , \"change\" : { \"actions\" : [ \"string - create, update, delete or no-op\" ], \"before\" : \"optional object - content of the resource\" , \"after\" : \"optional object - content of the resource\" } } ], \"terraform_version\" : \"string\" } } Aliases \u00bb In addition to our helper functions , we provide aliases for commonly used parts of the input data: Alias Description affected_resources List of the resources that will be created, deleted, and updated by Terraform created_resources List of the resources that will be created by Terraform deleted_resources List of the resources that will be deleted by Terraform recreated_resources List of the resources that will be deleted and then created by Terraform updated_resources List of the resources that will be updated by Terraform String Sanitization \u00bb Sensitive properties in \"before\" and \"after\" objects will be sanitized to protect secret values. Sanitization hashes the value and takes the last 8 bytes of the hash. If you need to compare a string property to a constant, you can use the sanitized(string) helper function. 1 2 3 4 5 6 7 8 9 10 package spacelift deny [ \"must not target the forbidden endpoint: forbidden.endpoint/webhook\" ] { resource : = input . terraform . resource_changes [ _ ] actions : = { \"create\" , \"delete\" , \"update\" } actions [ resource . change . actions [ _ ]] resource . change . after . endpoint == sanitized ( \"forbidden.endpoint/webhook\" ) } Custom inputs \u00bb Sometimes you might want to pass some additional data to your policy input. For example, you may want to pass the configuration data from the Terraform plan, the result of a third-party API or tool call. You can do that by generating a JSON file with the data you need at the root of your project. The file name must follow the pattern $key.custom.spacelift.json and must represent a valid JSON object . The object will be merged with the rest of the input data, as input.third_party_metadata.custom.$key . Be aware that the file name is case-sensitive. Below are two examples, one exposing Terraform configuration and the other exposing the result of a third-party security tool. Example: exposing Terraform configuration to the plan policy \u00bb Let's say you want to expose the Terraform configuration to the plan policy to ensure that only the \"blessed\" modules are used to provision resources. You would then add the following command to the list of after_plan hooks : 1 terraform show -json spacelift.plan | jq -c '.configuration' > configuration.custom.spacelift.json The data will be available in the policy input as input.third_party_metadata.custom.configuration . Note that this depends on the jq tool being available in the runner image (it is installed by default on our standard image). Example: passing custom tool output to the plan policy \u00bb For this example, let's use the awesome open-source Terraform security scanner called tfsec . What you want to accomplish is to generate tfsec warnings as JSON and have them reported and processed using the plan policy. In this case, you can run tfsec as a before_init hook and save the output to a file: 1 tfsec -s --format = json . > tfsec.custom.spacelift.json The data will be available in the policy input as input.third_party_metadata.custom.tfsec . Note that this depends on the tfsec tool being available in the runner image - you will need to install it yourself, either directly on the image, or as part of your before_init hook. Some vulnerability scanning tools, like tfsec, will return a non-zero exit code when they encounter vulnerabilities, which will result in a stack failure. As the majority of these tools provide a soft scanning option that will show all the vulnerabilities without considering the command as failed, we can leverage those. However, if there is a tool that doesn't offer that possibility, you can easily overcome this by appending || true at the end of the command as this will always return a zero exit code. Use cases \u00bb Since plan policies get access to the changes to your infrastructure that are about to be introduced, they are the right place to run all sorts of checks against those changes. We believe that there are two main use cases for those checks - hard rule enforcement preventing shooting yourself in the foot and automated code review that augments human decision-making. Organizational rule enforcement \u00bb In every organization, there are things you just don't do. Hard-won knowledge embodied by lengthy comments explaining that if you touch this particular line the site will go hard down and the on-call folks will be after you. Potential security vulnerabilities that can expose all your infra to the wrong crowd. Spacelift allows turning them into policies that simply can't be broken. In this case, you will most likely want to exclusively use deny rules. Let's start by introducing a very simple and reasonable rule - never create static AWS credentials: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package spacelift # Note that the message here is dynamic and captures resource address to provide # appropriate context to anyone affected by this policy . For the sake of your # sanity and that of your colleagues , please always add a message when denying a change . deny [ sprintf ( message , [ resource . address ])] { message : = \"static AWS credentials are evil (%s)\" resource : = input . terraform . resource_changes [ _ ] resource . change . actions [ _ ] == \"create\" # This is what decides whether the rule captures a resource . # There may be an arbitrary number of conditions , and they all must # succeed for the rule to take effect . resource . type == \"aws_iam_access_key\" } Here's a minimal example of this rule in the Rego playground . If that makes sense, let's try defining a policy that implements a slightly more sophisticated piece of knowledge - that when some resources are recreated, they should be created before they're destroyed or an outage will follow. We found that to be the case with the aws_batch_compute_environment , among others. So here it is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package spacelift # This is what Rego calls a set . You can add further elements to it as necessary . always_create_first : = { \"aws_batch_compute_environment\" } deny [ sprintf ( message , [ resource . address ])] { message : = \"always create before deleting (%s)\" resource : = input . terraform . resource_changes [ _ ] # Make sure the type is on the list . always_create_first [ resource . type ] some i_create , i_delete resource . change . actions [ i_create ] == \"create\" resource . change . actions [ i_delete ] == \"delete\" i_delete < i_create } Here's the obligatory Rego playground example . While in most cases you'll want your rules to only look at resources affected by the change, you're not limited to doing so. You can also look at all resources and force teams to remove certain resources. Here's an example - until all AWS resources are removed all in one go, no further changes can take place: 1 2 3 4 5 6 7 8 9 10 11 package spacelift deny [ sprintf ( message , [ resource . address ])] { message : = \"we've moved to GCP, find an equivalent there (%s)\" resource : = input . terraform . resource_changes [ _ ] resource . provider_name == \"aws\" # If you 're just deleting , all good . resource . change . actions != [ \"delete\" ] } Feel free to play with a minimal example of this policy in the Rego playground . Automated code review \u00bb OK, so rule enforcement is very powerful, sometimes you want something more sophisticated. Instead of (or in addition to) enforcing hard rules, you can use plan policy rules to help humans understand changes better, and make informed decisions what looks good and what does not. This time we'll be adding more color to our policies and start using warn rules in addition to deny ones we've already seen in action. You've already seen warn rules in the first section of this article but here it is in action again: It won't fail your plan and it looks relatively benign, but this little yellow note can provide great help to a human reviewer, especially when multiple changes are introduced. Also, if a stack is set to autodeploy , the presence of a single warning is enough to flag the run for a human review. The best way to use warn and deny rules together depends on your preferred Git workflow. We've found short-lived feature branches with Pull Requests to the tracked branch to work relatively well. In this scenario, the type of the run is important - it's PROPOSED for commits to feature branches, and TRACKED on commits to the tracked branch. You will probably want at least some of your rules to take that into account and use this mechanism to balance comprehensive feedback on Pull Requests and flexibility of being able to deploy things that humans deem appropriate. As a general rule when using plan policies for code review, deny when run type is PROPOSED and warn when it is TRACKED . Denying tracked runs unconditionally may be a good idea for most egregious violations for which you will not consider an exception, but when this approach is taken to an extreme it can make your life difficult. We thus suggest that you at most deny when the run is PROPOSED , which will send a failure status to the GitHub commit, but will give the reviewer a chance to approve the change nevertheless. If you want a human to take another look before those changes go live, either set stack autodeploy to false , or explicitly warn about potential violations. Here's an example of how to reuse the same rule to deny or warn depending on the run type: 1 2 3 4 5 6 7 8 9 10 11 12 13 package spacelift proposed : = input . spacelift . run . type == \"PROPOSED\" deny [ reason ] { proposed ; reason : = iam_user_created [ _ ] } warn [ reason ] { not proposed ; reason : = iam_user_created [ _ ] } iam_user_created [ sprintf ( \"do not create IAM users: (%s)\" , [ resource . address ])] { resource : = input . terraform . resource_changes [ _ ] resource . change . actions [ _ ] == \"create\" resource . type == \"aws_iam_user\" } Predictably, this fails when committed to a non-tracked (feature) branch: ...but as a GitHub repo admin you can still merge it if you've set your branch protection rules accordingly: Cool, let's merge it and see what happens: Cool, so the run stopped in its tracks and awaits human decision. At this point we still have a choice to either confirm or discard the run. In the latter case, you will likely want to revert the commit that caused the problem - otherwise all subsequent runs will be affected. The minimal example for the above rule is available in the Rego playground . Examples \u00bb Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need. Require human review when resources are deleted or updated \u00bb Adding resources may ultimately cost a lot of money but it's generally pretty safe from an operational perspective. Let's use a warn rule to allow changes with only added resources to get automatically applied, and require all others to get a human review: 1 2 3 4 5 6 7 8 9 10 11 package spacelift warn [ sprintf ( message , [ action , resource . address ])] { message : = \"action '%s' requires human review (%s)\" review : = { \"update\" , \"delete\" } resource : = input . terraform . resource_changes [ _ ] action : = resource . change . actions [ _ ] review [ action ] } Here's a minimal example to play with. Automatically deploy changes from selected individuals \u00bb Sometimes there are folks who really know what they're doing and changes they introduce can get deployed automatically, especially if they already went through code review. Below is an example that allows commits from whitelisted individuals to be deployed automatically (assumes Stack is set to autodeploy ): 1 2 3 4 5 6 7 8 9 package spacelift warn [ sprintf ( message , [ author ])] { message : = \"%s is not on the whitelist - human review required\" author : = input . spacelift . commit . author whitelisted : = { \"alice\" , \"bob\" , \"charlie\" } not whitelisted [ author ] } Here's the playground example for your enjoyment. Require commits to be reasonably sized \u00bb Massive changes make reviewers miserable. Let's automatically fail all changes that affect more than 50 resources. Let's also allow them to be deployed with mandatory human review nevertheless: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package spacelift proposed : = input . spacelift . run . type == \"PROPOSED\" deny [ msg ] { proposed ; msg : = too_many_changes [ _ ] } warn [ msg ] { not proposed ; msg : = too_many_changes [ _ ] } too_many_changes [ msg ] { threshold : = 50 res : = input . terraform . resource_changes ret : = count ([ r | r : = res [ _ ]; r . change . actions != [ \"no-op\" ]]) msg : = sprintf ( \"more than %d changes (%d)\" , [ threshold , ret ]) ret > threshold } Here's the above example in the Rego playground , with the threshold set to 1 for simplicity. Back-of-the-envelope blast radius \u00bb This is a fancy contrived example building on top of the previous one. However, rather than just looking at the total number of affected resources, it attempts to create a metric called a \"blast radius\" - that is how much the change will affect the whole stack. It assigns special multipliers to some types of resources changed and treats different types of changes differently: deletes and updates are more \"expensive\" because they affect live resources, while new resources are generally safer and thus \"cheaper\". As per our automated code review pattern, we will fail Pull Requests with changes violating this policy, but require human action through warnings when these changes hit the tracked branch. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package spacelift proposed : = input . spacelift . run . type == \"PROPOSED\" deny [ msg ] { proposed ; msg : = blast_radius_too_high [ _ ] } warn [ msg ] { not proposed ; msg : = blast_radius_too_high [ _ ] } blast_radius_too_high [ sprintf ( \"change blast radius too high (%d/100)\" , [ blast_radius ])] { blast_radius : = sum ([ blast | resource : = input . terraform . resource_changes [ _ ]; blast : = blast_radius_for_resource ( resource )]) blast_radius > 100 } blast_radius_for_resource ( resource ) = ret { blasts_radii_by_action : = { \"delete\" : 10 , \"update\" : 5 , \"create\" : 1 , \"no-op\" : 0 } ret : = sum ([ value | action : = resource . change . actions [ _ ] action_impact : = blasts_radii_by_action [ action ] type_impact : = blast_radius_for_type ( resource . type ) value : = action_impact * type_impact ]) } # Let 's give some types of resources special blast multipliers . blasts_radii_by_type : = { \"aws_ecs_cluster\" : 20 , \"aws_ecs_user\" : 10 , \"aws_ecs_role\" : 5 } # By default , blast radius has a value of 1 . blast_radius_for_type ( type ) = 1 { not blasts_radii_by_type [ type ] } blast_radius_for_type ( type ) = ret { blasts_radii_by_type [ type ] = ret } You can play with a minimal example of this policy in The Rego Playground . Cost management \u00bb Thanks to our Infracost integration, you can take cost information into account when deciding whether to ask for human approval or to block changes entirely.","title":"Plan policy"},{"location":"concepts/policy/terraform-plan-policy.html#plan-policy","text":"","title":"Plan policy"},{"location":"concepts/policy/terraform-plan-policy.html#purpose","text":"Plan policies are evaluated during a planning phase after vendor-specific change preview command (eg. terraform plan ) executes successfully. The body of the change is exported to JSON and parts of it are combined with Spacelift metadata to form the data input to the policy. Plan policies are the only ones that have access to the actual changes to the managed resources, so this is probably the best place to enforce organizational rules and best practices as well as do automated code review. There are two types of rules here that Spacelift will care about: deny and warn . Each of them must come with an appropriate message that will be shown in the logs. Any deny rules will print in red and will automatically fail the run, while warn rules will print in yellow and will at most mark the run for human review if the change affects the tracked branch and the Stack is set to autodeploy . Here is a super simple policy that will show both types of rules in action: 1 2 3 4 5 6 7 8 9 package spacelift deny [ \"you shall not pass\" ] { true # true means \"match everything\" } warn [ \"hey, you look suspicious\" ] { true } Let's create this policy, attach it to a Stack and take it for a spin by triggering a run : Yay, that works (and it fails our plan, too), but it's not terribly useful - unless of course you want to block all changes to your Stack in a really clumsy way. Let's look a bit deeper into the document that each plan policy receives, two possible use cases - rule enforcement and automated code review - and some cookbook examples .","title":"Purpose"},{"location":"concepts/policy/terraform-plan-policy.html#data-input","text":"This is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 { \"spacelift:\" : { \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"request\" : { \"timestamp_ns\" : \"number - current Unix timestamp in nanoseconds\" }, \"run\" : { \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch the run was triggered from\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" }, \"phase\" : \"string enum - plan | apply\" } ], \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"drift_detection\" : \"boolean - is this a drift detection run\" , \"flags\" : [ \"string - list of flags set on the run by other policies\" ], \"id\" : \"string - the run ID\" , \"runtime_config\" : { \"before_init\" : [ \"string - command to run before run initialization\" ], \"project_root\" : \"string - root of the Terraform project\" , \"runner_image\" : \"string - Docker image used to execute the run\" , \"terraform_version\" : \"string - Terraform version used to for the run\" }, \"state\" : \"string - the current run state\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - type of the run\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }, \"stack\" : { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"name\" : \"string - name of the stack\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"tracked_commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } } }, \"terraform\" : { \"resource_changes\" : [ { \"address\" : \"string - full address of the resource, including modules\" , \"type\" : \"string - type of the resource, eg. aws_iam_user\" , \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the resource, without type\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"provider_name\" : \"string - provider managing the resource, eg. aws\" , \"change\" : { \"actions\" : [ \"string - create, update, delete or no-op\" ], \"before\" : \"optional object - content of the resource\" , \"after\" : \"optional object - content of the resource\" } } ], \"terraform_version\" : \"string\" } }","title":"Data input"},{"location":"concepts/policy/terraform-plan-policy.html#aliases","text":"In addition to our helper functions , we provide aliases for commonly used parts of the input data: Alias Description affected_resources List of the resources that will be created, deleted, and updated by Terraform created_resources List of the resources that will be created by Terraform deleted_resources List of the resources that will be deleted by Terraform recreated_resources List of the resources that will be deleted and then created by Terraform updated_resources List of the resources that will be updated by Terraform","title":"Aliases"},{"location":"concepts/policy/terraform-plan-policy.html#string-sanitization","text":"Sensitive properties in \"before\" and \"after\" objects will be sanitized to protect secret values. Sanitization hashes the value and takes the last 8 bytes of the hash. If you need to compare a string property to a constant, you can use the sanitized(string) helper function. 1 2 3 4 5 6 7 8 9 10 package spacelift deny [ \"must not target the forbidden endpoint: forbidden.endpoint/webhook\" ] { resource : = input . terraform . resource_changes [ _ ] actions : = { \"create\" , \"delete\" , \"update\" } actions [ resource . change . actions [ _ ]] resource . change . after . endpoint == sanitized ( \"forbidden.endpoint/webhook\" ) }","title":"String Sanitization"},{"location":"concepts/policy/terraform-plan-policy.html#custom-inputs","text":"Sometimes you might want to pass some additional data to your policy input. For example, you may want to pass the configuration data from the Terraform plan, the result of a third-party API or tool call. You can do that by generating a JSON file with the data you need at the root of your project. The file name must follow the pattern $key.custom.spacelift.json and must represent a valid JSON object . The object will be merged with the rest of the input data, as input.third_party_metadata.custom.$key . Be aware that the file name is case-sensitive. Below are two examples, one exposing Terraform configuration and the other exposing the result of a third-party security tool.","title":"Custom inputs"},{"location":"concepts/policy/terraform-plan-policy.html#example-exposing-terraform-configuration-to-the-plan-policy","text":"Let's say you want to expose the Terraform configuration to the plan policy to ensure that only the \"blessed\" modules are used to provision resources. You would then add the following command to the list of after_plan hooks : 1 terraform show -json spacelift.plan | jq -c '.configuration' > configuration.custom.spacelift.json The data will be available in the policy input as input.third_party_metadata.custom.configuration . Note that this depends on the jq tool being available in the runner image (it is installed by default on our standard image).","title":"Example: exposing Terraform configuration to the plan policy"},{"location":"concepts/policy/terraform-plan-policy.html#example-passing-custom-tool-output-to-the-plan-policy","text":"For this example, let's use the awesome open-source Terraform security scanner called tfsec . What you want to accomplish is to generate tfsec warnings as JSON and have them reported and processed using the plan policy. In this case, you can run tfsec as a before_init hook and save the output to a file: 1 tfsec -s --format = json . > tfsec.custom.spacelift.json The data will be available in the policy input as input.third_party_metadata.custom.tfsec . Note that this depends on the tfsec tool being available in the runner image - you will need to install it yourself, either directly on the image, or as part of your before_init hook. Some vulnerability scanning tools, like tfsec, will return a non-zero exit code when they encounter vulnerabilities, which will result in a stack failure. As the majority of these tools provide a soft scanning option that will show all the vulnerabilities without considering the command as failed, we can leverage those. However, if there is a tool that doesn't offer that possibility, you can easily overcome this by appending || true at the end of the command as this will always return a zero exit code.","title":"Example: passing custom tool output to the plan policy"},{"location":"concepts/policy/terraform-plan-policy.html#use-cases","text":"Since plan policies get access to the changes to your infrastructure that are about to be introduced, they are the right place to run all sorts of checks against those changes. We believe that there are two main use cases for those checks - hard rule enforcement preventing shooting yourself in the foot and automated code review that augments human decision-making.","title":"Use cases"},{"location":"concepts/policy/terraform-plan-policy.html#organizational-rule-enforcement","text":"In every organization, there are things you just don't do. Hard-won knowledge embodied by lengthy comments explaining that if you touch this particular line the site will go hard down and the on-call folks will be after you. Potential security vulnerabilities that can expose all your infra to the wrong crowd. Spacelift allows turning them into policies that simply can't be broken. In this case, you will most likely want to exclusively use deny rules. Let's start by introducing a very simple and reasonable rule - never create static AWS credentials: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package spacelift # Note that the message here is dynamic and captures resource address to provide # appropriate context to anyone affected by this policy . For the sake of your # sanity and that of your colleagues , please always add a message when denying a change . deny [ sprintf ( message , [ resource . address ])] { message : = \"static AWS credentials are evil (%s)\" resource : = input . terraform . resource_changes [ _ ] resource . change . actions [ _ ] == \"create\" # This is what decides whether the rule captures a resource . # There may be an arbitrary number of conditions , and they all must # succeed for the rule to take effect . resource . type == \"aws_iam_access_key\" } Here's a minimal example of this rule in the Rego playground . If that makes sense, let's try defining a policy that implements a slightly more sophisticated piece of knowledge - that when some resources are recreated, they should be created before they're destroyed or an outage will follow. We found that to be the case with the aws_batch_compute_environment , among others. So here it is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package spacelift # This is what Rego calls a set . You can add further elements to it as necessary . always_create_first : = { \"aws_batch_compute_environment\" } deny [ sprintf ( message , [ resource . address ])] { message : = \"always create before deleting (%s)\" resource : = input . terraform . resource_changes [ _ ] # Make sure the type is on the list . always_create_first [ resource . type ] some i_create , i_delete resource . change . actions [ i_create ] == \"create\" resource . change . actions [ i_delete ] == \"delete\" i_delete < i_create } Here's the obligatory Rego playground example . While in most cases you'll want your rules to only look at resources affected by the change, you're not limited to doing so. You can also look at all resources and force teams to remove certain resources. Here's an example - until all AWS resources are removed all in one go, no further changes can take place: 1 2 3 4 5 6 7 8 9 10 11 package spacelift deny [ sprintf ( message , [ resource . address ])] { message : = \"we've moved to GCP, find an equivalent there (%s)\" resource : = input . terraform . resource_changes [ _ ] resource . provider_name == \"aws\" # If you 're just deleting , all good . resource . change . actions != [ \"delete\" ] } Feel free to play with a minimal example of this policy in the Rego playground .","title":"Organizational rule enforcement"},{"location":"concepts/policy/terraform-plan-policy.html#automated-code-review","text":"OK, so rule enforcement is very powerful, sometimes you want something more sophisticated. Instead of (or in addition to) enforcing hard rules, you can use plan policy rules to help humans understand changes better, and make informed decisions what looks good and what does not. This time we'll be adding more color to our policies and start using warn rules in addition to deny ones we've already seen in action. You've already seen warn rules in the first section of this article but here it is in action again: It won't fail your plan and it looks relatively benign, but this little yellow note can provide great help to a human reviewer, especially when multiple changes are introduced. Also, if a stack is set to autodeploy , the presence of a single warning is enough to flag the run for a human review. The best way to use warn and deny rules together depends on your preferred Git workflow. We've found short-lived feature branches with Pull Requests to the tracked branch to work relatively well. In this scenario, the type of the run is important - it's PROPOSED for commits to feature branches, and TRACKED on commits to the tracked branch. You will probably want at least some of your rules to take that into account and use this mechanism to balance comprehensive feedback on Pull Requests and flexibility of being able to deploy things that humans deem appropriate. As a general rule when using plan policies for code review, deny when run type is PROPOSED and warn when it is TRACKED . Denying tracked runs unconditionally may be a good idea for most egregious violations for which you will not consider an exception, but when this approach is taken to an extreme it can make your life difficult. We thus suggest that you at most deny when the run is PROPOSED , which will send a failure status to the GitHub commit, but will give the reviewer a chance to approve the change nevertheless. If you want a human to take another look before those changes go live, either set stack autodeploy to false , or explicitly warn about potential violations. Here's an example of how to reuse the same rule to deny or warn depending on the run type: 1 2 3 4 5 6 7 8 9 10 11 12 13 package spacelift proposed : = input . spacelift . run . type == \"PROPOSED\" deny [ reason ] { proposed ; reason : = iam_user_created [ _ ] } warn [ reason ] { not proposed ; reason : = iam_user_created [ _ ] } iam_user_created [ sprintf ( \"do not create IAM users: (%s)\" , [ resource . address ])] { resource : = input . terraform . resource_changes [ _ ] resource . change . actions [ _ ] == \"create\" resource . type == \"aws_iam_user\" } Predictably, this fails when committed to a non-tracked (feature) branch: ...but as a GitHub repo admin you can still merge it if you've set your branch protection rules accordingly: Cool, let's merge it and see what happens: Cool, so the run stopped in its tracks and awaits human decision. At this point we still have a choice to either confirm or discard the run. In the latter case, you will likely want to revert the commit that caused the problem - otherwise all subsequent runs will be affected. The minimal example for the above rule is available in the Rego playground .","title":"Automated code review"},{"location":"concepts/policy/terraform-plan-policy.html#examples","text":"Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.","title":"Examples"},{"location":"concepts/policy/terraform-plan-policy.html#require-human-review-when-resources-are-deleted-or-updated","text":"Adding resources may ultimately cost a lot of money but it's generally pretty safe from an operational perspective. Let's use a warn rule to allow changes with only added resources to get automatically applied, and require all others to get a human review: 1 2 3 4 5 6 7 8 9 10 11 package spacelift warn [ sprintf ( message , [ action , resource . address ])] { message : = \"action '%s' requires human review (%s)\" review : = { \"update\" , \"delete\" } resource : = input . terraform . resource_changes [ _ ] action : = resource . change . actions [ _ ] review [ action ] } Here's a minimal example to play with.","title":"Require human review when resources are deleted or updated"},{"location":"concepts/policy/terraform-plan-policy.html#automatically-deploy-changes-from-selected-individuals","text":"Sometimes there are folks who really know what they're doing and changes they introduce can get deployed automatically, especially if they already went through code review. Below is an example that allows commits from whitelisted individuals to be deployed automatically (assumes Stack is set to autodeploy ): 1 2 3 4 5 6 7 8 9 package spacelift warn [ sprintf ( message , [ author ])] { message : = \"%s is not on the whitelist - human review required\" author : = input . spacelift . commit . author whitelisted : = { \"alice\" , \"bob\" , \"charlie\" } not whitelisted [ author ] } Here's the playground example for your enjoyment.","title":"Automatically deploy changes from selected individuals"},{"location":"concepts/policy/terraform-plan-policy.html#require-commits-to-be-reasonably-sized","text":"Massive changes make reviewers miserable. Let's automatically fail all changes that affect more than 50 resources. Let's also allow them to be deployed with mandatory human review nevertheless: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package spacelift proposed : = input . spacelift . run . type == \"PROPOSED\" deny [ msg ] { proposed ; msg : = too_many_changes [ _ ] } warn [ msg ] { not proposed ; msg : = too_many_changes [ _ ] } too_many_changes [ msg ] { threshold : = 50 res : = input . terraform . resource_changes ret : = count ([ r | r : = res [ _ ]; r . change . actions != [ \"no-op\" ]]) msg : = sprintf ( \"more than %d changes (%d)\" , [ threshold , ret ]) ret > threshold } Here's the above example in the Rego playground , with the threshold set to 1 for simplicity.","title":"Require commits to be reasonably sized"},{"location":"concepts/policy/terraform-plan-policy.html#back-of-the-envelope-blast-radius","text":"This is a fancy contrived example building on top of the previous one. However, rather than just looking at the total number of affected resources, it attempts to create a metric called a \"blast radius\" - that is how much the change will affect the whole stack. It assigns special multipliers to some types of resources changed and treats different types of changes differently: deletes and updates are more \"expensive\" because they affect live resources, while new resources are generally safer and thus \"cheaper\". As per our automated code review pattern, we will fail Pull Requests with changes violating this policy, but require human action through warnings when these changes hit the tracked branch. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package spacelift proposed : = input . spacelift . run . type == \"PROPOSED\" deny [ msg ] { proposed ; msg : = blast_radius_too_high [ _ ] } warn [ msg ] { not proposed ; msg : = blast_radius_too_high [ _ ] } blast_radius_too_high [ sprintf ( \"change blast radius too high (%d/100)\" , [ blast_radius ])] { blast_radius : = sum ([ blast | resource : = input . terraform . resource_changes [ _ ]; blast : = blast_radius_for_resource ( resource )]) blast_radius > 100 } blast_radius_for_resource ( resource ) = ret { blasts_radii_by_action : = { \"delete\" : 10 , \"update\" : 5 , \"create\" : 1 , \"no-op\" : 0 } ret : = sum ([ value | action : = resource . change . actions [ _ ] action_impact : = blasts_radii_by_action [ action ] type_impact : = blast_radius_for_type ( resource . type ) value : = action_impact * type_impact ]) } # Let 's give some types of resources special blast multipliers . blasts_radii_by_type : = { \"aws_ecs_cluster\" : 20 , \"aws_ecs_user\" : 10 , \"aws_ecs_role\" : 5 } # By default , blast radius has a value of 1 . blast_radius_for_type ( type ) = 1 { not blasts_radii_by_type [ type ] } blast_radius_for_type ( type ) = ret { blasts_radii_by_type [ type ] = ret } You can play with a minimal example of this policy in The Rego Playground .","title":"Back-of-the-envelope blast radius"},{"location":"concepts/policy/terraform-plan-policy.html#cost-management","text":"Thanks to our Infracost integration, you can take cost information into account when deciding whether to ask for human approval or to block changes entirely.","title":"Cost management"},{"location":"concepts/policy/trigger-policy.html","text":"Trigger policy \u00bb Purpose \u00bb Frequently, your infrastructure consists of a number of projects ( stacks in Spacelift parlance) that are connected in some way - either depend logically on one another, or must be deployed in a particular order for some other reason - for example, a rolling deploy in multiple regions. Enter trigger policies. Trigger policies are evaluated at the end of each stack-blocking run (which includes tracked runs and tasks ) as well as on module version releases and allow you to decide if some tracked Runs should be triggered. This is a very powerful feature, effectively turning Spacelift into a Turing machine. Warning Note that in order to support various use cases this policy type is currently evaluated every time a blocking Run reaches a terminal state , which includes states like Canceled , Discarded , Stopped or Failed in addition to the more obvious Finished . This allows for very interesting and complex workflows (eg. automated retry logic) but please be aware of that when writing your own policies. All runs triggered - directly or indirectly - by trigger policies as a result of the same initial run are grouped into a so-called workflow. In the trigger policy you can access all other runs in the same workflow as the currently finished run, regardless of their Stack. This lets you coordinate executions of multiple Stacks and build workflows which require multiple runs to finish in order to commence to the next stage (and trigger another Stack). Data input \u00bb When triggered by a run , this is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 { \"run\" : { // the run metadata \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch the run was triggered from\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" }, \"phase\" : \"string enum - plan | apply\" } ], \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"creator_session\" : { \"admin\" : \"boolean - is the current user a Spacelift admin\" , \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - username of the creator\" , \"name\" : \"string - full name of the creator\" , \"teams\" : [ \"string - names of teams the creator was a member of\" ], \"machine\" : \"boolean - whether the run was initiated by a human or a machine\" }, \"drift_detection\" : \"boolean - is this a drift detection run\" , \"flags\" : [ \"string - list of flags set on the run by other policies\" ], \"id\" : \"string - the run ID\" , \"runtime_config\" : { \"before_init\" : [ \"string - command to run before run initialization\" ], \"project_root\" : \"string - root of the Terraform project\" , \"runner_image\" : \"string - Docker image used to execute the run\" , \"terraform_version\" : \"string - Terraform version used to for the run\" }, \"state\" : \"string - the current run state\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - type of the run\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }, \"stack\" : { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"id\" : \"string - unique stack identifier\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"tracked_commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } }, \"stacks\" : [ { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"id\" : \"string - unique stack identifier\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"tracked_commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } } ], \"workflow\" : [ { \"id\" : \"string - Unique ID of the Run\" , \"stack_id\" : \"string - unique stack identifier\" , \"state\" : \"state - one of the states of the Run\" , \"type\" : \"string - TRACKED or TASK\" } ] } Info Note the presence of two similar keys: stack and stacks . The former is the Stack that the newly finished Run belongs to. The other is a list of all Stacks in the account. The schema for both is the same. When triggered by a new module version , this is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 { \"module\" : { // Module for which the new version was released \"id\" : \"string - unique ID of the module\" , \"administrative\" : \"boolean - is the stack administrative\" , \"branch\" : \"string - tracked branch of the module\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Module, if any\" , \"repository\" : \"string - name of the source repository\" , \"terraform_provider\" : \"string - name of the main Terraform provider used by the module\" , \"version\" : { // Newly released module version \"number\" : \"string - semver version number\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , } } \"stacks\" : [ // List of consumers of the newest available module version { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"id\" : \"string - unique stack identifier\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"tracked_commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } } ] } Examples \u00bb Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need. Since trigger policies turn Spacelift into a Turing machine, you could probably use them to implement Conway's Game of Life , but there are a few more obvious use cases. Let's have a look at two of them - interdependent Stacks and automated retries. Interdependent stacks \u00bb The purpose here is to create a complex workflow that spans multiple Stacks. We will want to trigger a predefined list of Stacks when a Run finishes successfully. Here's our first take: 1 2 3 4 5 6 7 8 9 10 package spacelift trigger [ \"stack-one\" ] { finished } trigger [ \"stack-two\" ] { finished } trigger [ \"stack-three\" ] { finished } finished { input . run . state == \"FINISHED\" input . run . type == \"TRACKED\" } Here's a minimal example of this rule in the Rego playground . But it's far from ideal . We can't be guaranteed that stacks with these IDs still exist in this account. Spacelift will handle that just fine, but you'll likely find if confusing. Also, for any new Stack that appears you will need to explicitly add it to the list. That's annoying. We can do better, and to do that, we'll use Stack labels . Labels are completely arbitrary strings that you can attach to individual Stacks, and we can use them to do something magical - have \"client\" Stacks \"subscribe\" to \"parent\" ones. So how's that: 1 2 3 4 5 6 7 8 package spacelift trigger [ stack . id ] { stack : = input . stacks [ _ ] input . run . state == \"FINISHED\" input . run . type == \"TRACKED\" stack . labels [ _ ] == concat ( \"\" , [ \"depends-on:\" , input . stack . id ]) } Here's a minimal example of this rule in the Rego playground . The benefit of this policy is that you can attach it to all your stacks, and it will just work for your entire organization. Can we do better? Sure, we can even have stacks use labels to decide which types of runs or state changes they care about. Here's a mind-bending example: 1 2 3 4 5 6 7 8 9 10 package spacelift trigger [ stack . id ] { stack : = input . stacks [ _ ] input . run . type == \"TRACKED\" stack . labels [ _ ] == concat ( \"\" , [ \"depends-on:\" , input . stack . id , \"|state:\" , input . run . state ], ) } Another Rego example to play with . Now, how cool is that? Automated retries \u00bb Here's another use case - sometimes Terraform or Pulumi deployments fail for a reason that has nothing to do with the code - think eventual consistency between various cloud subsystems, transient API errors etc. It would be great if you could restart the failed run. Oh, and let's make sure new runs are not created in a crazy loop - since policy-triggered runs trigger another policy evaluation: 1 2 3 4 5 6 7 8 package spacelift trigger [ stack . id ] { stack : = input . stack input . run . state == \"FAILED\" input . run . type == \"TRACKED\" is_null ( input . run . triggered_by ) } Info Note that this will also prevent user-triggered runs from being retried. Which is usually what you want in the first place, because a triggering human is probably already babysitting the Stack anyway. Diamond Problem \u00bb The diamond problem happens when your stacks and their dependencies form a shape like in the following diagram: graph LR 1 --> 2a; 1 --> 2b; 2a --> 3; 2b --> 3; Which means that Stack 1 triggers both Stack 2a and 2b, and we only want to trigger Stack 3 when both predecessors finish. This can be elegantly solved using workflows. First we'll have to create a trigger policy for Stack 1: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 package spacelift trigger [ \"stack-2a\" ] { tracked_and_finished } trigger [ \"stack-2b\" ] { tracked_and_finished } tracked_and_finished { input . run . state == \"FINISHED\" input . run . type == \"TRACKED\" } This will trigger both Stack 2a and 2b whenever a run finishes on Stack 1. Now onto a trigger policy for Stack 2a and 2b: 1 2 3 4 5 6 7 8 9 10 package spacelift trigger [ \"stack-3\" ] { run_a : = input . workflow [ _ ] run_b : = input . workflow [ _ ] run_a . stack_id == \"stack-2a\" run_b . stack_id == \"stack-2b\" run_a . state == \"FINISHED\" run_b . state == \"FINISHED\" } Here we trigger Stack 3, whenever the runs in Stack 2a and 2b are both finished. You can also easily extend this to work with a label-based approach, so that you could define Stack 3's dependencies by attaching a depends-on:stack-2a,stack-2b label to it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package spacelift # Helper with stack_id 's of workflow runs which have already finished . already_finished [ run . stack_id ] { run : = input . workflow [ _ ] run . state == \"FINISHED\" } trigger [ stack . id ] { input . run . state == \"FINISHED\" input . run . type == \"TRACKED\" # For each Stack which has a depends - on label , # get a list of its dependencies . stack : = input . stacks [ _ ] label : = stack . labels [ _ ] startswith ( label , \"depends-on:\" ) dependencies : = split ( trim_prefix ( label , \"depends-on:\" ), \",\" ) # The current Stack is one of the dependencies . input . stack . id == dependencies [ _ ] finished_dependencies : = [ dependency | dependency : = dependencies [ _ ] already_finished [ dependency ]] # Check if all dependencies have finished . count ( finished_dependencies ) == count ( dependencies ) } Module updates \u00bb Trigger policies can be attached to modules as well. Modules track the consumers of each of their versions. When a new module version is released, the consumers of the previously newest version are assumed to be potential consumers of the newly released one. Hence, the trigger policy for a module can be used to trigger a run on all of these stacks. The module version will be updated as long as the version constraints allow the newest version to be used. Here is a simple trigger policy that will trigger a run on all stacks that use the latest version of the module when a new version is released: 1 2 3 package spacelift trigger [ stack . id ] { stack : = input . stacks [ _ ] }","title":"Trigger policy"},{"location":"concepts/policy/trigger-policy.html#trigger-policy","text":"","title":"Trigger policy"},{"location":"concepts/policy/trigger-policy.html#purpose","text":"Frequently, your infrastructure consists of a number of projects ( stacks in Spacelift parlance) that are connected in some way - either depend logically on one another, or must be deployed in a particular order for some other reason - for example, a rolling deploy in multiple regions. Enter trigger policies. Trigger policies are evaluated at the end of each stack-blocking run (which includes tracked runs and tasks ) as well as on module version releases and allow you to decide if some tracked Runs should be triggered. This is a very powerful feature, effectively turning Spacelift into a Turing machine. Warning Note that in order to support various use cases this policy type is currently evaluated every time a blocking Run reaches a terminal state , which includes states like Canceled , Discarded , Stopped or Failed in addition to the more obvious Finished . This allows for very interesting and complex workflows (eg. automated retry logic) but please be aware of that when writing your own policies. All runs triggered - directly or indirectly - by trigger policies as a result of the same initial run are grouped into a so-called workflow. In the trigger policy you can access all other runs in the same workflow as the currently finished run, regardless of their Stack. This lets you coordinate executions of multiple Stacks and build workflows which require multiple runs to finish in order to commence to the next stage (and trigger another Stack).","title":"Purpose"},{"location":"concepts/policy/trigger-policy.html#data-input","text":"When triggered by a run , this is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 { \"run\" : { // the run metadata \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch the run was triggered from\" , \"changes\" : [ { \"action\" : \"string enum - added | changed | deleted\" , \"entity\" : { \"address\" : \"string - full address of the entity\" , \"name\" : \"string - name of the entity\" , \"type\" : \"string - full resource type or \\\"output\\\" for outputs\" , \"entity_vendor\" : \"string - the name of the vendor\" , \"entity_type\" : \"string - the type of entity, possible values depend on the vendor\" , \"data\" : \"object - detailed information about the entity, shape depends on the vendor and type\" }, \"phase\" : \"string enum - plan | apply\" } ], \"commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"creator_session\" : { \"admin\" : \"boolean - is the current user a Spacelift admin\" , \"creator_ip\" : \"string - IP address of the user who created the session\" , \"login\" : \"string - username of the creator\" , \"name\" : \"string - full name of the creator\" , \"teams\" : [ \"string - names of teams the creator was a member of\" ], \"machine\" : \"boolean - whether the run was initiated by a human or a machine\" }, \"drift_detection\" : \"boolean - is this a drift detection run\" , \"flags\" : [ \"string - list of flags set on the run by other policies\" ], \"id\" : \"string - the run ID\" , \"runtime_config\" : { \"before_init\" : [ \"string - command to run before run initialization\" ], \"project_root\" : \"string - root of the Terraform project\" , \"runner_image\" : \"string - Docker image used to execute the run\" , \"terraform_version\" : \"string - Terraform version used to for the run\" }, \"state\" : \"string - the current run state\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - type of the run\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }, \"stack\" : { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"id\" : \"string - unique stack identifier\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"tracked_commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } }, \"stacks\" : [ { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"id\" : \"string - unique stack identifier\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"tracked_commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } } ], \"workflow\" : [ { \"id\" : \"string - Unique ID of the Run\" , \"stack_id\" : \"string - unique stack identifier\" , \"state\" : \"state - one of the states of the Run\" , \"type\" : \"string - TRACKED or TASK\" } ] } Info Note the presence of two similar keys: stack and stacks . The former is the Stack that the newly finished Run belongs to. The other is a list of all Stacks in the account. The schema for both is the same. When triggered by a new module version , this is the schema of the data input that each policy request will receive: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 { \"module\" : { // Module for which the new version was released \"id\" : \"string - unique ID of the module\" , \"administrative\" : \"boolean - is the stack administrative\" , \"branch\" : \"string - tracked branch of the module\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Module, if any\" , \"repository\" : \"string - name of the source repository\" , \"terraform_provider\" : \"string - name of the main Terraform provider used by the module\" , \"version\" : { // Newly released module version \"number\" : \"string - semver version number\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , } } \"stacks\" : [ // List of consumers of the newest available module version { \"administrative\" : \"boolean - is the stack administrative\" , \"autodeploy\" : \"boolean - is the stack currently set to autodeploy\" , \"branch\" : \"string - tracked branch of the stack\" , \"id\" : \"string - unique stack identifier\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string - name of the stack\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string - name of the source GitHub repository\" , \"state\" : \"string - current state of the stack\" , \"terraform_version\" : \"string or null - last Terraform version used to apply changes\" , \"tracked_commit\" : { \"author\" : \"string - GitHub login if available, name otherwise\" , \"branch\" : \"string - branch to which the commit was pushed\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"hash\" : \"string - the commit hash\" , \"message\" : \"string - commit message\" }, \"worker_pool\" : { \"id\" : \"string - the worker pool ID, if it is private\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors, if the worker pool is private\" ], \"name\" : \"string - name of the worker pool, if it is private\" , \"public\" : \"boolean - is the worker pool public\" } } ] }","title":"Data input"},{"location":"concepts/policy/trigger-policy.html#examples","text":"Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need. Since trigger policies turn Spacelift into a Turing machine, you could probably use them to implement Conway's Game of Life , but there are a few more obvious use cases. Let's have a look at two of them - interdependent Stacks and automated retries.","title":"Examples"},{"location":"concepts/policy/trigger-policy.html#interdependent-stacks","text":"The purpose here is to create a complex workflow that spans multiple Stacks. We will want to trigger a predefined list of Stacks when a Run finishes successfully. Here's our first take: 1 2 3 4 5 6 7 8 9 10 package spacelift trigger [ \"stack-one\" ] { finished } trigger [ \"stack-two\" ] { finished } trigger [ \"stack-three\" ] { finished } finished { input . run . state == \"FINISHED\" input . run . type == \"TRACKED\" } Here's a minimal example of this rule in the Rego playground . But it's far from ideal . We can't be guaranteed that stacks with these IDs still exist in this account. Spacelift will handle that just fine, but you'll likely find if confusing. Also, for any new Stack that appears you will need to explicitly add it to the list. That's annoying. We can do better, and to do that, we'll use Stack labels . Labels are completely arbitrary strings that you can attach to individual Stacks, and we can use them to do something magical - have \"client\" Stacks \"subscribe\" to \"parent\" ones. So how's that: 1 2 3 4 5 6 7 8 package spacelift trigger [ stack . id ] { stack : = input . stacks [ _ ] input . run . state == \"FINISHED\" input . run . type == \"TRACKED\" stack . labels [ _ ] == concat ( \"\" , [ \"depends-on:\" , input . stack . id ]) } Here's a minimal example of this rule in the Rego playground . The benefit of this policy is that you can attach it to all your stacks, and it will just work for your entire organization. Can we do better? Sure, we can even have stacks use labels to decide which types of runs or state changes they care about. Here's a mind-bending example: 1 2 3 4 5 6 7 8 9 10 package spacelift trigger [ stack . id ] { stack : = input . stacks [ _ ] input . run . type == \"TRACKED\" stack . labels [ _ ] == concat ( \"\" , [ \"depends-on:\" , input . stack . id , \"|state:\" , input . run . state ], ) } Another Rego example to play with . Now, how cool is that?","title":"Interdependent stacks"},{"location":"concepts/policy/trigger-policy.html#automated-retries","text":"Here's another use case - sometimes Terraform or Pulumi deployments fail for a reason that has nothing to do with the code - think eventual consistency between various cloud subsystems, transient API errors etc. It would be great if you could restart the failed run. Oh, and let's make sure new runs are not created in a crazy loop - since policy-triggered runs trigger another policy evaluation: 1 2 3 4 5 6 7 8 package spacelift trigger [ stack . id ] { stack : = input . stack input . run . state == \"FAILED\" input . run . type == \"TRACKED\" is_null ( input . run . triggered_by ) } Info Note that this will also prevent user-triggered runs from being retried. Which is usually what you want in the first place, because a triggering human is probably already babysitting the Stack anyway.","title":"Automated retries"},{"location":"concepts/policy/trigger-policy.html#diamond-problem","text":"The diamond problem happens when your stacks and their dependencies form a shape like in the following diagram: graph LR 1 --> 2a; 1 --> 2b; 2a --> 3; 2b --> 3; Which means that Stack 1 triggers both Stack 2a and 2b, and we only want to trigger Stack 3 when both predecessors finish. This can be elegantly solved using workflows. First we'll have to create a trigger policy for Stack 1: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 package spacelift trigger [ \"stack-2a\" ] { tracked_and_finished } trigger [ \"stack-2b\" ] { tracked_and_finished } tracked_and_finished { input . run . state == \"FINISHED\" input . run . type == \"TRACKED\" } This will trigger both Stack 2a and 2b whenever a run finishes on Stack 1. Now onto a trigger policy for Stack 2a and 2b: 1 2 3 4 5 6 7 8 9 10 package spacelift trigger [ \"stack-3\" ] { run_a : = input . workflow [ _ ] run_b : = input . workflow [ _ ] run_a . stack_id == \"stack-2a\" run_b . stack_id == \"stack-2b\" run_a . state == \"FINISHED\" run_b . state == \"FINISHED\" } Here we trigger Stack 3, whenever the runs in Stack 2a and 2b are both finished. You can also easily extend this to work with a label-based approach, so that you could define Stack 3's dependencies by attaching a depends-on:stack-2a,stack-2b label to it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package spacelift # Helper with stack_id 's of workflow runs which have already finished . already_finished [ run . stack_id ] { run : = input . workflow [ _ ] run . state == \"FINISHED\" } trigger [ stack . id ] { input . run . state == \"FINISHED\" input . run . type == \"TRACKED\" # For each Stack which has a depends - on label , # get a list of its dependencies . stack : = input . stacks [ _ ] label : = stack . labels [ _ ] startswith ( label , \"depends-on:\" ) dependencies : = split ( trim_prefix ( label , \"depends-on:\" ), \",\" ) # The current Stack is one of the dependencies . input . stack . id == dependencies [ _ ] finished_dependencies : = [ dependency | dependency : = dependencies [ _ ] already_finished [ dependency ]] # Check if all dependencies have finished . count ( finished_dependencies ) == count ( dependencies ) }","title":"Diamond Problem"},{"location":"concepts/policy/trigger-policy.html#module-updates","text":"Trigger policies can be attached to modules as well. Modules track the consumers of each of their versions. When a new module version is released, the consumers of the previously newest version are assumed to be potential consumers of the newly released one. Hence, the trigger policy for a module can be used to trigger a run on all of these stacks. The module version will be updated as long as the version constraints allow the newest version to be used. Here is a simple trigger policy that will trigger a run on all stacks that use the latest version of the module when a new version is released: 1 2 3 package spacelift trigger [ stack . id ] { stack : = input . stacks [ _ ] }","title":"Module updates"},{"location":"concepts/policy/push-policy/index.html","text":"Push policy \u00bb Purpose \u00bb Git push policies are triggered on a per-stack basis to determine the action that should be taken for each individual Stack or Module in response to a Git push or Pull Request notification. There are three possible outcomes: track : set the new head commit on the stack / module and create a tracked Run , ie. one that can be applied ; propose : create a proposed Run against a proposed version of infrastructure; ignore : do not schedule a new Run; Using this policy it is possible to create a very sophisticated, custom-made setup. We can think of two main - and not mutually exclusive - use cases. The first one would be to ignore changes to certain paths - something you'd find useful both with classic monorepos and repositories containing multiple Terraform projects under different paths. The second one would be to only attempt to apply a subset of changes - for example, only commits tagged in a certain way. Git push policy and tracked branch \u00bb Each stack and module points at a particular Git branch called a tracked branch . By default, any push to the tracked branch that changes a file in the project root triggers a tracked Run that can be applied . This logic can be changed entirely by a Git push policy, but the tracked branch is always reported as part of the Stack input to the policy evaluator and can be used as a point of reference. When a push policy does not track a new push, the head commit of the stack/module will not be set to the tracked branch head commit. We can address this by navigating to that stack and pressing the sync button (this syncs the tracked branch head commit with the head commit of the stack/module). Push and Pull Request events \u00bb Spacelift can currently react to two types of events - push and pull request (also called merge request by GitLab). Push events are the default - even if you don't have a push policy set up, we will respond to those events. Pull request events are supported for some VCS providers and are generally received when you open, synchronize (push a new commit), label, or merge the pull request. There are some valid reasons to use pull request events in addition or indeed instead of push ones. One is that when making decisions based on the paths of affected files, push events are often confusing: they contain affected files for all commits in a push, not just the head commit; they are not context-aware, making it hard to work with pull requests - if a given push is ignored on an otherwise relevant PR, then the Spacelift status check is not provided; But there are more reasons depending on how you want to structure your workflow. Here are a few samples of PR-driven policies from real-life use cases, each reflecting a slightly different way of doing things. First, let's only trigger proposed runs if a PR exists, and allow any push to the tracked branch to trigger a tracked run: 1 2 3 4 5 package spacelift track { input . push . branch == input . stack . branch } propose { not is_null ( input . pull_request ) } ignore { not track ; not propose } If you want to enforce that tracked runs are always created from PR merges (and not from direct pushes to the tracked branch), you can tweak the above policy accordingly to just ignore all non-PR events: 1 2 3 4 5 6 package spacelift track { is_pr ; input . push . branch == input . stack . branch } propose { is_pr } ignore { not is_pr } is_pr { not is_null ( input . pull_request ) } Here's another example where you respond to a particular PR label (\"deploy\") to automatically deploy changes: 1 2 3 4 5 6 package spacelift track { is_pr ; labeled } propose { true } is_pr { not is_null ( input . pull_request ) } labeled { input . pull_request . labels [ _ ] == \"deploy\" } Info When a run is triggered from a GitHub Pull Request and the Pull Request is mergeable (ie. there are no merge conflicts), we check out the code for something they call the \"potential merge commit\" - a virtual commit that represents the potential result of merging the Pull Request into its base branch. This should provide better quality, less confusing feedback. Let us know if you notice any irregularities. Deduplicating events \u00bb If you're using pull requests in your flow, it is possible that we'll receive duplicate events. For example, if you push to a feature branch and then open a pull request, we first receive a push event, then a separate pull request (opened) event. When you push another commit to that feature branch, we again receive two events - push and pull request (synchronized). When you merge the pull request, we get two more - push and pull request (closed) . It is possible that push policies resolve to the same actionable (not ignore ) outcome (eg. track or propose ). In those cases instead of creating two separate runs, we debounce the events by deduplicating runs created by them on a per-stack basis. The deduplication key consists of the commit SHA and run type. If your policy returns two different actionable outcomes for two different events associated with a given SHA, both runs will be created. In practice, this would be an unusual corner case and a good occasion to revisit your workflow. When events are deduplicated and you're sampling policy evaluations, you may notice that there are two samples for the same SHA, each with different input. You can generally assume that it's the first one that creates a run. Canceling in-progress runs \u00bb The push policy can also be used to have the new run pre-empt any runs that are currently in progress. The input document includes the in_progress key, which contains an array of runs that are currently either still queued or are awaiting human confirmation . You can use it in conjunction with the cancel rule like this: 1 cancel [ run . id ] { run : = input . in_progress [ _ ] } Of course, you can use a more sophisticated approach and only choose to cancel a certain type of run, or runs in a particular state. For example, the rule below will only cancel proposed runs that are currently queued (waiting for the worker): 1 2 3 4 5 cancel [ run . id ] { run : = input . in_progress [ _ ] run . type == \"PROPOSED\" run . state == \"QUEUED\" } Please note that you cannot cancel module test runs. Only proposed and tracked stack runs can be canceled. Info Note that run preemption is best effort and not guaranteed. If the run is either picked up by the worker or approved by a human in the meantime then the cancelation itself is canceled. Corner case: track, don't trigger \u00bb The track decision sets the new head commit on the affected stack or module . This head commit is what is going to be used when a tracked run is manually triggered , or a task is started on the stack. Usually what you want in this case is to have a new tracked Run, so this is what we do by default. Sometimes, however, you may want to trigger those tracked runs in a specific order or under specific circumstances - either manually or using a trigger policy . So what you want is an option to set the head commit, but not trigger a run. This is what the boolean notrigger rule can do for you. notrigger will only work in conjunction with track decision and will prevent the tracked run from being created. Please note that notrigger does not depend in any way on the track rule - they're entirely independent. Only when interpreting the result of the policy, we will only look at notrigger if track evaluates to true . Here's an example of using the two rules together to always set the new commit on the stack, but not trigger a run - for example, because it's either always triggered manually , through the API , or using a trigger policy : 1 2 3 track { input . push . branch == input . stack . branch } propose { not track } notrigger { true } Take Action from Comment(s) on Pull Request(s) \u00bb For more information on taking action from comments on Pull Requests, please view the documentation on pull request comments . Customize Spacelift Ignore Event Behavior \u00bb Customize VCS Check Messages for Ignored Run Events \u00bb If you would like to customize messages sent back to your VCS when Spacelift runs are ignored, you can do so using the message function within your Push policy. Please see the example policy below as a reference for this functionality. Customize Check Status for Ignored Run Events \u00bb By default, ignored runs on a stack will return a \"skipped\" status check event, rather than a fail event. If you would like ignored run events to have a failed status check on your VCS, you can do so using the fail function within your Push policy. If a fail result is desired, set this value to true. Example Policy \u00bb The following Push policy does not trigger any run within Spacelift. Using this policy, we can ensure that the status check within our VCS (in this case, GitHub) fails and returns the message \"I love bacon.\" 1 2 fail { true } message [ \"I love bacon\" ] { true } As a result of the above policy, users would then see this behavior within their GitHub status check: Info Note that this behavior (customization of the message and failing of the check within the VCS), is only applicable when runs do not take place within Spacelift. Tag-driven Terraform Module Release Flow \u00bb Some users prefer to manage their Terraform Module versions using git tags, and would like git tag events to push their module to the Spacelift module registry. Using a fairly simple Push policy, this is supported. To do this, you'll want to make sure of the module_version block within a Push policy attached your module, and then set the version using the tag information from the git push event. For example, the following example Push policy will trigger a tracked run when a tag event is detected. The policy then parses the tag event data and uses that value for the module version (in the below example we remove a git tag prefixed with v as the Terraform Module Registry only supports versions in a numeric X.X.X format. 1 2 3 4 5 6 7 8 package spacelift module_version : = version { version : = trim_prefix ( input . push . tag , \"v\" ) } propose { true } track { input . push . tag != \"\" } Allow forks \u00bb By default, we don't trigger runs when a forked repository opens a pull request against your repository. This is because of a security concern: if let's say your infrastructure is open source, someone forks it, implements some unwanted junk in there, then opens a pull request for the original repository, it'd run automatically with the prankster's code included. Info The cause is very similar to GitHub Actions where they don't expose repository secrets when forked repositories open pull requests. If you still want to allow it, you can explicitly do it with allow_fork rule. For example, if you trust certain people or organizations: 1 2 3 4 5 propose { true } allow_fork { validOwners : = { \"johnwayne\" , \"microsoft\" } validOwners [ input . pull_request . head_owner ] } In the above case, we'll allow a forked repository to run, only if the owner of the forked repository is either johnwayne or microsoft . head_owner field means different things in different VCS providers: GitHub / GitHub Enterprise \u00bb In GitHub, head_owner is the organization or the person owning the forked repository. It's typically in the URL: https://github.com/<head_owner>/<forked_repository> GitLab \u00bb In GitLab, it is the group of the repository which is typically the URL of the repository: https://gitlab.com/<head_owner>/<forked_repository> Azure DevOps \u00bb Azure DevOps is a special case because they don't provide us the friendly name of the head_owner . In this case, we need to refer to head_owner as the ID of the forked repository's project which is a UUID. One way to figure out this UUID is to open https://dev.azure.com/<organization>/_apis/projects website which lists all projects with their unique IDs. You don't need any special access to this API, you can just simply open it in your browser. Official documentation of the API. Bitbucket Cloud \u00bb In Bitbucket Cloud, head_owner means workspace . It's in the URL of the repository: https://www.bitbucket.org/<workspace>/<forked_repository> . Bitbucket Datacenter/Server \u00bb In Bitbucket Datacenter/Server, it is the project key of the repository. The project key is not the display name of the project, but the abbreviation in all caps. Approval and Mergeability \u00bb The pull_request property on the input to a push policy contains the following fields: approved - indicates whether the PR has been approved. mergeable - indicates whether the PR can be merged. undiverged - indicates that the PR branch is not behind the target branch. The following example shows a push policy that will automatically deploy a PR's changes once it has been approved, any required checks have completed, and the PR has a deploy label added to it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package spacelift # Trigger a tracked run if a change is pushed to the stack branch track { affected input.push.branch == input.stack.branch } # Trigger a tracked run if a PR is approved, mergeable, undiverged and has a deploy label track { is_pr is_clean is_approved is_marked_for_deploy } # Trigger a proposed run if a PR is opened propose { is_pr } is_pr { not is_null(input.pull_request) } is_clean { input.pull_request.mergeable input.pull_request.undiverged } is_approved { input.pull_request.approved } is_marked_for_deploy { input.pull_request.labels[_] == \"deploy\" } Each source control provider has slightly different features, and because of this the exact definition of approved and mergeable varies slightly between providers. The following sections explain the differences. Azure DevOps \u00bb approved means the PR has at least one approving review (including approved with suggestions). mergeable means that the PR branch has no conflicts with the target branch, and any blocking policies are approved. Info Please note that we are unable to calculate divergance across forks in Azure DevOps, so the undiverged property will always be false for PRs created from forks. Bitbucket Cloud \u00bb approved means that the PR has at least one approving review from someone other than the PR author. mergeable means that the PR branch has no conflicts with the target branch. Bitbucket Datacenter/Server \u00bb approved means that the PR has at least one approving review from someone other than the PR author. mergeable means that the PR branch has no conflicts with the target branch. GitHub / GitHub Enterprise \u00bb approved means that the PR has at least one approval, and also meets any minimum approval requirements for the repo. mergeable means that the PR branch has no conflicts with the target branch, and any branch protection rules have been met. GitLab \u00bb approved means that the PR has at least one approval. If approvals are required, it is only true when all required approvals have been made. mergeable means that the PR branch has no conflicts with the target branch, any blocking discussions have been resolved, and any required approvals have been made. Data input \u00bb As input, Git push policy receives the following document: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 { \"in_progress\" : [{ \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch this run is based on\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - run type: proposed, tracked, task, etc.\" , \"state\" : \"string - run state: queued, unconfirmed, etc.\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }], \"pull_request\" : { \"action\" : \"string - opened, reopened, closed, merged, edited, labeled, synchronize, unlabeled\" , \"action_initiator\" : \"string\" , \"approved\" : \"boolean - indicates whether the PR has been approved\" , \"author\" : \"string\" , \"base\" : { \"affected_files\" : [ \"string\" ], \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"message\" : \"string\" , \"tag\" : \"string\" }, \"closed\" : \"boolean\" , \"diff\" : [ \"string - list of files changed between base and head commit\" ], \"draft\" : \"boolean - indicates whether the PR is marked as draft\" , \"head\" : { \"affected_files\" : [ \"string\" ], \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"message\" : \"string\" , \"tag\" : \"string\" }, \"head_owner\" : \"string\" , \"id\" : \"number\" , \"labels\" : [ \"string\" ], \"mergeable\" : \"boolean - indicates whether the PR can be merged\" , \"title\" : \"string\" , \"undiverged\" : \"boolean - indicates whether the PR is up to date with the target branch\" } \"push\" : { // For Git push events, this contains the pushed commit. // For Pull Request events, // this contains the head commit or merge commit if available (merge event). \"affected_files\" : [ \"string\" ], \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"message\" : \"string\" , \"tag\" : \"string\" }, \"stack\" : { \"administrative\" : \"boolean\" , \"autodeploy\" : \"boolean\" , \"branch\" : \"string\" , \"id\" : \"string\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string\" , \"state\" : \"string\" , \"terraform_version\" : \"string or null\" , \"tracked_commit\" : { \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"hash\" : \"string\" , \"message\" : \"string\" }, \"worker_pool\" : { \"public\" : \"boolean - indicates whether the worker pool is public or not\" } } } Based on this input, the policy may define boolean track , propose and ignore rules. The positive outcome of at least one ignore rule causes the push to be ignored, no matter the outcome of other rules. The positive outcome of at least one track rule triggers a tracked run. The positive outcome of at least one propose rule triggers a proposed run. Warning If no rules are matched, the default is to ignore the push. Therefore it is important to always supply an exhaustive set of policies - that is, making sure that they define what to track and what to propose in addition to defining what they ignore . It is also possible to define an auxiliary rule called ignore_track , which overrides a positive outcome of the track rule but does not affect other rules, most notably the propose one. This can be used to turn some of the pushes that would otherwise be applied into test runs. Examples \u00bb Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need. Ignoring certain paths \u00bb Ignoring changes to certain paths is something you'd find useful both with classic monorepos and repositories containing multiple Terraform projects under different paths. When evaluating a push, we determine the list of affected files by looking at all the files touched by any of the commits in a given push. Info This list may include false positives - eg. in a situation where you delete a given file in one commit, then bring it back in another commit, and then push multiple commits at once. This is a safer default than trying to figure out the exact scope of each push. Let's imagine a situation where you only want to look at changes to Terraform definitions - in HCL or JSON - inside one the production/ or modules/ directory, and have track and propose use their default settings: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package spacelift track { input . push . branch == input . stack . branch } propose { input . push . branch != \"\" } ignore { not affected } affected { some i , j , k tracked_directories : = { \"modules/\" , \"production/\" } tracked_extensions : = { \".tf\" , \".tf.json\" } path : = input . push . affected_files [ i ] startswith ( path , tracked_directories [ j ]) endswith ( path , tracked_extensions [ k ]) } As an aside, note that in order to keep the example readable we had to define ignore in a negative way as per the Anna Karenina principle . A minimal example of this policy is available here . Status checks and ignored pushes \u00bb By default when the push policy instructs Spacelift to ignore a certain change, no commit status check is sent back to the VCS. This behavior is explicitly designed to prevent noise in monorepo scenarios where a large number of stacks are linked to the same Git repo. However, in certain cases one may still be interested in learning that the push was ignored, or just getting a commit status check for a given stack when it's set as required as part of GitHub branch protection set of rules, or simply your internal organization rules. In that case, you may find the notify rule useful. The purpose of this rule is to override default notification settings. So if you want to notify your VCS vendor even when a commit is ignored, you can define it like this: 1 2 3 4 5 package spacelift # other rules ( including ignore ), see above notify { ignore } Info The notify rule ( false by default) only applies to ignored pushes, so you can't set it to false to silence commit status checks for proposed runs . Applying from a tag \u00bb Another possible use case of a Git push policy would be to apply from a newly created tag rather than from a branch. This in turn can be useful in multiple scenarios - for example, a staging/QA environment could be deployed every time a certain tag type is applied to a tested branch, thereby providing inline feedback on a GitHub Pull Request from the actual deployment rather than a plan/test. One could also constrain production to only apply from tags unless a Run is explicitly triggered by the user. Here's an example of one such policy: 1 2 3 4 package spacelift track { re_match ( `^\\d+\\.\\d+\\.\\d+$` , input . push . tag ) } propose { input . push . branch != input . stack . branch } Default Git push policy \u00bb If no Git push policies are attached to a stack or a module, the default behavior is equivalent to this policy: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package spacelift track { affected input . push . branch == input . stack . branch } propose { affected input . push . branch != \"\" } ignore { input . push . branch == \"\" } affected { strings . any_prefix_match ( input . push . affected_files , input . stack . project_root ) } Waiting for CI/CD artifacts \u00bb There are cases where you want pushes to your repo to trigger a run in Spacelift, but only after a CI/CD pipeline (or a part of it) has completed. An example would be when you want to trigger an infra deploy after some docker image has been built and pushed to a registry. This is achievable via push policies by using the External Dependencies feature.","title":"Push policy"},{"location":"concepts/policy/push-policy/index.html#push-policy","text":"","title":"Push policy"},{"location":"concepts/policy/push-policy/index.html#purpose","text":"Git push policies are triggered on a per-stack basis to determine the action that should be taken for each individual Stack or Module in response to a Git push or Pull Request notification. There are three possible outcomes: track : set the new head commit on the stack / module and create a tracked Run , ie. one that can be applied ; propose : create a proposed Run against a proposed version of infrastructure; ignore : do not schedule a new Run; Using this policy it is possible to create a very sophisticated, custom-made setup. We can think of two main - and not mutually exclusive - use cases. The first one would be to ignore changes to certain paths - something you'd find useful both with classic monorepos and repositories containing multiple Terraform projects under different paths. The second one would be to only attempt to apply a subset of changes - for example, only commits tagged in a certain way.","title":"Purpose"},{"location":"concepts/policy/push-policy/index.html#git-push-policy-and-tracked-branch","text":"Each stack and module points at a particular Git branch called a tracked branch . By default, any push to the tracked branch that changes a file in the project root triggers a tracked Run that can be applied . This logic can be changed entirely by a Git push policy, but the tracked branch is always reported as part of the Stack input to the policy evaluator and can be used as a point of reference. When a push policy does not track a new push, the head commit of the stack/module will not be set to the tracked branch head commit. We can address this by navigating to that stack and pressing the sync button (this syncs the tracked branch head commit with the head commit of the stack/module).","title":"Git push policy and tracked branch"},{"location":"concepts/policy/push-policy/index.html#push-and-pull-request-events","text":"Spacelift can currently react to two types of events - push and pull request (also called merge request by GitLab). Push events are the default - even if you don't have a push policy set up, we will respond to those events. Pull request events are supported for some VCS providers and are generally received when you open, synchronize (push a new commit), label, or merge the pull request. There are some valid reasons to use pull request events in addition or indeed instead of push ones. One is that when making decisions based on the paths of affected files, push events are often confusing: they contain affected files for all commits in a push, not just the head commit; they are not context-aware, making it hard to work with pull requests - if a given push is ignored on an otherwise relevant PR, then the Spacelift status check is not provided; But there are more reasons depending on how you want to structure your workflow. Here are a few samples of PR-driven policies from real-life use cases, each reflecting a slightly different way of doing things. First, let's only trigger proposed runs if a PR exists, and allow any push to the tracked branch to trigger a tracked run: 1 2 3 4 5 package spacelift track { input . push . branch == input . stack . branch } propose { not is_null ( input . pull_request ) } ignore { not track ; not propose } If you want to enforce that tracked runs are always created from PR merges (and not from direct pushes to the tracked branch), you can tweak the above policy accordingly to just ignore all non-PR events: 1 2 3 4 5 6 package spacelift track { is_pr ; input . push . branch == input . stack . branch } propose { is_pr } ignore { not is_pr } is_pr { not is_null ( input . pull_request ) } Here's another example where you respond to a particular PR label (\"deploy\") to automatically deploy changes: 1 2 3 4 5 6 package spacelift track { is_pr ; labeled } propose { true } is_pr { not is_null ( input . pull_request ) } labeled { input . pull_request . labels [ _ ] == \"deploy\" } Info When a run is triggered from a GitHub Pull Request and the Pull Request is mergeable (ie. there are no merge conflicts), we check out the code for something they call the \"potential merge commit\" - a virtual commit that represents the potential result of merging the Pull Request into its base branch. This should provide better quality, less confusing feedback. Let us know if you notice any irregularities.","title":"Push and Pull Request events"},{"location":"concepts/policy/push-policy/index.html#deduplicating-events","text":"If you're using pull requests in your flow, it is possible that we'll receive duplicate events. For example, if you push to a feature branch and then open a pull request, we first receive a push event, then a separate pull request (opened) event. When you push another commit to that feature branch, we again receive two events - push and pull request (synchronized). When you merge the pull request, we get two more - push and pull request (closed) . It is possible that push policies resolve to the same actionable (not ignore ) outcome (eg. track or propose ). In those cases instead of creating two separate runs, we debounce the events by deduplicating runs created by them on a per-stack basis. The deduplication key consists of the commit SHA and run type. If your policy returns two different actionable outcomes for two different events associated with a given SHA, both runs will be created. In practice, this would be an unusual corner case and a good occasion to revisit your workflow. When events are deduplicated and you're sampling policy evaluations, you may notice that there are two samples for the same SHA, each with different input. You can generally assume that it's the first one that creates a run.","title":"Deduplicating events"},{"location":"concepts/policy/push-policy/index.html#canceling-in-progress-runs","text":"The push policy can also be used to have the new run pre-empt any runs that are currently in progress. The input document includes the in_progress key, which contains an array of runs that are currently either still queued or are awaiting human confirmation . You can use it in conjunction with the cancel rule like this: 1 cancel [ run . id ] { run : = input . in_progress [ _ ] } Of course, you can use a more sophisticated approach and only choose to cancel a certain type of run, or runs in a particular state. For example, the rule below will only cancel proposed runs that are currently queued (waiting for the worker): 1 2 3 4 5 cancel [ run . id ] { run : = input . in_progress [ _ ] run . type == \"PROPOSED\" run . state == \"QUEUED\" } Please note that you cannot cancel module test runs. Only proposed and tracked stack runs can be canceled. Info Note that run preemption is best effort and not guaranteed. If the run is either picked up by the worker or approved by a human in the meantime then the cancelation itself is canceled.","title":"Canceling in-progress runs"},{"location":"concepts/policy/push-policy/index.html#corner-case-track-dont-trigger","text":"The track decision sets the new head commit on the affected stack or module . This head commit is what is going to be used when a tracked run is manually triggered , or a task is started on the stack. Usually what you want in this case is to have a new tracked Run, so this is what we do by default. Sometimes, however, you may want to trigger those tracked runs in a specific order or under specific circumstances - either manually or using a trigger policy . So what you want is an option to set the head commit, but not trigger a run. This is what the boolean notrigger rule can do for you. notrigger will only work in conjunction with track decision and will prevent the tracked run from being created. Please note that notrigger does not depend in any way on the track rule - they're entirely independent. Only when interpreting the result of the policy, we will only look at notrigger if track evaluates to true . Here's an example of using the two rules together to always set the new commit on the stack, but not trigger a run - for example, because it's either always triggered manually , through the API , or using a trigger policy : 1 2 3 track { input . push . branch == input . stack . branch } propose { not track } notrigger { true }","title":"Corner case: track, don't trigger"},{"location":"concepts/policy/push-policy/index.html#take-action-from-comments-on-pull-requests","text":"For more information on taking action from comments on Pull Requests, please view the documentation on pull request comments .","title":"Take Action from Comment(s) on Pull Request(s)"},{"location":"concepts/policy/push-policy/index.html#customize-spacelift-ignore-event-behavior","text":"","title":"Customize Spacelift Ignore Event Behavior"},{"location":"concepts/policy/push-policy/index.html#customize-vcs-check-messages-for-ignored-run-events","text":"If you would like to customize messages sent back to your VCS when Spacelift runs are ignored, you can do so using the message function within your Push policy. Please see the example policy below as a reference for this functionality.","title":"Customize VCS Check Messages for Ignored Run Events"},{"location":"concepts/policy/push-policy/index.html#customize-check-status-for-ignored-run-events","text":"By default, ignored runs on a stack will return a \"skipped\" status check event, rather than a fail event. If you would like ignored run events to have a failed status check on your VCS, you can do so using the fail function within your Push policy. If a fail result is desired, set this value to true.","title":"Customize Check Status for Ignored Run Events"},{"location":"concepts/policy/push-policy/index.html#example-policy","text":"The following Push policy does not trigger any run within Spacelift. Using this policy, we can ensure that the status check within our VCS (in this case, GitHub) fails and returns the message \"I love bacon.\" 1 2 fail { true } message [ \"I love bacon\" ] { true } As a result of the above policy, users would then see this behavior within their GitHub status check: Info Note that this behavior (customization of the message and failing of the check within the VCS), is only applicable when runs do not take place within Spacelift.","title":"Example Policy"},{"location":"concepts/policy/push-policy/index.html#tag-driven-terraform-module-release-flow","text":"Some users prefer to manage their Terraform Module versions using git tags, and would like git tag events to push their module to the Spacelift module registry. Using a fairly simple Push policy, this is supported. To do this, you'll want to make sure of the module_version block within a Push policy attached your module, and then set the version using the tag information from the git push event. For example, the following example Push policy will trigger a tracked run when a tag event is detected. The policy then parses the tag event data and uses that value for the module version (in the below example we remove a git tag prefixed with v as the Terraform Module Registry only supports versions in a numeric X.X.X format. 1 2 3 4 5 6 7 8 package spacelift module_version : = version { version : = trim_prefix ( input . push . tag , \"v\" ) } propose { true } track { input . push . tag != \"\" }","title":"Tag-driven Terraform Module Release Flow"},{"location":"concepts/policy/push-policy/index.html#allow-forks","text":"By default, we don't trigger runs when a forked repository opens a pull request against your repository. This is because of a security concern: if let's say your infrastructure is open source, someone forks it, implements some unwanted junk in there, then opens a pull request for the original repository, it'd run automatically with the prankster's code included. Info The cause is very similar to GitHub Actions where they don't expose repository secrets when forked repositories open pull requests. If you still want to allow it, you can explicitly do it with allow_fork rule. For example, if you trust certain people or organizations: 1 2 3 4 5 propose { true } allow_fork { validOwners : = { \"johnwayne\" , \"microsoft\" } validOwners [ input . pull_request . head_owner ] } In the above case, we'll allow a forked repository to run, only if the owner of the forked repository is either johnwayne or microsoft . head_owner field means different things in different VCS providers:","title":"Allow forks"},{"location":"concepts/policy/push-policy/index.html#github-github-enterprise","text":"In GitHub, head_owner is the organization or the person owning the forked repository. It's typically in the URL: https://github.com/<head_owner>/<forked_repository>","title":"GitHub / GitHub Enterprise"},{"location":"concepts/policy/push-policy/index.html#gitlab","text":"In GitLab, it is the group of the repository which is typically the URL of the repository: https://gitlab.com/<head_owner>/<forked_repository>","title":"GitLab"},{"location":"concepts/policy/push-policy/index.html#azure-devops","text":"Azure DevOps is a special case because they don't provide us the friendly name of the head_owner . In this case, we need to refer to head_owner as the ID of the forked repository's project which is a UUID. One way to figure out this UUID is to open https://dev.azure.com/<organization>/_apis/projects website which lists all projects with their unique IDs. You don't need any special access to this API, you can just simply open it in your browser. Official documentation of the API.","title":"Azure DevOps"},{"location":"concepts/policy/push-policy/index.html#bitbucket-cloud","text":"In Bitbucket Cloud, head_owner means workspace . It's in the URL of the repository: https://www.bitbucket.org/<workspace>/<forked_repository> .","title":"Bitbucket Cloud"},{"location":"concepts/policy/push-policy/index.html#bitbucket-datacenterserver","text":"In Bitbucket Datacenter/Server, it is the project key of the repository. The project key is not the display name of the project, but the abbreviation in all caps.","title":"Bitbucket Datacenter/Server"},{"location":"concepts/policy/push-policy/index.html#approval-and-mergeability","text":"The pull_request property on the input to a push policy contains the following fields: approved - indicates whether the PR has been approved. mergeable - indicates whether the PR can be merged. undiverged - indicates that the PR branch is not behind the target branch. The following example shows a push policy that will automatically deploy a PR's changes once it has been approved, any required checks have completed, and the PR has a deploy label added to it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package spacelift # Trigger a tracked run if a change is pushed to the stack branch track { affected input.push.branch == input.stack.branch } # Trigger a tracked run if a PR is approved, mergeable, undiverged and has a deploy label track { is_pr is_clean is_approved is_marked_for_deploy } # Trigger a proposed run if a PR is opened propose { is_pr } is_pr { not is_null(input.pull_request) } is_clean { input.pull_request.mergeable input.pull_request.undiverged } is_approved { input.pull_request.approved } is_marked_for_deploy { input.pull_request.labels[_] == \"deploy\" } Each source control provider has slightly different features, and because of this the exact definition of approved and mergeable varies slightly between providers. The following sections explain the differences.","title":"Approval and Mergeability"},{"location":"concepts/policy/push-policy/index.html#azure-devops_1","text":"approved means the PR has at least one approving review (including approved with suggestions). mergeable means that the PR branch has no conflicts with the target branch, and any blocking policies are approved. Info Please note that we are unable to calculate divergance across forks in Azure DevOps, so the undiverged property will always be false for PRs created from forks.","title":"Azure DevOps "},{"location":"concepts/policy/push-policy/index.html#bitbucket-cloud_1","text":"approved means that the PR has at least one approving review from someone other than the PR author. mergeable means that the PR branch has no conflicts with the target branch.","title":"Bitbucket Cloud "},{"location":"concepts/policy/push-policy/index.html#bitbucket-datacenterserver_1","text":"approved means that the PR has at least one approving review from someone other than the PR author. mergeable means that the PR branch has no conflicts with the target branch.","title":"Bitbucket Datacenter/Server "},{"location":"concepts/policy/push-policy/index.html#github-github-enterprise_1","text":"approved means that the PR has at least one approval, and also meets any minimum approval requirements for the repo. mergeable means that the PR branch has no conflicts with the target branch, and any branch protection rules have been met.","title":"GitHub / GitHub Enterprise "},{"location":"concepts/policy/push-policy/index.html#gitlab_1","text":"approved means that the PR has at least one approval. If approvals are required, it is only true when all required approvals have been made. mergeable means that the PR branch has no conflicts with the target branch, any blocking discussions have been resolved, and any required approvals have been made.","title":"GitLab "},{"location":"concepts/policy/push-policy/index.html#data-input","text":"As input, Git push policy receives the following document: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 { \"in_progress\" : [{ \"based_on_local_workspace\" : \"boolean - whether the run stems from a local preview\" , \"branch\" : \"string - the branch this run is based on\" , \"created_at\" : \"number - creation Unix timestamp in nanoseconds\" , \"triggered_by\" : \"string or null - user or trigger policy who triggered the run, if applicable\" , \"type\" : \"string - run type: proposed, tracked, task, etc.\" , \"state\" : \"string - run state: queued, unconfirmed, etc.\" , \"updated_at\" : \"number - last update Unix timestamp in nanoseconds\" , \"user_provided_metadata\" : [ \"string - blobs of metadata provided using spacectl or the API when interacting with this run\" ] }], \"pull_request\" : { \"action\" : \"string - opened, reopened, closed, merged, edited, labeled, synchronize, unlabeled\" , \"action_initiator\" : \"string\" , \"approved\" : \"boolean - indicates whether the PR has been approved\" , \"author\" : \"string\" , \"base\" : { \"affected_files\" : [ \"string\" ], \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"message\" : \"string\" , \"tag\" : \"string\" }, \"closed\" : \"boolean\" , \"diff\" : [ \"string - list of files changed between base and head commit\" ], \"draft\" : \"boolean - indicates whether the PR is marked as draft\" , \"head\" : { \"affected_files\" : [ \"string\" ], \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"message\" : \"string\" , \"tag\" : \"string\" }, \"head_owner\" : \"string\" , \"id\" : \"number\" , \"labels\" : [ \"string\" ], \"mergeable\" : \"boolean - indicates whether the PR can be merged\" , \"title\" : \"string\" , \"undiverged\" : \"boolean - indicates whether the PR is up to date with the target branch\" } \"push\" : { // For Git push events, this contains the pushed commit. // For Pull Request events, // this contains the head commit or merge commit if available (merge event). \"affected_files\" : [ \"string\" ], \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"message\" : \"string\" , \"tag\" : \"string\" }, \"stack\" : { \"administrative\" : \"boolean\" , \"autodeploy\" : \"boolean\" , \"branch\" : \"string\" , \"id\" : \"string\" , \"labels\" : [ \"string - list of arbitrary, user-defined selectors\" ], \"locked_by\" : \"optional string - if the stack is locked, this is the name of the user who did it\" , \"name\" : \"string\" , \"namespace\" : \"string - repository namespace, only relevant to GitLab repositories\" , \"project_root\" : \"optional string - project root as set on the Stack, if any\" , \"repository\" : \"string\" , \"state\" : \"string\" , \"terraform_version\" : \"string or null\" , \"tracked_commit\" : { \"author\" : \"string\" , \"branch\" : \"string\" , \"created_at\" : \"number (timestamp in nanoseconds)\" , \"hash\" : \"string\" , \"message\" : \"string\" }, \"worker_pool\" : { \"public\" : \"boolean - indicates whether the worker pool is public or not\" } } } Based on this input, the policy may define boolean track , propose and ignore rules. The positive outcome of at least one ignore rule causes the push to be ignored, no matter the outcome of other rules. The positive outcome of at least one track rule triggers a tracked run. The positive outcome of at least one propose rule triggers a proposed run. Warning If no rules are matched, the default is to ignore the push. Therefore it is important to always supply an exhaustive set of policies - that is, making sure that they define what to track and what to propose in addition to defining what they ignore . It is also possible to define an auxiliary rule called ignore_track , which overrides a positive outcome of the track rule but does not affect other rules, most notably the propose one. This can be used to turn some of the pushes that would otherwise be applied into test runs.","title":"Data input"},{"location":"concepts/policy/push-policy/index.html#examples","text":"Tip We maintain a library of example policies that are ready to use or that you could tweak to meet your specific needs. If you cannot find what you are looking for below or in the library, please reach out to our support and we will craft a policy to do exactly what you need.","title":"Examples"},{"location":"concepts/policy/push-policy/index.html#ignoring-certain-paths","text":"Ignoring changes to certain paths is something you'd find useful both with classic monorepos and repositories containing multiple Terraform projects under different paths. When evaluating a push, we determine the list of affected files by looking at all the files touched by any of the commits in a given push. Info This list may include false positives - eg. in a situation where you delete a given file in one commit, then bring it back in another commit, and then push multiple commits at once. This is a safer default than trying to figure out the exact scope of each push. Let's imagine a situation where you only want to look at changes to Terraform definitions - in HCL or JSON - inside one the production/ or modules/ directory, and have track and propose use their default settings: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package spacelift track { input . push . branch == input . stack . branch } propose { input . push . branch != \"\" } ignore { not affected } affected { some i , j , k tracked_directories : = { \"modules/\" , \"production/\" } tracked_extensions : = { \".tf\" , \".tf.json\" } path : = input . push . affected_files [ i ] startswith ( path , tracked_directories [ j ]) endswith ( path , tracked_extensions [ k ]) } As an aside, note that in order to keep the example readable we had to define ignore in a negative way as per the Anna Karenina principle . A minimal example of this policy is available here .","title":"Ignoring certain paths"},{"location":"concepts/policy/push-policy/index.html#status-checks-and-ignored-pushes","text":"By default when the push policy instructs Spacelift to ignore a certain change, no commit status check is sent back to the VCS. This behavior is explicitly designed to prevent noise in monorepo scenarios where a large number of stacks are linked to the same Git repo. However, in certain cases one may still be interested in learning that the push was ignored, or just getting a commit status check for a given stack when it's set as required as part of GitHub branch protection set of rules, or simply your internal organization rules. In that case, you may find the notify rule useful. The purpose of this rule is to override default notification settings. So if you want to notify your VCS vendor even when a commit is ignored, you can define it like this: 1 2 3 4 5 package spacelift # other rules ( including ignore ), see above notify { ignore } Info The notify rule ( false by default) only applies to ignored pushes, so you can't set it to false to silence commit status checks for proposed runs .","title":"Status checks and ignored pushes"},{"location":"concepts/policy/push-policy/index.html#applying-from-a-tag","text":"Another possible use case of a Git push policy would be to apply from a newly created tag rather than from a branch. This in turn can be useful in multiple scenarios - for example, a staging/QA environment could be deployed every time a certain tag type is applied to a tested branch, thereby providing inline feedback on a GitHub Pull Request from the actual deployment rather than a plan/test. One could also constrain production to only apply from tags unless a Run is explicitly triggered by the user. Here's an example of one such policy: 1 2 3 4 package spacelift track { re_match ( `^\\d+\\.\\d+\\.\\d+$` , input . push . tag ) } propose { input . push . branch != input . stack . branch }","title":"Applying from a tag"},{"location":"concepts/policy/push-policy/index.html#default-git-push-policy","text":"If no Git push policies are attached to a stack or a module, the default behavior is equivalent to this policy: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package spacelift track { affected input . push . branch == input . stack . branch } propose { affected input . push . branch != \"\" } ignore { input . push . branch == \"\" } affected { strings . any_prefix_match ( input . push . affected_files , input . stack . project_root ) }","title":"Default Git push policy"},{"location":"concepts/policy/push-policy/index.html#waiting-for-cicd-artifacts","text":"There are cases where you want pushes to your repo to trigger a run in Spacelift, but only after a CI/CD pipeline (or a part of it) has completed. An example would be when you want to trigger an infra deploy after some docker image has been built and pushed to a registry. This is achievable via push policies by using the External Dependencies feature.","title":"Waiting for CI/CD artifacts"},{"location":"concepts/policy/push-policy/run-external-dependencies.html","text":"External Dependencies \u00bb Purpose \u00bb External dependencies is a feature within push policies that allows a user to define a set of dependencies, which, while being external to Spacelift, must be completed before a Spacelift run can start. A common use case of this feature is making Spacelift wait for a CI/CD pipeline to complete before executing a run. How it works \u00bb Using this feature consists of two parts: defining dependencies in push policies marking dependencies as finished or failed using spacectl Defining dependencies \u00bb To define dependencies, you need to add a external_dependency rule to your push policy definition. This way, any run that gets created via this policy, also has the dependency defined. The following rule adds a dependency to all runs created by a policy. 1 external_dependency[sprintf(\"%s-binary-build\", [input.push.hash])] { true } You can of course have more complex rules, that decide on the set of external dependencies based on e.g. the current stack's labels. Warning Make sure to include unique strings such as commit hashes in the dependencies names, as this is the only way to ensure that the dependency is unique for each source control event. Marking dependencies as finished or failed \u00bb To mark a dependency as finished or failed, you need to use the spacectl command line tool. You can do so with following commands: 1 2 3 spacectl run-external-dependency mark-completed --id \"<commit-sha>-binary-build\" --status finished spacectl run-external-dependency mark-completed --id \"<commit-sha>-binary-build\" --status failed Info Run will be eligible for execution only after all of its dependencies are marked as finished. At the same time, if any of the dependencies has failed, the run will be marked as failed as well. Warning In order to mark a run dependency as finished or failed, spacectl needs to be authenticated and have write access to all the spaces that have runs with the given dependency defined. Example with GH Actions \u00bb The following example shows how to use this feature with GitHub Actions. The first thing we need to do is to define a push policy with dependencies. Our policy will look like this: 1 2 3 4 5 6 7 8 9 package spacelift track { input.push != null input.push.branch == input.stack.branch } external_dependency[sprintf(\"%s-binary-build\", [input.push.hash])] { true } external_dependency[sprintf(\"%s-docker-image-build\", [input.push.hash])] { true } We are defining two dependencies. One for a binary build and one for a docker image build. Next, we need to create a GitHub Action pipeline that will mark the dependencies as finished or failed. This pipeline will define two jobs, one for each dependency. We will use sleep to mock the build process. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 name : Build on : push : jobs : build-binaries : name : Build binaries runs-on : ubuntu-latest steps : - name : Install spacectl uses : spacelift-io/setup-spacectl@main env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} - name : Check out repository code uses : actions/checkout@v3 - name : Build binaries run : | sleep 15 echo \"building binaries done\" - name : Notify Spacelift of build completion (success) if : success() env : SPACELIFT_API_KEY_ENDPOINT : https://<youraccount>.app.spacelift.io SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-binary-build\" --status finished - name : Notify Spacelift of build completion (failed) if : failure() env : SPACELIFT_API_KEY_ENDPOINT : https://<youraccount>.app.spacelift.io SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-binary-build\" --status failed build-docker-images : name : Build docker images runs-on : ubuntu-latest steps : - name : Install spacectl uses : spacelift-io/setup-spacectl@main env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} - name : Check out repository code uses : actions/checkout@v3 - name : Build docker images run : | sleep 30 echo \"building images done\" - name : Notify Spacelift of build completion (success) if : success() env : SPACELIFT_API_KEY_ENDPOINT : https://<youraccount>.app.spacelift.io SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-docker-image-build\" --status finished - name : Notify Spacelift of build completion (failed) if : failure() env : SPACELIFT_API_KEY_ENDPOINT : https://<youraccount>.app.spacelift.io SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-docker-image-build\" --status failed Warning Make sure to replace <youraccount> with your Spacelift account name and fill in necessary secrets if you decide to use this example. Testing example \u00bb Having the policy and the pipeline defined, we can now test it. Creating a new commit in the repository will trigger the pipeline. As we can see a run was created in Spacelift, but it's in queued state. The run will not start until all the dependencies are marked as finished. After the binary-build dependency has been marked as completed the run is still queued, as the docker-image-build dependency is still not resolved. The run starts only after all the dependencies are marked as finished. We can also test what happens if a step in the pipeline fails. In order to test this, we can change the build-binaries job in the pipeline from 1 2 3 4 - name : Build binaries run : | sleep 15 echo \"building binaries done\" to 1 2 3 4 5 - name : Build binaries run : | sleep 15 echo \"building binaries failed\" exit 1 Now, when we push a commit to the repo, after a while, we will see that the new run is marked as failed, with a note explaining that one of the dependencies is marked as failed.","title":"External Dependencies"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#external-dependencies","text":"","title":"External Dependencies"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#purpose","text":"External dependencies is a feature within push policies that allows a user to define a set of dependencies, which, while being external to Spacelift, must be completed before a Spacelift run can start. A common use case of this feature is making Spacelift wait for a CI/CD pipeline to complete before executing a run.","title":"Purpose"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#how-it-works","text":"Using this feature consists of two parts: defining dependencies in push policies marking dependencies as finished or failed using spacectl","title":"How it works"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#defining-dependencies","text":"To define dependencies, you need to add a external_dependency rule to your push policy definition. This way, any run that gets created via this policy, also has the dependency defined. The following rule adds a dependency to all runs created by a policy. 1 external_dependency[sprintf(\"%s-binary-build\", [input.push.hash])] { true } You can of course have more complex rules, that decide on the set of external dependencies based on e.g. the current stack's labels. Warning Make sure to include unique strings such as commit hashes in the dependencies names, as this is the only way to ensure that the dependency is unique for each source control event.","title":"Defining dependencies"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#marking-dependencies-as-finished-or-failed","text":"To mark a dependency as finished or failed, you need to use the spacectl command line tool. You can do so with following commands: 1 2 3 spacectl run-external-dependency mark-completed --id \"<commit-sha>-binary-build\" --status finished spacectl run-external-dependency mark-completed --id \"<commit-sha>-binary-build\" --status failed Info Run will be eligible for execution only after all of its dependencies are marked as finished. At the same time, if any of the dependencies has failed, the run will be marked as failed as well. Warning In order to mark a run dependency as finished or failed, spacectl needs to be authenticated and have write access to all the spaces that have runs with the given dependency defined.","title":"Marking dependencies as finished or failed"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#example-with-gh-actions","text":"The following example shows how to use this feature with GitHub Actions. The first thing we need to do is to define a push policy with dependencies. Our policy will look like this: 1 2 3 4 5 6 7 8 9 package spacelift track { input.push != null input.push.branch == input.stack.branch } external_dependency[sprintf(\"%s-binary-build\", [input.push.hash])] { true } external_dependency[sprintf(\"%s-docker-image-build\", [input.push.hash])] { true } We are defining two dependencies. One for a binary build and one for a docker image build. Next, we need to create a GitHub Action pipeline that will mark the dependencies as finished or failed. This pipeline will define two jobs, one for each dependency. We will use sleep to mock the build process. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 name : Build on : push : jobs : build-binaries : name : Build binaries runs-on : ubuntu-latest steps : - name : Install spacectl uses : spacelift-io/setup-spacectl@main env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} - name : Check out repository code uses : actions/checkout@v3 - name : Build binaries run : | sleep 15 echo \"building binaries done\" - name : Notify Spacelift of build completion (success) if : success() env : SPACELIFT_API_KEY_ENDPOINT : https://<youraccount>.app.spacelift.io SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-binary-build\" --status finished - name : Notify Spacelift of build completion (failed) if : failure() env : SPACELIFT_API_KEY_ENDPOINT : https://<youraccount>.app.spacelift.io SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-binary-build\" --status failed build-docker-images : name : Build docker images runs-on : ubuntu-latest steps : - name : Install spacectl uses : spacelift-io/setup-spacectl@main env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} - name : Check out repository code uses : actions/checkout@v3 - name : Build docker images run : | sleep 30 echo \"building images done\" - name : Notify Spacelift of build completion (success) if : success() env : SPACELIFT_API_KEY_ENDPOINT : https://<youraccount>.app.spacelift.io SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-docker-image-build\" --status finished - name : Notify Spacelift of build completion (failed) if : failure() env : SPACELIFT_API_KEY_ENDPOINT : https://<youraccount>.app.spacelift.io SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : spacectl run-external-dependency mark-completed --id \"${GITHUB_SHA}-docker-image-build\" --status failed Warning Make sure to replace <youraccount> with your Spacelift account name and fill in necessary secrets if you decide to use this example.","title":"Example with GH Actions"},{"location":"concepts/policy/push-policy/run-external-dependencies.html#testing-example","text":"Having the policy and the pipeline defined, we can now test it. Creating a new commit in the repository will trigger the pipeline. As we can see a run was created in Spacelift, but it's in queued state. The run will not start until all the dependencies are marked as finished. After the binary-build dependency has been marked as completed the run is still queued, as the docker-image-build dependency is still not resolved. The run starts only after all the dependencies are marked as finished. We can also test what happens if a step in the pipeline fails. In order to test this, we can change the build-binaries job in the pipeline from 1 2 3 4 - name : Build binaries run : | sleep 15 echo \"building binaries done\" to 1 2 3 4 5 - name : Build binaries run : | sleep 15 echo \"building binaries failed\" exit 1 Now, when we push a commit to the repo, after a while, we will see that the new run is marked as failed, with a note explaining that one of the dependencies is marked as failed.","title":"Testing example"},{"location":"concepts/run/index.html","text":"Run \u00bb Every job that can touch your Spacelift-managed infrastructure is called a Run. There are four main types of runs, and each of them warrants a separate section. Three of them are children of Stacks : task , which is a freeform command you can execute on your infrastructure; proposed run , which serves as a preview of introduced changes; tracked run , which is a form of deployment; There's also a fourth type of run - module test case . Very similar to a tracked run, it's executed on a Terraform module. Execution model \u00bb In Spacelift, each run is executed on a worker node, inside a Docker container. We maintain a number of these worker nodes (collectively known as the public worker pool ) that are available to all customers, but also allow individual customers to run our agent on their end, for their exclusive use. You can read more about worker pools here . Regardless of whether you end up using a private or a public worker pool, each Spacelift run involves a handover between spacelift.io (which we like to call the mothership ) and the worker node. After the handover, the worker node is fully responsible for running the job and communicating the results of the job back to the mothership . Info It's important to know that it's always the worker node executing the run and accessing your infrastructure, never the mothership. Common run states \u00bb Regardless of the type of the job performed, some phases and terminal states are common. We discuss them here, so that we can refer to them when describing various types of runs in more detail. Queued \u00bb Queued means that no worker has picked up the run yet . There are two possible reasons for that: there are no available workers, or the run is not eligible for processing. Let's start with the latter - more common - scenario. Spacelift serializes all state-changing operations to the Stack. Both tracked runs and tasks have the capacity to change the state, they're never allowed to run in parallel. Instead, each of them gets an exclusive lock on the stack, blocking others from starting. If you run or task is currently blocked by something else holding the lock on the stack, you'll see the link to the blocker in the header: The other scenario is just running out of available workers . If you're using the public worker pool, you can track its availability on our status page . In particular, you should look at a system metric called Public worker queuing time. It indicates how long has the oldest run spent in the queue since becoming eligible for processing: If you're using private workers, please refer to the worker pools documentation for troubleshooting advice. Queued is a passive state meaning no operations are performed while a run is in this state. When a run is eligible for processing and a worker is available to pick it up, the state will automatically transition to Preparing . The user can cancel the run while it's still queued, transitioning it to the terminal Canceled state. Canceled \u00bb Canceled state means that the user has manually stopped a Queued run or task even before it had the chance to be picked up by the worker. Canceled is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can follow it. Preparing \u00bb The preparing state is the first one where real work is done. At this point both the run is eligible for processing and there's a worker node ready to process it. The preparing state is all about the handover and dialog between spacelift.io and the worker. Here's an example of one such handover: Note that Ground Control refers to the bit directly controlled by us, in a nod to late David Bowie . The main purpose of this phase is for Ground Control to make sure that the worker node gets everything that's required to perform the job, and that it can take over the execution. Once the worker is able to pull the Docker image and use it to start the container, this phase is over and the initialization phase begins. If the process fails for whatever reason, the run is marked as failed . Initializing \u00bb The last phase where actual work is done and which is common to all types of run is the initialization . This phase is handled exclusively by the worker and involves running pre-initialization hooks and vendor-specific initialization process. For Terraform stacks it would mean running terraform init , in the right directory and with the right parameters. Important thing to note with regards to pre-initialization hooks and the rest of the initialization process is that all these run in the same shell session, so environment variables exported by pre-initialization hooks are accessible to the vendor-specific initialization process. This is often the desired outcome when working with external secret managers like HashiCorp Vault. If this phase fails for whatever reason, the run is marked as failed . Otherwise, the next step is determined by the type of the run being executed. Failed \u00bb If a run transitions into the failed state means that something, at some point went wrong and this state can follow any state. In most cases this will be something related to your project like: errors in the source code; pre-initialization checks failing; plan policies rejecting your change; deployment-time errors; In rare cases errors in Spacelift application code or third party dependencies can also make the job fail. These cases are clearly marked by a notice corresponding to the failure mode, reported through our exception tracker and immediately investigated. Failed is a passive state meaning no operations are performed while the run is in this state. It's also a terminal state meaning that no further state can supersede it. Finished \u00bb Finished state means that the run was successful, though the success criteria will depend on the type of run. Please read the documentation for the relevant run type for more details. Finished is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it. Stopping runs \u00bb Some types of runs in some phases may safely be interrupted. We allow sending a stop signal from the GUI and API to the run, which is then passed to the worker handling the job. It's then up to the worker to handle or ignore that signal. Stopped state indicates that a run has been stopped while Initializing or Planning , either manually by the user or - for proposed changes - also by Spacelift. Proposed changes will automatically be stopped when a newer version of the code is pushed to their branch. This is mainly designed to limit the number of unnecessary API calls to your resource providers, though it saves us a few bucks on EC2, too. Here's an example of a run manually stopped while Initializing : Stopped is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it. Logs Retention \u00bb Run logs are kept for 60 days. Zero Trust Model \u00bb For your most sensitive Stacks you can use additional verification of Runs based on arbitrary metadata you can provide to Runs when creating or confirming them. This metadata can be passed through the API or by using the spacectl CLI to create or confirm runs. Every time such an interaction happens, you can add a new piece of metadata, which will form a list of metadata blobs inside of the Run. This will then be available in policies, including the private-worker side initialization policy. This way you can i.e. sign the runs when you confirm them and later verify this signature inside of the private worker, through the initialization policy. There you can use the exec function, which lets you run an arbitrary binary inside of the docker image. This binary would verify that the content of the run and signature match and that the signature is a proper signature of somebody from your company. This works for any kind of Run, including tasks.","title":"Run"},{"location":"concepts/run/index.html#run","text":"Every job that can touch your Spacelift-managed infrastructure is called a Run. There are four main types of runs, and each of them warrants a separate section. Three of them are children of Stacks : task , which is a freeform command you can execute on your infrastructure; proposed run , which serves as a preview of introduced changes; tracked run , which is a form of deployment; There's also a fourth type of run - module test case . Very similar to a tracked run, it's executed on a Terraform module.","title":"Run"},{"location":"concepts/run/index.html#execution-model","text":"In Spacelift, each run is executed on a worker node, inside a Docker container. We maintain a number of these worker nodes (collectively known as the public worker pool ) that are available to all customers, but also allow individual customers to run our agent on their end, for their exclusive use. You can read more about worker pools here . Regardless of whether you end up using a private or a public worker pool, each Spacelift run involves a handover between spacelift.io (which we like to call the mothership ) and the worker node. After the handover, the worker node is fully responsible for running the job and communicating the results of the job back to the mothership . Info It's important to know that it's always the worker node executing the run and accessing your infrastructure, never the mothership.","title":"Execution model"},{"location":"concepts/run/index.html#common-run-states","text":"Regardless of the type of the job performed, some phases and terminal states are common. We discuss them here, so that we can refer to them when describing various types of runs in more detail.","title":"Common run states"},{"location":"concepts/run/index.html#queued","text":"Queued means that no worker has picked up the run yet . There are two possible reasons for that: there are no available workers, or the run is not eligible for processing. Let's start with the latter - more common - scenario. Spacelift serializes all state-changing operations to the Stack. Both tracked runs and tasks have the capacity to change the state, they're never allowed to run in parallel. Instead, each of them gets an exclusive lock on the stack, blocking others from starting. If you run or task is currently blocked by something else holding the lock on the stack, you'll see the link to the blocker in the header: The other scenario is just running out of available workers . If you're using the public worker pool, you can track its availability on our status page . In particular, you should look at a system metric called Public worker queuing time. It indicates how long has the oldest run spent in the queue since becoming eligible for processing: If you're using private workers, please refer to the worker pools documentation for troubleshooting advice. Queued is a passive state meaning no operations are performed while a run is in this state. When a run is eligible for processing and a worker is available to pick it up, the state will automatically transition to Preparing . The user can cancel the run while it's still queued, transitioning it to the terminal Canceled state.","title":"Queued"},{"location":"concepts/run/index.html#canceled","text":"Canceled state means that the user has manually stopped a Queued run or task even before it had the chance to be picked up by the worker. Canceled is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can follow it.","title":"Canceled"},{"location":"concepts/run/index.html#preparing","text":"The preparing state is the first one where real work is done. At this point both the run is eligible for processing and there's a worker node ready to process it. The preparing state is all about the handover and dialog between spacelift.io and the worker. Here's an example of one such handover: Note that Ground Control refers to the bit directly controlled by us, in a nod to late David Bowie . The main purpose of this phase is for Ground Control to make sure that the worker node gets everything that's required to perform the job, and that it can take over the execution. Once the worker is able to pull the Docker image and use it to start the container, this phase is over and the initialization phase begins. If the process fails for whatever reason, the run is marked as failed .","title":"Preparing"},{"location":"concepts/run/index.html#initializing","text":"The last phase where actual work is done and which is common to all types of run is the initialization . This phase is handled exclusively by the worker and involves running pre-initialization hooks and vendor-specific initialization process. For Terraform stacks it would mean running terraform init , in the right directory and with the right parameters. Important thing to note with regards to pre-initialization hooks and the rest of the initialization process is that all these run in the same shell session, so environment variables exported by pre-initialization hooks are accessible to the vendor-specific initialization process. This is often the desired outcome when working with external secret managers like HashiCorp Vault. If this phase fails for whatever reason, the run is marked as failed . Otherwise, the next step is determined by the type of the run being executed.","title":"Initializing"},{"location":"concepts/run/index.html#failed","text":"If a run transitions into the failed state means that something, at some point went wrong and this state can follow any state. In most cases this will be something related to your project like: errors in the source code; pre-initialization checks failing; plan policies rejecting your change; deployment-time errors; In rare cases errors in Spacelift application code or third party dependencies can also make the job fail. These cases are clearly marked by a notice corresponding to the failure mode, reported through our exception tracker and immediately investigated. Failed is a passive state meaning no operations are performed while the run is in this state. It's also a terminal state meaning that no further state can supersede it.","title":"Failed"},{"location":"concepts/run/index.html#finished","text":"Finished state means that the run was successful, though the success criteria will depend on the type of run. Please read the documentation for the relevant run type for more details. Finished is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it.","title":"Finished"},{"location":"concepts/run/index.html#stopping-runs","text":"Some types of runs in some phases may safely be interrupted. We allow sending a stop signal from the GUI and API to the run, which is then passed to the worker handling the job. It's then up to the worker to handle or ignore that signal. Stopped state indicates that a run has been stopped while Initializing or Planning , either manually by the user or - for proposed changes - also by Spacelift. Proposed changes will automatically be stopped when a newer version of the code is pushed to their branch. This is mainly designed to limit the number of unnecessary API calls to your resource providers, though it saves us a few bucks on EC2, too. Here's an example of a run manually stopped while Initializing : Stopped is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it.","title":"Stopping runs"},{"location":"concepts/run/index.html#logs-retention","text":"Run logs are kept for 60 days.","title":"Logs Retention"},{"location":"concepts/run/index.html#zero-trust-model","text":"For your most sensitive Stacks you can use additional verification of Runs based on arbitrary metadata you can provide to Runs when creating or confirming them. This metadata can be passed through the API or by using the spacectl CLI to create or confirm runs. Every time such an interaction happens, you can add a new piece of metadata, which will form a list of metadata blobs inside of the Run. This will then be available in policies, including the private-worker side initialization policy. This way you can i.e. sign the runs when you confirm them and later verify this signature inside of the private worker, through the initialization policy. There you can use the exec function, which lets you run an arbitrary binary inside of the docker image. This binary would verify that the content of the run and signature match and that the signature is a proper signature of somebody from your company. This works for any kind of Run, including tasks.","title":"Zero Trust Model"},{"location":"concepts/run/proposed.html","text":"Proposed run (preview) \u00bb Proposed runs are previews of changes that would be applied to your infrastructure if the new code was to somehow become canonical, for example by pushing it to the tracked branch . Proposed runs are generally triggered by Git push events. By default, whenever a push occurs to any branch other than the tracked branch , a proposed run is started - one for each of the affected stacks. This default behavior can be extensively customized using our push policies . The purpose of proposed runs is not to make changes to your infrastructure but to merely preview and report them during the planning phase. Planning \u00bb Once the workspace is prepared by the Initializing phase, planning runs a vendor-specific preview command and interprets the results. For Terraform that command is terraform plan , for Pulumi - pulumi preview . The result of the planning phase is the collection of currently managed resources and outputs as well as planned changes. This is used as an input to plan policies (optional) and to calculate the delta - always. Note that the Planning phase can be safely stopped by the user . On Ansible stacks, this phase can be skipped without execution by setting the SPACELIFT_SKIP_PLANNING environment variable to true in the stack's environment variables . Plan policies \u00bb If any plan policies are attached to the current stack, each of these policies is evaluated to automatically determine whether the change is acceptable according to the rules adopted by your organization. Here is an example of an otherwise successful planning phase that still fails due to policy violations: You can read more about plan policies here . Delta \u00bb If the planning phase is successful (which includes policy evaluation), Spacelift analyses the diff and counts the resources and outputs that would be added, changed and deleted if the changes were to be applied. Here's one example of one such delta being reported: Success criteria \u00bb The planning phase will fail if: infrastructure definitions are incorrect - eg. malformed, invalid etc.; external APIs (eg. AWS, GCP, Azure etc.) fail when previewing changes; plan policies return one or more deny reasons; a worker node crashes - eg. you kill a private worker node while it's executing the job; If that happens, the run will transition to the failed state. Otherwise, the proposed run terminates in the finished state. Reporting \u00bb The results of proposed runs are reported in multiple ways: always - in VCS, as commit statuses and pull request comments - please refer to GitHub and GitLab documentation for the exact details; through Slack notifications - if set up; through webhooks - if set up;","title":"Proposed run (preview)"},{"location":"concepts/run/proposed.html#proposed-run-preview","text":"Proposed runs are previews of changes that would be applied to your infrastructure if the new code was to somehow become canonical, for example by pushing it to the tracked branch . Proposed runs are generally triggered by Git push events. By default, whenever a push occurs to any branch other than the tracked branch , a proposed run is started - one for each of the affected stacks. This default behavior can be extensively customized using our push policies . The purpose of proposed runs is not to make changes to your infrastructure but to merely preview and report them during the planning phase.","title":"Proposed run (preview)"},{"location":"concepts/run/proposed.html#planning","text":"Once the workspace is prepared by the Initializing phase, planning runs a vendor-specific preview command and interprets the results. For Terraform that command is terraform plan , for Pulumi - pulumi preview . The result of the planning phase is the collection of currently managed resources and outputs as well as planned changes. This is used as an input to plan policies (optional) and to calculate the delta - always. Note that the Planning phase can be safely stopped by the user . On Ansible stacks, this phase can be skipped without execution by setting the SPACELIFT_SKIP_PLANNING environment variable to true in the stack's environment variables .","title":"Planning"},{"location":"concepts/run/proposed.html#plan-policies","text":"If any plan policies are attached to the current stack, each of these policies is evaluated to automatically determine whether the change is acceptable according to the rules adopted by your organization. Here is an example of an otherwise successful planning phase that still fails due to policy violations: You can read more about plan policies here .","title":"Plan policies"},{"location":"concepts/run/proposed.html#delta","text":"If the planning phase is successful (which includes policy evaluation), Spacelift analyses the diff and counts the resources and outputs that would be added, changed and deleted if the changes were to be applied. Here's one example of one such delta being reported:","title":"Delta"},{"location":"concepts/run/proposed.html#success-criteria","text":"The planning phase will fail if: infrastructure definitions are incorrect - eg. malformed, invalid etc.; external APIs (eg. AWS, GCP, Azure etc.) fail when previewing changes; plan policies return one or more deny reasons; a worker node crashes - eg. you kill a private worker node while it's executing the job; If that happens, the run will transition to the failed state. Otherwise, the proposed run terminates in the finished state.","title":"Success criteria"},{"location":"concepts/run/proposed.html#reporting","text":"The results of proposed runs are reported in multiple ways: always - in VCS, as commit statuses and pull request comments - please refer to GitHub and GitLab documentation for the exact details; through Slack notifications - if set up; through webhooks - if set up;","title":"Reporting"},{"location":"concepts/run/pull-request-comments.html","text":"Pull Request Comments \u00bb Pull Request Plan Commenting \u00bb To enable this feature, simply add the label feature:add_plan_pr_comment to the stacks you wish to have plan commenting enabled for on pull requests. Once enabled, on any future pull request activity, the result of the plan will be commented onto the pull request. Pull Request Comment-Driven Actions \u00bb To enable support for pull request comment events in Spacelift, you will need to ensure the following permissions are enabled within your VCS app integration. Note that if your VCS integration was created using the Spacelift VCS setup wizard, then these permissions have already been set up automatically, and no action is needed. Read access to issues repository permissions Subscribe to issues:comments event Assuming the above permissions are configured on your VCS application, you can then access pull request comment event data from within your Push policy, and build customizable workflows using this data. Warning Please note that Spacelift will only evaluate comments that begin with /spacelift to prevent users from unintended actions against their resources managed by Spacelift. Furthermore, Spacelift only processes event data for new comments, and will not receive event data for edited or deleted comments. Example Push policy to trigger a tracked run from a pull request comment event: 1 2 3 4 5 6 package spacelift track { input . pull_request . action == \"commented\" input . pull_request . comment == concat ( \" \" , [ \"/spacelift\" , \"deploy\" , input . stack . id ]) } Using a Push policy such as the example above, a user could trigger a tracked run on their Spacelift stack by commenting something such as: 1 /spacelift deploy my-stack-id","title":"Pull Request Comments"},{"location":"concepts/run/pull-request-comments.html#pull-request-comments","text":"","title":"Pull Request Comments"},{"location":"concepts/run/pull-request-comments.html#pull-request-plan-commenting","text":"To enable this feature, simply add the label feature:add_plan_pr_comment to the stacks you wish to have plan commenting enabled for on pull requests. Once enabled, on any future pull request activity, the result of the plan will be commented onto the pull request.","title":"Pull Request Plan Commenting"},{"location":"concepts/run/pull-request-comments.html#pull-request-comment-driven-actions","text":"To enable support for pull request comment events in Spacelift, you will need to ensure the following permissions are enabled within your VCS app integration. Note that if your VCS integration was created using the Spacelift VCS setup wizard, then these permissions have already been set up automatically, and no action is needed. Read access to issues repository permissions Subscribe to issues:comments event Assuming the above permissions are configured on your VCS application, you can then access pull request comment event data from within your Push policy, and build customizable workflows using this data. Warning Please note that Spacelift will only evaluate comments that begin with /spacelift to prevent users from unintended actions against their resources managed by Spacelift. Furthermore, Spacelift only processes event data for new comments, and will not receive event data for edited or deleted comments. Example Push policy to trigger a tracked run from a pull request comment event: 1 2 3 4 5 6 package spacelift track { input . pull_request . action == \"commented\" input . pull_request . comment == concat ( \" \" , [ \"/spacelift\" , \"deploy\" , input . stack . id ]) } Using a Push policy such as the example above, a user could trigger a tracked run on their Spacelift stack by commenting something such as: 1 /spacelift deploy my-stack-id","title":"Pull Request Comment-Driven Actions"},{"location":"concepts/run/run-promotion.html","text":"Run Promotion \u00bb What is Run Promotion? \u00bb As a quick summary of the differences between the two types of runs: proposed runs only display changes to be made, while tracked runs apply (deploy) the proposed changes. Promoting a proposed run is triggering a tracked run for the same Git commit. Using Run Promotion \u00bb Pre-Requisites \u00bb For a run to be promote-able, the proposed run must point to a commit that is newer than the stack's current commit . To promote a run, you first need to ensure that you have Allow run promotion enabled in the stack settings of your stack(s) in which you'd like to promote runs. Stack Settings > Behavior > Advanced Options > Allow Run Promotion Promote from Proposed Run View \u00bb Assuming you've enabled Run Promotion within the stack settings, and the commit to be promoted is newer than the stack's current commit. On a given proposed run, you will then see the \"Promote\" button as seen in the screenshot below. You simply need to click this button to promote the proposed run into a tracked run. Promote from a Pull Request \u00bb For Spacelift users utilizing GitHub, a similar feature is available directly from the GitHub Pull Request. Assuming the same criteria is met as mentioned previously: 1) The commit to be promoted is newer than the stack's current commit 2) Run Promotion is enabled on the stack - Then, you will see a Deploy button available within the Checks tab of the pull request. This button will promote your proposed run into a tracked run.","title":"Run Promotion"},{"location":"concepts/run/run-promotion.html#run-promotion","text":"","title":"Run Promotion"},{"location":"concepts/run/run-promotion.html#what-is-run-promotion","text":"As a quick summary of the differences between the two types of runs: proposed runs only display changes to be made, while tracked runs apply (deploy) the proposed changes. Promoting a proposed run is triggering a tracked run for the same Git commit.","title":"What is Run Promotion?"},{"location":"concepts/run/run-promotion.html#using-run-promotion","text":"","title":"Using Run Promotion"},{"location":"concepts/run/run-promotion.html#pre-requisites","text":"For a run to be promote-able, the proposed run must point to a commit that is newer than the stack's current commit . To promote a run, you first need to ensure that you have Allow run promotion enabled in the stack settings of your stack(s) in which you'd like to promote runs. Stack Settings > Behavior > Advanced Options > Allow Run Promotion","title":"Pre-Requisites"},{"location":"concepts/run/run-promotion.html#promote-from-proposed-run-view","text":"Assuming you've enabled Run Promotion within the stack settings, and the commit to be promoted is newer than the stack's current commit. On a given proposed run, you will then see the \"Promote\" button as seen in the screenshot below. You simply need to click this button to promote the proposed run into a tracked run.","title":"Promote from Proposed Run View"},{"location":"concepts/run/run-promotion.html#promote-from-a-pull-request","text":"For Spacelift users utilizing GitHub, a similar feature is available directly from the GitHub Pull Request. Assuming the same criteria is met as mentioned previously: 1) The commit to be promoted is newer than the stack's current commit 2) Run Promotion is enabled on the stack - Then, you will see a Deploy button available within the Checks tab of the pull request. This button will promote your proposed run into a tracked run.","title":"Promote from a Pull Request"},{"location":"concepts/run/task.html","text":"Task \u00bb While tasks enjoy the privilege of having their own GUI screen, they're just another type of run . The core difference is that after the common initialization phase, a task will run your custom command instead of a string of preordained vendor-specific commands. The purpose of tasks \u00bb The main purpose of task is to perform arbitrary changes to your infrastructure in a coordinated, safe and audited way. Tasks allow ultimate flexibility and can be used to check the environment (see the humble ls -la on the above screenshot), perform benign read-only operations like showing parts of the Terraform state , or even make changes to the state itself, like tainting a resource . Given that thanks to the Docker integration you have full control over the execution environment of your workloads, there's hardly a limit to what you can do. Danger Obvious abuse of shared workers will get you kicked out of the platform. But you can abuse private workers all you like. With the above caveat, let's go through the main benefits of using Spacelift tasks. Coordinated \u00bb Tasks are always treated as operations that may change the underlying state, and are thus serialized. No two tasks will ever run simultaneously, nor will a task execute while a tracked run is in progress. This prevents possible concurrent updates to the state that would be possible without a centrally managed mutex. What's more, some tasks will be more sensitive than others. While a simple ls is probably nothing to be afraid of, the two-way state migration described above could have gone wrong in great many different ways. The stack locking mechanism thus allows taking exclusive control over one or more stacks by a single individual, taking the possibility of coordination to a whole new level. Safe \u00bb Any non-trivial infrastructure project will inevitably be full of credentials and secrets which are possibly too sensitive to be stored even on a work laptop. Tasks allow any operation to be executed remotely, preventing the leak of sensitive data. Spacelift's integration with infra providers like AWS also allows authentication without any credentials whatsoever, which further protects you from the shame and humiliation of having the keys to the kingdom leaked by running the occasional env command, as you do. Actually, let's run it in Spacelift to see what gives: Yes, the secrets are masked in the output and won't leak due to an honest mistake. Danger There are limits to the extent we can protect you from a determined attacker with write access to your stack. We don't want to give you a false sense of security where none is warranted. You may want to look into task policies to prevent certain (or even all) tasks from being executed. Audited \u00bb Unlike arbitrary operations performed on your local machine, tasks are recorded for eternity, so in cases where some archaeology is necessary, it's easy to see what happened and when. Tasks are attributed to individuals (or API keys ) that triggered them and the access model ensures that only stack writers can trigger tasks, giving you even more control over your infrastructure. Performing a task \u00bb Apart from the common run phases described in the general run documentation, tasks have just one extra state - performing. That's when the arbitrary user-supplied command is executed, wrapped in sh -c to support all the shell goodies we all love to abuse. In particular, you can use as many && and || as you wish. Performing a task will succeed and the task will transition to the finished state iff the exit code of your command is 0 (the Unix standard). Otherwise the task is marked as failed . Performing cannot be stopped since we must assume that it involves state changes. Skipping initialization \u00bb In rare cases it may be useful to perform tasks without initialization - like when the initialization would fail without some changes being introduced. An obvious example here are Terraform version migrations . This corner case is served by explicitly skipping the initialization. In the GUI (on by default), you will find the toggle to control this behavior: Let's execute a task without initialization on a Terraform stack: Notice how the operation failed because it is expected to be executed on an initialized Terraform workspace. But the same operation would easily succeed if we were to run it in the default mode, with initialization:","title":"Task"},{"location":"concepts/run/task.html#task","text":"While tasks enjoy the privilege of having their own GUI screen, they're just another type of run . The core difference is that after the common initialization phase, a task will run your custom command instead of a string of preordained vendor-specific commands.","title":"Task"},{"location":"concepts/run/task.html#the-purpose-of-tasks","text":"The main purpose of task is to perform arbitrary changes to your infrastructure in a coordinated, safe and audited way. Tasks allow ultimate flexibility and can be used to check the environment (see the humble ls -la on the above screenshot), perform benign read-only operations like showing parts of the Terraform state , or even make changes to the state itself, like tainting a resource . Given that thanks to the Docker integration you have full control over the execution environment of your workloads, there's hardly a limit to what you can do. Danger Obvious abuse of shared workers will get you kicked out of the platform. But you can abuse private workers all you like. With the above caveat, let's go through the main benefits of using Spacelift tasks.","title":"The purpose of tasks"},{"location":"concepts/run/task.html#coordinated","text":"Tasks are always treated as operations that may change the underlying state, and are thus serialized. No two tasks will ever run simultaneously, nor will a task execute while a tracked run is in progress. This prevents possible concurrent updates to the state that would be possible without a centrally managed mutex. What's more, some tasks will be more sensitive than others. While a simple ls is probably nothing to be afraid of, the two-way state migration described above could have gone wrong in great many different ways. The stack locking mechanism thus allows taking exclusive control over one or more stacks by a single individual, taking the possibility of coordination to a whole new level.","title":"Coordinated"},{"location":"concepts/run/task.html#safe","text":"Any non-trivial infrastructure project will inevitably be full of credentials and secrets which are possibly too sensitive to be stored even on a work laptop. Tasks allow any operation to be executed remotely, preventing the leak of sensitive data. Spacelift's integration with infra providers like AWS also allows authentication without any credentials whatsoever, which further protects you from the shame and humiliation of having the keys to the kingdom leaked by running the occasional env command, as you do. Actually, let's run it in Spacelift to see what gives: Yes, the secrets are masked in the output and won't leak due to an honest mistake. Danger There are limits to the extent we can protect you from a determined attacker with write access to your stack. We don't want to give you a false sense of security where none is warranted. You may want to look into task policies to prevent certain (or even all) tasks from being executed.","title":"Safe"},{"location":"concepts/run/task.html#audited","text":"Unlike arbitrary operations performed on your local machine, tasks are recorded for eternity, so in cases where some archaeology is necessary, it's easy to see what happened and when. Tasks are attributed to individuals (or API keys ) that triggered them and the access model ensures that only stack writers can trigger tasks, giving you even more control over your infrastructure.","title":"Audited"},{"location":"concepts/run/task.html#performing-a-task","text":"Apart from the common run phases described in the general run documentation, tasks have just one extra state - performing. That's when the arbitrary user-supplied command is executed, wrapped in sh -c to support all the shell goodies we all love to abuse. In particular, you can use as many && and || as you wish. Performing a task will succeed and the task will transition to the finished state iff the exit code of your command is 0 (the Unix standard). Otherwise the task is marked as failed . Performing cannot be stopped since we must assume that it involves state changes.","title":"Performing a task"},{"location":"concepts/run/task.html#skipping-initialization","text":"In rare cases it may be useful to perform tasks without initialization - like when the initialization would fail without some changes being introduced. An obvious example here are Terraform version migrations . This corner case is served by explicitly skipping the initialization. In the GUI (on by default), you will find the toggle to control this behavior: Let's execute a task without initialization on a Terraform stack: Notice how the operation failed because it is expected to be executed on an initialized Terraform workspace. But the same operation would easily succeed if we were to run it in the default mode, with initialization:","title":"Skipping initialization"},{"location":"concepts/run/test-case.html","text":"Module test case \u00bb Module test cases are special types of runs that are executed not on Stacks but on Spacelift-managed Terraform modules . Note that this article does not cover modules specifically - for that please refer directly to their documentation . The purpose of this article is to explain how modules test cases are executed and how they're different from other types of runs. In a nutshell, module test cases are almost identical to autodeployed tracked runs . But unlike stacks, modules are stateless and do not manage any resources directly, so just after the changes are applied and resources are created, they are immediately destroyed during the destroying phase. Here is what a fully successful module test case looks like: Note that the destroying phase will run regardless of whether the applying phase succeeds or fails. This is because the failure could have been partial, and some resources may still have been created. The destroying phase can be skipped without execution by setting the SPACELIFT_SKIP_DESTROYING environment variable to true in the stack's environment variables . Success criteria \u00bb A module test case will only transition to the successful finished state if all the previous phases succeed. If any of the phases fails, the run will be marked as failed .","title":"Module test case"},{"location":"concepts/run/test-case.html#module-test-case","text":"Module test cases are special types of runs that are executed not on Stacks but on Spacelift-managed Terraform modules . Note that this article does not cover modules specifically - for that please refer directly to their documentation . The purpose of this article is to explain how modules test cases are executed and how they're different from other types of runs. In a nutshell, module test cases are almost identical to autodeployed tracked runs . But unlike stacks, modules are stateless and do not manage any resources directly, so just after the changes are applied and resources are created, they are immediately destroyed during the destroying phase. Here is what a fully successful module test case looks like: Note that the destroying phase will run regardless of whether the applying phase succeeds or fails. This is because the failure could have been partial, and some resources may still have been created. The destroying phase can be skipped without execution by setting the SPACELIFT_SKIP_DESTROYING environment variable to true in the stack's environment variables .","title":"Module test case"},{"location":"concepts/run/test-case.html#success-criteria","text":"A module test case will only transition to the successful finished state if all the previous phases succeed. If any of the phases fails, the run will be marked as failed .","title":"Success criteria"},{"location":"concepts/run/tracked.html","text":"Tracked run (deployment) \u00bb Tracked runs represent the actual changes to your infrastructure caused by changes to your infrastructure definitions and/or configuration. In that sense, they can be also called deployments. Tracked runs are effectively an extension of proposed runs - instead of stopping at the planning phase, they also allow you to apply the previewed changes. They are presented on the Runs screen, which is the main screen of the Stack view: Each of the tracked runs is represented by a separate element containing some information about the attempted deployment: Also worth noting is the colorful strip present on some runs - as long as the planning phase was successful this visually represents the resources and outputs diff introduced by the change: Triggering tracked runs \u00bb Tracked runs can be triggered in of the three ways - manually by the user, by a Git push or by a trigger policy . Triggering manually \u00bb Any account admin or stack writer can trigger a tracked run on a stack: Runs triggered by individuals and machine users are marked accordingly: Triggering from Git events \u00bb Tracked runs can also be triggered by Git push and tag events. By default, whenever a push occurs to the tracked branch , a tracked run is started - one for each of the affected stacks. This default behavior can be extensively customized using our push policies . Runs triggered by Git push and/or tag events can are marked accordingly: Triggering from policies \u00bb Trigger policies can be used to create sophisticated workflows representing arbitrarily complex processes like staged rollouts or cascading updates. This is an advanced topic, which is described in more detail in its dedicated section . But if you see something like this, be aware of the fact that a trigger policy must have been involved: Handling no-op changes \u00bb If the planning phase detects no changes to the resources and outputs managed by the stack, the tracked run is considered a no-op. In that case it transitions directly from planning to finished state, just like a proposed run . Otherwise, it will go through the approval flow . Approval flow \u00bb If the tracked run detects a change to its managed resources or outputs, it goes through the approval flow. This can be automated or manual. The automated flow involves a direct transition between the planning and applying phase, without an extra human intervention. This is a convenient but not always the safest option. Changes can be automatically applied if both these conditions are met: autodeploy is turned \"on\" for the Stack; if plan policies are attached, none of them returns any warnings; Otherwise, the change will go through the manual flow described below. Unconfirmed \u00bb If a change is detected and human approval is required, a tracked run will transition from the planning state to unconfirmed . At that point the worker node encrypts uploads the entire workspace to a dedicated Amazon S3 location and finishes its involvement with the run. The resulting changes are shown to the user for the final approval: Unconfirmed is a passive state meaning no operations are performed while a run is in this state. If the user approves (confirms) the plan, the run transitions to the temporary Confirmed state and waits for a worker node to pick it up. If the user doesn't like the plan and discards it, the run transitions to the terminal Discarded state. Targeted replan \u00bb When a run is in the Unconfirmed state it's also possible to replan it. When replanning, a user is able to generate a new plan to apply by only picking specific changes from the current plan. Warning Targeted replan feature is currently in open public beta and is free to use regardless of your pricing plan. Once GA, it will be available as part of our Enterprise plan . Discarded \u00bb Discarded state follows Unconfirmed and indicates that the user did not like the changes detected by the Planning phase. Discarded is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it. Confirmed \u00bb Confirmed state follows Unconfirmed indicates that a user has accepted the plan generated in the Planning phase and wants to apply it but no worker has picked up the job yet. This state is similar to Queued in a sense that shows only temporarily until one of the workers picks up the associated job and changes the state to Applying . On the other hand, there is no way to stop a run once it's confirmed. Confirmed is a passive state meaning no operations are performed while a run is in this state. Applying \u00bb If the run required a manual approval step, this phase is preceded by another handover ( preparing phase) since the run again needs to be yielded to a worker node. This preparing phase is subtly different internally but ultimately serves the same purpose from the user perspective. Here's an example: This preparation phase is very unlikely to fail, but if it does (eg. the worker node becomes unavailable during the transition), the run will transition to the terminal failed state. If the handover succeeds, or the run does not go through the manual approval process, the applying phase begins and attempts to deploy the changes. Here's an example: This phase can be skipped without execution by setting the SPACELIFT_SKIP_APPLYING environment variable to true in the stack's environment variables . Success criteria \u00bb If the run is a no-op or the applying phase succeeds, the run transitions to the finished state. On the other hand, if anything goes wrong, the run is marked as failed . Reporting \u00bb The results of tracked runs are reported in multiple ways: as deployments in VCS unless the change is a no-op - please refer to GitHub and GitLab documentation for the exact details; through Slack notifications - if set up; through webhooks - if set up;","title":"Tracked run (deployment)"},{"location":"concepts/run/tracked.html#tracked-run-deployment","text":"Tracked runs represent the actual changes to your infrastructure caused by changes to your infrastructure definitions and/or configuration. In that sense, they can be also called deployments. Tracked runs are effectively an extension of proposed runs - instead of stopping at the planning phase, they also allow you to apply the previewed changes. They are presented on the Runs screen, which is the main screen of the Stack view: Each of the tracked runs is represented by a separate element containing some information about the attempted deployment: Also worth noting is the colorful strip present on some runs - as long as the planning phase was successful this visually represents the resources and outputs diff introduced by the change:","title":"Tracked run (deployment)"},{"location":"concepts/run/tracked.html#triggering-tracked-runs","text":"Tracked runs can be triggered in of the three ways - manually by the user, by a Git push or by a trigger policy .","title":"Triggering tracked runs"},{"location":"concepts/run/tracked.html#triggering-manually","text":"Any account admin or stack writer can trigger a tracked run on a stack: Runs triggered by individuals and machine users are marked accordingly:","title":"Triggering manually"},{"location":"concepts/run/tracked.html#triggering-from-git-events","text":"Tracked runs can also be triggered by Git push and tag events. By default, whenever a push occurs to the tracked branch , a tracked run is started - one for each of the affected stacks. This default behavior can be extensively customized using our push policies . Runs triggered by Git push and/or tag events can are marked accordingly:","title":"Triggering from Git events"},{"location":"concepts/run/tracked.html#triggering-from-policies","text":"Trigger policies can be used to create sophisticated workflows representing arbitrarily complex processes like staged rollouts or cascading updates. This is an advanced topic, which is described in more detail in its dedicated section . But if you see something like this, be aware of the fact that a trigger policy must have been involved:","title":"Triggering from policies"},{"location":"concepts/run/tracked.html#handling-no-op-changes","text":"If the planning phase detects no changes to the resources and outputs managed by the stack, the tracked run is considered a no-op. In that case it transitions directly from planning to finished state, just like a proposed run . Otherwise, it will go through the approval flow .","title":"Handling no-op changes"},{"location":"concepts/run/tracked.html#approval-flow","text":"If the tracked run detects a change to its managed resources or outputs, it goes through the approval flow. This can be automated or manual. The automated flow involves a direct transition between the planning and applying phase, without an extra human intervention. This is a convenient but not always the safest option. Changes can be automatically applied if both these conditions are met: autodeploy is turned \"on\" for the Stack; if plan policies are attached, none of them returns any warnings; Otherwise, the change will go through the manual flow described below.","title":"Approval flow"},{"location":"concepts/run/tracked.html#unconfirmed","text":"If a change is detected and human approval is required, a tracked run will transition from the planning state to unconfirmed . At that point the worker node encrypts uploads the entire workspace to a dedicated Amazon S3 location and finishes its involvement with the run. The resulting changes are shown to the user for the final approval: Unconfirmed is a passive state meaning no operations are performed while a run is in this state. If the user approves (confirms) the plan, the run transitions to the temporary Confirmed state and waits for a worker node to pick it up. If the user doesn't like the plan and discards it, the run transitions to the terminal Discarded state.","title":"Unconfirmed"},{"location":"concepts/run/tracked.html#targeted-replan","text":"When a run is in the Unconfirmed state it's also possible to replan it. When replanning, a user is able to generate a new plan to apply by only picking specific changes from the current plan. Warning Targeted replan feature is currently in open public beta and is free to use regardless of your pricing plan. Once GA, it will be available as part of our Enterprise plan .","title":"Targeted replan"},{"location":"concepts/run/tracked.html#discarded","text":"Discarded state follows Unconfirmed and indicates that the user did not like the changes detected by the Planning phase. Discarded is a passive state meaning no operations are performed while a run is in this state. It's also a terminal state meaning that no further state can supersede it.","title":"Discarded"},{"location":"concepts/run/tracked.html#confirmed","text":"Confirmed state follows Unconfirmed indicates that a user has accepted the plan generated in the Planning phase and wants to apply it but no worker has picked up the job yet. This state is similar to Queued in a sense that shows only temporarily until one of the workers picks up the associated job and changes the state to Applying . On the other hand, there is no way to stop a run once it's confirmed. Confirmed is a passive state meaning no operations are performed while a run is in this state.","title":"Confirmed"},{"location":"concepts/run/tracked.html#applying","text":"If the run required a manual approval step, this phase is preceded by another handover ( preparing phase) since the run again needs to be yielded to a worker node. This preparing phase is subtly different internally but ultimately serves the same purpose from the user perspective. Here's an example: This preparation phase is very unlikely to fail, but if it does (eg. the worker node becomes unavailable during the transition), the run will transition to the terminal failed state. If the handover succeeds, or the run does not go through the manual approval process, the applying phase begins and attempts to deploy the changes. Here's an example: This phase can be skipped without execution by setting the SPACELIFT_SKIP_APPLYING environment variable to true in the stack's environment variables .","title":"Applying"},{"location":"concepts/run/tracked.html#success-criteria","text":"If the run is a no-op or the applying phase succeeds, the run transitions to the finished state. On the other hand, if anything goes wrong, the run is marked as failed .","title":"Success criteria"},{"location":"concepts/run/tracked.html#reporting","text":"The results of tracked runs are reported in multiple ways: as deployments in VCS unless the change is a no-op - please refer to GitHub and GitLab documentation for the exact details; through Slack notifications - if set up; through webhooks - if set up;","title":"Reporting"},{"location":"concepts/run/user-provided-metadata.html","text":"User-Provided Metadata \u00bb Occasionally you might want to add additional information to your Runs which isn\u2019t handled on a first-class basis by Spacelift. You can attach this kind of information using the run metadata parameter, which is available through spacectl as well as the GraphQL API. Usage \u00bb Let\u2019s start with a small example. You\u2019ll need a private worker for this. On the machine where the worker resides, create a simple policy in a file: 1 2 package spacelift sample { true } And then start the worker with an additional environment variable: 1 SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY = /path/to/your/policy.rego This policy will make our launcher sample each initialization policy evaluation and print it as a log on stderr. We\u2019ll also need a Stack to which this worker is attached . We can now trigger a run and provide an arbitrary metadata string: 1 2 3 ~> spacectl stack deploy --id testing-spacelift --run-metadata \"deploy-metadata\" You have successfully created a deployment The live run can be visited at http://cube2222.app.spacelift.tf/stack/testing-spacelift/run/01FEKAGP4AYV0DWP4QDFTANRES And in the private worker logs we should suitably see (formatted for readability): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 { \"caller\" : \"setup.go:201\" , \"level\" : \"info\" , \"msg\" : \"Sample 0/INITIALIZATION/7YGHCNF7W6VMBQ49XQ42MH4JD1/allow\" , \"sample\" : { \"body\" : \"package spacelift\\nsample { true }\\n\" , \"input\" : { \"docker_image\" : \"\" , \"run\" : { \"based_on_local_workspace\" : false , \"changes\" : [], \"commit\" : { \"author\" : \"cube2222\" , \"branch\" : \"master\" , \"created_at\" : 1628243895000000000 , \"message\" : \"Update main.tf\" }, \"created_at\" : 1630588655754344000 , \"id\" : \"01FEKAGP4AYV0DWP4QDFTANRES\" , \"state\" : \"PREPARING\" , \"triggered_by\" : \"api::01FEGXFB7TWQ2NNF95W7HPRE2E\" , \"updated_at\" : 1630588656197898500 , \"user_provided_metadata\" : [ \"deploy-metadata\" . // (1) ] }, \"static_run_environment\" : { \"account_name\" : \"cube2222\" , \"auto_deploy\" : false , \"before_apply\" : null , \"before_init\" : null , \"command\" : \"\" , \"commit_branch\" : \"master\" , \"commit_sha\" : \"7d629c6c3f3b6da07e28a87727f0586e577d98c1\" , \"endpoint_logs\" : \"tcp://169.254.0.3:1983\" , \"endpoint_registry\" : \"registry.spacelift.io\" , \"environment_variables\" : {}, \"project_root\" : \"\" , \"refresh_state\" : true , \"repository_path\" : \"cube2222/testing-spacelift\" , \"run_type\" : \"TRACKED\" , \"run_ulid\" : \"01FEKAGP4AYV0DWP4QDFTANRES\" , \"skip_init\" : false , \"stack_labels\" : null , \"stack_slug\" : \"testing-spacelift\" , \"terraform_version\" : \"0.14.10\" , \"vendor_specific_config\" : { \"vendor\" : \"terraform\" , \"typed_config\" : { \"use_terragrunt\" : false , \"use_infracost\" : false } } }, \"worker_version\" : \"development\" }, \"outcome\" : \"allow\" , \"results\" : { \"deny\" : [], \"sample\" : true }, \"error\" : \"\" }, \"ts\" : \"2021-09-02T13:17:37.785219048Z\" } The metadata string Great! We can now go ahead and confirm this run: 1 2 3 ~> spacectl stack confirm --id testing-spacelift --run-metadata \"confirm-metadata\" --run 01FEKAGP4AYV0DWP4QDFTANRES You have successfully confirmed a deployment The live run can be visited at http://cube2222.app.spacelift.tf/stack/testing-spacelift/run/01FEKAGP4AYV0DWP4QDFTANRES In the policy sample log for the relevant metadata key we\u2019ll see an additional entry, which was added when confirming: 1 2 3 4 \"user_provided_metadata\" : [ \"deploy-metadata\" , \"confirm-metadata\" ] And that's basically it! It's a very flexible building block which lets you build various automation and compliance helper tooling. Run signatures \u00bb A standard use case for this feature would be to sign your runs when you\u2019re creating them. You'll have to bring the infrastructure for managing keys and signatures yourself - usually you'll already have something like that internally. But in short you can create a cryptographic signature of the parameters for a run you\u2019re about to create - based on the commit SHA, run type, stack, date, etc. - and then you can pass that signature to Spacelift when creating the run. Later, in the initialization policy you can use the exec function to run your custom binary for verifying that signature. This way - for your most sensitive stacks - you can verify whether runs you are receiving from the Spacelift backend are legit, intentionally created by an employee of your company. Tip We created a reference implementation to demonstrate how to sign runs and verify signatures.","title":"User-Provided Metadata"},{"location":"concepts/run/user-provided-metadata.html#user-provided-metadata","text":"Occasionally you might want to add additional information to your Runs which isn\u2019t handled on a first-class basis by Spacelift. You can attach this kind of information using the run metadata parameter, which is available through spacectl as well as the GraphQL API.","title":"User-Provided Metadata"},{"location":"concepts/run/user-provided-metadata.html#usage","text":"Let\u2019s start with a small example. You\u2019ll need a private worker for this. On the machine where the worker resides, create a simple policy in a file: 1 2 package spacelift sample { true } And then start the worker with an additional environment variable: 1 SPACELIFT_LAUNCHER_RUN_INITIALIZATION_POLICY = /path/to/your/policy.rego This policy will make our launcher sample each initialization policy evaluation and print it as a log on stderr. We\u2019ll also need a Stack to which this worker is attached . We can now trigger a run and provide an arbitrary metadata string: 1 2 3 ~> spacectl stack deploy --id testing-spacelift --run-metadata \"deploy-metadata\" You have successfully created a deployment The live run can be visited at http://cube2222.app.spacelift.tf/stack/testing-spacelift/run/01FEKAGP4AYV0DWP4QDFTANRES And in the private worker logs we should suitably see (formatted for readability): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 { \"caller\" : \"setup.go:201\" , \"level\" : \"info\" , \"msg\" : \"Sample 0/INITIALIZATION/7YGHCNF7W6VMBQ49XQ42MH4JD1/allow\" , \"sample\" : { \"body\" : \"package spacelift\\nsample { true }\\n\" , \"input\" : { \"docker_image\" : \"\" , \"run\" : { \"based_on_local_workspace\" : false , \"changes\" : [], \"commit\" : { \"author\" : \"cube2222\" , \"branch\" : \"master\" , \"created_at\" : 1628243895000000000 , \"message\" : \"Update main.tf\" }, \"created_at\" : 1630588655754344000 , \"id\" : \"01FEKAGP4AYV0DWP4QDFTANRES\" , \"state\" : \"PREPARING\" , \"triggered_by\" : \"api::01FEGXFB7TWQ2NNF95W7HPRE2E\" , \"updated_at\" : 1630588656197898500 , \"user_provided_metadata\" : [ \"deploy-metadata\" . // (1) ] }, \"static_run_environment\" : { \"account_name\" : \"cube2222\" , \"auto_deploy\" : false , \"before_apply\" : null , \"before_init\" : null , \"command\" : \"\" , \"commit_branch\" : \"master\" , \"commit_sha\" : \"7d629c6c3f3b6da07e28a87727f0586e577d98c1\" , \"endpoint_logs\" : \"tcp://169.254.0.3:1983\" , \"endpoint_registry\" : \"registry.spacelift.io\" , \"environment_variables\" : {}, \"project_root\" : \"\" , \"refresh_state\" : true , \"repository_path\" : \"cube2222/testing-spacelift\" , \"run_type\" : \"TRACKED\" , \"run_ulid\" : \"01FEKAGP4AYV0DWP4QDFTANRES\" , \"skip_init\" : false , \"stack_labels\" : null , \"stack_slug\" : \"testing-spacelift\" , \"terraform_version\" : \"0.14.10\" , \"vendor_specific_config\" : { \"vendor\" : \"terraform\" , \"typed_config\" : { \"use_terragrunt\" : false , \"use_infracost\" : false } } }, \"worker_version\" : \"development\" }, \"outcome\" : \"allow\" , \"results\" : { \"deny\" : [], \"sample\" : true }, \"error\" : \"\" }, \"ts\" : \"2021-09-02T13:17:37.785219048Z\" } The metadata string Great! We can now go ahead and confirm this run: 1 2 3 ~> spacectl stack confirm --id testing-spacelift --run-metadata \"confirm-metadata\" --run 01FEKAGP4AYV0DWP4QDFTANRES You have successfully confirmed a deployment The live run can be visited at http://cube2222.app.spacelift.tf/stack/testing-spacelift/run/01FEKAGP4AYV0DWP4QDFTANRES In the policy sample log for the relevant metadata key we\u2019ll see an additional entry, which was added when confirming: 1 2 3 4 \"user_provided_metadata\" : [ \"deploy-metadata\" , \"confirm-metadata\" ] And that's basically it! It's a very flexible building block which lets you build various automation and compliance helper tooling.","title":"Usage"},{"location":"concepts/run/user-provided-metadata.html#run-signatures","text":"A standard use case for this feature would be to sign your runs when you\u2019re creating them. You'll have to bring the infrastructure for managing keys and signatures yourself - usually you'll already have something like that internally. But in short you can create a cryptographic signature of the parameters for a run you\u2019re about to create - based on the commit SHA, run type, stack, date, etc. - and then you can pass that signature to Spacelift when creating the run. Later, in the initialization policy you can use the exec function to run your custom binary for verifying that signature. This way - for your most sensitive stacks - you can verify whether runs you are receiving from the Spacelift backend are legit, intentionally created by an employee of your company. Tip We created a reference implementation to demonstrate how to sign runs and verify signatures.","title":"Run signatures"},{"location":"concepts/spaces/index.html","text":"Spaces \u00bb Introduction \u00bb With increased usage comes a bigger need for access control and self-service. Having a single team of admins doesn't scale when you start having tens or hundreds of people using Spacelift daily. You may want to delegate partial admin rights to those teams, enable them to manage their own limited environments, but without giving them keys to the whole account and other teams' environments. In Spacelift, this can be achieved by splitting your account into multiple Spaces. Spaces are sets that can be filled with various kinds of Spacelift entities: Stacks , Policies , Contexts , Modules , Worker Pools , and Cloud Integrations . Initially, you start with a root and a legacy space. The root space is the top-level space of your account, while the legacy space exists for backward compatibility with pre-spaces RBAC. You can then create more spaces , ending up with a big tree of segregated environments. What problems do Spaces solve? \u00bb First and foremost, Spaces let you give users limited admin access. This means that, in their space, they can create Stacks, Policies, etc. while not interfering with entities present in other Spaces. Additionally, Spaces bring the ability to share resources between all these isolated environments, which means you can have a single worker pool and a single set of policies that can be reused by the whole organization.","title":"Spaces"},{"location":"concepts/spaces/index.html#spaces","text":"","title":"Spaces"},{"location":"concepts/spaces/index.html#introduction","text":"With increased usage comes a bigger need for access control and self-service. Having a single team of admins doesn't scale when you start having tens or hundreds of people using Spacelift daily. You may want to delegate partial admin rights to those teams, enable them to manage their own limited environments, but without giving them keys to the whole account and other teams' environments. In Spacelift, this can be achieved by splitting your account into multiple Spaces. Spaces are sets that can be filled with various kinds of Spacelift entities: Stacks , Policies , Contexts , Modules , Worker Pools , and Cloud Integrations . Initially, you start with a root and a legacy space. The root space is the top-level space of your account, while the legacy space exists for backward compatibility with pre-spaces RBAC. You can then create more spaces , ending up with a big tree of segregated environments.","title":"Introduction"},{"location":"concepts/spaces/index.html#what-problems-do-spaces-solve","text":"First and foremost, Spaces let you give users limited admin access. This means that, in their space, they can create Stacks, Policies, etc. while not interfering with entities present in other Spaces. Additionally, Spaces bring the ability to share resources between all these isolated environments, which means you can have a single worker pool and a single set of policies that can be reused by the whole organization.","title":"What problems do Spaces solve?"},{"location":"concepts/spaces/access-control.html","text":"Access Control \u00bb With Spaces, the whole permission management process is done within Login policies where you can specify what type of role a user gets within the given spaces. Roles \u00bb There are 3 roles that you can assign to users on a space-by-space basis: Read - cannot create or modify neither stacks nor any attachable entities, but can view them Write - an extension to Read , as it can actually trigger runs in the stacks it sees Admin - can create and modify stacks and attachable entities, as well as trigger runs A special case is someone who is given Admin permissions to the root space - we would call that person a Root Space Admin. Any Root Space Admin is perceived to be an admin of the whole account. Only they can modify the space tree or access account-wide settings. Info Administrative stacks get the Admin role in the space they belong to. Warning Administrative stacks in the legacy space get admin access to the root space for backward compatibility reasons. A comparison table of what users with given roles are capable of can be found below. Action\\Who Root Space Admin Admin Writer Reader Setup SSO \u2705 \u274c \u274c \u274c Setup VCS \u2705 \u274c \u274c \u274c Manage Sessions \u2705 \u274c \u274c \u274c Manage Spaces \u2705 \u274c \u274c \u274c Manage Login Policies \u2705 \u274c \u274c \u274c Manage Stacks \u2705 \u2705 \u274c \u274c Manage Worker Pools, Contexts \u2705 \u2705 \u274c \u274c Trigger runs \u2705 \u2705 \u2705 \u274c View Stacks \u2705 \u2705 \u2705 \u2705 View Spaces \u2705 \u2705 \u2705 \u2705 View Worker Pools, Contexts \u2705 \u2705 \u2705 \u2705 Login Policies \u00bb The way you can control access to Spaces in your Spacelift account is by using Login policies . We have introduced new rules that allow you to assign access to spaces: space_reader space_writer space_admin Here is a valid login policy that uses all of them: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package spacelift developers : = { \"bob\" } login : = input . session . login is_developer { developers [ login ] } allow { is_developer } # Let 's give every developer read access to any space space_read [ space . id ] { space : = input . spaces [ _ ] is_developer } # Assign write role to developers for spaces with \"developers-are-writers\" label space_write [ space . id ] { space : = input . spaces [ _ ] space . labels [ _ ] == \"developers-are-writers\" is_developer } # Assign admin role for the root space for anyone in the admin team space_admin [ \"root\" ] { input . session . teams [ _ ] == \"admin\" } Warning Please note that Login policies are only allowed to be created in the root space, therefore only root space admins and administrative stacks, as well as legacy space administrative stacks can create or modify them. Warning A logged-in user's access levels only get updated when they log out and in again, so newly added spaces might not be visible to some users. An exception is that the space's creator immediately gets access to it. Inheritance \u00bb Inheritance is a toggle that defines whether a space inherits resources from its parent space or not. When set to true, any stack in the child space can use resources such as worker pools or contexts from the parent space. If a space inherits from a parent and its parent inherits from the grandparent, then the space inherits from the grandparent as well. Inheritance also modifies how roles propagate between spaces. In a scenario when inheritance between spaces is turned off, the roles are propagated only down the space tree. On the other hand, when inheritance is enabled, then a user with any role in the child space also gets Read role in their parent. Below is a diagram that demonstrates how this all works in practice. This is a view for a user that was given the following roles by Login policies: Read in read access space Write in write access space Admin in admin access space Dashed lines indicate a lack of inheritance, while when it's enabled the lines are solid. Let's analyze the tree starting from the left. As mentioned, the user was granted Write access to the write access space space. Because inheritance is enabled, they also received Read access to the access propagates up space and the root space. The reason for that is to allow users to see resources that their space can now use. Next, the user was given Admin access to the admin access space space. Regardless of the inheritance being off, they also received Admin access to the access propagates down space. This makes sense, as we want to allow admins to still manage their spaces subtree even if they want to disable resource sharing between some spaces. Finally, the user was given Read access to the read access space space. Because inheritance is off, they did not receive Read access to the legacy space. Info Inheritance works well with Policy Autoattachment. By creating a policy with an autoattach:* label you enforce it on all the stacks in all the spaces that inherit the space where the policy resides.","title":"Access Control"},{"location":"concepts/spaces/access-control.html#access-control","text":"With Spaces, the whole permission management process is done within Login policies where you can specify what type of role a user gets within the given spaces.","title":"Access Control"},{"location":"concepts/spaces/access-control.html#roles","text":"There are 3 roles that you can assign to users on a space-by-space basis: Read - cannot create or modify neither stacks nor any attachable entities, but can view them Write - an extension to Read , as it can actually trigger runs in the stacks it sees Admin - can create and modify stacks and attachable entities, as well as trigger runs A special case is someone who is given Admin permissions to the root space - we would call that person a Root Space Admin. Any Root Space Admin is perceived to be an admin of the whole account. Only they can modify the space tree or access account-wide settings. Info Administrative stacks get the Admin role in the space they belong to. Warning Administrative stacks in the legacy space get admin access to the root space for backward compatibility reasons. A comparison table of what users with given roles are capable of can be found below. Action\\Who Root Space Admin Admin Writer Reader Setup SSO \u2705 \u274c \u274c \u274c Setup VCS \u2705 \u274c \u274c \u274c Manage Sessions \u2705 \u274c \u274c \u274c Manage Spaces \u2705 \u274c \u274c \u274c Manage Login Policies \u2705 \u274c \u274c \u274c Manage Stacks \u2705 \u2705 \u274c \u274c Manage Worker Pools, Contexts \u2705 \u2705 \u274c \u274c Trigger runs \u2705 \u2705 \u2705 \u274c View Stacks \u2705 \u2705 \u2705 \u2705 View Spaces \u2705 \u2705 \u2705 \u2705 View Worker Pools, Contexts \u2705 \u2705 \u2705 \u2705","title":"Roles"},{"location":"concepts/spaces/access-control.html#login-policies","text":"The way you can control access to Spaces in your Spacelift account is by using Login policies . We have introduced new rules that allow you to assign access to spaces: space_reader space_writer space_admin Here is a valid login policy that uses all of them: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package spacelift developers : = { \"bob\" } login : = input . session . login is_developer { developers [ login ] } allow { is_developer } # Let 's give every developer read access to any space space_read [ space . id ] { space : = input . spaces [ _ ] is_developer } # Assign write role to developers for spaces with \"developers-are-writers\" label space_write [ space . id ] { space : = input . spaces [ _ ] space . labels [ _ ] == \"developers-are-writers\" is_developer } # Assign admin role for the root space for anyone in the admin team space_admin [ \"root\" ] { input . session . teams [ _ ] == \"admin\" } Warning Please note that Login policies are only allowed to be created in the root space, therefore only root space admins and administrative stacks, as well as legacy space administrative stacks can create or modify them. Warning A logged-in user's access levels only get updated when they log out and in again, so newly added spaces might not be visible to some users. An exception is that the space's creator immediately gets access to it.","title":"Login Policies"},{"location":"concepts/spaces/access-control.html#inheritance","text":"Inheritance is a toggle that defines whether a space inherits resources from its parent space or not. When set to true, any stack in the child space can use resources such as worker pools or contexts from the parent space. If a space inherits from a parent and its parent inherits from the grandparent, then the space inherits from the grandparent as well. Inheritance also modifies how roles propagate between spaces. In a scenario when inheritance between spaces is turned off, the roles are propagated only down the space tree. On the other hand, when inheritance is enabled, then a user with any role in the child space also gets Read role in their parent. Below is a diagram that demonstrates how this all works in practice. This is a view for a user that was given the following roles by Login policies: Read in read access space Write in write access space Admin in admin access space Dashed lines indicate a lack of inheritance, while when it's enabled the lines are solid. Let's analyze the tree starting from the left. As mentioned, the user was granted Write access to the write access space space. Because inheritance is enabled, they also received Read access to the access propagates up space and the root space. The reason for that is to allow users to see resources that their space can now use. Next, the user was given Admin access to the admin access space space. Regardless of the inheritance being off, they also received Admin access to the access propagates down space. This makes sense, as we want to allow admins to still manage their spaces subtree even if they want to disable resource sharing between some spaces. Finally, the user was given Read access to the read access space space. Because inheritance is off, they did not receive Read access to the legacy space. Info Inheritance works well with Policy Autoattachment. By creating a policy with an autoattach:* label you enforce it on all the stacks in all the spaces that inherit the space where the policy resides.","title":"Inheritance"},{"location":"concepts/spaces/creating-a-space.html","text":"Creating a Space \u00bb Creating and modifying spaces takes place in the Spaces tab in the UI. Spaces View \u00bb The view shows a tree of all the spaces visible to you in your account. The immutable root space is at the top, and from there you can build any tree structure you want. This view behaves a bit differently for users that are admins of the root space. If you are not an admin of the root space, you will only see the spaces that you have access to, additionally, you will see a path from the spaces you have access to, to the root space. Each space card would indicate what access level to that space you have. If you are an admin of the root space you don't see individual access levels, as you are automatically an admin of all spaces. Creating a Single Space \u00bb If you are an admin of the root space you can create a space either by clicking a Create space button in the top right corner of the view, or by clicking the blue addition button at the bottom of a space card. Clicking either will open a modal where you can enter the name of the space, its parent and optionally a description and labels. You then can click Create to create the space. Editing the Space \u00bb An admin of the root space has the ability to modify spaces. This can be done by clicking on a space card, which opens up a form similar to the one used for creating a space. After performing any changes you can click Save to save them.","title":"Creating a Space"},{"location":"concepts/spaces/creating-a-space.html#creating-a-space","text":"Creating and modifying spaces takes place in the Spaces tab in the UI.","title":"Creating a Space"},{"location":"concepts/spaces/creating-a-space.html#spaces-view","text":"The view shows a tree of all the spaces visible to you in your account. The immutable root space is at the top, and from there you can build any tree structure you want. This view behaves a bit differently for users that are admins of the root space. If you are not an admin of the root space, you will only see the spaces that you have access to, additionally, you will see a path from the spaces you have access to, to the root space. Each space card would indicate what access level to that space you have. If you are an admin of the root space you don't see individual access levels, as you are automatically an admin of all spaces.","title":"Spaces View"},{"location":"concepts/spaces/creating-a-space.html#creating-a-single-space","text":"If you are an admin of the root space you can create a space either by clicking a Create space button in the top right corner of the view, or by clicking the blue addition button at the bottom of a space card. Clicking either will open a modal where you can enter the name of the space, its parent and optionally a description and labels. You then can click Create to create the space.","title":"Creating a Single Space"},{"location":"concepts/spaces/creating-a-space.html#editing-the-space","text":"An admin of the root space has the ability to modify spaces. This can be done by clicking on a space card, which opens up a form similar to the one used for creating a space. After performing any changes you can click Save to save them.","title":"Editing the Space"},{"location":"concepts/spaces/deleting-a-space.html","text":"Deleting a Space \u00bb Click on the Delete space button to delete a space: The delete operation is only permitted if the space is empty, and it is a leaf of the space tree. If any of the following conditions is true, the space cannot be deleted: The space has any child spaces The space contains any stacks or modules The space contains any attachable entities such as worker pools, contexts, cloud integrations, etc. Info You cannot delete neither the root nor the legacy space.","title":"Deleting a Space"},{"location":"concepts/spaces/deleting-a-space.html#deleting-a-space","text":"Click on the Delete space button to delete a space: The delete operation is only permitted if the space is empty, and it is a leaf of the space tree. If any of the following conditions is true, the space cannot be deleted: The space has any child spaces The space contains any stacks or modules The space contains any attachable entities such as worker pools, contexts, cloud integrations, etc. Info You cannot delete neither the root nor the legacy space.","title":"Deleting a Space"},{"location":"concepts/spaces/migrating-out-of-the-legacy-space.html","text":"Migrating out of the Legacy Space \u00bb After the introduction of Spaces most of your resources will end up in the legacy space. This space is there to provide backward-compatibility with your existing setup, and it's the only place Access policies still work. To get the most out of Spacelift, you'll want to move out of the legacy space into other spaces. For each entity, you can do that by going into the entity's settings and choosing a new space for it. However, you'll have to keep a few things in mind. First, moving entities can't break any relationships between them. For example, if you have a policy attached to a stack, you can't move the policy to a space where it won't be accessible to the stack. Second, you have to move entities one by one, which means that moving stacks with their attachments (policies, contexts, worker pools, etc.) needs to be done in multiple steps. Create your new space as a child of the root space with inheritance enabled. Move all the attachable entities (policies, contexts, worker pools, etc.) from the legacy space to the root space. This way they're accessible from both the legacy space and your new space. Move the stacks to your new space. Move the attachable entities to your new space. Additionally, when you're ready to stop using access policies in the legacy space , you can start providing users a level of access to the legacy space using the space_admin , space_write , and space_read directives of the Login policy. The moment the Login policy specifies any of these for a user for the legacy space, Access policies will stop being evaluated for this user.","title":"Migrating out of the Legacy Space"},{"location":"concepts/spaces/migrating-out-of-the-legacy-space.html#migrating-out-of-the-legacy-space","text":"After the introduction of Spaces most of your resources will end up in the legacy space. This space is there to provide backward-compatibility with your existing setup, and it's the only place Access policies still work. To get the most out of Spacelift, you'll want to move out of the legacy space into other spaces. For each entity, you can do that by going into the entity's settings and choosing a new space for it. However, you'll have to keep a few things in mind. First, moving entities can't break any relationships between them. For example, if you have a policy attached to a stack, you can't move the policy to a space where it won't be accessible to the stack. Second, you have to move entities one by one, which means that moving stacks with their attachments (policies, contexts, worker pools, etc.) needs to be done in multiple steps. Create your new space as a child of the root space with inheritance enabled. Move all the attachable entities (policies, contexts, worker pools, etc.) from the legacy space to the root space. This way they're accessible from both the legacy space and your new space. Move the stacks to your new space. Move the attachable entities to your new space. Additionally, when you're ready to stop using access policies in the legacy space , you can start providing users a level of access to the legacy space using the space_admin , space_write , and space_read directives of the Login policy. The moment the Login policy specifies any of these for a user for the legacy space, Access policies will stop being evaluated for this user.","title":"Migrating out of the Legacy Space"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html","text":"Moving a Space or any Entity \u00bb If after creating multiple Spaces you conclude that the tree structure could be better, it is possible to restructure the tree completely. Restructuring the tree can be achieved either by creating new spaces and then moving entities (stacks, policies, etc.) to them or by moving (re-parenting) spaces (which includes the entities they contain). Both approaches are valid, but some conditions must be satisfied for the move operations to succeed. Moving a Stack \u00bb Stacks can be moved to a different space using the Name tab in the Stack's settings. For the move operation to succeed, the stack has to maintain access to any attachable resources it uses (worker pools, contexts, cloud integrations, etc.). In other words, the new space the stack will be in must either inherit (directly or indirectly via parental chain) from the spaces that the used attachable resources are in or those resources have to be defined in the new space. Moving an Attachable Entity (Worker Pool, Context, etc.) \u00bb All the attachable entities can be moved to a different space either: Moving entities is possible only if all the stacks that have been using them would still be able to access them after the move in compliance with the inheritance rules. Moving a Space \u00bb Moving a space is essentially changing its parent space. This can be done by choosing a new parent in the space's settings: This operation affects the whole subtree, because all the children of the space being moved remain its children, so they change their location in the tree as well. Changing a parent for a space is possible only if after the re-parenting process all the stacks in the whole affected subtree would still be able to access any entities that are attached outside of the subtree, in compliance with the inheritance rules.","title":"Moving a Space or any Entity"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-a-space-or-any-entity","text":"If after creating multiple Spaces you conclude that the tree structure could be better, it is possible to restructure the tree completely. Restructuring the tree can be achieved either by creating new spaces and then moving entities (stacks, policies, etc.) to them or by moving (re-parenting) spaces (which includes the entities they contain). Both approaches are valid, but some conditions must be satisfied for the move operations to succeed.","title":"Moving a Space or any Entity"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-a-stack","text":"Stacks can be moved to a different space using the Name tab in the Stack's settings. For the move operation to succeed, the stack has to maintain access to any attachable resources it uses (worker pools, contexts, cloud integrations, etc.). In other words, the new space the stack will be in must either inherit (directly or indirectly via parental chain) from the spaces that the used attachable resources are in or those resources have to be defined in the new space.","title":"Moving a Stack"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-an-attachable-entity-worker-pool-context-etc","text":"All the attachable entities can be moved to a different space either: Moving entities is possible only if all the stacks that have been using them would still be able to access them after the move in compliance with the inheritance rules.","title":"Moving an Attachable Entity (Worker Pool, Context, etc.)"},{"location":"concepts/spaces/moving-a-space-or-an-entity.html#moving-a-space","text":"Moving a space is essentially changing its parent space. This can be done by choosing a new parent in the space's settings: This operation affects the whole subtree, because all the children of the space being moved remain its children, so they change their location in the tree as well. Changing a parent for a space is possible only if after the re-parenting process all the stacks in the whole affected subtree would still be able to access any entities that are attached outside of the subtree, in compliance with the inheritance rules.","title":"Moving a Space"},{"location":"concepts/spaces/structuring-your-spaces-tree.html","text":"Structuring your Spaces Tree \u00bb Based on experience with other tools, a first intuition might be to structure your spaces team-first or service-first. Something akin to the following picture: However, there are likely Spacelift resources you will want to reuse across all development environment services, not across all environments for a single service. For example, a Worker Pool . That is because resources like Worker Pools are usually shared across security domains, not logical domains. Due to this an architecture akin to the following is more advisable: This way you can create your Worker Pools, Contexts and Policies in the dev , preprod , and prod spaces, and then reuse them in all spaces below those.","title":"Structuring your Spaces Tree"},{"location":"concepts/spaces/structuring-your-spaces-tree.html#structuring-your-spaces-tree","text":"Based on experience with other tools, a first intuition might be to structure your spaces team-first or service-first. Something akin to the following picture: However, there are likely Spacelift resources you will want to reuse across all development environment services, not across all environments for a single service. For example, a Worker Pool . That is because resources like Worker Pools are usually shared across security domains, not logical domains. Due to this an architecture akin to the following is more advisable: This way you can create your Worker Pools, Contexts and Policies in the dev , preprod , and prod spaces, and then reuse them in all spaces below those.","title":"Structuring your Spaces Tree"},{"location":"concepts/stack/index.html","text":"Stack \u00bb Stack is one of the core concepts in Spacelift. A stack is an isolated, independent entity and the choice of the word mirrors products like AWS CloudFormation , or Pulumi (of which we support both). You can think about a stack as a combination of source code, current state of the managed infrastructure (eg. Terraform state file) and configuration in the form of environment variables and mounted files. Unless you're using Spacelift only to host and test private Terraform modules , your account should probably contain one or more stacks to be of any use. For example: Here's a few helpful articles about stacks: In this article , you can learn how to create a new stack; Here you can see all the settings that are available for the stack; Here you can learn about stack locking; Stack state \u00bb Similar to runs and tasks , stacks also have states. A stack's state is the last state of its most recently processed tracked run that has progressed beyond the queued state and which was not canceled . Only if the stack has no runs yet a special state \"None\" is applied: Stack states allow users to see at a glance the overall health of their infrastructure, and the level of development activity associated with it.","title":"Stack"},{"location":"concepts/stack/index.html#stack","text":"Stack is one of the core concepts in Spacelift. A stack is an isolated, independent entity and the choice of the word mirrors products like AWS CloudFormation , or Pulumi (of which we support both). You can think about a stack as a combination of source code, current state of the managed infrastructure (eg. Terraform state file) and configuration in the form of environment variables and mounted files. Unless you're using Spacelift only to host and test private Terraform modules , your account should probably contain one or more stacks to be of any use. For example: Here's a few helpful articles about stacks: In this article , you can learn how to create a new stack; Here you can see all the settings that are available for the stack; Here you can learn about stack locking;","title":"Stack"},{"location":"concepts/stack/index.html#stack-state","text":"Similar to runs and tasks , stacks also have states. A stack's state is the last state of its most recently processed tracked run that has progressed beyond the queued state and which was not canceled . Only if the stack has no runs yet a special state \"None\" is applied: Stack states allow users to see at a glance the overall health of their infrastructure, and the level of development activity associated with it.","title":"Stack state"},{"location":"concepts/stack/creating-a-stack.html","text":"Creating a stack \u00bb Unless you're defining a stack programmatically using our Terraform provider , you will be creating one from the root of your Spacelift account: Info You need to be an admin to create a stack. By default, GitHub account owners and admins are automatically given Spacelift admin privileges, but this can be customized using login policies and/or SSO integration . The stack creation process involves four simple steps: Naming, describing and labeling ; Creating a link between your new stack and an existing Git repository ; Defining backend-specific behavior (different for each supported backend, eg. Terraform , AWS CloudFormation , Pulumi , or Kubernetes Defining common behavior of the stack ; Video Walkthrough \u00bb Please see below for a step-by-step walkthrough and explanation or watch the video for quick consumption. Name your stack \u00bb Staring with the most difficult step - naming things. Here's where you give your new stack a nice informative name and an optional description - this one even supports Markdown: You'll be able to change the name and description later, too - with one caveat. Based on the original name , Spacelift generates an immutable slug that serves as a unique identifier of this stack. If the name and the slug diverge significantly, things may become confusing. Here you will be able to choose which space your stack belongs to. Initially, you start with a root and a legacy space. The root space is the top-level space of your account, while the legacy space exists for backward compatibility with pre-spaces RBAC. Also, this is the opportunity to set a few labels . Labels are useful for searching and grouping things, but also work extremely well with policies. Integrate VCS \u00bb In this step, you will need to tell Spacelift where to look for the Terraform code for the stack - a combination of Git repository and one of its existing branches. The branch that you specify set here is what we called a tracked branch. By default, anything that you push to this branch will be considered for deployment. Anything you push to a different branch will be tested for changes against the current state. The project root configuration is where inside the repository Spacelift should look for the infra project source code (e.g. create a stack for a specific folder in the repository). A few things worth noting: you can point multiple Spacelift stacks to the same repository, even the same branch; the default behavior can be tweaked extensively to work with all sorts of Git and deployment workflows (yes, we like monorepos, too) using push and trigger policies, which are more advanced topics; in order to learn what exactly our Git hosting provider integration means, please refer to GitHub and GitLab integration documentation; Info If you're using our default GitHub App integration, we only list the repositories you've given us access to. If some repositories appear to be missing in the selection dropdown, it's likely that you've installed the app on a few selected repositories. That's fine, too, just whitelist the desired repositories and retry. Configure backend \u00bb At this point you'll probably know whether you want to create a Terraform , AWS CloudFormation , Pulumi , or Kubernetes stack. Each of the supported vendors has some settings that are specific to it, and the backend configuration step is where you can define them. Terraform \u00bb When selecting Terraform , you can choose which version of Terraform to start with - we support Terraform 0.12.0 and above. You don't need to dwell on this decision since you can change the version later - Spacelift supports full Terraform version management allowing you to even preview the impact of upgrading to a newer version. The next decisions involves your Terraform state. First, whether you want us to provide a Terraform state backend for your state. We do offer that as a convenience feature, though Spacelift works just fine with any remote backend, like Amazon S3. Info If you want to bring your own backend, there's no point in doing additional state locking - Spacelift itself provides a more sophisticated state access control mechanism than Terraform. If you choose not to use our state backend, feel free to proceed. If you do want us to manage your state, you have an option to import an existing state file from your previous backend. This is only relevant if you're migrating an existing Terraform project to Spacelift. If you have no state yet and Spacelift will be creating resources from scratch, this step is unnecessary. Warning Remember - this is the only time you can ask Spacelift to be the state backend for a given stack, so choose wisely. You can read more about state management here . In addition to these options, we also offer external state access for read-only purposes, this is available for administrative stacks or users with write permission to this Stack's space. Pulumi \u00bb When creating a Pulumi stack, you will need to provide two things. First, the login URL to your Pulumi state backend, as currently we don't provide one like we do for Terraform, so you will need to bring your own. Second, you need to specify the name of the Pulumi stack. This is separate from the name of the Spacelift stack, which you will specify in the next step . That said, nothing prevents you from keeping them in sync. CloudFormation \u00bb If you're using CloudFormation with Spacelift, there are a few pieces of information you'll need to provide. First, you'll need to specify the region where your CloudFormation stack will be located. Additionally, you'll need to provide the name of the corresponding CloudFormation stack for this Spacelift stack. This will help us keep track of the different resources in your infrastructure. You'll also need to provide the path to the template file in your repository that describes the root CloudFormation stack and finally you'll need to specify the S3 bucket where your processed CloudFormation templates will be stored. This will enable us to manage your CloudFormation state and ensure that all changes are properly applied. Kubernetes \u00bb When you create a Kubernetes stack in Spacelift, you have the option to specify the namespace of the Kubernetes cluster that you want to run commands on. You can leave this empty for multi-namespace Stacks. You can also provide the version of kubectl that you want the worker to download. This is useful if you need to work with a specific version of kubectl for compatibility or testing purposes. The worker will download the specified version of kubectl at runtime, ensuring that the correct version is available for executing commands on the cluster. Define behavior \u00bb Regardless of which of the supported backends (Terraform, Pulumi etc.) you're setting up your stack to use, there are a few common settings that apply to all of them. You'll have a chance to define them in the next step: The basic settings are: whether the stack is administrative ; worker pool to use, if applicable (default uses the Spacelift public worker pool); The advanced settings are: whether the changes should automatically deploy ; whether obsolete tests should be automatically retried ; whether or not to protect the stack from deletion; whether or not to enable the local preview spacectl CLI feature; whether or not run promotion is enabled; optionally specify a custom Docker image to use to for your job container; list of commands to run before/after any of the workflow stages;","title":"Creating a stack"},{"location":"concepts/stack/creating-a-stack.html#creating-a-stack","text":"Unless you're defining a stack programmatically using our Terraform provider , you will be creating one from the root of your Spacelift account: Info You need to be an admin to create a stack. By default, GitHub account owners and admins are automatically given Spacelift admin privileges, but this can be customized using login policies and/or SSO integration . The stack creation process involves four simple steps: Naming, describing and labeling ; Creating a link between your new stack and an existing Git repository ; Defining backend-specific behavior (different for each supported backend, eg. Terraform , AWS CloudFormation , Pulumi , or Kubernetes Defining common behavior of the stack ;","title":"Creating a stack"},{"location":"concepts/stack/creating-a-stack.html#video-walkthrough","text":"Please see below for a step-by-step walkthrough and explanation or watch the video for quick consumption.","title":"Video Walkthrough"},{"location":"concepts/stack/creating-a-stack.html#name-your-stack","text":"Staring with the most difficult step - naming things. Here's where you give your new stack a nice informative name and an optional description - this one even supports Markdown: You'll be able to change the name and description later, too - with one caveat. Based on the original name , Spacelift generates an immutable slug that serves as a unique identifier of this stack. If the name and the slug diverge significantly, things may become confusing. Here you will be able to choose which space your stack belongs to. Initially, you start with a root and a legacy space. The root space is the top-level space of your account, while the legacy space exists for backward compatibility with pre-spaces RBAC. Also, this is the opportunity to set a few labels . Labels are useful for searching and grouping things, but also work extremely well with policies.","title":"Name your stack"},{"location":"concepts/stack/creating-a-stack.html#integrate-vcs","text":"In this step, you will need to tell Spacelift where to look for the Terraform code for the stack - a combination of Git repository and one of its existing branches. The branch that you specify set here is what we called a tracked branch. By default, anything that you push to this branch will be considered for deployment. Anything you push to a different branch will be tested for changes against the current state. The project root configuration is where inside the repository Spacelift should look for the infra project source code (e.g. create a stack for a specific folder in the repository). A few things worth noting: you can point multiple Spacelift stacks to the same repository, even the same branch; the default behavior can be tweaked extensively to work with all sorts of Git and deployment workflows (yes, we like monorepos, too) using push and trigger policies, which are more advanced topics; in order to learn what exactly our Git hosting provider integration means, please refer to GitHub and GitLab integration documentation; Info If you're using our default GitHub App integration, we only list the repositories you've given us access to. If some repositories appear to be missing in the selection dropdown, it's likely that you've installed the app on a few selected repositories. That's fine, too, just whitelist the desired repositories and retry.","title":"Integrate VCS"},{"location":"concepts/stack/creating-a-stack.html#configure-backend","text":"At this point you'll probably know whether you want to create a Terraform , AWS CloudFormation , Pulumi , or Kubernetes stack. Each of the supported vendors has some settings that are specific to it, and the backend configuration step is where you can define them.","title":"Configure backend"},{"location":"concepts/stack/creating-a-stack.html#terraform","text":"When selecting Terraform , you can choose which version of Terraform to start with - we support Terraform 0.12.0 and above. You don't need to dwell on this decision since you can change the version later - Spacelift supports full Terraform version management allowing you to even preview the impact of upgrading to a newer version. The next decisions involves your Terraform state. First, whether you want us to provide a Terraform state backend for your state. We do offer that as a convenience feature, though Spacelift works just fine with any remote backend, like Amazon S3. Info If you want to bring your own backend, there's no point in doing additional state locking - Spacelift itself provides a more sophisticated state access control mechanism than Terraform. If you choose not to use our state backend, feel free to proceed. If you do want us to manage your state, you have an option to import an existing state file from your previous backend. This is only relevant if you're migrating an existing Terraform project to Spacelift. If you have no state yet and Spacelift will be creating resources from scratch, this step is unnecessary. Warning Remember - this is the only time you can ask Spacelift to be the state backend for a given stack, so choose wisely. You can read more about state management here . In addition to these options, we also offer external state access for read-only purposes, this is available for administrative stacks or users with write permission to this Stack's space.","title":"Terraform"},{"location":"concepts/stack/creating-a-stack.html#pulumi","text":"When creating a Pulumi stack, you will need to provide two things. First, the login URL to your Pulumi state backend, as currently we don't provide one like we do for Terraform, so you will need to bring your own. Second, you need to specify the name of the Pulumi stack. This is separate from the name of the Spacelift stack, which you will specify in the next step . That said, nothing prevents you from keeping them in sync.","title":"Pulumi"},{"location":"concepts/stack/creating-a-stack.html#cloudformation","text":"If you're using CloudFormation with Spacelift, there are a few pieces of information you'll need to provide. First, you'll need to specify the region where your CloudFormation stack will be located. Additionally, you'll need to provide the name of the corresponding CloudFormation stack for this Spacelift stack. This will help us keep track of the different resources in your infrastructure. You'll also need to provide the path to the template file in your repository that describes the root CloudFormation stack and finally you'll need to specify the S3 bucket where your processed CloudFormation templates will be stored. This will enable us to manage your CloudFormation state and ensure that all changes are properly applied.","title":"CloudFormation"},{"location":"concepts/stack/creating-a-stack.html#kubernetes","text":"When you create a Kubernetes stack in Spacelift, you have the option to specify the namespace of the Kubernetes cluster that you want to run commands on. You can leave this empty for multi-namespace Stacks. You can also provide the version of kubectl that you want the worker to download. This is useful if you need to work with a specific version of kubectl for compatibility or testing purposes. The worker will download the specified version of kubectl at runtime, ensuring that the correct version is available for executing commands on the cluster.","title":"Kubernetes"},{"location":"concepts/stack/creating-a-stack.html#define-behavior","text":"Regardless of which of the supported backends (Terraform, Pulumi etc.) you're setting up your stack to use, there are a few common settings that apply to all of them. You'll have a chance to define them in the next step: The basic settings are: whether the stack is administrative ; worker pool to use, if applicable (default uses the Spacelift public worker pool); The advanced settings are: whether the changes should automatically deploy ; whether obsolete tests should be automatically retried ; whether or not to protect the stack from deletion; whether or not to enable the local preview spacectl CLI feature; whether or not run promotion is enabled; optionally specify a custom Docker image to use to for your job container; list of commands to run before/after any of the workflow stages;","title":"Define behavior"},{"location":"concepts/stack/drift-detection.html","text":"Drift detection \u00bb Drift happens \u00bb In infrastructure-as-code , the concept of drift represents the difference between the desired and the actual state of the infrastructure managed by your tool of choice - Terraform , Pulumi , AWS CloudFormation , etc. In practice, there are two sources of drift . The first source covers changes directly introduced by external actors - either humans or machines (scripts). If an on-call SRE changes your database parameters otherwise controlled by Terraform, you've introduced drift . If an external script updates your Kubernetes cluster in a way that conflicts with its Pulumi definition, it's drift as well. The other source of drift comes from the dependency of your resources on external data sources. For example, if your load balancer only expects to receive traffic from Cloudflare , you may want to restrict ingress to a predefined range of IPs. However, that range may be dynamic, and your IaC tool queries it every time it runs. If there's any change to the external data source, it's showing up as drift, too. In the first scenario, drift is an unwanted by-product of emergencies or broken processes. In the latter, it's both desired and inevitable - it's proof that your otherwise declarative system responds to external changes. In other words - drift happens, so deal with it. \ud83d\ude0e Video Walkthrough \u00bb How Spacelift helps \u00bb Spacelift comes with a built-in mechanism to detect and - optionally - reconcile drift. We do it by periodically executing proposed runs on your stable infrastructure (in Spacelift, we generally represent it by the FINISHED stack state ) and checking for any changes. To get started, create a drift detection configuration from the Scheduling section of your stack settings. You will be able to add multiple cron rules to define when your reconciliation jobs should be scheduled, as well as decide whether you want your jobs to trigger tracked runs ( reconciliation jobs) in response to detected drift : Info Note that, at least currently, drift detection only works on private workers. To reconcile, or not to reconcile \u00bb We generally suggest turning reconciliation \"on\" as it ensures that you get the most out of drift detection. Reconciliation jobs are equivalent to manually triggering tracked runs and obey the same rules and constraints. In particular, they respect their stacks' auto-deploy setting and trigger plan policies - see this section for more details. However, if you choose not to reconcile changes, you can still get value out of drift detection - in this case, drifted resources can be seen in the Resources view, both on the stack and account level. Also, drift detection jobs trigger webhooks like regular runs, where they're clearly marked as such ( driftDetection field). Drift detection in practice \u00bb With drift detection enabled on the stack, proposed runs are quietly executing in the background. If they do not detect any changes, the only way you'd know about them is by viewing all runs in the Account > Runs section and filtering or grouping by drift detection parameter - here is an example: But once your job detects drift (and you've enabled reconciliation ), it triggers a regular tracked run. This run is subject to the same rules as a regular tracked run is. For example, if you set your stack not to deploy changes automatically , the run will end up in an Unconfirmed state, waiting for your decision. The same thing will happen if a plan policy produces a warning using a matched warn rule. Policy input \u00bb The only real difference between a drift detection job and one triggered manually is that the run section of your policy input will have the drift_detection field set to true - and this applies to both plan and trigger policies. You can use this mechanism to add extra controls to your drift detection strategy. For example, if you're automatically deploying your changes but want a human to look at drift before reconciling it, you can add the following section to your plan policy body: 1 2 3 4 5 package spacelift warn [ \"Drift reconciliation requires manual approval\" ] { input . spacelift . run . drift_detection }","title":"Drift detection"},{"location":"concepts/stack/drift-detection.html#drift-detection","text":"","title":"Drift detection"},{"location":"concepts/stack/drift-detection.html#drift-happens","text":"In infrastructure-as-code , the concept of drift represents the difference between the desired and the actual state of the infrastructure managed by your tool of choice - Terraform , Pulumi , AWS CloudFormation , etc. In practice, there are two sources of drift . The first source covers changes directly introduced by external actors - either humans or machines (scripts). If an on-call SRE changes your database parameters otherwise controlled by Terraform, you've introduced drift . If an external script updates your Kubernetes cluster in a way that conflicts with its Pulumi definition, it's drift as well. The other source of drift comes from the dependency of your resources on external data sources. For example, if your load balancer only expects to receive traffic from Cloudflare , you may want to restrict ingress to a predefined range of IPs. However, that range may be dynamic, and your IaC tool queries it every time it runs. If there's any change to the external data source, it's showing up as drift, too. In the first scenario, drift is an unwanted by-product of emergencies or broken processes. In the latter, it's both desired and inevitable - it's proof that your otherwise declarative system responds to external changes. In other words - drift happens, so deal with it. \ud83d\ude0e","title":"Drift happens"},{"location":"concepts/stack/drift-detection.html#video-walkthrough","text":"","title":"Video Walkthrough"},{"location":"concepts/stack/drift-detection.html#how-spacelift-helps","text":"Spacelift comes with a built-in mechanism to detect and - optionally - reconcile drift. We do it by periodically executing proposed runs on your stable infrastructure (in Spacelift, we generally represent it by the FINISHED stack state ) and checking for any changes. To get started, create a drift detection configuration from the Scheduling section of your stack settings. You will be able to add multiple cron rules to define when your reconciliation jobs should be scheduled, as well as decide whether you want your jobs to trigger tracked runs ( reconciliation jobs) in response to detected drift : Info Note that, at least currently, drift detection only works on private workers.","title":"How Spacelift helps"},{"location":"concepts/stack/drift-detection.html#to-reconcile-or-not-to-reconcile","text":"We generally suggest turning reconciliation \"on\" as it ensures that you get the most out of drift detection. Reconciliation jobs are equivalent to manually triggering tracked runs and obey the same rules and constraints. In particular, they respect their stacks' auto-deploy setting and trigger plan policies - see this section for more details. However, if you choose not to reconcile changes, you can still get value out of drift detection - in this case, drifted resources can be seen in the Resources view, both on the stack and account level. Also, drift detection jobs trigger webhooks like regular runs, where they're clearly marked as such ( driftDetection field).","title":"To reconcile, or not to reconcile"},{"location":"concepts/stack/drift-detection.html#drift-detection-in-practice","text":"With drift detection enabled on the stack, proposed runs are quietly executing in the background. If they do not detect any changes, the only way you'd know about them is by viewing all runs in the Account > Runs section and filtering or grouping by drift detection parameter - here is an example: But once your job detects drift (and you've enabled reconciliation ), it triggers a regular tracked run. This run is subject to the same rules as a regular tracked run is. For example, if you set your stack not to deploy changes automatically , the run will end up in an Unconfirmed state, waiting for your decision. The same thing will happen if a plan policy produces a warning using a matched warn rule.","title":"Drift detection in practice"},{"location":"concepts/stack/drift-detection.html#policy-input","text":"The only real difference between a drift detection job and one triggered manually is that the run section of your policy input will have the drift_detection field set to true - and this applies to both plan and trigger policies. You can use this mechanism to add extra controls to your drift detection strategy. For example, if you're automatically deploying your changes but want a human to look at drift before reconciling it, you can add the following section to your plan policy body: 1 2 3 4 5 package spacelift warn [ \"Drift reconciliation requires manual approval\" ] { input . spacelift . run . drift_detection }","title":"Policy input"},{"location":"concepts/stack/organizing-stacks.html","text":"Organizing stacks \u00bb Depending on the complexity of your infrastructure, the size of your team, your particular needs and your preferred way of working you may end up managing a lot of stacks. This obviously makes it harder to quickly find what you're looking for. As practitioners ourselves, we're providing you a few tools to make this process more manageable - from the basic query-based searching to filtering by status and the coolest of all, label-based folders . Video Walkthrough \u00bb Query-based searching and filtering \u00bb Historically the first tool we offered was the search bar: The search bar allows you to search and filter by the following stack properties: name; ID (slug); any of its labels ; Note how the search phrase is highlighted, and irrelevant stacks are filtered out: Filtering by status \u00bb Filtering stacks by status is a very useful mechanism for identifying action items like plans pending confirmation ( unconfirmed state) or failed jobs that require fixing. For that, use the Filter stacks by status section on the sidebar to the left. If you click on any of the statuses, the list of stacks will be filtered to only include stacks with a given status: Note that if no stacks in the account have a particular status at the moment, that status is missing from the list. Label-based folders \u00bb Probably the most useful way of grouping stacks is by attaching folder labels to them. You can read more about labels here, including how to set them, and folder labels are just regular labels, prefixed with folder: . In order to make it more obvious in the GUI and save some screen real estate, we replace the folder: prefix by the folder icon... ...but once you start editing labels, the magic is gone: For every folder label, a sidebar section is included in the Folders menu, allowing you to search by that label. The number to the right hand side indicates that number of stacks with that label: Nesting and multiple folder labels \u00bb Perhaps worth mentioning is the fact that folder labels can be nested, allowing you to create either hierarchies, or arbitrary classifications of your stacks. Also, a single stack can have any number of folder labels set, in which case it belongs to all the collections. In that, folder labels are like labels in Gmail rather than directories in your filesystem. Saving filters in views \u00bb It is possible to save your filters with a Filters Tab. You can select all the filters that you would like to apply, add a search query or sorting in the top right corner, click New View, enter the view name, and click Save. This view is now saved for this account. You can also mark your new view as your default view during creation. Next time you log in or navigate to stacks, your personal default view will be applied. If you forgot to mark your view as default then you can easily do the same thing in the Views Tab. Shared views \u00bb Views can be shared or private. While first creating the view, it is available only to your user. If you have admin access, you can make it public for all the users of the account by hovering over the saved view and clicking the small eye icon \"Share within account\". This way, all the users within this application can see the saved view and who created it. Reseting \u00bb To quickly reset your default view to Spacelift default state, click the \"Reset to Spacelift default view\" button. It will result in clearing all sorting, search, and filter parameters, as well as managed filter settings. Manage view \u00bb If you change your filter, search and/or sorting settings, you can update the currently selected view by clicking on Update item under \"Manage view\" button. The blue icon on the manage view button indicates an update possibility. Edit name allows editing name of the current view Delete allows removing your private view (Shared and Spacelift default views can not be removed). You can delete the view from the Views tab as well.","title":"Organizing stacks"},{"location":"concepts/stack/organizing-stacks.html#organizing-stacks","text":"Depending on the complexity of your infrastructure, the size of your team, your particular needs and your preferred way of working you may end up managing a lot of stacks. This obviously makes it harder to quickly find what you're looking for. As practitioners ourselves, we're providing you a few tools to make this process more manageable - from the basic query-based searching to filtering by status and the coolest of all, label-based folders .","title":"Organizing stacks"},{"location":"concepts/stack/organizing-stacks.html#video-walkthrough","text":"","title":"Video Walkthrough"},{"location":"concepts/stack/organizing-stacks.html#query-based-searching-and-filtering","text":"Historically the first tool we offered was the search bar: The search bar allows you to search and filter by the following stack properties: name; ID (slug); any of its labels ; Note how the search phrase is highlighted, and irrelevant stacks are filtered out:","title":"Query-based searching and filtering"},{"location":"concepts/stack/organizing-stacks.html#filtering-by-status","text":"Filtering stacks by status is a very useful mechanism for identifying action items like plans pending confirmation ( unconfirmed state) or failed jobs that require fixing. For that, use the Filter stacks by status section on the sidebar to the left. If you click on any of the statuses, the list of stacks will be filtered to only include stacks with a given status: Note that if no stacks in the account have a particular status at the moment, that status is missing from the list.","title":"Filtering by status"},{"location":"concepts/stack/organizing-stacks.html#label-based-folders","text":"Probably the most useful way of grouping stacks is by attaching folder labels to them. You can read more about labels here, including how to set them, and folder labels are just regular labels, prefixed with folder: . In order to make it more obvious in the GUI and save some screen real estate, we replace the folder: prefix by the folder icon... ...but once you start editing labels, the magic is gone: For every folder label, a sidebar section is included in the Folders menu, allowing you to search by that label. The number to the right hand side indicates that number of stacks with that label:","title":"Label-based folders"},{"location":"concepts/stack/organizing-stacks.html#nesting-and-multiple-folder-labels","text":"Perhaps worth mentioning is the fact that folder labels can be nested, allowing you to create either hierarchies, or arbitrary classifications of your stacks. Also, a single stack can have any number of folder labels set, in which case it belongs to all the collections. In that, folder labels are like labels in Gmail rather than directories in your filesystem.","title":"Nesting and multiple folder labels"},{"location":"concepts/stack/organizing-stacks.html#saving-filters-in-views","text":"It is possible to save your filters with a Filters Tab. You can select all the filters that you would like to apply, add a search query or sorting in the top right corner, click New View, enter the view name, and click Save. This view is now saved for this account. You can also mark your new view as your default view during creation. Next time you log in or navigate to stacks, your personal default view will be applied. If you forgot to mark your view as default then you can easily do the same thing in the Views Tab.","title":"Saving filters in views"},{"location":"concepts/stack/organizing-stacks.html#shared-views","text":"Views can be shared or private. While first creating the view, it is available only to your user. If you have admin access, you can make it public for all the users of the account by hovering over the saved view and clicking the small eye icon \"Share within account\". This way, all the users within this application can see the saved view and who created it.","title":"Shared views"},{"location":"concepts/stack/organizing-stacks.html#reseting","text":"To quickly reset your default view to Spacelift default state, click the \"Reset to Spacelift default view\" button. It will result in clearing all sorting, search, and filter parameters, as well as managed filter settings.","title":"Reseting"},{"location":"concepts/stack/organizing-stacks.html#manage-view","text":"If you change your filter, search and/or sorting settings, you can update the currently selected view by clicking on Update item under \"Manage view\" button. The blue icon on the manage view button indicates an update possibility. Edit name allows editing name of the current view Delete allows removing your private view (Shared and Spacelift default views can not be removed). You can delete the view from the Views tab as well.","title":"Manage view"},{"location":"concepts/stack/scheduling.html","text":"Scheduling \u00bb What is scheduling? \u00bb Info Note that, at least currently, scheduling only works on private workers. Scheduling allows you to trigger a stack deletion or task at a specific time or periodically based on the cron rules defined. Scheduled Delete Stack (TTL) \u00bb A Delete Stack schedule allows you to delete the stack and (optionally) its resources at the specific timestamp (UNIX timestamp). Add a schedule with the Delete Stack type from the Scheduling section of your stack settings. Actions when the schedule defines that the resources should be deleted: a destruction run will be triggered at the specified time. after this run is successful, the stack will be deleted. When the resources should not be deleted, we will delete the stack at the specified time. Scheduled Task \u00bb A scheduled task enables you to run a command at a specific timestamp or periodically based on the cron rules defined. Add a schedule with the Task type from the Scheduling section of your stack settings. After creating this schedule, a task will be triggered with the defined command (at a specific timestamp or periodically based on the cron rules defined).","title":"Scheduling"},{"location":"concepts/stack/scheduling.html#scheduling","text":"","title":"Scheduling"},{"location":"concepts/stack/scheduling.html#what-is-scheduling","text":"Info Note that, at least currently, scheduling only works on private workers. Scheduling allows you to trigger a stack deletion or task at a specific time or periodically based on the cron rules defined.","title":"What is scheduling?"},{"location":"concepts/stack/scheduling.html#scheduled-delete-stack-ttl","text":"A Delete Stack schedule allows you to delete the stack and (optionally) its resources at the specific timestamp (UNIX timestamp). Add a schedule with the Delete Stack type from the Scheduling section of your stack settings. Actions when the schedule defines that the resources should be deleted: a destruction run will be triggered at the specified time. after this run is successful, the stack will be deleted. When the resources should not be deleted, we will delete the stack at the specified time.","title":"Scheduled Delete Stack (TTL)"},{"location":"concepts/stack/scheduling.html#scheduled-task","text":"A scheduled task enables you to run a command at a specific timestamp or periodically based on the cron rules defined. Add a schedule with the Task type from the Scheduling section of your stack settings. After creating this schedule, a task will be triggered with the defined command (at a specific timestamp or periodically based on the cron rules defined).","title":"Scheduled Task"},{"location":"concepts/stack/stack-dependencies.html","text":"Stack dependencies \u00bb Stacks can depend on other stacks. This is useful when you want to run a stack only after another stacks have finished running. For example, you might want to deploy a database stack before a stack that uses the database. Info Stack dependencies only respect tracked runs . Proposed runs and tasks are not considered. Goals \u00bb Stack dependencies aim to solve the problem of ordering the execution of related runs triggered by the same VCS event. Stack dependencies do not manage stack lifecycle events such as creating or deleting stacks. In fact, you cannot delete a stack if it has dependencies. Defining stack dependencies \u00bb Stack dependencies can be defined in the Dependencies tab of the stack. Info You can only create dependencies between stacks that you're both an admin of. See Spaces Access Control for more information. Dependencies overview \u00bb In the Dependencies tab of the stack, there is a button called Dependencies graph to view the full dependency graph of the stack. How it works \u00bb Stack dependencies are directed acyclic graphs ( DAGs ). This means that a stack can depend on multiple stacks, and a stack can be depended on by multiple stacks but there cannot be loops: you will receive an error if you try to add a stack to a dependency graph that will create a cycle. When a tracked run is created in the stack (either triggered manually or by a VCS event), and the stack is a dependency of other stack(s), those stacks will queue up tracked runs and wait until the current stack's tracked run has finished running. If a run fails in the dependency chain, all subsequent runs will be cancelled. It will be easier to understand in a second. Examples \u00bb Scenario 1 \u00bb graph TD; BaseInfra-->Database; BaseInfra-->networkColor(Network); BaseInfra-->Storage; Database-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); Database-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style networkColor fill:#51cbad style paymentSvcColor fill:#51abcb style cartSvcColor fill:#51abcb In the above example, if Network stack receives a push event to the tracked branch, it will start a run immediately and queue up PaymentService and CartService . When Network finishes running, those two will start running. Since PaymentService and CartService does not depend on each other, they can run in parallel. BaseInfra remains untouched, we never go up in the dependency graph. Scenario 2 \u00bb graph TD; baseInfraColor(BaseInfra)-->databaseColor(Database); baseInfraColor(BaseInfra)-->networkColor(Network); baseInfraColor(BaseInfra)-->storageColor(Storage); databaseColor(Database)-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); databaseColor(Database)-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style baseInfraColor fill:#51cbad style networkColor fill:#51abcb style paymentSvcColor fill:#51abcb style cartSvcColor fill:#51abcb style storageColor fill:#51abcb style databaseColor fill:#51abcb If BaseInfra receives a push event, it will start running immediately and queue up all of the stacks below. The order of the runs: BaseInfra , then Database & Network & Storage in parallel, finally PaymentService & CartService in parallel. Note: since PaymentService and CartService does not depend on Storage , they will not wait until it finishes running. Scenario 3 \u00bb graph TD; baseInfraColor(BaseInfra)-->databaseColor(Database); baseInfraColor(BaseInfra)-->networkColor(Network); baseInfraColor(BaseInfra)-->storageColor(Storage); databaseColor(Database)-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); databaseColor(Database)-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style baseInfraColor fill:#51cbad style networkColor fill:#e21316 style paymentSvcColor fill:#ecd309 style cartSvcColor fill:#ecd309 style storageColor fill:#51abcb style databaseColor fill:#51abcb In this scenario, similarly to the previous one BaseInfra received a push, started running and queued up all of the stacks below. However, Network stack has failed which means that the rest of the runs ( PaymentService and CartService ) will be skipped. Same level stacks ( Database & Storage ) are not affected by the failure. Scenario 4 \u00bb graph TD; baseInfraColor(BaseInfra)-->databaseColor(Database); baseInfraColor(BaseInfra)-->networkColor(Network); baseInfraColor(BaseInfra)-->storageColor(Storage); databaseColor(Database)-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); databaseColor(Database)-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style baseInfraColor fill:#51cbad style networkColor fill:#51cbad style paymentSvcColor fill:#51abcb style cartSvcColor fill:#51abcb style storageColor fill:#51cbad style databaseColor fill:#51cbad Let's assume that the infrastructure ( BaseInfra , Database , Network and Storage ) is a monorepo, and a push event affects all 4 stacks. The situation isn't any different than Scenario 2 . The dependencies are still respected and the stacks will run in the proper order: BaseInfra first, then Database & Network & Storage in parallel, finally PaymentService & CartService in parallel. Scenario 5 \u00bb graph TD; baseInfraColor(BaseInfra)-->databaseColor(Database); baseInfraColor(BaseInfra)-->networkColor(Network); baseInfraColor(BaseInfra)-->storageColor(Storage); databaseColor(Database)-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); databaseColor(Database)-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style baseInfraColor fill:#51cbad style networkColor fill:#51abcb style paymentSvcColor fill:#51abcb style cartSvcColor fill:#51abcb style storageColor fill:#51abcb style databaseColor fill:#51cbad If BaseInfra and Database are a monorepo and a push event affects both of them, this scenario isn't any different than Scenario 2 and Scenario 4 . The order from top to bottom is still the same: BaseInfra first, then Database & Network & Storage in parallel, finally PaymentService & CartService in parallel. Trigger policies \u00bb Stack dependencies are meant to be a replacement of trigger policies . There is no connection between the two features, and the two shouldn't be combined ideally to avoid confusion. However, if you have a trigger policy that is not covered by the stack dependencies, you can still use it. Stack deletion \u00bb A stack cannot be deleted if it has upstream or downstream dependencies. If you want to delete a stack, you need to delete all of its dependencies first. Ordered Stack creation and deletion \u00bb As mentioned earlier , Stack Dependencies do not aim to handle the lifecycle of the stacks. Ordering the creation and deletion of stacks in a specific order is not impossible though. If you manage your Spacelift stacks with the Spacelift Terraform Provider , you can easily do it by setting spacelift_stack_destructor resources and setting the depends_on Terraform attribute on them. Here is a simple example of creating a dependency between two stacks, immediately triggering a run on the parent stack (which cascades to the child stack) and setting up a destructor for them. By setting up a destructor resource with the proper depends_on attribute, it ensures that the deletion of the stacks will happen in the proper order. First child, then parent. This is also an easy way to create short-lived environments. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # Parent stack resource \"spacelift_stack\" \"infra\" { name = \"Base infrastructure\" repository = \"infra\" branch = \"main\" autodeploy = true } # Child stack resource \"spacelift_stack\" \"app\" { name = \"Application\" repository = \"app\" branch = \"main\" autodeploy = true } # Create the parent-child dependency for run execution ordering resource \"spacelift_stack_dependency\" \"this\" { stack_id = spacelift_stack.app.id depends_on_stack_id = spacelift_stack.infra.id depends_on = [ spacelift_stack_destructor.app , spacelift_stack_destructor.infra ] } # Trigger a run on the parent stack, to create the infrastructure # and deploy the application. resource \"spacelift_run\" \"this\" { stack_id = spacelift_stack.infra.id keepers = { branch = spacelift_stack.infra.branch } # Make sure the dependency exists before triggering the run depends_on = [ spacelift_stack_dependency.this ] } # Create the destructor for the parent stack resource \"spacelift_stack_destructor\" \"infra\" { stack_id = spacelift_stack.infra.id } # Create the destructor for the child stack resource \"spacelift_stack_destructor\" \"app\" { stack_id = spacelift_stack.app.id depends_on = [ spacelift_stack_destructor.infra ] } What happens during terraform apply : Terraform creates the two stacks Sets up the dependency between them Triggers a run on the parent stack ( infra ) Which in turn automatically triggers a run on the child stack ( app ) as well You might notice the two destructors at the end. They don't do anything yet, but they will be used during terraform destroy . Destroy order: Terraform destroys the dependency Destroys the child stack ( app ) and its resources Finally, destroys the parent stack ( infra ) and its resources","title":"Stack dependencies"},{"location":"concepts/stack/stack-dependencies.html#stack-dependencies","text":"Stacks can depend on other stacks. This is useful when you want to run a stack only after another stacks have finished running. For example, you might want to deploy a database stack before a stack that uses the database. Info Stack dependencies only respect tracked runs . Proposed runs and tasks are not considered.","title":"Stack dependencies"},{"location":"concepts/stack/stack-dependencies.html#goals","text":"Stack dependencies aim to solve the problem of ordering the execution of related runs triggered by the same VCS event. Stack dependencies do not manage stack lifecycle events such as creating or deleting stacks. In fact, you cannot delete a stack if it has dependencies.","title":"Goals"},{"location":"concepts/stack/stack-dependencies.html#defining-stack-dependencies","text":"Stack dependencies can be defined in the Dependencies tab of the stack. Info You can only create dependencies between stacks that you're both an admin of. See Spaces Access Control for more information.","title":"Defining stack dependencies"},{"location":"concepts/stack/stack-dependencies.html#dependencies-overview","text":"In the Dependencies tab of the stack, there is a button called Dependencies graph to view the full dependency graph of the stack.","title":"Dependencies overview"},{"location":"concepts/stack/stack-dependencies.html#how-it-works","text":"Stack dependencies are directed acyclic graphs ( DAGs ). This means that a stack can depend on multiple stacks, and a stack can be depended on by multiple stacks but there cannot be loops: you will receive an error if you try to add a stack to a dependency graph that will create a cycle. When a tracked run is created in the stack (either triggered manually or by a VCS event), and the stack is a dependency of other stack(s), those stacks will queue up tracked runs and wait until the current stack's tracked run has finished running. If a run fails in the dependency chain, all subsequent runs will be cancelled. It will be easier to understand in a second.","title":"How it works"},{"location":"concepts/stack/stack-dependencies.html#examples","text":"","title":"Examples"},{"location":"concepts/stack/stack-dependencies.html#scenario-1","text":"graph TD; BaseInfra-->Database; BaseInfra-->networkColor(Network); BaseInfra-->Storage; Database-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); Database-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style networkColor fill:#51cbad style paymentSvcColor fill:#51abcb style cartSvcColor fill:#51abcb In the above example, if Network stack receives a push event to the tracked branch, it will start a run immediately and queue up PaymentService and CartService . When Network finishes running, those two will start running. Since PaymentService and CartService does not depend on each other, they can run in parallel. BaseInfra remains untouched, we never go up in the dependency graph.","title":"Scenario 1"},{"location":"concepts/stack/stack-dependencies.html#scenario-2","text":"graph TD; baseInfraColor(BaseInfra)-->databaseColor(Database); baseInfraColor(BaseInfra)-->networkColor(Network); baseInfraColor(BaseInfra)-->storageColor(Storage); databaseColor(Database)-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); databaseColor(Database)-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style baseInfraColor fill:#51cbad style networkColor fill:#51abcb style paymentSvcColor fill:#51abcb style cartSvcColor fill:#51abcb style storageColor fill:#51abcb style databaseColor fill:#51abcb If BaseInfra receives a push event, it will start running immediately and queue up all of the stacks below. The order of the runs: BaseInfra , then Database & Network & Storage in parallel, finally PaymentService & CartService in parallel. Note: since PaymentService and CartService does not depend on Storage , they will not wait until it finishes running.","title":"Scenario 2"},{"location":"concepts/stack/stack-dependencies.html#scenario-3","text":"graph TD; baseInfraColor(BaseInfra)-->databaseColor(Database); baseInfraColor(BaseInfra)-->networkColor(Network); baseInfraColor(BaseInfra)-->storageColor(Storage); databaseColor(Database)-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); databaseColor(Database)-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style baseInfraColor fill:#51cbad style networkColor fill:#e21316 style paymentSvcColor fill:#ecd309 style cartSvcColor fill:#ecd309 style storageColor fill:#51abcb style databaseColor fill:#51abcb In this scenario, similarly to the previous one BaseInfra received a push, started running and queued up all of the stacks below. However, Network stack has failed which means that the rest of the runs ( PaymentService and CartService ) will be skipped. Same level stacks ( Database & Storage ) are not affected by the failure.","title":"Scenario 3"},{"location":"concepts/stack/stack-dependencies.html#scenario-4","text":"graph TD; baseInfraColor(BaseInfra)-->databaseColor(Database); baseInfraColor(BaseInfra)-->networkColor(Network); baseInfraColor(BaseInfra)-->storageColor(Storage); databaseColor(Database)-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); databaseColor(Database)-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style baseInfraColor fill:#51cbad style networkColor fill:#51cbad style paymentSvcColor fill:#51abcb style cartSvcColor fill:#51abcb style storageColor fill:#51cbad style databaseColor fill:#51cbad Let's assume that the infrastructure ( BaseInfra , Database , Network and Storage ) is a monorepo, and a push event affects all 4 stacks. The situation isn't any different than Scenario 2 . The dependencies are still respected and the stacks will run in the proper order: BaseInfra first, then Database & Network & Storage in parallel, finally PaymentService & CartService in parallel.","title":"Scenario 4"},{"location":"concepts/stack/stack-dependencies.html#scenario-5","text":"graph TD; baseInfraColor(BaseInfra)-->databaseColor(Database); baseInfraColor(BaseInfra)-->networkColor(Network); baseInfraColor(BaseInfra)-->storageColor(Storage); databaseColor(Database)-->paymentSvcColor(PaymentService); networkColor(Network)-->paymentSvcColor(PaymentService); databaseColor(Database)-->cartSvcColor(CartService); networkColor(Network)-->cartSvcColor(CartService); style baseInfraColor fill:#51cbad style networkColor fill:#51abcb style paymentSvcColor fill:#51abcb style cartSvcColor fill:#51abcb style storageColor fill:#51abcb style databaseColor fill:#51cbad If BaseInfra and Database are a monorepo and a push event affects both of them, this scenario isn't any different than Scenario 2 and Scenario 4 . The order from top to bottom is still the same: BaseInfra first, then Database & Network & Storage in parallel, finally PaymentService & CartService in parallel.","title":"Scenario 5"},{"location":"concepts/stack/stack-dependencies.html#trigger-policies","text":"Stack dependencies are meant to be a replacement of trigger policies . There is no connection between the two features, and the two shouldn't be combined ideally to avoid confusion. However, if you have a trigger policy that is not covered by the stack dependencies, you can still use it.","title":"Trigger policies"},{"location":"concepts/stack/stack-dependencies.html#stack-deletion","text":"A stack cannot be deleted if it has upstream or downstream dependencies. If you want to delete a stack, you need to delete all of its dependencies first.","title":"Stack deletion"},{"location":"concepts/stack/stack-dependencies.html#ordered-stack-creation-and-deletion","text":"As mentioned earlier , Stack Dependencies do not aim to handle the lifecycle of the stacks. Ordering the creation and deletion of stacks in a specific order is not impossible though. If you manage your Spacelift stacks with the Spacelift Terraform Provider , you can easily do it by setting spacelift_stack_destructor resources and setting the depends_on Terraform attribute on them. Here is a simple example of creating a dependency between two stacks, immediately triggering a run on the parent stack (which cascades to the child stack) and setting up a destructor for them. By setting up a destructor resource with the proper depends_on attribute, it ensures that the deletion of the stacks will happen in the proper order. First child, then parent. This is also an easy way to create short-lived environments. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # Parent stack resource \"spacelift_stack\" \"infra\" { name = \"Base infrastructure\" repository = \"infra\" branch = \"main\" autodeploy = true } # Child stack resource \"spacelift_stack\" \"app\" { name = \"Application\" repository = \"app\" branch = \"main\" autodeploy = true } # Create the parent-child dependency for run execution ordering resource \"spacelift_stack_dependency\" \"this\" { stack_id = spacelift_stack.app.id depends_on_stack_id = spacelift_stack.infra.id depends_on = [ spacelift_stack_destructor.app , spacelift_stack_destructor.infra ] } # Trigger a run on the parent stack, to create the infrastructure # and deploy the application. resource \"spacelift_run\" \"this\" { stack_id = spacelift_stack.infra.id keepers = { branch = spacelift_stack.infra.branch } # Make sure the dependency exists before triggering the run depends_on = [ spacelift_stack_dependency.this ] } # Create the destructor for the parent stack resource \"spacelift_stack_destructor\" \"infra\" { stack_id = spacelift_stack.infra.id } # Create the destructor for the child stack resource \"spacelift_stack_destructor\" \"app\" { stack_id = spacelift_stack.app.id depends_on = [ spacelift_stack_destructor.infra ] } What happens during terraform apply : Terraform creates the two stacks Sets up the dependency between them Triggers a run on the parent stack ( infra ) Which in turn automatically triggers a run on the child stack ( app ) as well You might notice the two destructors at the end. They don't do anything yet, but they will be used during terraform destroy . Destroy order: Terraform destroys the dependency Destroys the child stack ( app ) and its resources Finally, destroys the parent stack ( infra ) and its resources","title":"Ordered Stack creation and deletion"},{"location":"concepts/stack/stack-locking.html","text":"Stack locking \u00bb Spacelift supports locking the stack for one person's exclusive use. This can be very handy if a delicate operation is taking place that could easily be affected by someone else making changes in the meantime. Every stack writer can lock the stack unless it's already locked. The owner of the lock is the only one who can trigger runs and tasks for the entire duration of the lock. Locks never expire, and only its creator and Spacelift admins can release it. Info Note that while a stack is locked, auto deploy is disabled to prevent accidental deployments.","title":"Stack locking"},{"location":"concepts/stack/stack-locking.html#stack-locking","text":"Spacelift supports locking the stack for one person's exclusive use. This can be very handy if a delicate operation is taking place that could easily be affected by someone else making changes in the meantime. Every stack writer can lock the stack unless it's already locked. The owner of the lock is the only one who can trigger runs and tasks for the entire duration of the lock. Locks never expire, and only its creator and Spacelift admins can release it. Info Note that while a stack is locked, auto deploy is disabled to prevent accidental deployments.","title":"Stack locking"},{"location":"concepts/stack/stack-settings.html","text":"Stack settings \u00bb This article covers all settings that are set directly on the stack . It's important to note that these are not the only settings that affect how runs and tasks within a given stack are processed - environment , attached contexts , runtime configuration and various integrations will all play a role here, too. Video Walkthrough \u00bb Common settings \u00bb Administrative \u00bb This setting indicates whether a stack has administrative privileges within the space it lives in. Runs executed by administrative stacks receive an API token that gives them administrative access to a subset of the Spacelift API used by our Terraform provider , which means they can create, update and destroy Spacelift resources. The main use case is to create one or a small number of administrative stacks that declaratively define the rest of Spacelift resources like other stacks, their environments , contexts , policies , modules , worker pools etc. in order to avoid ClickOps. Another pattern we've seen is stacks exporting their outputs as a context to avoid exposing their entire state through the Terraform remote state pattern or using external storage mechanisms, like AWS Parameter Store or Secrets Manager . If this sounds interesting and you want to give it a try, please refer to the help article exclusively dedicated to Spacelift's Terraform provider . Autodeploy \u00bb Indicates whether changes to the stack can be applied automatically. When autodeploy is set to true , any change to the tracked branch will automatically be applied if the planning phase was successful and there are no plan policy warnings. Consider setting it to true if you always do a code review before merging to the tracked branch, and/or want to rely on plan policies to automatically flag potential problems. If each candidate change goes through a meaningful human code review with stack writers as reviewers, having a separate step to confirm deployment may be overkill. You may also want to refer to a dedicated section on using plan policies for automated code review. Autoretry \u00bb Indicates whether obsolete proposed changes will be retried automatically. When autoretry is set to true and a change gets applied, all Pull Requests to the tracked branch conflicting with that change will be reevaluated based on the changed state. This saves you from manually retrying runs on Pull Requests when the state changes. This way it also gives you more confidence, that the proposed changes will actually be the actual changes you get after merging the Pull Request. Autoretry is only supported for Stacks with a private Worker Pool attached. Customizing workflow \u00bb Spacelift workflow can be customized by adding extra commands to be executed before and after each of the following phases: Initialization ( before_init and after_init , respectively) Planning ( before_plan and after_plan , respectively) Applying ( before_apply and after_apply , respectively) Destroying ( before_destroy and after_destroy , respectively) Performing ( before_perform and after_perform , respectively) You can also set up hooks ( after_run ) that will execute after each actively processed run, regardless of its outcome. These hooks will execute as part of the last \"active\" state of the run and will have access to an environment variable called TF_VAR_spacelift_final_run_state indicating the final state of the run. Note here that all hooks, including the after_run ones, execute on the worker. Hence, the after_run hooks will not fire if the run is not being processed by the worker - for example, if the run is terminated outside of the worker (eg. canceled, discarded), there is an issue setting up the workspace or starting the worker container, or the worker container is killed while processing the run. These commands may serve one of two general purposes - either to make some modifications to your workspace (eg. set up symlinks, move files around etc.) or perhaps to run validations using something like tfsec , tflint or terraform fmt . Danger When a run resumes after having been paused for any reason (e.g., confirmation, approval policy), the remaining phases are run in a new container. As a result, any tool installed in a phase that occurred before the pause won't be available in the subsequent phases. A better way to achieve this would be to bake the tool into a custom runner image . Info If any of the \"before\" hooks fail (non-zero exit code), the relevant phase is not executed. If the phase itself fails, none of the \"after\" hooks get executed. The workflow can be customized either using our Terraform provider or in the GUI. The GUI has a very nice editor that allows you to select the phase you want to customize and add commands before and after each phase. You will be able to add and remove commands, reorder them using drag and drop and edit them in-line. Note how the commands that precede the customized phase are the \"before\" hooks ( ps aux and ls in the example below), and the ones that go after it are the \"after\" hooks ( ls -la .terraform ): Perhaps worth noting is the fact that these commands run in the same shell session as the phase itself. So the phase will have access to any shell variables exported by the preceding scripts. Environment variables are preserved from one phase to the next. Info These scripts can be overridden by the runtime configuration specified in the .spacelift/config.yml file. Enable local preview \u00bb Indicates whether creating proposed Runs based on user-uploaded local workspaces is allowed. If this is enabled, you can use spacectl to create a proposed run based on the directory you're in: 1 spacectl stack local-preview --id <stack-id> Danger This in effect allows anybody with write access to the Stack to execute arbitrary code with access to all the environment variables configured in the Stack. Use with caution. Name and description \u00bb Stack name and description are pretty self-explanatory. The required name is what you'll see in the stack list on the home screen and menu selection dropdown. Make sure that it's informative enough to be able to immediately communicate the purpose of the stack, but short enough so that it fits nicely in the dropdown, and no important information is cut off. The optional description is completely free-form and it supports Markdown . This is perhaps a good place for a thorough explanation of the purpose of the stack, perhaps a link or two, and an obligatory cat GIF. Warning Based on the original name , Spacelift generates an immutable slug that serves as a unique identifier of this stack. If the name and the slug diverge significantly, things may become confusing. So even though you can change the stack name at any point, we strongly discourage all non-trivial changes. Labels \u00bb Labels are arbitrary, user-defined tags that can be attached to Stacks. A single Stack can have an arbitrary number of these, but they must be unique. Labels can be used for any purpose, including UI filtering, but one area where they shine most is user-defined policies which can modify their behavior based on the presence (or lack thereof) of a particular label. Project root \u00bb Project root points to the directory within the repo where the project should start executing. This is especially useful for monorepos, or indeed repositories hosting multiple somewhat independent projects. This setting plays very well with Git push policies , allowing you to easily express generic rules on what it means for the stack to be affected by a code change. Info The project root can be overridden by the runtime configuration specified in the .spacelift/config.yml file. Repository and branch \u00bb Repository and branch point to the location of the source code for a stack. The repository must either belong to the GitHub account linked to Spacelift (its choice may further be limited by the way the Spacelift GitHub app has been installed) or to the GitLab server integrated with your Spacelift account. For more information about these integrations, please refer to our GitHub and GitLab documentation respectively. Thanks to the strong integration between GitHub and Spacelift, the link between a stack and a repository can survive the repository being renamed in GitHub. If you're storing your repositories in GitLab then you need to make sure to manually (or programmatically, using Terraform ) point the stack to the new location of the source code. Info Spacelift does not support moving repositories between GitHub accounts, since Spacelift accounts are strongly linked to GitHub ones. In that case the best course of action is to take your Terraform state, download it and import it while recreating the stack (or multiple stacks) in a different account. After that, all the stacks pointing to the old repository can be safely deleted. Moving a repository between GitHub and GitLab or the other way around is simple, however. Just change the provider setting on the Spacelift project, and point the stack to the new source code location. Branch signifies the repository branch tracked by the stack. By default, that is unless a Git push policy explicitly determines otherwise, a commit pushed to the tracked branch triggers a deployment represented by a tracked run. A push to any other branch by default triggers a test represented by a proposed run. More information about git push policies, tracked branches, and head commits can be found here . Results of both tracked and proposed runs are displayed in the source control provider using their specific APIs - please refer to our GitHub and GitLab documentation respectively to understand how Spacelift feedback is provided for your infrastructure changes. Info A branch must exist before it's pointed to in Spacelift. Runner image \u00bb Since every Spacelift job (which we call runs ) is executed in a separate Docker container, setting a custom runner image provides a convenient way to prepare the exact runtime environment your infra-as-code flow is designed to use. Additionally, for our Pulumi integration overriding the default runner image is the canonical way of selecting the exact Pulumi version and its corresponding language SDK. You can find more information about our use of Docker in this dedicated help article . Info Runner image can be overridden by the runtime configuration specified in the .spacelift/config.yml file. Warning On the public worker pool, Docker images can only be pulled from allowed registries . On private workers, images can be stored in any registry, including self-hosted ones. Worker pool \u00bb Terraform-specific settings \u00bb Version \u00bb The Terraform version is set when a stack is created to indicate the version of Terraform that will be used with this project. However, Spacelift covers the entire Terraform version management story, and applying a change with a newer version will automatically update the version on the stack. Workspace \u00bb Terraform workspaces are supported by Spacelift, too, as long as your state backend supports them. If the workspace is set, Spacelift will try to first select , and then - should that fail - automatically create the required workspace on the state backend. If you're managing Terraform state through Spacelift , the workspace argument is ignored since Spacelift gives each stack a separate workspace by default. Pulumi-specific settings \u00bb Login URL \u00bb Login URL is the address Pulumi should log into during Run initialization. Since we do not yet provide a full-featured Pulumi state backend, you need to bring your own (eg. Amazon S3 ). You can read more about the login process here . More general explanation of Pulumi state management and backends is available here . Stack name \u00bb The name of the Pulumi stack which should be selected for backend operations. Please do not confuse it with the Spacelift stack name - they may be different, though it's probably good if you can keep them identical.","title":"Stack settings"},{"location":"concepts/stack/stack-settings.html#stack-settings","text":"This article covers all settings that are set directly on the stack . It's important to note that these are not the only settings that affect how runs and tasks within a given stack are processed - environment , attached contexts , runtime configuration and various integrations will all play a role here, too.","title":"Stack settings"},{"location":"concepts/stack/stack-settings.html#video-walkthrough","text":"","title":"Video Walkthrough"},{"location":"concepts/stack/stack-settings.html#common-settings","text":"","title":"Common settings"},{"location":"concepts/stack/stack-settings.html#administrative","text":"This setting indicates whether a stack has administrative privileges within the space it lives in. Runs executed by administrative stacks receive an API token that gives them administrative access to a subset of the Spacelift API used by our Terraform provider , which means they can create, update and destroy Spacelift resources. The main use case is to create one or a small number of administrative stacks that declaratively define the rest of Spacelift resources like other stacks, their environments , contexts , policies , modules , worker pools etc. in order to avoid ClickOps. Another pattern we've seen is stacks exporting their outputs as a context to avoid exposing their entire state through the Terraform remote state pattern or using external storage mechanisms, like AWS Parameter Store or Secrets Manager . If this sounds interesting and you want to give it a try, please refer to the help article exclusively dedicated to Spacelift's Terraform provider .","title":"Administrative"},{"location":"concepts/stack/stack-settings.html#autodeploy","text":"Indicates whether changes to the stack can be applied automatically. When autodeploy is set to true , any change to the tracked branch will automatically be applied if the planning phase was successful and there are no plan policy warnings. Consider setting it to true if you always do a code review before merging to the tracked branch, and/or want to rely on plan policies to automatically flag potential problems. If each candidate change goes through a meaningful human code review with stack writers as reviewers, having a separate step to confirm deployment may be overkill. You may also want to refer to a dedicated section on using plan policies for automated code review.","title":"Autodeploy"},{"location":"concepts/stack/stack-settings.html#autoretry","text":"Indicates whether obsolete proposed changes will be retried automatically. When autoretry is set to true and a change gets applied, all Pull Requests to the tracked branch conflicting with that change will be reevaluated based on the changed state. This saves you from manually retrying runs on Pull Requests when the state changes. This way it also gives you more confidence, that the proposed changes will actually be the actual changes you get after merging the Pull Request. Autoretry is only supported for Stacks with a private Worker Pool attached.","title":"Autoretry"},{"location":"concepts/stack/stack-settings.html#customizing-workflow","text":"Spacelift workflow can be customized by adding extra commands to be executed before and after each of the following phases: Initialization ( before_init and after_init , respectively) Planning ( before_plan and after_plan , respectively) Applying ( before_apply and after_apply , respectively) Destroying ( before_destroy and after_destroy , respectively) Performing ( before_perform and after_perform , respectively) You can also set up hooks ( after_run ) that will execute after each actively processed run, regardless of its outcome. These hooks will execute as part of the last \"active\" state of the run and will have access to an environment variable called TF_VAR_spacelift_final_run_state indicating the final state of the run. Note here that all hooks, including the after_run ones, execute on the worker. Hence, the after_run hooks will not fire if the run is not being processed by the worker - for example, if the run is terminated outside of the worker (eg. canceled, discarded), there is an issue setting up the workspace or starting the worker container, or the worker container is killed while processing the run. These commands may serve one of two general purposes - either to make some modifications to your workspace (eg. set up symlinks, move files around etc.) or perhaps to run validations using something like tfsec , tflint or terraform fmt . Danger When a run resumes after having been paused for any reason (e.g., confirmation, approval policy), the remaining phases are run in a new container. As a result, any tool installed in a phase that occurred before the pause won't be available in the subsequent phases. A better way to achieve this would be to bake the tool into a custom runner image . Info If any of the \"before\" hooks fail (non-zero exit code), the relevant phase is not executed. If the phase itself fails, none of the \"after\" hooks get executed. The workflow can be customized either using our Terraform provider or in the GUI. The GUI has a very nice editor that allows you to select the phase you want to customize and add commands before and after each phase. You will be able to add and remove commands, reorder them using drag and drop and edit them in-line. Note how the commands that precede the customized phase are the \"before\" hooks ( ps aux and ls in the example below), and the ones that go after it are the \"after\" hooks ( ls -la .terraform ): Perhaps worth noting is the fact that these commands run in the same shell session as the phase itself. So the phase will have access to any shell variables exported by the preceding scripts. Environment variables are preserved from one phase to the next. Info These scripts can be overridden by the runtime configuration specified in the .spacelift/config.yml file.","title":"Customizing workflow"},{"location":"concepts/stack/stack-settings.html#enable-local-preview","text":"Indicates whether creating proposed Runs based on user-uploaded local workspaces is allowed. If this is enabled, you can use spacectl to create a proposed run based on the directory you're in: 1 spacectl stack local-preview --id <stack-id> Danger This in effect allows anybody with write access to the Stack to execute arbitrary code with access to all the environment variables configured in the Stack. Use with caution.","title":"Enable local preview"},{"location":"concepts/stack/stack-settings.html#name-and-description","text":"Stack name and description are pretty self-explanatory. The required name is what you'll see in the stack list on the home screen and menu selection dropdown. Make sure that it's informative enough to be able to immediately communicate the purpose of the stack, but short enough so that it fits nicely in the dropdown, and no important information is cut off. The optional description is completely free-form and it supports Markdown . This is perhaps a good place for a thorough explanation of the purpose of the stack, perhaps a link or two, and an obligatory cat GIF. Warning Based on the original name , Spacelift generates an immutable slug that serves as a unique identifier of this stack. If the name and the slug diverge significantly, things may become confusing. So even though you can change the stack name at any point, we strongly discourage all non-trivial changes.","title":"Name and description"},{"location":"concepts/stack/stack-settings.html#labels","text":"Labels are arbitrary, user-defined tags that can be attached to Stacks. A single Stack can have an arbitrary number of these, but they must be unique. Labels can be used for any purpose, including UI filtering, but one area where they shine most is user-defined policies which can modify their behavior based on the presence (or lack thereof) of a particular label.","title":"Labels"},{"location":"concepts/stack/stack-settings.html#project-root","text":"Project root points to the directory within the repo where the project should start executing. This is especially useful for monorepos, or indeed repositories hosting multiple somewhat independent projects. This setting plays very well with Git push policies , allowing you to easily express generic rules on what it means for the stack to be affected by a code change. Info The project root can be overridden by the runtime configuration specified in the .spacelift/config.yml file.","title":"Project root"},{"location":"concepts/stack/stack-settings.html#repository-and-branch","text":"Repository and branch point to the location of the source code for a stack. The repository must either belong to the GitHub account linked to Spacelift (its choice may further be limited by the way the Spacelift GitHub app has been installed) or to the GitLab server integrated with your Spacelift account. For more information about these integrations, please refer to our GitHub and GitLab documentation respectively. Thanks to the strong integration between GitHub and Spacelift, the link between a stack and a repository can survive the repository being renamed in GitHub. If you're storing your repositories in GitLab then you need to make sure to manually (or programmatically, using Terraform ) point the stack to the new location of the source code. Info Spacelift does not support moving repositories between GitHub accounts, since Spacelift accounts are strongly linked to GitHub ones. In that case the best course of action is to take your Terraform state, download it and import it while recreating the stack (or multiple stacks) in a different account. After that, all the stacks pointing to the old repository can be safely deleted. Moving a repository between GitHub and GitLab or the other way around is simple, however. Just change the provider setting on the Spacelift project, and point the stack to the new source code location. Branch signifies the repository branch tracked by the stack. By default, that is unless a Git push policy explicitly determines otherwise, a commit pushed to the tracked branch triggers a deployment represented by a tracked run. A push to any other branch by default triggers a test represented by a proposed run. More information about git push policies, tracked branches, and head commits can be found here . Results of both tracked and proposed runs are displayed in the source control provider using their specific APIs - please refer to our GitHub and GitLab documentation respectively to understand how Spacelift feedback is provided for your infrastructure changes. Info A branch must exist before it's pointed to in Spacelift.","title":"Repository and branch"},{"location":"concepts/stack/stack-settings.html#runner-image","text":"Since every Spacelift job (which we call runs ) is executed in a separate Docker container, setting a custom runner image provides a convenient way to prepare the exact runtime environment your infra-as-code flow is designed to use. Additionally, for our Pulumi integration overriding the default runner image is the canonical way of selecting the exact Pulumi version and its corresponding language SDK. You can find more information about our use of Docker in this dedicated help article . Info Runner image can be overridden by the runtime configuration specified in the .spacelift/config.yml file. Warning On the public worker pool, Docker images can only be pulled from allowed registries . On private workers, images can be stored in any registry, including self-hosted ones.","title":"Runner image"},{"location":"concepts/stack/stack-settings.html#worker-pool","text":"","title":"Worker pool"},{"location":"concepts/stack/stack-settings.html#terraform-specific-settings","text":"","title":"Terraform-specific settings"},{"location":"concepts/stack/stack-settings.html#terraform-version","text":"The Terraform version is set when a stack is created to indicate the version of Terraform that will be used with this project. However, Spacelift covers the entire Terraform version management story, and applying a change with a newer version will automatically update the version on the stack.","title":"Version"},{"location":"concepts/stack/stack-settings.html#terraform-workspace","text":"Terraform workspaces are supported by Spacelift, too, as long as your state backend supports them. If the workspace is set, Spacelift will try to first select , and then - should that fail - automatically create the required workspace on the state backend. If you're managing Terraform state through Spacelift , the workspace argument is ignored since Spacelift gives each stack a separate workspace by default.","title":"Workspace"},{"location":"concepts/stack/stack-settings.html#pulumi-specific-settings","text":"","title":"Pulumi-specific settings"},{"location":"concepts/stack/stack-settings.html#pulumi-login-url","text":"Login URL is the address Pulumi should log into during Run initialization. Since we do not yet provide a full-featured Pulumi state backend, you need to bring your own (eg. Amazon S3 ). You can read more about the login process here . More general explanation of Pulumi state management and backends is available here .","title":"Login URL"},{"location":"concepts/stack/stack-settings.html#pulumi-stackname","text":"The name of the Pulumi stack which should be selected for backend operations. Please do not confuse it with the Spacelift stack name - they may be different, though it's probably good if you can keep them identical.","title":"Stack name"},{"location":"faq/index.html","text":"FAQ \u00bb Spacelift has many features and hidden nuggets so it is easy to overlook some of them but we have you covered with this list of frequently asked questions. If you still cannot find the answer to your question below, please reach out to our support team . Platforms \u00bb Terraform \u00bb How do I import the Terraform state for my stack? \u00bb The Terraform state file can be imported during the creation of a stack . How do I export the Terraform state for my stack? \u00bb The Terraform state file can be pulled and then exported using a Task . For example, to export the state to an Amazon S3 bucket, you would run the following command as a Task: 1 terraform state pull > state.json && aws s3 cp state.json s3://<PATH> Warning For that example to work, the stack needs to have write access to the AWS S3 bucket, possibly via an AWS Integration . How do I switch from Spacelift managing the Terraform state to me managing it? \u00bb You would first need to export the state file to a suitable location. The state management setting can not be changed once a stack has been created so you will need to recreate the stack and make sure that the \"Manage state\" setting is disabled. How do I manipulate the Terraform state file? \u00bb You can manipulate the Terraform state by running terraform state <SUBCOMMAND> commands in a Task . This applies whether you or Spacelift manages the Terraform state file. How do I import existing resources into a Terraform stack? \u00bb Just run the terraform import \u2026 in a Task . This applies whether you or Spacelift manages the Terraform state file. Policies \u00bb My policy works fine in the workbench but not on my stack/module \u00bb Except for the Login policies, all policies must be attached to stacks or modules to be evaluated so let's first confirm this by verifying that the stack or module is listed in the \"Used by\" section on the policy page. If it does not show up there, you will need to attach the policy . If your policy is attached to your stack/module and you still do not see the expected behavior from that policy, you should make sure that sampling is enabled for that policy, and then review the recorded samples in the Policy Workbench . That should give you valuable insight. If you do not see any sampled events despite sampling being enabled and having performed events that should have triggered events, make sure that the appropriate type was selected when the policy was created. I do not see some samples for my Login policy \u00bb Login policies are not evaluated for account creators and SSO admins who always get admin access to their respective Spacelift accounts. This is to avoid a situation where a bad Login policy locks out everyone from the account. The side-effect is that you will not see samples for these users. Are Approval policies and run confirmation the same thing? \u00bb Approval policies and run confirmation are related but different concepts. Just think about how GitHub's Pull Requests work - you can approve a PR before merging it in a separate step. Just like a PR approval means \"I'm OK with this being merged\", a run approval means \"I'm OK with that action being executed\" but nothing will happen until someone clicks on the \"Merge\" or \"Confirm\" button, respectively.","title":"\u270b FAQ"},{"location":"faq/index.html#faq","text":"Spacelift has many features and hidden nuggets so it is easy to overlook some of them but we have you covered with this list of frequently asked questions. If you still cannot find the answer to your question below, please reach out to our support team .","title":"FAQ"},{"location":"faq/index.html#platforms","text":"","title":"Platforms"},{"location":"faq/index.html#terraform","text":"","title":"Terraform"},{"location":"faq/index.html#how-do-i-import-the-terraform-state-for-my-stack","text":"The Terraform state file can be imported during the creation of a stack .","title":"How do I import the Terraform state for my stack?"},{"location":"faq/index.html#how-do-i-export-the-terraform-state-for-my-stack","text":"The Terraform state file can be pulled and then exported using a Task . For example, to export the state to an Amazon S3 bucket, you would run the following command as a Task: 1 terraform state pull > state.json && aws s3 cp state.json s3://<PATH> Warning For that example to work, the stack needs to have write access to the AWS S3 bucket, possibly via an AWS Integration .","title":"How do I export the Terraform state for my stack?"},{"location":"faq/index.html#how-do-i-switch-from-spacelift-managing-the-terraform-state-to-me-managing-it","text":"You would first need to export the state file to a suitable location. The state management setting can not be changed once a stack has been created so you will need to recreate the stack and make sure that the \"Manage state\" setting is disabled.","title":"How do I switch from Spacelift managing the Terraform state to me managing it?"},{"location":"faq/index.html#how-do-i-manipulate-the-terraform-state-file","text":"You can manipulate the Terraform state by running terraform state <SUBCOMMAND> commands in a Task . This applies whether you or Spacelift manages the Terraform state file.","title":"How do I manipulate the Terraform state file?"},{"location":"faq/index.html#how-do-i-import-existing-resources-into-a-terraform-stack","text":"Just run the terraform import \u2026 in a Task . This applies whether you or Spacelift manages the Terraform state file.","title":"How do I import existing resources into a Terraform stack?"},{"location":"faq/index.html#policies","text":"","title":"Policies"},{"location":"faq/index.html#my-policy-works-fine-in-the-workbench-but-not-on-my-stackmodule","text":"Except for the Login policies, all policies must be attached to stacks or modules to be evaluated so let's first confirm this by verifying that the stack or module is listed in the \"Used by\" section on the policy page. If it does not show up there, you will need to attach the policy . If your policy is attached to your stack/module and you still do not see the expected behavior from that policy, you should make sure that sampling is enabled for that policy, and then review the recorded samples in the Policy Workbench . That should give you valuable insight. If you do not see any sampled events despite sampling being enabled and having performed events that should have triggered events, make sure that the appropriate type was selected when the policy was created.","title":"My policy works fine in the workbench but not on my stack/module"},{"location":"faq/index.html#i-do-not-see-some-samples-for-my-login-policy","text":"Login policies are not evaluated for account creators and SSO admins who always get admin access to their respective Spacelift accounts. This is to avoid a situation where a bad Login policy locks out everyone from the account. The side-effect is that you will not see samples for these users.","title":"I do not see some samples for my Login policy"},{"location":"faq/index.html#are-approval-policies-and-run-confirmation-the-same-thing","text":"Approval policies and run confirmation are related but different concepts. Just think about how GitHub's Pull Requests work - you can approve a PR before merging it in a separate step. Just like a PR approval means \"I'm OK with this being merged\", a run approval means \"I'm OK with that action being executed\" but nothing will happen until someone clicks on the \"Merge\" or \"Confirm\" button, respectively.","title":"Are Approval policies and run confirmation the same thing?"},{"location":"integrations/api.html","text":"GraphQL API \u00bb GraphQL \u00bb GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools. Spacelift provides a GraphQL API for you to control your Spacelift account programmatically and/or through an API Client if you choose to do so. A smaller subset of this API is also used by the Spacelift Terraform provider , as well as the Spacelift CLI ( spacectl ). The API can be accessed at the /graphql endpoint of your account using POST HTTP method. An example of request and response 1 2 3 4 5 $ curl --request POST \\ --url http://<account-name>.app.spacelift.io/graphql \\ --header 'Authorization: Bearer <token>' \\ --header 'Content-Type: application/json' \\ --data '{\"query\":\"{ stacks { id name, administrative, createdAt, description }}\"}' The request body looks like this when formatted a bit nicer: 1 2 3 4 5 6 7 8 9 10 { stacks { id name, administrative, createdAt, description } } And the response looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"data\" : { \"stacks\" : [ { \"id\" : \"my-stack-1\" , \"name\" : \"My Stack 1\" , \"administrative\" : false , \"createdAt\" : 1672916942 , \"description\" : \"The is my first stack\" }, { \"id\" : \"my-stack-2\" , \"name\" : \"My Stack 2\" , \"administrative\" : false , \"createdAt\" : 1674218834 , \"description\" : \"The is my second stack\" } ] } } Recommendation \u00bb Our recommendation is to use the Spacelift API Key to authenticate with the GraphQL API. As of today, Postman does not support GraphQL natively, so our choice of tool is Insomnia . Insomnia is a free, open-source tool that allows you to easily create and manage API requests. Usage Demo \u00bb The below guide walks through an example of generating your Spacelift token with spacectl and using it to communicate with Spacelift. Prerequisites: Insomnia downloaded and installed Spacelift account with admin access (for ability to create API Keys) Authenticating with the GraphQL API \u00bb If your Spacelift account is called example you would be able to access your GraphQL by sending POST requests to: https://example.app.spacelift.io/graphql All requests need to be authenticated using a JWT bearer token, which we will discuss in more detail below. There are currently three ways of obtaining this token: Spacelift API Key > Token - for long-term usage ( recommended ) SpaceCTL CLI > Token - for temporary usage Personal GitHub Token > Token Spacelift API Key > Token \u00bb Spacelift supports creating and managing machine users with programmatic access to the Spacelift GraphQL API. These \"machine users\" are called API Keys and can be created by Spacelift admins through the Settings panel. Note API keys are virtual users and are billed like regular users, too. Thus, each API key used (exchanged for a token) during any given billing cycle counts against the total number of users. Steps to create API key in the UI: Click on Settings in the bottom left corner of the UI Choose API Keys menu and click on Add new API key The API key creation form will allow you to specify an arbitrary key name, along with the Admin setting and the list of teams . If the key is given admin privileges, it has full access to the Spacelift API and won't be subject to access policies . For non-administrative keys, you may want to add a virtual list of teams that the key should \"belong to\" so that existing access policies based on GitHub teams or SAML assertions can work with your API keys just as they do with regular users. Without further ado, let's create a non-administrative API key with virtual membership in two teams: Developers and DevOps: Once you click the Add Key button, the API Key will be generated and a file will be automatically downloaded. The file contains the API token in two forms - one to be used with our API, and the other one as a .terraformrc snippet to access your private modules outside of Spacelift: The config file looks something like this: 1 2 3 4 5 6 7 8 9 10 11 Please use the following API secret when communicating with Spacelift programmatically: SECRET_VALUE40ffc46887297384892384789239 Please add this snippet to your .terraformrc file if you want to use this API key to access Spacelift-hosted Terraform modules outside of Spacelift: credentials \"spacelift.io\" { token = \"TOKEN_VALUEQwZmZjNDY4ODdiMjI2ZWE4NDhjMWQwNWZiMWE5MGU4NWMwZTFlY2Q4NDAxMGI2ZjA2NzkwMmI1YmVlMWNmMGE\" } Warning Make sure you persist this data somewhere on your end - we don't store the token and it cannot be retrieved or recreated afterwards. SpaceCTL CLI > Token \u00bb One approach to generating this token is using the Spacelift spacectl CLI. We consider this the easiest method, as the heavy lifting to obtain the token is done for you. Steps: Follow the instructions on the spacectl GitHub repository to install the CLI on your machine. Authenticate to your Spacelift account using spacectl profile login Once authenticated, run spacectl profile export-token to receive the bearer token needed for future GraphQL queries/mutations. Personal GitHub Token > Token \u00bb Info This option is only available to those using GitHub as their identity provider. If you have enabled any other Single Sign-On methods on your account, this method will not work. If this applies to you, you will need to use the Spacelift API Key > Token method instead. Steps: Using a GitHub Account that has access to your Spacelift account, create a GitHub Personal Access Token . Copy the value of this token to a secure location, as you'll need it in the next step. Using your favorite API Client (e.g. Insomnia or GraphiQL ). Make a GraphQL POST request to your account's GraphQL endpoint (example below). Request Details: POST to https://example.app.spacelift.io/graphql Info Replace \"example\" with the name of your Spacelift account. Query: 1 2 3 4 5 mutation GetSpaceliftToken($token: String!) { oauthUser(token: $token) { jwt } } Info You'll need to pass in token as a query variable for the above example query to work. When making a GraphQL query with your favorite API Client, you should see a section called GraphQL variables where you can pass in an input. GraphQL Variables Input: 1 2 3 { \"token\": \"PASTE-TOKEN-VALUE-HERE\" } Assuming all went well, the result of the above query will return your JWT bearer token, which you will now be able to use to authenticate other queries. Once acquired, ensure you use this bearer token in your requests. If you want to access the API reliably in an automated way, we suggest using the Spacelift API Key > JWT Token approach as Spacelift tokens expire after 1 hour. Insomnia setup \u00bb You can create request libraries in Insomnia to make it easier to work with the Spacelift API. You can also automate the JWT token generation process using the Environment Variables feature. Copy the following JSON to your clipboard: Click here to expand 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 { \"_type\" : \"export\" , \"__export_format\" : 4 , \"__export_date\" : \"2023-01-23T19:49:05.605Z\" , \"__export_source\" : \"insomnia.desktop.app:v2022.7.0\" , \"resources\" : [ { \"_id\" : \"req_d7fb83c13cc945da9e21cd9b94722d3d\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1674497188638 , \"created\" : 1656577781496 , \"url\" : \"{{ _.BASE_URL }}/graphql\" , \"name\" : \"Authentication - Get JWT\" , \"description\" : \"\" , \"method\" : \"POST\" , \"body\" : { \"mimeType\" : \"application/graphql\" , \"text\" : \"{\\\"query\\\":\\\"mutation GetSpaceliftToken($keyId: ID!, $keySecret: String!) {\\\\n apiKeyUser(id: $keyId, secret: $keySecret) {\\\\n id\\\\n\\\\t\\\\tjwt\\\\n }\\\\n}\\\",\\\"variables\\\":{\\\"keyId\\\":\\\"{{ _.API_KEY_ID }}\\\",\\\"keySecret\\\":\\\"{{ _.API_KEY_SECRET }}\\\"},\\\"operationName\\\":\\\"GetSpaceliftToken\\\"}\" }, \"parameters\" : [], \"headers\" : [ { \"name\" : \"Content-Type\" , \"value\" : \"application/json\" , \"id\" : \"pair_85e4a9afc2e6491ca59b52f77d94e81f\" } ], \"authentication\" : {}, \"metaSortKey\" : -1656577781496 , \"isPrivate\" : false , \"settingStoreCookies\" : true , \"settingSendCookies\" : true , \"settingDisableRenderRequestBody\" : false , \"settingEncodeUrl\" : true , \"settingRebuildPath\" : true , \"settingFollowRedirects\" : \"global\" , \"_type\" : \"request\" }, { \"_id\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"parentId\" : null , \"modified\" : 1656576979763 , \"created\" : 1656576979763 , \"name\" : \"Spacelift\" , \"description\" : \"\" , \"scope\" : \"collection\" , \"_type\" : \"workspace\" }, { \"_id\" : \"req_83de84158a16459fa4bfce6042859df6\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1674497166036 , \"created\" : 1656577541263 , \"url\" : \"{{ _.BASE_URL }}/graphql\" , \"name\" : \"Get Stacks\" , \"description\" : \"\" , \"method\" : \"POST\" , \"body\" : { \"mimeType\" : \"application/graphql\" , \"text\" : \"{\\\"query\\\":\\\"{ \\\\n\\\\tstacks\\\\n\\\\t{\\\\n\\\\t\\\\tid\\\\n\\\\t\\\\tname,\\\\n\\\\t\\\\tadministrative,\\\\n\\\\t\\\\tcreatedAt,\\\\n\\\\t\\\\tdescription\\\\n\\\\t}\\\\n}\\\"}\" }, \"parameters\" : [], \"headers\" : [ { \"name\" : \"Content-Type\" , \"value\" : \"application/json\" , \"id\" : \"pair_80893dda7c0f4266b48bd09d0eaa3222\" } ], \"authentication\" : { \"type\" : \"bearer\" , \"token\" : \"{{ _.API_TOKEN }}\" }, \"metaSortKey\" : -1656577721437.75 , \"isPrivate\" : false , \"settingStoreCookies\" : true , \"settingSendCookies\" : true , \"settingDisableRenderRequestBody\" : false , \"settingEncodeUrl\" : true , \"settingRebuildPath\" : true , \"settingFollowRedirects\" : \"global\" , \"_type\" : \"request\" }, { \"_id\" : \"env_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1660646140956 , \"created\" : 1656576979773 , \"name\" : \"Base Environment\" , \"data\" : {}, \"dataPropertyOrder\" : {}, \"color\" : null , \"isPrivate\" : false , \"metaSortKey\" : 1656576979773 , \"_type\" : \"environment\" }, { \"_id\" : \"jar_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1656576979775 , \"created\" : 1656576979775 , \"name\" : \"Default Jar\" , \"cookies\" : [], \"_type\" : \"cookie_jar\" }, { \"_id\" : \"spc_dbcf993f70b44bb18eee1b2362bb5bdc\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1656576979770 , \"created\" : 1656576979770 , \"fileName\" : \"Spacelift\" , \"contents\" : \"\" , \"contentType\" : \"yaml\" , \"_type\" : \"api_spec\" }, { \"_id\" : \"env_ea5c30c23af449f792c71d160678eff5\" , \"parentId\" : \"env_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\" , \"modified\" : 1669716444669 , \"created\" : 1669716373608 , \"name\" : \"Spacelift\" , \"data\" : { \"BASE_URL\" : \"https://ACCOUNT_NAME.app.spacelift.io\" , \"API_KEY_ID\" : \"insert-your-real-api-key-here\" , \"API_KEY_SECRET\" : \"insert-your-real-api-secret-here\" , \"API_TOKEN\" : \"{% response 'body', 'req_d7fb83c13cc945da9e21cd9b94722d3d', 'b64::JC5kYXRhLmFwaUtleVVzZXIuand0::46b', 'never', 60 %}\" }, \"dataPropertyOrder\" : { \"&\" : [ \"BASE_URL\" , \"API_KEY_ID\" , \"API_KEY_SECRET\" , \"API_TOKEN\" ] }, \"color\" : \"#6b84ff\" , \"isPrivate\" : false , \"metaSortKey\" : 828288489886.5 , \"_type\" : \"environment\" } ] } In the home screen of Insomnia, click on Import From then click on Clipboard . The Spacelift collection will appear. Click on it. On the top left corner, click the \ud83d\udd35 Spacelift icon, then choose Manage Environments . Here, make sure you fill the first three variables properly: BASE_URL should be the URL of your Spacelift account. For example, https://my-account.app.spacelift.io API_KEY_ID is the ID of the API key you created in the previous step . It should be a 26-character ULID . API_KEY_SECRET can be found in the file that was downloaded when you created the API key. Don't worry about the 4th. That's it! Now just send an Authentication - Get JWT request which populates API_TOKEN environment variable, then send the other Get Stacks request to see the list of stacks in your account. If you want to create another request, just right click on Get Stacks and duplicate it. Then, change the query to whatever you want. Hint Don't forget that the JWT expires after 10 hours. Run the authentication request again to get a new one. Viewing the GraphQL Schema \u00bb Our GraphQL schema is self-documenting. The best way to view the latest documentation is using a dedicated GraphQL client like Insomnia or GraphiQL . Note: As of the writing of these examples, the latest version of Postman does not currently support viewing GraphQL Schemas from a URL, but does support autocompletion. Warning Please replace the URL in the below examples with the one pointing to your Spacelift account. Insomnia \u00bb GraphiQL \u00bb Input your GraphQL Endpoint for your Spacelift Account. Use the Documentation Explorer within GraphiQL","title":"GraphQL API"},{"location":"integrations/api.html#graphql-api","text":"","title":"GraphQL API"},{"location":"integrations/api.html#graphql","text":"GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools. Spacelift provides a GraphQL API for you to control your Spacelift account programmatically and/or through an API Client if you choose to do so. A smaller subset of this API is also used by the Spacelift Terraform provider , as well as the Spacelift CLI ( spacectl ). The API can be accessed at the /graphql endpoint of your account using POST HTTP method. An example of request and response 1 2 3 4 5 $ curl --request POST \\ --url http://<account-name>.app.spacelift.io/graphql \\ --header 'Authorization: Bearer <token>' \\ --header 'Content-Type: application/json' \\ --data '{\"query\":\"{ stacks { id name, administrative, createdAt, description }}\"}' The request body looks like this when formatted a bit nicer: 1 2 3 4 5 6 7 8 9 10 { stacks { id name, administrative, createdAt, description } } And the response looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"data\" : { \"stacks\" : [ { \"id\" : \"my-stack-1\" , \"name\" : \"My Stack 1\" , \"administrative\" : false , \"createdAt\" : 1672916942 , \"description\" : \"The is my first stack\" }, { \"id\" : \"my-stack-2\" , \"name\" : \"My Stack 2\" , \"administrative\" : false , \"createdAt\" : 1674218834 , \"description\" : \"The is my second stack\" } ] } }","title":"GraphQL"},{"location":"integrations/api.html#recommendation","text":"Our recommendation is to use the Spacelift API Key to authenticate with the GraphQL API. As of today, Postman does not support GraphQL natively, so our choice of tool is Insomnia . Insomnia is a free, open-source tool that allows you to easily create and manage API requests.","title":"Recommendation"},{"location":"integrations/api.html#usage-demo","text":"The below guide walks through an example of generating your Spacelift token with spacectl and using it to communicate with Spacelift. Prerequisites: Insomnia downloaded and installed Spacelift account with admin access (for ability to create API Keys)","title":"Usage Demo"},{"location":"integrations/api.html#authenticating-with-the-graphql-api","text":"If your Spacelift account is called example you would be able to access your GraphQL by sending POST requests to: https://example.app.spacelift.io/graphql All requests need to be authenticated using a JWT bearer token, which we will discuss in more detail below. There are currently three ways of obtaining this token: Spacelift API Key > Token - for long-term usage ( recommended ) SpaceCTL CLI > Token - for temporary usage Personal GitHub Token > Token","title":"Authenticating with the GraphQL API"},{"location":"integrations/api.html#spacelift-api-key-token","text":"Spacelift supports creating and managing machine users with programmatic access to the Spacelift GraphQL API. These \"machine users\" are called API Keys and can be created by Spacelift admins through the Settings panel. Note API keys are virtual users and are billed like regular users, too. Thus, each API key used (exchanged for a token) during any given billing cycle counts against the total number of users. Steps to create API key in the UI: Click on Settings in the bottom left corner of the UI Choose API Keys menu and click on Add new API key The API key creation form will allow you to specify an arbitrary key name, along with the Admin setting and the list of teams . If the key is given admin privileges, it has full access to the Spacelift API and won't be subject to access policies . For non-administrative keys, you may want to add a virtual list of teams that the key should \"belong to\" so that existing access policies based on GitHub teams or SAML assertions can work with your API keys just as they do with regular users. Without further ado, let's create a non-administrative API key with virtual membership in two teams: Developers and DevOps: Once you click the Add Key button, the API Key will be generated and a file will be automatically downloaded. The file contains the API token in two forms - one to be used with our API, and the other one as a .terraformrc snippet to access your private modules outside of Spacelift: The config file looks something like this: 1 2 3 4 5 6 7 8 9 10 11 Please use the following API secret when communicating with Spacelift programmatically: SECRET_VALUE40ffc46887297384892384789239 Please add this snippet to your .terraformrc file if you want to use this API key to access Spacelift-hosted Terraform modules outside of Spacelift: credentials \"spacelift.io\" { token = \"TOKEN_VALUEQwZmZjNDY4ODdiMjI2ZWE4NDhjMWQwNWZiMWE5MGU4NWMwZTFlY2Q4NDAxMGI2ZjA2NzkwMmI1YmVlMWNmMGE\" } Warning Make sure you persist this data somewhere on your end - we don't store the token and it cannot be retrieved or recreated afterwards.","title":"Spacelift API Key &gt; Token"},{"location":"integrations/api.html#spacectl-cli-token","text":"One approach to generating this token is using the Spacelift spacectl CLI. We consider this the easiest method, as the heavy lifting to obtain the token is done for you. Steps: Follow the instructions on the spacectl GitHub repository to install the CLI on your machine. Authenticate to your Spacelift account using spacectl profile login Once authenticated, run spacectl profile export-token to receive the bearer token needed for future GraphQL queries/mutations.","title":"SpaceCTL CLI &gt; Token"},{"location":"integrations/api.html#personal-github-token-token","text":"Info This option is only available to those using GitHub as their identity provider. If you have enabled any other Single Sign-On methods on your account, this method will not work. If this applies to you, you will need to use the Spacelift API Key > Token method instead. Steps: Using a GitHub Account that has access to your Spacelift account, create a GitHub Personal Access Token . Copy the value of this token to a secure location, as you'll need it in the next step. Using your favorite API Client (e.g. Insomnia or GraphiQL ). Make a GraphQL POST request to your account's GraphQL endpoint (example below). Request Details: POST to https://example.app.spacelift.io/graphql Info Replace \"example\" with the name of your Spacelift account. Query: 1 2 3 4 5 mutation GetSpaceliftToken($token: String!) { oauthUser(token: $token) { jwt } } Info You'll need to pass in token as a query variable for the above example query to work. When making a GraphQL query with your favorite API Client, you should see a section called GraphQL variables where you can pass in an input. GraphQL Variables Input: 1 2 3 { \"token\": \"PASTE-TOKEN-VALUE-HERE\" } Assuming all went well, the result of the above query will return your JWT bearer token, which you will now be able to use to authenticate other queries. Once acquired, ensure you use this bearer token in your requests. If you want to access the API reliably in an automated way, we suggest using the Spacelift API Key > JWT Token approach as Spacelift tokens expire after 1 hour.","title":"Personal GitHub Token &gt; Token"},{"location":"integrations/api.html#insomnia-setup","text":"You can create request libraries in Insomnia to make it easier to work with the Spacelift API. You can also automate the JWT token generation process using the Environment Variables feature. Copy the following JSON to your clipboard: Click here to expand 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 { \"_type\" : \"export\" , \"__export_format\" : 4 , \"__export_date\" : \"2023-01-23T19:49:05.605Z\" , \"__export_source\" : \"insomnia.desktop.app:v2022.7.0\" , \"resources\" : [ { \"_id\" : \"req_d7fb83c13cc945da9e21cd9b94722d3d\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1674497188638 , \"created\" : 1656577781496 , \"url\" : \"{{ _.BASE_URL }}/graphql\" , \"name\" : \"Authentication - Get JWT\" , \"description\" : \"\" , \"method\" : \"POST\" , \"body\" : { \"mimeType\" : \"application/graphql\" , \"text\" : \"{\\\"query\\\":\\\"mutation GetSpaceliftToken($keyId: ID!, $keySecret: String!) {\\\\n apiKeyUser(id: $keyId, secret: $keySecret) {\\\\n id\\\\n\\\\t\\\\tjwt\\\\n }\\\\n}\\\",\\\"variables\\\":{\\\"keyId\\\":\\\"{{ _.API_KEY_ID }}\\\",\\\"keySecret\\\":\\\"{{ _.API_KEY_SECRET }}\\\"},\\\"operationName\\\":\\\"GetSpaceliftToken\\\"}\" }, \"parameters\" : [], \"headers\" : [ { \"name\" : \"Content-Type\" , \"value\" : \"application/json\" , \"id\" : \"pair_85e4a9afc2e6491ca59b52f77d94e81f\" } ], \"authentication\" : {}, \"metaSortKey\" : -1656577781496 , \"isPrivate\" : false , \"settingStoreCookies\" : true , \"settingSendCookies\" : true , \"settingDisableRenderRequestBody\" : false , \"settingEncodeUrl\" : true , \"settingRebuildPath\" : true , \"settingFollowRedirects\" : \"global\" , \"_type\" : \"request\" }, { \"_id\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"parentId\" : null , \"modified\" : 1656576979763 , \"created\" : 1656576979763 , \"name\" : \"Spacelift\" , \"description\" : \"\" , \"scope\" : \"collection\" , \"_type\" : \"workspace\" }, { \"_id\" : \"req_83de84158a16459fa4bfce6042859df6\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1674497166036 , \"created\" : 1656577541263 , \"url\" : \"{{ _.BASE_URL }}/graphql\" , \"name\" : \"Get Stacks\" , \"description\" : \"\" , \"method\" : \"POST\" , \"body\" : { \"mimeType\" : \"application/graphql\" , \"text\" : \"{\\\"query\\\":\\\"{ \\\\n\\\\tstacks\\\\n\\\\t{\\\\n\\\\t\\\\tid\\\\n\\\\t\\\\tname,\\\\n\\\\t\\\\tadministrative,\\\\n\\\\t\\\\tcreatedAt,\\\\n\\\\t\\\\tdescription\\\\n\\\\t}\\\\n}\\\"}\" }, \"parameters\" : [], \"headers\" : [ { \"name\" : \"Content-Type\" , \"value\" : \"application/json\" , \"id\" : \"pair_80893dda7c0f4266b48bd09d0eaa3222\" } ], \"authentication\" : { \"type\" : \"bearer\" , \"token\" : \"{{ _.API_TOKEN }}\" }, \"metaSortKey\" : -1656577721437.75 , \"isPrivate\" : false , \"settingStoreCookies\" : true , \"settingSendCookies\" : true , \"settingDisableRenderRequestBody\" : false , \"settingEncodeUrl\" : true , \"settingRebuildPath\" : true , \"settingFollowRedirects\" : \"global\" , \"_type\" : \"request\" }, { \"_id\" : \"env_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1660646140956 , \"created\" : 1656576979773 , \"name\" : \"Base Environment\" , \"data\" : {}, \"dataPropertyOrder\" : {}, \"color\" : null , \"isPrivate\" : false , \"metaSortKey\" : 1656576979773 , \"_type\" : \"environment\" }, { \"_id\" : \"jar_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1656576979775 , \"created\" : 1656576979775 , \"name\" : \"Default Jar\" , \"cookies\" : [], \"_type\" : \"cookie_jar\" }, { \"_id\" : \"spc_dbcf993f70b44bb18eee1b2362bb5bdc\" , \"parentId\" : \"wrk_3b73a2a7403445a48acdc8396803c4e8\" , \"modified\" : 1656576979770 , \"created\" : 1656576979770 , \"fileName\" : \"Spacelift\" , \"contents\" : \"\" , \"contentType\" : \"yaml\" , \"_type\" : \"api_spec\" }, { \"_id\" : \"env_ea5c30c23af449f792c71d160678eff5\" , \"parentId\" : \"env_36e5a9fc63b6443ed4d0a656800d202bcd1f5286\" , \"modified\" : 1669716444669 , \"created\" : 1669716373608 , \"name\" : \"Spacelift\" , \"data\" : { \"BASE_URL\" : \"https://ACCOUNT_NAME.app.spacelift.io\" , \"API_KEY_ID\" : \"insert-your-real-api-key-here\" , \"API_KEY_SECRET\" : \"insert-your-real-api-secret-here\" , \"API_TOKEN\" : \"{% response 'body', 'req_d7fb83c13cc945da9e21cd9b94722d3d', 'b64::JC5kYXRhLmFwaUtleVVzZXIuand0::46b', 'never', 60 %}\" }, \"dataPropertyOrder\" : { \"&\" : [ \"BASE_URL\" , \"API_KEY_ID\" , \"API_KEY_SECRET\" , \"API_TOKEN\" ] }, \"color\" : \"#6b84ff\" , \"isPrivate\" : false , \"metaSortKey\" : 828288489886.5 , \"_type\" : \"environment\" } ] } In the home screen of Insomnia, click on Import From then click on Clipboard . The Spacelift collection will appear. Click on it. On the top left corner, click the \ud83d\udd35 Spacelift icon, then choose Manage Environments . Here, make sure you fill the first three variables properly: BASE_URL should be the URL of your Spacelift account. For example, https://my-account.app.spacelift.io API_KEY_ID is the ID of the API key you created in the previous step . It should be a 26-character ULID . API_KEY_SECRET can be found in the file that was downloaded when you created the API key. Don't worry about the 4th. That's it! Now just send an Authentication - Get JWT request which populates API_TOKEN environment variable, then send the other Get Stacks request to see the list of stacks in your account. If you want to create another request, just right click on Get Stacks and duplicate it. Then, change the query to whatever you want. Hint Don't forget that the JWT expires after 10 hours. Run the authentication request again to get a new one.","title":"Insomnia setup"},{"location":"integrations/api.html#viewing-the-graphql-schema","text":"Our GraphQL schema is self-documenting. The best way to view the latest documentation is using a dedicated GraphQL client like Insomnia or GraphiQL . Note: As of the writing of these examples, the latest version of Postman does not currently support viewing GraphQL Schemas from a URL, but does support autocompletion. Warning Please replace the URL in the below examples with the one pointing to your Spacelift account.","title":"Viewing the GraphQL Schema"},{"location":"integrations/api.html#insomnia","text":"","title":"Insomnia"},{"location":"integrations/api.html#graphiql","text":"Input your GraphQL Endpoint for your Spacelift Account. Use the Documentation Explorer within GraphiQL","title":"GraphiQL"},{"location":"integrations/audit-trail.html","text":"Audit trail \u00bb Spacelift optionally supports auditing all operations that change Spacelift resources. This is handled by asynchronously sending webhooks to a user-supplied endpoint. Setup \u00bb In order to set up the audit trail, navigate to the Audit trail section of your account settings and click the Set up button: You will then need to provide a webhook endpoint and an arbitrary secret that you can later use for verifying payload . Let's use ngrok for the purpose of this tutorial: If you choose to automatically enable the functionality, clicking the Save button will verify that payloads can be delivered (the endpoint returns a 2xx status code). This gives us an opportunity to look at the payload: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"account\" : \"example\" , \"action\" : \"audit_trail_webhook.set\" , \"actor\" : \"github::name\" , \"context\" : { \"mutation\" : \"auditTrailSetWebhook\" }, \"data\" : { \"args\" : { \"Enabled\" : true , \"Endpoint\" : \"https://example-audithook.com/\" , \"SecretSHA\" : \"xxxfffdddwww\" } }, \"remoteIP\" : \"0.0.0.0\" , \"timestamp\" : 1674124447947 } ...and the headers - the interesting ones are highlighted: Usage \u00bb Every audit trail payload conforms to the same schema: account : name (subdomain) of the affected Spacelift account; action : name of the performed action; actor : actor performing the action - the :: format shows both the actor identity (second element), and the source of the identity (first element) context : some contextual metadata about the request; data : action-specific payload showing arguments passed to the request. Any sensitive arguments (like secrets) are sanitized; Below is a sample: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 { \"account\" : \"example\" , \"action\" : \"stack.create\" , \"actor\" : \"github::name\" , \"context\" : { \"mutation\" : \"stackCreate\" }, \"data\" : { \"ID\" : \"audit-trail-demo\" , \"args\" : { \"Input\" : { \"Administrative\" : false , \"AfterApply\" : [], \"AfterDestroy\" : [], \"AfterInit\" : [], \"AfterPerform\" : [], \"AfterPlan\" : [], \"AfterRun\" : [], \"Autodeploy\" : false , \"Autoretry\" : false , \"BeforeApply\" : [], \"BeforeDestroy\" : [], \"BeforeInit\" : [], \"BeforePerform\" : [], \"BeforePlan\" : [], \"Branch\" : \"showcase\" , \"Description\" : \"\" , \"GithubActionDeploy\" : true , \"IsDisabled\" : null , \"Labels\" : [], \"LocalPreviewEnabled\" : false , \"Name\" : \"audit-trail-demo\" , \"Namespace\" : \"spacelift-io\" , \"ProjectRoot\" : \"\" , \"ProtectFromDeletion\" : false , \"Provider\" : \"SHOWCASE\" , \"Repository\" : \"terraform-starter\" , \"RunnerImage\" : null , \"Space\" : \"legacy\" , \"TerraformVersion\" : null , \"VendorConfig\" : { \"Ansible\" : null , \"CloudFormation\" : null , \"Kubernetes\" : null , \"Pulumi\" : null , \"Terraform\" : { \"use_smart_sanitization\" : null , \"version\" : \"1.3.7\" , \"workspace\" : null } }, \"WorkerPool\" : null }, \"ManageState\" : true , \"Slug\" : null , \"StackObjectID\" : null } }, \"remoteIP\" : \"0.0.0.0\" , \"timestamp\" : 1674124447947 } Disabling and deleting the audit trail \u00bb The audit trail can be disabled and deleted at any point, but for both events we will send the appropriate payload. We suggest that you always treat these at least as important security signals, if not alerting conditions: 1 2 3 4 5 6 7 8 9 10 11 { \"account\" : \"example\" , \"action\" : \"audit_trail_webhook.delete\" , \"actor\" : \"github::user\" , \"context\" : { \"mutation\" : \"auditTrailDeleteWebhook\" }, \"data\" : {}, \"remoteIP\" : \"0.0.0.0\" , \"timestamp\" : 1674124447947 } Verifying payload \u00bb Spacelift uses the same similar verification mechanism as GitHub. With each payload we send 2 headers, X-Signature and X-Signature-256 . X-Signature header contains the SHA1 hash of the payload, while X-Signature-256 contains the SHA256 hash. We're using the exact same mechanism as GitHub to generate signatures, please refer to this article for details. Sending logs to AWS \u00bb We provide a reference implementation for sending the Audit Trail logs to an AWS S3 bucket. It works as-is but can also be tweaked to route the logs to other destinations with minimal effort.","title":"Audit trail"},{"location":"integrations/audit-trail.html#audit-trail","text":"Spacelift optionally supports auditing all operations that change Spacelift resources. This is handled by asynchronously sending webhooks to a user-supplied endpoint.","title":"Audit trail"},{"location":"integrations/audit-trail.html#setup","text":"In order to set up the audit trail, navigate to the Audit trail section of your account settings and click the Set up button: You will then need to provide a webhook endpoint and an arbitrary secret that you can later use for verifying payload . Let's use ngrok for the purpose of this tutorial: If you choose to automatically enable the functionality, clicking the Save button will verify that payloads can be delivered (the endpoint returns a 2xx status code). This gives us an opportunity to look at the payload: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"account\" : \"example\" , \"action\" : \"audit_trail_webhook.set\" , \"actor\" : \"github::name\" , \"context\" : { \"mutation\" : \"auditTrailSetWebhook\" }, \"data\" : { \"args\" : { \"Enabled\" : true , \"Endpoint\" : \"https://example-audithook.com/\" , \"SecretSHA\" : \"xxxfffdddwww\" } }, \"remoteIP\" : \"0.0.0.0\" , \"timestamp\" : 1674124447947 } ...and the headers - the interesting ones are highlighted:","title":"Setup"},{"location":"integrations/audit-trail.html#usage","text":"Every audit trail payload conforms to the same schema: account : name (subdomain) of the affected Spacelift account; action : name of the performed action; actor : actor performing the action - the :: format shows both the actor identity (second element), and the source of the identity (first element) context : some contextual metadata about the request; data : action-specific payload showing arguments passed to the request. Any sensitive arguments (like secrets) are sanitized; Below is a sample: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 { \"account\" : \"example\" , \"action\" : \"stack.create\" , \"actor\" : \"github::name\" , \"context\" : { \"mutation\" : \"stackCreate\" }, \"data\" : { \"ID\" : \"audit-trail-demo\" , \"args\" : { \"Input\" : { \"Administrative\" : false , \"AfterApply\" : [], \"AfterDestroy\" : [], \"AfterInit\" : [], \"AfterPerform\" : [], \"AfterPlan\" : [], \"AfterRun\" : [], \"Autodeploy\" : false , \"Autoretry\" : false , \"BeforeApply\" : [], \"BeforeDestroy\" : [], \"BeforeInit\" : [], \"BeforePerform\" : [], \"BeforePlan\" : [], \"Branch\" : \"showcase\" , \"Description\" : \"\" , \"GithubActionDeploy\" : true , \"IsDisabled\" : null , \"Labels\" : [], \"LocalPreviewEnabled\" : false , \"Name\" : \"audit-trail-demo\" , \"Namespace\" : \"spacelift-io\" , \"ProjectRoot\" : \"\" , \"ProtectFromDeletion\" : false , \"Provider\" : \"SHOWCASE\" , \"Repository\" : \"terraform-starter\" , \"RunnerImage\" : null , \"Space\" : \"legacy\" , \"TerraformVersion\" : null , \"VendorConfig\" : { \"Ansible\" : null , \"CloudFormation\" : null , \"Kubernetes\" : null , \"Pulumi\" : null , \"Terraform\" : { \"use_smart_sanitization\" : null , \"version\" : \"1.3.7\" , \"workspace\" : null } }, \"WorkerPool\" : null }, \"ManageState\" : true , \"Slug\" : null , \"StackObjectID\" : null } }, \"remoteIP\" : \"0.0.0.0\" , \"timestamp\" : 1674124447947 }","title":"Usage"},{"location":"integrations/audit-trail.html#disabling-and-deleting-the-audit-trail","text":"The audit trail can be disabled and deleted at any point, but for both events we will send the appropriate payload. We suggest that you always treat these at least as important security signals, if not alerting conditions: 1 2 3 4 5 6 7 8 9 10 11 { \"account\" : \"example\" , \"action\" : \"audit_trail_webhook.delete\" , \"actor\" : \"github::user\" , \"context\" : { \"mutation\" : \"auditTrailDeleteWebhook\" }, \"data\" : {}, \"remoteIP\" : \"0.0.0.0\" , \"timestamp\" : 1674124447947 }","title":"Disabling and deleting the audit trail"},{"location":"integrations/audit-trail.html#verifying-payload","text":"Spacelift uses the same similar verification mechanism as GitHub. With each payload we send 2 headers, X-Signature and X-Signature-256 . X-Signature header contains the SHA1 hash of the payload, while X-Signature-256 contains the SHA256 hash. We're using the exact same mechanism as GitHub to generate signatures, please refer to this article for details.","title":"Verifying payload"},{"location":"integrations/audit-trail.html#sending-logs-to-aws","text":"We provide a reference implementation for sending the Audit Trail logs to an AWS S3 bucket. It works as-is but can also be tweaked to route the logs to other destinations with minimal effort.","title":"Sending logs to AWS"},{"location":"integrations/docker.html","text":"Docker \u00bb Every job in Spacelift is processed inside a fresh, isolated Docker container. This approach provides reasonable isolation and resource allocation and - let's face it - is a pretty standard approach these days . Standard runner image \u00bb By default, Spacelift uses the latest version of the public.ecr.aws/spacelift/runner-terraform image, a simple Alpine image with a small bunch of universally useful packages. Feel free to refer to the very simple Dockerfile that builds this image. Info Given that we use Continuous Deployment on our backend and Terraform provider, we explicitly don't want to version the runner image. Feature previews are available under a future tag, but we'd advise against using these as the API might change unexpectedly. Allowed registries on public worker pools \u00bb On public worker pools, only Docker images from the following registries are allowed to be used for runner images: azurecr.io (Azure Container Registry) dkr.ecr.<region>.amazonaws.com (All regions are supported) docker.io docker.pkg.dev gcr.io (Google Cloud Container Registry) ghcr.io (GitHub Container Registry) public.ecr.aws quay.io registry.gitlab.com registry.hub.docker.com Customizing the runner image \u00bb The best way to customizing your Terraform execution environment is to build a custom runner image and use runtime configuration to tell Spacelift to use it instead of the standard runner. If you're not using Spacelift provider with Terraform 0.12, you can use any image supporting (by far the most popular) AMD64 architecture and add your dependencies to it. If you want our tooling in your image, there are two possible approaches. The first approach is to build on top of our image . We'd suggest doing that only if your customizations are relatively simple. For example, let's add a custom CircleCI provider to your image. They have a releases page allowing you to just curl the right version of the binary and put it in the /bin directory: Dockerfile 1 2 3 4 5 6 FROM public.ecr.aws/spacelift/runner-terraform:latest WORKDIR /tmp RUN curl -O -L https://github.com/mrolla/terraform-provider-circleci/releases/download/v0.3.0/terraform-provider-circleci-linux-amd64 \\ && mv terraform-provider-circleci-linux-amd64 /bin/terraform-provider-circleci \\ && chmod +x /bin/terraform-provider-circleci For more sophisticated use cases it may be cleaner to use Docker's multistage build feature to build your image and add our tooling on top of it. As an example, here's the case of us building a Terraform sops provider from source using a particular version. We want to keep our image small so we'll use a separate builder stage. The following approach works for Terraform version 0.12 and below, where custom Terraform providers colocated with the Terraform binary are automatically used. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 FROM public.ecr.aws/spacelift/runner-terraform:latest as spacelift FROM golang:1.13-alpine as builder WORKDIR /tmp # Note how we don't bother building the provider statically because # we're using Alpine for our final runner image, too. RUN git clone https://github.com/carlpett/terraform-provider-sops.git \\ && cd terraform-provider-sops \\ && git checkout c5ffe6ebfac0a56fd60d5e7d77e0f2a73c34c3b7 \\ && go build -o /terraform-provider-sops FROM alpine:3.10 COPY --from = spacelift /bin/terraform-provider-spacelift /bin/terraform-provider-spacelift COPY --from = builder /terraform-provider-sops /bin/terraform-provider-sops RUN adduser --disabled-password --no-create-home --uid = 1983 spacelift Info Note the adduser bit. Spacelift runs its Docker workflows as user spacelift with UID 1983 , so make sure that: this user exists and has the right UID, otherwise you won't have access to your files; whatever you need accessed and executed in your custom image has the right ownership and/or permissions; Depending on your image flavor, the exact command to add the user may be different. Tip Any ENTRYPOINT and CMD customization will be ignored because the Spacelift worker binary must be the root process in the container. If you need to customize the shell (e.g., dynamically set environment variables or export functions), you can do so in a before_init hook . Custom providers from Terraform 0.13 onwards \u00bb Since Terraform 0.13, custom providers require a slightly different approach. You will build them the same way as described above, but the path now will be different. In order to work with the new API, we require that you put the provider binaries in the /plugins directory and maintain a particular naming scheme. The above sops provider example will work with Terraform 0.13 if the following stanza is added to the Dockerfile . 1 COPY --from = builder /terraform-provider-sops /plugins/registry.myorg.io/myorg/sops/1.0.0/linux_amd64/terraform-provider-sops In addition, the custom provider must be explicitly required in the Terraform code, like this: 1 2 3 4 5 6 7 terraform { required_providers { spacelift = { source = \"registry.myorg.io/myorg/sops\" } } } Note that the source as defined above and the plugin path as defined in the Dockerfile are entirely arbitrary but must match . You can read more in the official Terraform 0.13 upgrade documentation . Using private Docker images \u00bb If you're using Spacelift's default public worker pool , you're required to use public images. This is by design - if we allowed using private images, they would be cached by the Docker daemon and accessible to all customers using the same shared worker. Hence, only private workers support private Docker images. To enable private image support, first, execute docker login command with the proper registry credentials. Spacelift agent will read the credentials from Docker's configuration directory, but you will need to point it to the correct location by setting the SPACELIFT_DOCKER_CONFIG_DIR environment variable. Special case: ECR \u00bb ECR is a special case because those credentials tend to expire pretty quickly, and you'd need to add a mechanism to refresh them periodically if you wanted to maintain live access to the registry (cached images would not be affected by expired credentials). Given that many of our customers use EC2 to host their worker pools, we implemented a special mechanism to support private images hosted in ECR. This access is seamless - if the launcher detects that a runner image is hosted in ECR, it tries to use the existing credentials (e. g. EC2 instance role credentials) to generate the registry access token automatically on each job execution. With ECR images you don't even need to execute docker login . Best practices \u00bb Here's a bunch of things we consider essential to keep your Docker usage relatively safe. If unsure, build from source \u00bb Building from the source is generally safer than using a pre-built binary, especially if you can review the code beforehand and make sure you're always building the code you've reviewed. You can use a Git commit hash, like we did above. Use well-known bases \u00bb If you're building an image from a source other than public.ecr.aws/spacelift/runner-terraform , please prefer well-known and well-supported base images. Official images are generally safe, so choose something like golang:1.13-alpine over things like imtotallylegit/notascamipromise:best-golang-image . There's a bunch of services out there offering Docker image vulnerability scanning, so that's an option as well. Limit push access \u00bb Your stack is only as safe as the runner image you're using for it. A malicious actor is able to doctor your runner image in a way that will allow them to take over your stack and all its associated cloud provider accounts in a snap. Please always review the code, and only allow docker push access to your most trusted associates. Info In our default public worker pool, we only support publicly available Docker images . If you need private Docker images, you can log in to any Docker registry from a worker in a private worker pool .","title":"Docker"},{"location":"integrations/docker.html#docker","text":"Every job in Spacelift is processed inside a fresh, isolated Docker container. This approach provides reasonable isolation and resource allocation and - let's face it - is a pretty standard approach these days .","title":"Docker"},{"location":"integrations/docker.html#standard-runner-image","text":"By default, Spacelift uses the latest version of the public.ecr.aws/spacelift/runner-terraform image, a simple Alpine image with a small bunch of universally useful packages. Feel free to refer to the very simple Dockerfile that builds this image. Info Given that we use Continuous Deployment on our backend and Terraform provider, we explicitly don't want to version the runner image. Feature previews are available under a future tag, but we'd advise against using these as the API might change unexpectedly.","title":"Standard runner image"},{"location":"integrations/docker.html#allowed-registries-on-public-worker-pools","text":"On public worker pools, only Docker images from the following registries are allowed to be used for runner images: azurecr.io (Azure Container Registry) dkr.ecr.<region>.amazonaws.com (All regions are supported) docker.io docker.pkg.dev gcr.io (Google Cloud Container Registry) ghcr.io (GitHub Container Registry) public.ecr.aws quay.io registry.gitlab.com registry.hub.docker.com","title":"Allowed registries on public worker pools"},{"location":"integrations/docker.html#customizing-the-runner-image","text":"The best way to customizing your Terraform execution environment is to build a custom runner image and use runtime configuration to tell Spacelift to use it instead of the standard runner. If you're not using Spacelift provider with Terraform 0.12, you can use any image supporting (by far the most popular) AMD64 architecture and add your dependencies to it. If you want our tooling in your image, there are two possible approaches. The first approach is to build on top of our image . We'd suggest doing that only if your customizations are relatively simple. For example, let's add a custom CircleCI provider to your image. They have a releases page allowing you to just curl the right version of the binary and put it in the /bin directory: Dockerfile 1 2 3 4 5 6 FROM public.ecr.aws/spacelift/runner-terraform:latest WORKDIR /tmp RUN curl -O -L https://github.com/mrolla/terraform-provider-circleci/releases/download/v0.3.0/terraform-provider-circleci-linux-amd64 \\ && mv terraform-provider-circleci-linux-amd64 /bin/terraform-provider-circleci \\ && chmod +x /bin/terraform-provider-circleci For more sophisticated use cases it may be cleaner to use Docker's multistage build feature to build your image and add our tooling on top of it. As an example, here's the case of us building a Terraform sops provider from source using a particular version. We want to keep our image small so we'll use a separate builder stage. The following approach works for Terraform version 0.12 and below, where custom Terraform providers colocated with the Terraform binary are automatically used. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 FROM public.ecr.aws/spacelift/runner-terraform:latest as spacelift FROM golang:1.13-alpine as builder WORKDIR /tmp # Note how we don't bother building the provider statically because # we're using Alpine for our final runner image, too. RUN git clone https://github.com/carlpett/terraform-provider-sops.git \\ && cd terraform-provider-sops \\ && git checkout c5ffe6ebfac0a56fd60d5e7d77e0f2a73c34c3b7 \\ && go build -o /terraform-provider-sops FROM alpine:3.10 COPY --from = spacelift /bin/terraform-provider-spacelift /bin/terraform-provider-spacelift COPY --from = builder /terraform-provider-sops /bin/terraform-provider-sops RUN adduser --disabled-password --no-create-home --uid = 1983 spacelift Info Note the adduser bit. Spacelift runs its Docker workflows as user spacelift with UID 1983 , so make sure that: this user exists and has the right UID, otherwise you won't have access to your files; whatever you need accessed and executed in your custom image has the right ownership and/or permissions; Depending on your image flavor, the exact command to add the user may be different. Tip Any ENTRYPOINT and CMD customization will be ignored because the Spacelift worker binary must be the root process in the container. If you need to customize the shell (e.g., dynamically set environment variables or export functions), you can do so in a before_init hook .","title":"Customizing the runner image"},{"location":"integrations/docker.html#custom-providers-from-terraform-013-onwards","text":"Since Terraform 0.13, custom providers require a slightly different approach. You will build them the same way as described above, but the path now will be different. In order to work with the new API, we require that you put the provider binaries in the /plugins directory and maintain a particular naming scheme. The above sops provider example will work with Terraform 0.13 if the following stanza is added to the Dockerfile . 1 COPY --from = builder /terraform-provider-sops /plugins/registry.myorg.io/myorg/sops/1.0.0/linux_amd64/terraform-provider-sops In addition, the custom provider must be explicitly required in the Terraform code, like this: 1 2 3 4 5 6 7 terraform { required_providers { spacelift = { source = \"registry.myorg.io/myorg/sops\" } } } Note that the source as defined above and the plugin path as defined in the Dockerfile are entirely arbitrary but must match . You can read more in the official Terraform 0.13 upgrade documentation .","title":"Custom providers from Terraform 0.13 onwards"},{"location":"integrations/docker.html#using-private-docker-images","text":"If you're using Spacelift's default public worker pool , you're required to use public images. This is by design - if we allowed using private images, they would be cached by the Docker daemon and accessible to all customers using the same shared worker. Hence, only private workers support private Docker images. To enable private image support, first, execute docker login command with the proper registry credentials. Spacelift agent will read the credentials from Docker's configuration directory, but you will need to point it to the correct location by setting the SPACELIFT_DOCKER_CONFIG_DIR environment variable.","title":"Using private Docker images"},{"location":"integrations/docker.html#special-case-ecr","text":"ECR is a special case because those credentials tend to expire pretty quickly, and you'd need to add a mechanism to refresh them periodically if you wanted to maintain live access to the registry (cached images would not be affected by expired credentials). Given that many of our customers use EC2 to host their worker pools, we implemented a special mechanism to support private images hosted in ECR. This access is seamless - if the launcher detects that a runner image is hosted in ECR, it tries to use the existing credentials (e. g. EC2 instance role credentials) to generate the registry access token automatically on each job execution. With ECR images you don't even need to execute docker login .","title":"Special case: ECR"},{"location":"integrations/docker.html#best-practices","text":"Here's a bunch of things we consider essential to keep your Docker usage relatively safe.","title":"Best practices"},{"location":"integrations/docker.html#if-unsure-build-from-source","text":"Building from the source is generally safer than using a pre-built binary, especially if you can review the code beforehand and make sure you're always building the code you've reviewed. You can use a Git commit hash, like we did above.","title":"If unsure, build from source"},{"location":"integrations/docker.html#use-well-known-bases","text":"If you're building an image from a source other than public.ecr.aws/spacelift/runner-terraform , please prefer well-known and well-supported base images. Official images are generally safe, so choose something like golang:1.13-alpine over things like imtotallylegit/notascamipromise:best-golang-image . There's a bunch of services out there offering Docker image vulnerability scanning, so that's an option as well.","title":"Use well-known bases"},{"location":"integrations/docker.html#limit-push-access","text":"Your stack is only as safe as the runner image you're using for it. A malicious actor is able to doctor your runner image in a way that will allow them to take over your stack and all its associated cloud provider accounts in a snap. Please always review the code, and only allow docker push access to your most trusted associates. Info In our default public worker pool, we only support publicly available Docker images . If you need private Docker images, you can log in to any Docker registry from a worker in a private worker pool .","title":"Limit push access"},{"location":"integrations/webhooks.html","text":"Webhooks \u00bb Spacelift can be configured to send webhook notifications for various events to an HTTP endpoint of your choice. Setting up webhooks \u00bb Webhooks can be set up by Spacelift administrators. They can be easily created or modified in the Webhooks section. Navigate to the webhooks section \u00bb Fill required fields \u00bb Info The Secret parameter is optional and is used to validate the received payload. You can learn more about it in the validating payload section. Reference webhooks in policy rules \u00bb Webhook messages are delivered using the notification policy . When defining rules, the policy expects you to reference the webhook by its ID which you can copy from the webhook list view: Exploring deliveries \u00bb Webhook deliveries and their response statuses are stored and can be explored by selecting a specific webhook and viewing its details. You'll be presented with a list of deliveries, their status codes and when they happened. You can also click on each delivery to view more details about it: Default webhook payloads \u00bb The following section documents the default webhook payloads sent for each event type. However, if required, webhook payloads can be customized via a notification policy . Run events \u00bb Here's an example of the default webhook payload for a notification about a finished tracked run: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"account\" : \"spacelift-io\" , \"state\" : \"FINISHED\" , \"stateVersion\" : 4 , \"timestamp\" : 1596979684 , \"run\" : { \"id\" : \"01EF9PFPNFFM2MQXTJKHK1B869\" , \"branch\" : \"master\" , \"commit\" : { \"authorLogin\" : \"marcinwyszynski\" , \"authorName\" : \"Marcin Wyszynski\" , \"hash\" : \"0ee3a3b7266daf5a1d44a193a0f48ce995fa75eb\" , \"message\" : \"Update demo.tf\" , \"timestamp\" : 1596705932 , \"url\" : \"https://github.com/spacelift-io/demo/commit/0ee3a3b7266daf5a1d44a193a0f48ce995fa75eb\" }, \"createdAt\" : 1596979665 , \"delta\" : { \"added\" : 0 , \"changed\" : 0 , \"deleted\" : 0 , \"resources\" : 1 }, \"triggeredBy\" : \"marcinw@spacelift.io\" , \"type\" : \"TRACKED\" }, \"stack\" : { \"id\" : \"spacelift-demo\" , \"name\" : \"Spacelift demo\" , \"description\" : \"\" , \"labels\" : [] } } The payload consists of a few fields: account is the name (subdomain) of the account generating the webhook - useful in case you're pointing webhooks from various accounts at the same endpoint; state is a string representation of the run state at the time of the notification being triggered; stateVersion is the ordinal number of the state, which can be used to ensure that notifications that may be sent or received out-of-order are correctly processed; timestamp is the unix timestamp of the state transition; run contains information about the run, its associated commit and delta (if any); stack contains some basic information about the parent Stack of the run ; Internal error events \u00bb 1 2 3 4 5 6 7 { \"title\" : \"Invalid Stack Slug Triggered\" , \"body\" : \"policy tried to trigger Stack 'this-is-not-a-stack' which either doesn't exist or this policy doesn't have access to\" , \"error\" : \"policy triggered for Stack that doesn't exist\" , \"severity\" : \"ERROR\" , \"account\" : \"spacelift-io\" } Internal errors will always have the same fields set and some of them will be static for an event: title is the title (summary) of the error. body is the is the full explanation of what went wrong. error is a description of the error that happened. severity can be one of three different constants: INFO , WARNING , ERROR . account is the account for which the error happened. Validating payload \u00bb In order to validate the incoming payload, you will need to have the secret handy - the one you've generated yourself when creating or updating the webhook. Every webhook payload comes with two signature headers generated from the combination of the secret and payload. X-Signature header contains the SHA1 hash of the payload, while X-Signature-256 contains the SHA256 hash. We're using the exact same mechanism as GitHub to generate signatures, please refer to GitHub docs for details. Attaching webhooks to stacks \u00bb Warning We recommend that you use notification policies to route stack events to your webhooks. Stack webhook integrations are provided for backwards compatibility. Webhooks can be set up by Spacelift administrators on per-stack basis. In order to do that, navigate to the Integrations section of the stack settings view. From the list of available integrations, select the Add webhook option: Info You can set up as many webhooks for a Stack as you need, though each one must have a unique endpoint. You will be presented with a simple setup form that requires you to provide and endpoint to which the payload is sent, and an optional secret that you can use to validate that the incoming requests are indeed coming from Spacelift: Please note that it's up to you to come up with a reasonably non-obvious secret. Once saved, the webhook will appear on the list of integrations: Info Unlike some other secrets in Spacelift, the webhook secret can be viewed by anyone with read access to the stack. If you suspect foul play, consider regenerating your secret. By default webhooks are enabled which means that they are triggered every time there's a run state change event on the Stack they are attached to. If you want to temporarily disable some of the endpoints, you can do that without having to delete the whole integration. To do that, just click on the Edit button on the desired webhook integration section: ...and click on the Enabled toggle to see it going gray : Reversing this action is equally simple - just follow the same steps making sure that the toggle goes green :","title":"Webhooks"},{"location":"integrations/webhooks.html#webhooks","text":"Spacelift can be configured to send webhook notifications for various events to an HTTP endpoint of your choice.","title":"Webhooks"},{"location":"integrations/webhooks.html#setting-up-webhooks","text":"Webhooks can be set up by Spacelift administrators. They can be easily created or modified in the Webhooks section.","title":"Setting up webhooks"},{"location":"integrations/webhooks.html#navigate-to-the-webhooks-section","text":"","title":"Navigate to the webhooks section"},{"location":"integrations/webhooks.html#fill-required-fields","text":"Info The Secret parameter is optional and is used to validate the received payload. You can learn more about it in the validating payload section.","title":"Fill required fields"},{"location":"integrations/webhooks.html#reference-webhooks-in-policy-rules","text":"Webhook messages are delivered using the notification policy . When defining rules, the policy expects you to reference the webhook by its ID which you can copy from the webhook list view:","title":"Reference webhooks in policy rules"},{"location":"integrations/webhooks.html#exploring-deliveries","text":"Webhook deliveries and their response statuses are stored and can be explored by selecting a specific webhook and viewing its details. You'll be presented with a list of deliveries, their status codes and when they happened. You can also click on each delivery to view more details about it:","title":"Exploring deliveries"},{"location":"integrations/webhooks.html#default-webhook-payloads","text":"The following section documents the default webhook payloads sent for each event type. However, if required, webhook payloads can be customized via a notification policy .","title":"Default webhook payloads"},{"location":"integrations/webhooks.html#run-events","text":"Here's an example of the default webhook payload for a notification about a finished tracked run: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"account\" : \"spacelift-io\" , \"state\" : \"FINISHED\" , \"stateVersion\" : 4 , \"timestamp\" : 1596979684 , \"run\" : { \"id\" : \"01EF9PFPNFFM2MQXTJKHK1B869\" , \"branch\" : \"master\" , \"commit\" : { \"authorLogin\" : \"marcinwyszynski\" , \"authorName\" : \"Marcin Wyszynski\" , \"hash\" : \"0ee3a3b7266daf5a1d44a193a0f48ce995fa75eb\" , \"message\" : \"Update demo.tf\" , \"timestamp\" : 1596705932 , \"url\" : \"https://github.com/spacelift-io/demo/commit/0ee3a3b7266daf5a1d44a193a0f48ce995fa75eb\" }, \"createdAt\" : 1596979665 , \"delta\" : { \"added\" : 0 , \"changed\" : 0 , \"deleted\" : 0 , \"resources\" : 1 }, \"triggeredBy\" : \"marcinw@spacelift.io\" , \"type\" : \"TRACKED\" }, \"stack\" : { \"id\" : \"spacelift-demo\" , \"name\" : \"Spacelift demo\" , \"description\" : \"\" , \"labels\" : [] } } The payload consists of a few fields: account is the name (subdomain) of the account generating the webhook - useful in case you're pointing webhooks from various accounts at the same endpoint; state is a string representation of the run state at the time of the notification being triggered; stateVersion is the ordinal number of the state, which can be used to ensure that notifications that may be sent or received out-of-order are correctly processed; timestamp is the unix timestamp of the state transition; run contains information about the run, its associated commit and delta (if any); stack contains some basic information about the parent Stack of the run ;","title":"Run events"},{"location":"integrations/webhooks.html#internal-error-events","text":"1 2 3 4 5 6 7 { \"title\" : \"Invalid Stack Slug Triggered\" , \"body\" : \"policy tried to trigger Stack 'this-is-not-a-stack' which either doesn't exist or this policy doesn't have access to\" , \"error\" : \"policy triggered for Stack that doesn't exist\" , \"severity\" : \"ERROR\" , \"account\" : \"spacelift-io\" } Internal errors will always have the same fields set and some of them will be static for an event: title is the title (summary) of the error. body is the is the full explanation of what went wrong. error is a description of the error that happened. severity can be one of three different constants: INFO , WARNING , ERROR . account is the account for which the error happened.","title":"Internal error events"},{"location":"integrations/webhooks.html#validating-payload","text":"In order to validate the incoming payload, you will need to have the secret handy - the one you've generated yourself when creating or updating the webhook. Every webhook payload comes with two signature headers generated from the combination of the secret and payload. X-Signature header contains the SHA1 hash of the payload, while X-Signature-256 contains the SHA256 hash. We're using the exact same mechanism as GitHub to generate signatures, please refer to GitHub docs for details.","title":"Validating payload"},{"location":"integrations/webhooks.html#attaching-webhooks-to-stacks","text":"Warning We recommend that you use notification policies to route stack events to your webhooks. Stack webhook integrations are provided for backwards compatibility. Webhooks can be set up by Spacelift administrators on per-stack basis. In order to do that, navigate to the Integrations section of the stack settings view. From the list of available integrations, select the Add webhook option: Info You can set up as many webhooks for a Stack as you need, though each one must have a unique endpoint. You will be presented with a simple setup form that requires you to provide and endpoint to which the payload is sent, and an optional secret that you can use to validate that the incoming requests are indeed coming from Spacelift: Please note that it's up to you to come up with a reasonably non-obvious secret. Once saved, the webhook will appear on the list of integrations: Info Unlike some other secrets in Spacelift, the webhook secret can be viewed by anyone with read access to the stack. If you suspect foul play, consider regenerating your secret. By default webhooks are enabled which means that they are triggered every time there's a run state change event on the Stack they are attached to. If you want to temporarily disable some of the endpoints, you can do that without having to delete the whole integration. To do that, just click on the Edit button on the desired webhook integration section: ...and click on the Enabled toggle to see it going gray : Reversing this action is equally simple - just follow the same steps making sure that the toggle goes green :","title":"Attaching webhooks to stacks"},{"location":"integrations/chatops/msteams.html","text":"Microsoft Teams \u00bb Microsoft Teams is a Slack alternative and a part of the Microsoft Office 365 suite. It's a chat-based workspace where teams can organize and discuss their work. Many DevOps teams use it to communicate and collaborate on infrastructure and application deployments. Hence, Spacelift has a first-class integration with Microsoft Teams. The integration creates a webhook in Spacelift that will send notifications to a Microsoft Teams channel when: a tracked run needs confirmation ; a tracked run or a task finishes; a module version succeeds or fails; Based on this configuration, the module will send notifications that look like these: Prerequisites \u00bb In order to set up the integration, you'll to perform some manual steps in Microsoft Teams. The Spacelift end of the integration is handled programmatically, by a Terraform module . In Microsoft Teams \u00bb In order to set up the integration, you'll need to create a Microsoft Teams webhook and copy its URL. You can do this by following these steps: Open the channel in which you want to receive notifications from Spacelift. Click the ellipsis (...) next to the channel name and select Connectors Search for Incoming Webhook and click Configure . Click Add to create the webhook. Copy the webhook URL, you'll need it in the next step. In Spacelift \u00bb The Teams integration is based on our notification policy feature, which requires at least an active Cloud tier subscription. While building a notification-based Teams integration from scratch is possible, we've created a Terraform module that will set up all the necessary integration elements for you. This module will only create Spacelift assets: a notification policy that will send data to Microsoft Teams; a webhook endpoint that serve as a notification target for the policy; Monitoring and troubleshooting \u00bb Once the integration is set up, you can monitor the notifications in the My channel channel in Microsoft Teams. You can also monitor the notifications in the corresponding notification policy and its webhook endpoint in Spacelift.","title":"Microsoft Teams"},{"location":"integrations/chatops/msteams.html#microsoft-teams","text":"Microsoft Teams is a Slack alternative and a part of the Microsoft Office 365 suite. It's a chat-based workspace where teams can organize and discuss their work. Many DevOps teams use it to communicate and collaborate on infrastructure and application deployments. Hence, Spacelift has a first-class integration with Microsoft Teams. The integration creates a webhook in Spacelift that will send notifications to a Microsoft Teams channel when: a tracked run needs confirmation ; a tracked run or a task finishes; a module version succeeds or fails; Based on this configuration, the module will send notifications that look like these:","title":"Microsoft Teams"},{"location":"integrations/chatops/msteams.html#prerequisites","text":"In order to set up the integration, you'll to perform some manual steps in Microsoft Teams. The Spacelift end of the integration is handled programmatically, by a Terraform module .","title":"Prerequisites"},{"location":"integrations/chatops/msteams.html#in-microsoft-teams","text":"In order to set up the integration, you'll need to create a Microsoft Teams webhook and copy its URL. You can do this by following these steps: Open the channel in which you want to receive notifications from Spacelift. Click the ellipsis (...) next to the channel name and select Connectors Search for Incoming Webhook and click Configure . Click Add to create the webhook. Copy the webhook URL, you'll need it in the next step.","title":"In Microsoft Teams"},{"location":"integrations/chatops/msteams.html#in-spacelift","text":"The Teams integration is based on our notification policy feature, which requires at least an active Cloud tier subscription. While building a notification-based Teams integration from scratch is possible, we've created a Terraform module that will set up all the necessary integration elements for you. This module will only create Spacelift assets: a notification policy that will send data to Microsoft Teams; a webhook endpoint that serve as a notification target for the policy;","title":"In Spacelift"},{"location":"integrations/chatops/msteams.html#monitoring-and-troubleshooting","text":"Once the integration is set up, you can monitor the notifications in the My channel channel in Microsoft Teams. You can also monitor the notifications in the corresponding notification policy and its webhook endpoint in Spacelift.","title":"Monitoring and troubleshooting"},{"location":"integrations/chatops/slack.html","text":"Slack \u00bb At Spacelift, we're using Slack for internal communication. And we know that other tech companies do the same, so we've created a first-class integration that we ourselves enjoy using. Here are examples of messages the Spacelift application sends to Slack; Linking your Spacelift account to the Slack workspace \u00bb As a Spacelift and Slack admin, you can link your Spacelift account to the Slack workspace by going to the Slack section of the Settings screen. The integration is an OAuth2 exchange which installs Slack Spacelift app in your workspace. Once you install the Spacelift app, the account-level integration is finished and the Slack section of the Settings screen informs you that the two are talking to one another: Installing the Slack app doesn't automatically cause Spacelift to flood your Slack channels with torrents of notifications. These are set up on a per-stack basis using Slack commands and the management uses the Slack interface. Though before that happens, you need to allow requests coming from Slack to access Spacelift stacks. Managing access to Stacks with policies \u00bb Our Slack integration allows users in the Slack workspace to interact with stacks by adding the ability to change their run state or view changes that are planned or were applied. Similar to regular requests to our HTTP APIs, requests and actions coming from Slack are subject to the policy-based access validation. If you haven't had a chance to review the policy and Spaces documentation yet, please do it now before proceeding any further - you're risking a chance of getting lost. Available actions \u00bb Currently, we allow: Confirming and discarding tracked runs. Viewing planned and actual changes. Both of these actions require specific permissions to be configured using the login policy. Confirming or discarding runs requires Write level permissions while viewing changes requires Read level permissions. The documentation sections about policies below describe how to setup and manage these permissions. Info The default login policy decision for Slack requests is to deny all access. Login policy \u00bb Using login policies is the preferred way to control access for the Slack integration. Using them you can control who can access stacks which are in a specific Space . They allow for granular space access control using the provided policy data such as slack workspace details, Slack team information and user which interacted with the message data. Using the Login policy you can define rules which would allow to have Read or Write level permissions for certain actions. Login policies also don't need to be attached to a specific stack in order to work but are instead evaluated during every stack mutation or read attempt from the integration. Warning It's important to know that if you have multiple login policies, failing to evaluate one of them or having at least one of them result in a deny decision after the evaluation is done, will result in the overall decision being a deny all . Here is an example of data which the login policy receives when evaluating stack access for the integration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 { \"request\" : { \"timestamp_ns\" : \"<int> - a unix timestamp for when this request happened\" }, \"slack\" : { \"channel\" : { \"id\" : \"<string> - a channel ID, example: C042YPN0000\" , \"name\" : \"<string> - a channel name, example: spc-finished\" }, \"command\" : \"<string>\" , \"team\" : { \"id\" : \"<string> - a team ID for which this user belongs, example: T0431750000\" , \"name\" : \"<string> - a team name represented as string, example: slack-workspace-name\" }, \"user\" : { \"deleted\" : \"<boolean>\" , \"display_name\" : \"<string>\" , \"enterprise\" : { \"enterprise_id\" : \"<string>\" , \"enterprise_name\" : \"<string>\" , \"id\" : \"<string>\" , \"is_admin\" : \"<boolean>\" , \"is_owner\" : \"<boolean>\" , }, \"teams\" : { \"id\" : \"<string>\" , \"name\" : \"<string>\" }, \"id\" : \"<string> - a user which initially request ID, example: C042YPN1111\" , \"is_admin\" : \"<boolean> - is the user an admin\" , \"is_owner\" : \"<boolean> - is the workspace owner\" , \"is_primary_owner\" : \"<boolean>\" , \"is_restricted\" : \"<boolean>\" , \"is_stranger\" : \"<boolean>\" , \"is_ultra_restricted\" : \"<boolean>\" , \"has_2fa\" : \"<boolean>\" \"real_name\" : \"<string>\" , \"tz\" : \"<string>\" } }, \"spaces\" : [{ \"id\" : \"<string> - an ID for a Space in spacelift\" , \"labels\" : \"<stringArray> - a list of labels attached to this space\" , \"name\" : \"<string> - name for a Space in spacelift\" }, { \"id\" : \"<string>\" , \"labels\" : \"<stringArray>\" , \"name\" : \"<string>\" }] } Info The slack object in the policy input data is built using Slack provided data. See their official documentation for always up-to-date and full explanation of the slack object fields. Using the above data we can write policies which only allow for a specific user or slack team to access specific spaces in which your stacks reside. For example here is a policy which would allow anyone from a specific slack team to alter stacks in a particular space: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package spacelift # Allow access for anyone in team X allow { input . slack . team . id == \"X\" } # Deny access for everyone except team X deny { input . slack . team . id != \"\" input . slack . team . id != \"X\" } # Grant write access to stacks in Space Y for anyone in team X space_write [ \"Y\" ] { input . slack . team . id == \"X\" } Available slash commands \u00bb Warning It's recommended to instead use the notification policy in order to manage slack messages received from Spacelift. These slash commands are deprecated . Also, please note that slash commands only work if your Spacelift instance is publicly accessible by Slack. If your Spacelift installation uses an internal load balancer, for example, slash commands will not work. Three slash commands are currently available: /spacelift subscribe $stackId - subscribes a particular Slack channel to run state changes for a given Stack - requires ; /spacelift unsubscribe $stackId - unsubscribes a particular Slack channel from run state changes for a given Stack; /spacelift trigger $stackId - triggers a tracked run for the specified Stack;","title":"Slack"},{"location":"integrations/chatops/slack.html#slack","text":"At Spacelift, we're using Slack for internal communication. And we know that other tech companies do the same, so we've created a first-class integration that we ourselves enjoy using. Here are examples of messages the Spacelift application sends to Slack;","title":"Slack"},{"location":"integrations/chatops/slack.html#linking-your-spacelift-account-to-the-slack-workspace","text":"As a Spacelift and Slack admin, you can link your Spacelift account to the Slack workspace by going to the Slack section of the Settings screen. The integration is an OAuth2 exchange which installs Slack Spacelift app in your workspace. Once you install the Spacelift app, the account-level integration is finished and the Slack section of the Settings screen informs you that the two are talking to one another: Installing the Slack app doesn't automatically cause Spacelift to flood your Slack channels with torrents of notifications. These are set up on a per-stack basis using Slack commands and the management uses the Slack interface. Though before that happens, you need to allow requests coming from Slack to access Spacelift stacks.","title":"Linking your Spacelift account to the Slack workspace"},{"location":"integrations/chatops/slack.html#managing-access-to-stacks-with-policies","text":"Our Slack integration allows users in the Slack workspace to interact with stacks by adding the ability to change their run state or view changes that are planned or were applied. Similar to regular requests to our HTTP APIs, requests and actions coming from Slack are subject to the policy-based access validation. If you haven't had a chance to review the policy and Spaces documentation yet, please do it now before proceeding any further - you're risking a chance of getting lost.","title":"Managing access to Stacks with policies"},{"location":"integrations/chatops/slack.html#available-actions","text":"Currently, we allow: Confirming and discarding tracked runs. Viewing planned and actual changes. Both of these actions require specific permissions to be configured using the login policy. Confirming or discarding runs requires Write level permissions while viewing changes requires Read level permissions. The documentation sections about policies below describe how to setup and manage these permissions. Info The default login policy decision for Slack requests is to deny all access.","title":"Available actions"},{"location":"integrations/chatops/slack.html#login-policy","text":"Using login policies is the preferred way to control access for the Slack integration. Using them you can control who can access stacks which are in a specific Space . They allow for granular space access control using the provided policy data such as slack workspace details, Slack team information and user which interacted with the message data. Using the Login policy you can define rules which would allow to have Read or Write level permissions for certain actions. Login policies also don't need to be attached to a specific stack in order to work but are instead evaluated during every stack mutation or read attempt from the integration. Warning It's important to know that if you have multiple login policies, failing to evaluate one of them or having at least one of them result in a deny decision after the evaluation is done, will result in the overall decision being a deny all . Here is an example of data which the login policy receives when evaluating stack access for the integration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 { \"request\" : { \"timestamp_ns\" : \"<int> - a unix timestamp for when this request happened\" }, \"slack\" : { \"channel\" : { \"id\" : \"<string> - a channel ID, example: C042YPN0000\" , \"name\" : \"<string> - a channel name, example: spc-finished\" }, \"command\" : \"<string>\" , \"team\" : { \"id\" : \"<string> - a team ID for which this user belongs, example: T0431750000\" , \"name\" : \"<string> - a team name represented as string, example: slack-workspace-name\" }, \"user\" : { \"deleted\" : \"<boolean>\" , \"display_name\" : \"<string>\" , \"enterprise\" : { \"enterprise_id\" : \"<string>\" , \"enterprise_name\" : \"<string>\" , \"id\" : \"<string>\" , \"is_admin\" : \"<boolean>\" , \"is_owner\" : \"<boolean>\" , }, \"teams\" : { \"id\" : \"<string>\" , \"name\" : \"<string>\" }, \"id\" : \"<string> - a user which initially request ID, example: C042YPN1111\" , \"is_admin\" : \"<boolean> - is the user an admin\" , \"is_owner\" : \"<boolean> - is the workspace owner\" , \"is_primary_owner\" : \"<boolean>\" , \"is_restricted\" : \"<boolean>\" , \"is_stranger\" : \"<boolean>\" , \"is_ultra_restricted\" : \"<boolean>\" , \"has_2fa\" : \"<boolean>\" \"real_name\" : \"<string>\" , \"tz\" : \"<string>\" } }, \"spaces\" : [{ \"id\" : \"<string> - an ID for a Space in spacelift\" , \"labels\" : \"<stringArray> - a list of labels attached to this space\" , \"name\" : \"<string> - name for a Space in spacelift\" }, { \"id\" : \"<string>\" , \"labels\" : \"<stringArray>\" , \"name\" : \"<string>\" }] } Info The slack object in the policy input data is built using Slack provided data. See their official documentation for always up-to-date and full explanation of the slack object fields. Using the above data we can write policies which only allow for a specific user or slack team to access specific spaces in which your stacks reside. For example here is a policy which would allow anyone from a specific slack team to alter stacks in a particular space: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package spacelift # Allow access for anyone in team X allow { input . slack . team . id == \"X\" } # Deny access for everyone except team X deny { input . slack . team . id != \"\" input . slack . team . id != \"X\" } # Grant write access to stacks in Space Y for anyone in team X space_write [ \"Y\" ] { input . slack . team . id == \"X\" }","title":"Login policy"},{"location":"integrations/chatops/slack.html#available-slash-commands","text":"Warning It's recommended to instead use the notification policy in order to manage slack messages received from Spacelift. These slash commands are deprecated . Also, please note that slash commands only work if your Spacelift instance is publicly accessible by Slack. If your Spacelift installation uses an internal load balancer, for example, slash commands will not work. Three slash commands are currently available: /spacelift subscribe $stackId - subscribes a particular Slack channel to run state changes for a given Stack - requires ; /spacelift unsubscribe $stackId - unsubscribes a particular Slack channel from run state changes for a given Stack; /spacelift trigger $stackId - triggers a tracked run for the specified Stack;","title":"Available slash commands"},{"location":"integrations/cloud-providers/index.html","text":"Cloud Integrations \u00bb Cloud integrations allow Spacelift to manage your resources without the need for long-lived static credentials. When using infrastructure-as-code automation tools such as Terraform, AWS CloudFormation, or Pulumi, these tools typically require credentials to execute. Usually, these are very powerful credentials, administrative credentials, sometimes. And these can do a lot of damage . Typically, you'd provide those credentials statically - think AWS credentials, GCP service keys, etc. This is dangerous, and against security best practices. That's why Spacelift integrates with identity management systems from major cloud providers to dynamically generate short-lived access tokens that can be used to configure their corresponding Terraform providers. Currently AWS is natively supported. A generic OpenID Connect integration is also available to work with any compatible service provider. Hint This feature is designed for clients using the shared public worker pool. When hosting Spacelift workers on your infrastructure you can use your cloud providers' ambient credentials (eg. EC2 instance role or EKS worker role on AWS).","title":"Cloud Integrations"},{"location":"integrations/cloud-providers/index.html#cloud-integrations","text":"Cloud integrations allow Spacelift to manage your resources without the need for long-lived static credentials. When using infrastructure-as-code automation tools such as Terraform, AWS CloudFormation, or Pulumi, these tools typically require credentials to execute. Usually, these are very powerful credentials, administrative credentials, sometimes. And these can do a lot of damage . Typically, you'd provide those credentials statically - think AWS credentials, GCP service keys, etc. This is dangerous, and against security best practices. That's why Spacelift integrates with identity management systems from major cloud providers to dynamically generate short-lived access tokens that can be used to configure their corresponding Terraform providers. Currently AWS is natively supported. A generic OpenID Connect integration is also available to work with any compatible service provider. Hint This feature is designed for clients using the shared public worker pool. When hosting Spacelift workers on your infrastructure you can use your cloud providers' ambient credentials (eg. EC2 instance role or EKS worker role on AWS).","title":"Cloud Integrations"},{"location":"integrations/cloud-providers/aws.html","text":"Amazon Web Services (AWS) \u00bb Let's Explain \u00bb The AWS integration allows either Spacelift runs or tasks to automatically assume an IAM role in your AWS account, and in the process, generate a set of temporary credentials. These credentials are then exposed as computed environment variables during the run/task that takes place on the particular Spacelift stack that the integration is attached to. AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SECURITY_TOKEN AWS_SESSION_TOKEN This is enough for both the AWS Terraform provider and/or Amazon S3 state backend to generate a fully authenticated AWS session without further configuration. However, you will likely need to select one of the available regions with the former. Usage \u00bb To utilize the AWS integration, you need to set up at least one cloud integration, and then attach that integration to any stacks that need it. Please follow the Setup Guide for more information on this process. Trust Policy \u00bb When setting up a Spacelift AWS Cloud Integration you need to specify the ARN of an IAM Role to use. The Trust Policy for this role must be configured to allow Spacelift to assume the role and generate temporary credentials. When completing the role assumption, Spacelift will pass extra information in the ExternalId attribute, allowing you to optionally add additional layers of security to your role. External ID Format: <spacelift-account-name>@<integration-id>@<stack-slug>@<read|write> <spacelift-account-name> : the name of the Spacelift account that initiated the role assumption. <integration-id> : the ID of the AWS Cloud Integration that initiated the role assumption. <stack-slug> : the slug of the stack that the AWS Cloud Integration is attached to, that initiated the role assumption. <read|write> : set to either read or write based upon the event occurring that has initiated the role assumption. The Planning phase utilizes read while the Applying phase utilizes write . Setup Guide \u00bb Prerequisites: The ability to create IAM Roles in your AWS account. Admin access to your Spacelift account. Setup a Role in AWS \u00bb Before creating the Spacelift AWS integration, you need to have an AWS IAM Role within your AWS account that the cloud integration will use. Within your AWS account, navigate to AWS IAM and click the Create role button. Configure Trust Policy \u00bb Next, we want to configure the Trust Policy for the role to allow Spacelift to assume the role. Here's an example trust policy statement you can use, that allows any stack within your Spacelift account to use this IAM Role: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringLike\" : { \"sts:ExternalId\" : \"yourSpaceliftAccountName@*\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"324880187172\" } } ] } Info Be sure to replace yourSpaceliftAccountName in the example above with your actual Spacelift account name. Optionally Configure Further Constraints on the Trust Policy \u00bb Info By default, Spacelift passes the ExternalId value in this format: <spacelift-account-name>@<integration-id>@<stack-slug>@<read|write> Knowing the format of the External ID passed by Spacelift, you can further secure your IAM Role trust policies if you desire a deeper level of granular security. For example, you may wish to lock down an IAM Role so that it can only be used by a specific stack. The following example shows how to lock down an IAM Role so that it can only be assumed by the stack stack-a in a Spacelift account called example : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringLike\" : { \"sts:ExternalId\" : \"example@*@stack-a@*\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"324880187172\" } } ] } Configure Role Permissions \u00bb Next, you need to attach at least one IAM Policy to your IAM Role to provide it with sufficient permissions to deploy any resources that your IaC code defines. Info For Terraform users that are managing their own state file, don't forget to give your role sufficient permissions to access your state (Terraform documents the permissions required for S3-managed state here , and for DynamoDB state locking here ). Create IAM Role \u00bb Once you have your IAM Role's trust policy and IAM Policies configured, you can finish creating the role. Take a note of the IAM Role ARN, as you'll need this when setting up the integration in Spacelift in the next section. Navigate to Cloud Integrations \u00bb Now that you have an IAM Role created, navigate to the Cloud Integration page from the Spacelift navigation sidebar. Create an Integration \u00bb Click the add your first integration button to begin the creation of your first integration. When creating an integration, you will immediately notice that you need to specify two required fields: Name and Role ARN. Give the integration a name of your choosing, and paste in the ARN of the IAM Role that you just created. If you enable the assume role on worker option, the role assumption will be performed on your private worker rather than at Spacelift's end. When role assumption on the worker is enabled, you can also optionally specify a custom External ID to use during role assumption. Info When creating your role in AWS, you need to ensure the role has a trust policy that allows Spacelift to assume the role to generate temporary credentials for runs. Assuming you are following this guide, you should have configured this in the previous section . Using the Integration \u00bb Now that the integration has been created, you need to attach it to one or more stacks. To do this, navigate to a stack that you want to attach your integration to: Next, go to the stack's settings: Choose the integrations tab: Select the AWS option from the drop down, choose your integration, and select whether it should be used for read, write or both read and write phases: Info Once you have chosen your integration and specified whether it will be used for read or write phases, an example trust relationship statement will be displayed. This shows an example of how to configure your role for use by this exact stack, and based on whether the integration is being attached for read or write phases. This policy statement is provided for convenience only, and you can safely ignore it if you have already configured your trust relationship for your role. Read vs Write \u00bb You can attach an AWS integration as read, write or read-write, and you can attach at most two integrations to any single stack. Read indicates that this integration will be used during read phases of runs (for example, plans), and Write indicates that this integration will be used during write phases of runs (for example, applies). Role Assumption Verification \u00bb If the Cloud Integration has the \"Assume Role on Worker\" setting disabled, Spacelift will verify the role assumption as soon as you click the attach button. If role assumption succeeds, it will try to assume the role without the unique external ID , and this time it expects to fail . If Spacelift fails the latter check, we consider the integration is safely configured. Success This somewhat counterintuitive extra check is to prevent against malicious takeover of your account by someone who happens to know your AWS account ID, which isn't all that secret, really. The security vulnerability we're addressing here is known as the confused deputy problem . Programmatic Setup \u00bb You can also use the Spacelift Terraform provider in order to create an AWS Cloud integration from an administrative stack , including the trust relationship. Note that in order to do that, your administrative stack will require AWS credentials itself, and ones powerful enough to be able to deal with IAM. Here's a little example of what that might look like to create a Cloud Integration programmatically: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 data \"aws_caller_identity\" \"current\" {} locals { role_name = \"example-role\" role_arn = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\" } resource \"spacelift_stack\" \"this\" { name = \"Example Stack\" repository = \"your-awesome-repo\" branch = \"main\" } resource \"spacelift_aws_integration\" \"this\" { name = local.role_name # We need to set this manually rather than referencing the role to avoid a circular dependency role_arn = local.role_arn generate_credentials_in_worker = false } # The spacelift_aws_integration_attachment_external_id data source is used to help generate a trust policy for the integration data \"spacelift_aws_integration_attachment_external_id\" \"this\" { integration_id = spacelift_aws_integration.this.id stack_id = spacelift_stack.this.id read = true write = true } resource \"aws_iam_role\" \"this\" { name = local.role_name assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ jsondecode ( data.spacelift_aws_integration_attachment_external_id.this.assume_role_policy_statement ), ] }) } resource \"aws_iam_role_policy_attachment\" \"this\" { policy_arn = \"arn:aws:iam::aws:policy/PowerUserAccess\" role = aws_iam_role.this.name } resource \"spacelift_aws_integration_attachment\" \"this\" { integration_id = spacelift_aws_integration.this.id stack_id = spacelift_stack.this.id read = true write = true # The role needs to exist before we attach since we test role assumption during attachment. depends_on = [ aws_iam_role.this ] } Info Please always refer to the provider documentation for the most up-to-date documentation. Attaching a Role to Multiple Stacks \u00bb The previous example explained how to use the spacelift_aws_integration_attachment_external_id data-source to generate the assume role policy for using the integration with a single stack, but what if you want to attach the integration to multiple stacks? The simplest option would be to create multiple instances of the data-source - one for each stack - but you can also use a Terraform for_each condition to reduce the amount of code required: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 locals { role_name = \"multi-stack-integration\" role_arn = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\" # Define the stacks we want to attach the integration to stacks_to_attach = [ \"stack-1\", \"stack-2\", \"stack-3\" ] } data \"aws_caller_identity\" \"current\" {} data \"spacelift_account\" \"current\" {} resource \"spacelift_aws_integration\" \"integration\" { name = local.role_name role_arn = local.role_arn generate_credentials_in_worker = false } # Generate the External IDs required for creating our AssumeRole policy data \"spacelift_aws_integration_attachment_external_id\" \"integration\" { for_each = toset ( local.stacks_to_attach ) integration_id = spacelift_aws_integration.integration.id stack_id = each.key read = true write = true } resource \"aws_iam_role\" \"role\" { name = local.role_name assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ { Effect = \"Allow\" , \"Principal\" = { \"AWS\" : data.spacelift_account.current.aws_account_id }, \"Action\" = \"sts:AssumeRole\" , \"Condition\" = { \"StringEquals\" = { # Allow the external ID for any of the stacks to assume our role \"sts:ExternalId\" = [ for i in values ( data.spacelift_aws_integration_attachment_external_id.integration ) : i.external_id ], } } } ], }) } This will generate a trust relationship that looks something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::324880187172:root\" }, \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : [ \"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-1@write\" , \"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-2@write\" , \"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-3@write\" ] } } } ] } Is it safe? \u00bb Assuming roles and generating credentials on the private worker is perfectly safe . Those credentials are never leaked to us in any shape or form. Hence, the rest of this section discusses the trust relationship established between the Spacelift account and your AWS account for the purpose of dynamically generating short-lived credentials. So, how safe is that? Probably safer than storing static credentials in your stack environment. Unlike user keys that you'd normally have to use, role credentials are dynamically created and short-lived. We use the default expiration which is 1 hour , and do not store them anywhere. Leaking them accidentally through the logs is not an option either because we mask AWS credentials. The most tangible safety feature of the AWS integration is the breadcrumb trail it leaves in CloudTrail . Every resource change can be mapped to an individual Terraform run or task whose ID automatically becomes the username as the sts:AssumeRole API call with that ID as RoleSessionName . In conjunction with AWS tools like Config , it can be a very powerful compliance tool. Let's have a look at a CloudTrail event showing an IAM role being created by what seems to be a Spacelift run: 01DSJ63P40BAZY4VW8BXXG7M4K is indeed a run ID we can then trace back even further: Roles assuming other roles \u00bb OK, we get it. Using everyone's favorite Inception meme: Indeed, the AWS Terraform provider allows you to assume an IAM role during setup , effectively doing the same thing over again. This approach is especially useful if you want to control resources in multiple AWS accounts from a single Spacelift stack. This is totally fine - in IAM, roles can assume other roles, though what you need to do on your end is set up the trust relationship between the role you have Spacelift assume and the role for each provider instance to assume. But let's face it - at this level of sophistication, you sure know what you're doing. One bit you might not want to miss though, is the guaranteed ability to map the change to a particular run or task that we described in the previous section . One way of fixing that would be to use the TF_VAR_spacelift_run_id computed environment variable available to each Spacelift workflow. Conveniently, it's already a Terraform variable , so a setup like this should do the trick: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 variable \"spacelift_run_id\" {} # That's our default provider with credentials generated by Spacelift. provider \"aws\" {} # That's where Terraform needs to run sts:AssumeRole with your # Spacelift-generated credentials to obtain ones for the second account. provider \"aws\" { alias = \"second-account\" assume_role { role_arn = \"<up-to-you>\" session_name = var.spacelift_run_id external_id = \"<up-to-you>\" } } Migrating from \"Legacy\" integrations \u00bb Originally, when our AWS integration was created it did not support account-level integrations. Instead, each integration was created separately on a per-stack basis. This section provides instructions on how to migrate to the new account-level integrations, with different sets of instructions provided depending on whether you manage your integrations via the UI, or via the Spacelift Terraform Provider . Via the UI \u00bb Once the new Cloud Integrations UI is enabled for your account, you will still be able to view your legacy integrations in the \u201cIntegrations\u201d section of your stacks, but it will look slightly different: The first thing to do is to copy your Role ARN, then head over to the Cloud Integrations section. Once there, click on the \u201cAdd your first integration\u201d button to get started: Give your role a name, and paste in your existing ARN: The role name is completely up to you and is just used for managing the roles in your account. Maybe it\u2019s a role that allows management of S3 accounts, like in the example above, or maybe you have a single role that can manage your entire pre-production environment, in which case you might name it \u201cPreprod Writer\u201d. Now that you\u2019ve created an AWS integration, head back to your stack integration settings and click on the \u201cDelete\u201d button next to your legacy integration: Now choose the \u201cAWS\u201d option from the \u201cSelect integration to set up\u201d drop down, select the AWS integration you just created, and check the read and write checkboxes: You\u2019ll notice that we display an example trust relationship on this screen. You may or may not need to adjust the Trust Relationship of your role at this point. If you already use a StringLike condition with an <account-name>@* wildcard you shouldn\u2019t need to change anything. Click on the Attach button to attach your integration to your stack: Congrats - you\u2019ve now successfully migrated! Via the Terraform Provider \u00bb Migrating via the Terraform Provider is also very simple. The Terraform Provider now contains two new resources, along with an additional data source that you can use: spacelift_aws_integration - creates an account level integration. spacelift_aws_integration_attachment - attaches an integration to a stack. spacelift_aws_integration_attachment_external_id - a data source that can help you generate the correct trust relationship for attaching an integration to a stack. Starting Configuration \u00bb To begin with, let\u2019s assume you have the following Terraform configuration that creates an IAM role, a Stack, and connects the role to the stack: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 resource \"spacelift_stack\" \"this\" { name = \"AWS Integration Mig Test\" repository = \"spacelift-local\" branch = \"main\" project_root = \"stacks/aws-integration-testing\" } resource \"aws_iam_role\" \"this\" { name = \"adamc-v1-to-v2-migration-role\" assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ jsondecode ( spacelift_stack.this.aws_assume_role_policy_statement )] }) } resource \"aws_iam_role_policy\" \"this\" { name = aws_iam_role.this.name role = aws_iam_role.this.id policy = jsonencode ({ # Excluded for brevity }) } resource \"spacelift_aws_role\" \"this\" { stack_id = spacelift_stack.this.id role_arn = aws_iam_role.this.arn } Migration \u00bb To migrate to the new integration format, we need to take the following steps: Add a new AWS integration. (optional) Generate an AssumeRole policy for attaching it to our stack. (optional) Update the assume_role_policy on our aws_iam_role resource. Remove the spacelift_aws_role resource. Attach your new integration to your stack via the spacelift_aws_integration_attachment resource. Steps 2 and 3 are completely optional, and may not be required if you are using wildcard matching for the external ID, for example. Result \u00bb The following shows the finished result of the migrated Terraform: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 data \"aws_caller_identity\" \"current\" {} # In this example, we're using some locals to calculate the role ARN. This is to avoid a circular # dependency between the aws_iam_role resource and the spacelift_aws_integration resource. locals { role_name = \"adamc-v1-to-v2-migration-role\" role_arn = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\" } resource \"spacelift_stack\" \"this\" { name = \"AWS Integration Mig Test\" repository = \"spacelift-local\" branch = \"main\" project_root = \"stacks/aws-integration-testing\" } # 1. Add the new integration. resource \"spacelift_aws_integration\" \"this\" { name = local.role_name role_arn = local.role_arn generate_credentials_in_worker = false } # 2. Generate the AssumeRole policy for attaching it to our stack. data \"spacelift_aws_integration_attachment_external_id\" \"this\" { integration_id = spacelift_aws_integration.this.id stack_id = spacelift_stack.this.id read = true write = true } resource \"aws_iam_role\" \"this\" { name = local.role_name assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ # 3. Remove the old assume role policy statement, and add our new one. # jsondecode(spacelift_stack.this.aws_assume_role_policy_statement), jsondecode ( data.spacelift_aws_integration_attachment_external_id.this.assume_role_policy_statement ), ] }) } resource \"aws_iam_role_policy\" \"this\" { name = aws_iam_role.this.name role = aws_iam_role.this.id policy = jsonencode ({ # Excluded for brevity }) } # 4. Remove the old spacelift_aws_role resource # resource \"spacelift_aws_role\" \"this\" { # stack_id = spacelift_stack.this.id # role_arn = aws_iam_role.this.arn # } # 5. Add the new integration attachment resource \"spacelift_aws_integration_attachment\" \"this\" { integration_id = spacelift_aws_integration.this.id stack_id = spacelift_stack.this.id read = true write = true # The role needs to exist before we attach since we test role assumption during attachment. depends_on = [ aws_iam_role.this ] }","title":"Amazon Web Services (AWS)"},{"location":"integrations/cloud-providers/aws.html#amazon-web-services-aws","text":"","title":"Amazon Web Services (AWS)"},{"location":"integrations/cloud-providers/aws.html#lets-explain","text":"The AWS integration allows either Spacelift runs or tasks to automatically assume an IAM role in your AWS account, and in the process, generate a set of temporary credentials. These credentials are then exposed as computed environment variables during the run/task that takes place on the particular Spacelift stack that the integration is attached to. AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SECURITY_TOKEN AWS_SESSION_TOKEN This is enough for both the AWS Terraform provider and/or Amazon S3 state backend to generate a fully authenticated AWS session without further configuration. However, you will likely need to select one of the available regions with the former.","title":"Let's Explain"},{"location":"integrations/cloud-providers/aws.html#usage","text":"To utilize the AWS integration, you need to set up at least one cloud integration, and then attach that integration to any stacks that need it. Please follow the Setup Guide for more information on this process.","title":"Usage"},{"location":"integrations/cloud-providers/aws.html#trust-policy","text":"When setting up a Spacelift AWS Cloud Integration you need to specify the ARN of an IAM Role to use. The Trust Policy for this role must be configured to allow Spacelift to assume the role and generate temporary credentials. When completing the role assumption, Spacelift will pass extra information in the ExternalId attribute, allowing you to optionally add additional layers of security to your role. External ID Format: <spacelift-account-name>@<integration-id>@<stack-slug>@<read|write> <spacelift-account-name> : the name of the Spacelift account that initiated the role assumption. <integration-id> : the ID of the AWS Cloud Integration that initiated the role assumption. <stack-slug> : the slug of the stack that the AWS Cloud Integration is attached to, that initiated the role assumption. <read|write> : set to either read or write based upon the event occurring that has initiated the role assumption. The Planning phase utilizes read while the Applying phase utilizes write .","title":"Trust Policy"},{"location":"integrations/cloud-providers/aws.html#setup-guide","text":"Prerequisites: The ability to create IAM Roles in your AWS account. Admin access to your Spacelift account.","title":"Setup Guide"},{"location":"integrations/cloud-providers/aws.html#setup-a-role-in-aws","text":"Before creating the Spacelift AWS integration, you need to have an AWS IAM Role within your AWS account that the cloud integration will use. Within your AWS account, navigate to AWS IAM and click the Create role button.","title":"Setup a Role in AWS"},{"location":"integrations/cloud-providers/aws.html#configure-trust-policy","text":"Next, we want to configure the Trust Policy for the role to allow Spacelift to assume the role. Here's an example trust policy statement you can use, that allows any stack within your Spacelift account to use this IAM Role: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringLike\" : { \"sts:ExternalId\" : \"yourSpaceliftAccountName@*\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"324880187172\" } } ] } Info Be sure to replace yourSpaceliftAccountName in the example above with your actual Spacelift account name.","title":"Configure Trust Policy"},{"location":"integrations/cloud-providers/aws.html#optionally-configure-further-constraints-on-the-trust-policy","text":"Info By default, Spacelift passes the ExternalId value in this format: <spacelift-account-name>@<integration-id>@<stack-slug>@<read|write> Knowing the format of the External ID passed by Spacelift, you can further secure your IAM Role trust policies if you desire a deeper level of granular security. For example, you may wish to lock down an IAM Role so that it can only be used by a specific stack. The following example shows how to lock down an IAM Role so that it can only be assumed by the stack stack-a in a Spacelift account called example : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringLike\" : { \"sts:ExternalId\" : \"example@*@stack-a@*\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"324880187172\" } } ] }","title":"Optionally Configure Further Constraints on the Trust Policy"},{"location":"integrations/cloud-providers/aws.html#configure-role-permissions","text":"Next, you need to attach at least one IAM Policy to your IAM Role to provide it with sufficient permissions to deploy any resources that your IaC code defines. Info For Terraform users that are managing their own state file, don't forget to give your role sufficient permissions to access your state (Terraform documents the permissions required for S3-managed state here , and for DynamoDB state locking here ).","title":"Configure Role Permissions"},{"location":"integrations/cloud-providers/aws.html#create-iam-role","text":"Once you have your IAM Role's trust policy and IAM Policies configured, you can finish creating the role. Take a note of the IAM Role ARN, as you'll need this when setting up the integration in Spacelift in the next section.","title":"Create IAM Role"},{"location":"integrations/cloud-providers/aws.html#navigate-to-cloud-integrations","text":"Now that you have an IAM Role created, navigate to the Cloud Integration page from the Spacelift navigation sidebar.","title":"Navigate to Cloud Integrations"},{"location":"integrations/cloud-providers/aws.html#create-an-integration","text":"Click the add your first integration button to begin the creation of your first integration. When creating an integration, you will immediately notice that you need to specify two required fields: Name and Role ARN. Give the integration a name of your choosing, and paste in the ARN of the IAM Role that you just created. If you enable the assume role on worker option, the role assumption will be performed on your private worker rather than at Spacelift's end. When role assumption on the worker is enabled, you can also optionally specify a custom External ID to use during role assumption. Info When creating your role in AWS, you need to ensure the role has a trust policy that allows Spacelift to assume the role to generate temporary credentials for runs. Assuming you are following this guide, you should have configured this in the previous section .","title":"Create an Integration"},{"location":"integrations/cloud-providers/aws.html#using-the-integration","text":"Now that the integration has been created, you need to attach it to one or more stacks. To do this, navigate to a stack that you want to attach your integration to: Next, go to the stack's settings: Choose the integrations tab: Select the AWS option from the drop down, choose your integration, and select whether it should be used for read, write or both read and write phases: Info Once you have chosen your integration and specified whether it will be used for read or write phases, an example trust relationship statement will be displayed. This shows an example of how to configure your role for use by this exact stack, and based on whether the integration is being attached for read or write phases. This policy statement is provided for convenience only, and you can safely ignore it if you have already configured your trust relationship for your role.","title":"Using the Integration"},{"location":"integrations/cloud-providers/aws.html#read-vs-write","text":"You can attach an AWS integration as read, write or read-write, and you can attach at most two integrations to any single stack. Read indicates that this integration will be used during read phases of runs (for example, plans), and Write indicates that this integration will be used during write phases of runs (for example, applies).","title":"Read vs Write"},{"location":"integrations/cloud-providers/aws.html#role-assumption-verification","text":"If the Cloud Integration has the \"Assume Role on Worker\" setting disabled, Spacelift will verify the role assumption as soon as you click the attach button. If role assumption succeeds, it will try to assume the role without the unique external ID , and this time it expects to fail . If Spacelift fails the latter check, we consider the integration is safely configured. Success This somewhat counterintuitive extra check is to prevent against malicious takeover of your account by someone who happens to know your AWS account ID, which isn't all that secret, really. The security vulnerability we're addressing here is known as the confused deputy problem .","title":"Role Assumption Verification"},{"location":"integrations/cloud-providers/aws.html#programmatic-setup","text":"You can also use the Spacelift Terraform provider in order to create an AWS Cloud integration from an administrative stack , including the trust relationship. Note that in order to do that, your administrative stack will require AWS credentials itself, and ones powerful enough to be able to deal with IAM. Here's a little example of what that might look like to create a Cloud Integration programmatically: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 data \"aws_caller_identity\" \"current\" {} locals { role_name = \"example-role\" role_arn = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\" } resource \"spacelift_stack\" \"this\" { name = \"Example Stack\" repository = \"your-awesome-repo\" branch = \"main\" } resource \"spacelift_aws_integration\" \"this\" { name = local.role_name # We need to set this manually rather than referencing the role to avoid a circular dependency role_arn = local.role_arn generate_credentials_in_worker = false } # The spacelift_aws_integration_attachment_external_id data source is used to help generate a trust policy for the integration data \"spacelift_aws_integration_attachment_external_id\" \"this\" { integration_id = spacelift_aws_integration.this.id stack_id = spacelift_stack.this.id read = true write = true } resource \"aws_iam_role\" \"this\" { name = local.role_name assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ jsondecode ( data.spacelift_aws_integration_attachment_external_id.this.assume_role_policy_statement ), ] }) } resource \"aws_iam_role_policy_attachment\" \"this\" { policy_arn = \"arn:aws:iam::aws:policy/PowerUserAccess\" role = aws_iam_role.this.name } resource \"spacelift_aws_integration_attachment\" \"this\" { integration_id = spacelift_aws_integration.this.id stack_id = spacelift_stack.this.id read = true write = true # The role needs to exist before we attach since we test role assumption during attachment. depends_on = [ aws_iam_role.this ] } Info Please always refer to the provider documentation for the most up-to-date documentation.","title":"Programmatic Setup"},{"location":"integrations/cloud-providers/aws.html#attaching-a-role-to-multiple-stacks","text":"The previous example explained how to use the spacelift_aws_integration_attachment_external_id data-source to generate the assume role policy for using the integration with a single stack, but what if you want to attach the integration to multiple stacks? The simplest option would be to create multiple instances of the data-source - one for each stack - but you can also use a Terraform for_each condition to reduce the amount of code required: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 locals { role_name = \"multi-stack-integration\" role_arn = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\" # Define the stacks we want to attach the integration to stacks_to_attach = [ \"stack-1\", \"stack-2\", \"stack-3\" ] } data \"aws_caller_identity\" \"current\" {} data \"spacelift_account\" \"current\" {} resource \"spacelift_aws_integration\" \"integration\" { name = local.role_name role_arn = local.role_arn generate_credentials_in_worker = false } # Generate the External IDs required for creating our AssumeRole policy data \"spacelift_aws_integration_attachment_external_id\" \"integration\" { for_each = toset ( local.stacks_to_attach ) integration_id = spacelift_aws_integration.integration.id stack_id = each.key read = true write = true } resource \"aws_iam_role\" \"role\" { name = local.role_name assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ { Effect = \"Allow\" , \"Principal\" = { \"AWS\" : data.spacelift_account.current.aws_account_id }, \"Action\" = \"sts:AssumeRole\" , \"Condition\" = { \"StringEquals\" = { # Allow the external ID for any of the stacks to assume our role \"sts:ExternalId\" = [ for i in values ( data.spacelift_aws_integration_attachment_external_id.integration ) : i.external_id ], } } } ], }) } This will generate a trust relationship that looks something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::324880187172:root\" }, \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : [ \"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-1@write\" , \"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-2@write\" , \"spacelifter@01GE7K9SR2DQBCRQ90DH70JF6Y@stack-3@write\" ] } } } ] }","title":"Attaching a Role to Multiple Stacks"},{"location":"integrations/cloud-providers/aws.html#is-it-safe","text":"Assuming roles and generating credentials on the private worker is perfectly safe . Those credentials are never leaked to us in any shape or form. Hence, the rest of this section discusses the trust relationship established between the Spacelift account and your AWS account for the purpose of dynamically generating short-lived credentials. So, how safe is that? Probably safer than storing static credentials in your stack environment. Unlike user keys that you'd normally have to use, role credentials are dynamically created and short-lived. We use the default expiration which is 1 hour , and do not store them anywhere. Leaking them accidentally through the logs is not an option either because we mask AWS credentials. The most tangible safety feature of the AWS integration is the breadcrumb trail it leaves in CloudTrail . Every resource change can be mapped to an individual Terraform run or task whose ID automatically becomes the username as the sts:AssumeRole API call with that ID as RoleSessionName . In conjunction with AWS tools like Config , it can be a very powerful compliance tool. Let's have a look at a CloudTrail event showing an IAM role being created by what seems to be a Spacelift run: 01DSJ63P40BAZY4VW8BXXG7M4K is indeed a run ID we can then trace back even further:","title":"Is it safe?"},{"location":"integrations/cloud-providers/aws.html#roles-assuming-other-roles","text":"OK, we get it. Using everyone's favorite Inception meme: Indeed, the AWS Terraform provider allows you to assume an IAM role during setup , effectively doing the same thing over again. This approach is especially useful if you want to control resources in multiple AWS accounts from a single Spacelift stack. This is totally fine - in IAM, roles can assume other roles, though what you need to do on your end is set up the trust relationship between the role you have Spacelift assume and the role for each provider instance to assume. But let's face it - at this level of sophistication, you sure know what you're doing. One bit you might not want to miss though, is the guaranteed ability to map the change to a particular run or task that we described in the previous section . One way of fixing that would be to use the TF_VAR_spacelift_run_id computed environment variable available to each Spacelift workflow. Conveniently, it's already a Terraform variable , so a setup like this should do the trick: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 variable \"spacelift_run_id\" {} # That's our default provider with credentials generated by Spacelift. provider \"aws\" {} # That's where Terraform needs to run sts:AssumeRole with your # Spacelift-generated credentials to obtain ones for the second account. provider \"aws\" { alias = \"second-account\" assume_role { role_arn = \"<up-to-you>\" session_name = var.spacelift_run_id external_id = \"<up-to-you>\" } }","title":"Roles assuming other roles"},{"location":"integrations/cloud-providers/aws.html#migrating-from-legacy-integrations","text":"Originally, when our AWS integration was created it did not support account-level integrations. Instead, each integration was created separately on a per-stack basis. This section provides instructions on how to migrate to the new account-level integrations, with different sets of instructions provided depending on whether you manage your integrations via the UI, or via the Spacelift Terraform Provider .","title":"Migrating from \"Legacy\" integrations"},{"location":"integrations/cloud-providers/aws.html#via-the-ui","text":"Once the new Cloud Integrations UI is enabled for your account, you will still be able to view your legacy integrations in the \u201cIntegrations\u201d section of your stacks, but it will look slightly different: The first thing to do is to copy your Role ARN, then head over to the Cloud Integrations section. Once there, click on the \u201cAdd your first integration\u201d button to get started: Give your role a name, and paste in your existing ARN: The role name is completely up to you and is just used for managing the roles in your account. Maybe it\u2019s a role that allows management of S3 accounts, like in the example above, or maybe you have a single role that can manage your entire pre-production environment, in which case you might name it \u201cPreprod Writer\u201d. Now that you\u2019ve created an AWS integration, head back to your stack integration settings and click on the \u201cDelete\u201d button next to your legacy integration: Now choose the \u201cAWS\u201d option from the \u201cSelect integration to set up\u201d drop down, select the AWS integration you just created, and check the read and write checkboxes: You\u2019ll notice that we display an example trust relationship on this screen. You may or may not need to adjust the Trust Relationship of your role at this point. If you already use a StringLike condition with an <account-name>@* wildcard you shouldn\u2019t need to change anything. Click on the Attach button to attach your integration to your stack: Congrats - you\u2019ve now successfully migrated!","title":"Via the UI"},{"location":"integrations/cloud-providers/aws.html#via-the-terraform-provider","text":"Migrating via the Terraform Provider is also very simple. The Terraform Provider now contains two new resources, along with an additional data source that you can use: spacelift_aws_integration - creates an account level integration. spacelift_aws_integration_attachment - attaches an integration to a stack. spacelift_aws_integration_attachment_external_id - a data source that can help you generate the correct trust relationship for attaching an integration to a stack.","title":"Via the Terraform Provider"},{"location":"integrations/cloud-providers/aws.html#starting-configuration","text":"To begin with, let\u2019s assume you have the following Terraform configuration that creates an IAM role, a Stack, and connects the role to the stack: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 resource \"spacelift_stack\" \"this\" { name = \"AWS Integration Mig Test\" repository = \"spacelift-local\" branch = \"main\" project_root = \"stacks/aws-integration-testing\" } resource \"aws_iam_role\" \"this\" { name = \"adamc-v1-to-v2-migration-role\" assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ jsondecode ( spacelift_stack.this.aws_assume_role_policy_statement )] }) } resource \"aws_iam_role_policy\" \"this\" { name = aws_iam_role.this.name role = aws_iam_role.this.id policy = jsonencode ({ # Excluded for brevity }) } resource \"spacelift_aws_role\" \"this\" { stack_id = spacelift_stack.this.id role_arn = aws_iam_role.this.arn }","title":"Starting Configuration"},{"location":"integrations/cloud-providers/aws.html#migration","text":"To migrate to the new integration format, we need to take the following steps: Add a new AWS integration. (optional) Generate an AssumeRole policy for attaching it to our stack. (optional) Update the assume_role_policy on our aws_iam_role resource. Remove the spacelift_aws_role resource. Attach your new integration to your stack via the spacelift_aws_integration_attachment resource. Steps 2 and 3 are completely optional, and may not be required if you are using wildcard matching for the external ID, for example.","title":"Migration"},{"location":"integrations/cloud-providers/aws.html#result","text":"The following shows the finished result of the migrated Terraform: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 data \"aws_caller_identity\" \"current\" {} # In this example, we're using some locals to calculate the role ARN. This is to avoid a circular # dependency between the aws_iam_role resource and the spacelift_aws_integration resource. locals { role_name = \"adamc-v1-to-v2-migration-role\" role_arn = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.role_name}\" } resource \"spacelift_stack\" \"this\" { name = \"AWS Integration Mig Test\" repository = \"spacelift-local\" branch = \"main\" project_root = \"stacks/aws-integration-testing\" } # 1. Add the new integration. resource \"spacelift_aws_integration\" \"this\" { name = local.role_name role_arn = local.role_arn generate_credentials_in_worker = false } # 2. Generate the AssumeRole policy for attaching it to our stack. data \"spacelift_aws_integration_attachment_external_id\" \"this\" { integration_id = spacelift_aws_integration.this.id stack_id = spacelift_stack.this.id read = true write = true } resource \"aws_iam_role\" \"this\" { name = local.role_name assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ # 3. Remove the old assume role policy statement, and add our new one. # jsondecode(spacelift_stack.this.aws_assume_role_policy_statement), jsondecode ( data.spacelift_aws_integration_attachment_external_id.this.assume_role_policy_statement ), ] }) } resource \"aws_iam_role_policy\" \"this\" { name = aws_iam_role.this.name role = aws_iam_role.this.id policy = jsonencode ({ # Excluded for brevity }) } # 4. Remove the old spacelift_aws_role resource # resource \"spacelift_aws_role\" \"this\" { # stack_id = spacelift_stack.this.id # role_arn = aws_iam_role.this.arn # } # 5. Add the new integration attachment resource \"spacelift_aws_integration_attachment\" \"this\" { integration_id = spacelift_aws_integration.this.id stack_id = spacelift_stack.this.id read = true write = true # The role needs to exist before we attach since we test role assumption during attachment. depends_on = [ aws_iam_role.this ] }","title":"Result"},{"location":"integrations/cloud-providers/oidc.html","text":"OpenID Connect (OIDC) \u00bb Hint For this feature to work, the service you are integrating with needs to be able to access your Spacelift instance. For example, if you have deployed your Spacelift instance with an internal rather than public-facing load balancer, you will not be able to use OIDC Federation for AWS role assumption. OpenID Connect is a federated identity technology that allows you to exchange short-lived Spacelift credentials for temporary credentials valid for external service providers like AWS, GCP, Azure, HashiCorp Vault etc. This allows you to use Spacelift to manage your infrastructure on these cloud providers without the need of using static credentials. OIDC is also an attractive alternative to our native AWS integration in that it implements a common protocol, requires no additional configuration on the Spacelift side, supports a wider range of external service providers and empowers the user to construct more sophisticated access policies based on JWT claims. It is not the purpose of this document to explain the details of the OpenID Connect protocol. If you are not familiar with it, we recommend you read the OpenID Connect specification or GitHub's excellent introduction to security hardening with OpenID Connect . About the Spacelift OIDC token \u00bb The Spacelift OIDC token is a JSON Web Token that is signed by Spacelift and contains a set of claims that can be used to construct a set of temporary credentials for the external service provider. The token is valid for an hour and is available to every run in any paid Spacelift account. The token is available in the SPACELIFT_OIDC_TOKEN environment variable and in the /mnt/workspace/spacelift.oidc file. Standard claims \u00bb The token contains the following standard claims: iss - the issuer of the token - the URL of your Spacelift account, for example https://demo.app.spacelift.io . This is unique for each Spacelift account; sub - the subject of the token - some information about the Spacelift run that generated this token. The subject claim is constructed as follows: space:<space_id>:(stack|module):<stack_id|module_id>:run_type:<run_type>:scope:<read|write> . For example, space:legacy:stack:infra:run_type:TRACKED:scope:write . Individual values are also available as separate custom claims - see below ; aud - the audience of the token - the hostname of your Spacelift account. For example, demo.app.spacelift.io . This is unique for each Spacelift account; exp - the expiration time of the token - the time at which the token will expire, in seconds since the Unix epoch. The token is valid for one hour; iat - the time at which the token was issued, in seconds since the Unix epoch; jti - the unique identifier of the token; nbf - the time before which the token is not valid, in seconds since the Unix epoch. This is always set to the same value as iat ; Custom claims \u00bb The token also contains the following custom claims: spaceId - the ID of the space in which the run that owns the token was executed; callerType - the type of the caller, ie. the entity that owns the run - either stack or module ; callerId - the ID of the caller, ie. the stack or module that generated the run; runType - the type of the run ( PROPOSED , TRACKED , TASK , TESTING or DESTROY ; runId - the ID of the run that owns the token; scope - the scope of the token - either read or write . About scopes \u00bb Whether the token is given read or write scope depends on the type of the run that generated the token. Proposed runs get a read scope, while tracked , testing and destroy runs as well as tasks get a write scope. The only exception to that rule are tracked runs whose stack is not set to autodeploy . In that case, the token will have a read scope during the planning phase, and a write scope during the apply phase. This is because we know in advance that the tracked run requiring a manual approval should not perform write operations before human confirmation. Note that the scope claim, as well as other claims presented by the Spacelift token are merely advisory. It depends on you whether you want to control access to your external service provider based on the scope of the token or on some other claim like space, caller or run type. In other words, Spacelift just gives you the data and it's up to you to decide whether and how to use it. Using the Spacelift OIDC token \u00bb In this section we will show you how to use the Spacelift OIDC token to authenticate with AWS , GCP , Azure , and HashiCorp Vault . In particular, we will focus on setting up the integration and using it from these services' respective Terraform providers AWS \u00bb Warning While the Terraform AWS provider supports authenticating with OIDC , the AWS S3 state backend does not support it yet. If you need to use the AWS S3 state backend, you can use the following workaround: Add the following command as a before_init hook (make sure to replace <ROLE ARN> with your IAM role ARN) 1 export $( printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" $( aws sts assume-role-with-web-identity --web-identity-token \" $( cat /mnt/workspace/spacelift.oidc ) \" --role-arn <ROLE ARN> --role-session-name spacelift-run- ${ TF_VAR_spacelift_run_id } --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" --output text )) Comment out the role_arn argument in the backend block Comment out the assume_role_with_web_identity section in the AWS provider block Alternatively, you can use the dedicated AWS Cloud Integration that uses AWS STS to obtain temporary credentials. Configuring Spacelift as an Identity Provider \u00bb In order to be able to do that, you will need to set up Spacelift as a valid identity provider for your AWS account. This is done by creating an OpenID Connect identity provider . You can do it declaratively using any of the IaC providers, programmatically using the AWS CLI or simply use the console. For illustrative purposes, we will use the console: Go to the AWS console and select the IAM service; Click on the \"Identity providers\" link in the left-hand menu; Click on the \"Add provider\" button in the top bar Select \"OpenID Connect\" as the provider type Make sure to get the host thumbprint by clicking the \"Get thumbprint\" button. This is required by AWS and protects you from a certain class of MitM attacks. Once created, the identity provider will be listed in the \"Identity providers\" table. You can click on the provider name to see the details. From here, you will also be able to assign an IAM role to this new identity provider: A dialog will pop up, asking you to select whether you want to create a new role or use an existing one. Let's create a brand new role. The most important thing for us is to select the right trusted entity - the new Spacelift OIDC provider. Make sure you select the audience from the dropdown - there should be just one option to choose from: The rest of the process is the same as for any other role creation. You will be asked to select the policies that you want to attach to the role. You can also add tags and a description. Once you're done, click the \"Create role\" button. If you go to your new role's details page, in the Trust relationships section you will notice that it is now associated with the Spacelift OIDC provider: This trust relationship is very relaxed and will allow any stack or module in the demo Spacelift account to assume this role. If you want to be more restrictive, you will want to add more conditions. For example, we can restrict the role to be only assumable by stacks in the production space by adding the following condition: 1 2 3 \"StringLike\" : { \"demo.app.spacelift.io:sub\" : \"space:production:*\" } Hint You will need to replace demo.app.spacelift.io with the hostname of your Spacelift account. You can also restrict the role to be assumable only by a specific stack by matching on the stack ID: 1 2 3 \"StringLike\" : { \"demo.app.spacelift.io:sub\" : \"*:stack:oidc-is-awesome:*\" } You can mix and match these to get the exact constraints you need. It is not the purpose of this guide to go into the intricacies of AWS IAM conditions - you can learn all about these in the official doc . One important thing to remember though is that AWS does not seem to support custom claims so you will need to use the standard ones to do the matching - primarily sub , as shown above. Configuring the Terraform Provider \u00bb Once the Spacelift-AWS OIDC integration is set up, the provider can be configured without the need for any static credentials. The aws_role_arn variable should be set to the ARN of the role that you want to assume: 1 2 3 4 5 6 provider \"aws\" { assume_role_with_web_identity { role_arn = var.aws_role_arn web_identity_token_file = \"/mnt/workspace/spacelift.oidc\" } } GCP \u00bb Configuring Workload Identity \u00bb In order to enable Spacelift runs to access GCP resources, you need to set up Spacelift as a valid identity provider for your account. To do this you need to perform a number of steps within GCP: Create a workload identity pool and set up the Spacelift OIDC provider as an identity provider for it; Create a service account that will be used by Spacelift; Connect the service account to the workload identity pool; Let's go through these steps one by one. First, you will want to go to the GCP console and select the IAM service, then click on the \"Workload Identity Federation\" link in the left-hand menu: There, you will want to click on the Create pool button, which will take you to the pool creation form. First, give your new identity pool a name and optionally set a description. The next step is more interesting - you will need to set up an identity provider. The name is pretty much arbitrary but the rest of the fields are important to get right. The Issuer URL needs to be set to the URL of your Spacelift account (including the scheme). You will want to manually specify allowed audiences. There's just one you need - the hostname of your Spacelift account. Here is what a properly filled out form would look like: In the last step, you will need to configure a mapping between provider Spacelift token claims (assertions) and Google attributes. google.subject is a required mapping and should generally map to assertion.sub . Custom claims can be mapped to custom attributes, which need to start with the attribute. prefix. In the below example, we are also mapping Spacelift's spaceId claim to GCP's custom space attribute: To restrict which identities can authenticate using your workload identity pool you can specify extra conditions using Google's Common Expression Language . Last but not least, we will want to grant the workload identity pool the ability to impersonate the service account we will be using. Assuming we already have a service account, let's allow any token claiming to originate from the production space in our Spacelift account to impersonate it: Configuring the Terraform Provider \u00bb Once the Spacelift-GCP OIDC integration is set up, the Google Cloud Terraform provider can be configured without the need for any static credentials. You will however want to provide a configuration file telling the provider how to authenticate. The configuration file can be created manually or generated by the gcloud utility and would look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"type\" : \"external_account\" , \"audience\" : \"//iam.googleapis.com/projects/${PROJECT_ID}/locations/global/workloadIdentityPools/${WORKER_POOL_ID}/providers/${IDENTITY_PROVIDER_ID}\" , \"subject_token_type\" : \"urn:ietf:params:oauth:token-type:jwt\" , \"token_url\" : \"https://sts.googleapis.com/v1/token\" , \"credential_source\" : { \"file\" : \"/mnt/workspace/spacelift.oidc\" }, \"service_account_impersonation_url\" : \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/${SERVICE_ACCOUNT_EMAIL}:generateAccessToken\" , \"service_account_impersonation\" : { \"token_lifetime_seconds\" : 3600 } } Your Spacelift run needs to have access to this file, so you can check it in (there's nothing secret here), mount it on a stack or mount it in a context that is then attached to the stack. Note that you will also need to tell the provider how to find this configuration file. This bit is nicely documented in the Google Cloud Terraform provider docs . And here is an example of us using a Spacelift context to mount the file and configure the provider to be attached to an arbitrary number of stacks: Azure \u00bb Configuring workload identity federation \u00bb In order to enable Spacelift runs to access Azure resources, you need to set up Spacelift as a valid identity provider for your account. This is done using workload identity federation . The set up process involves creating an App Registration, and then adding federated credentials that tell Azure which Spacelift runs should be able to use which App Registrations. This process can be completed via the Azure Portal, Azure CLI or Terraform. For illustrative purposes we will use the Azure Portal. The first step is to go to the Azure AD section of the Azure Portal, go to App registrations , and then click on the New registration button: Specify a name for your registration, select the Accounts in this organizational directory only option, and click on the Register button: On the overview page, take a note of the Application (client) ID and Directory (tenant) ID - you will need them later when configuring the Terraform provider. Next, go to the Certificates & secrets section, select the Federated credentials tab and click on the Add credential button: On the next screen, choose Other issuer as the Federated credential scenario : The next step is to configure the trust relationship between Spacelift and Azure. In order to do this, we need to fill out the following pieces of information: Issuer - the URL of your Spacelift account, for example https://myaccount.app.spacelift.io . Subject identifier - the subject that a token must contain to be able to get credentials for your App. This uses the format mentioned in the Standard claims section. Name - a name for this credential. Audience - the hostname of your Spacelift account, for example myaccount.app.spacelift.io . Take a look at the following screenshot for an example allowing a proposed run to use our App: Workload federation in Azure requires the subject claim of the OIDC token to exactly match the federated credential, and doesn't allow wildcards. Because of this you will need to repeat the same process and add a number of different federated credentials in order to support all the different types of runs for your Stack or module. For example for a stack called azure-oidc-test in the legacy space you need to add credentials for the following subjects: 1 2 3 4 5 space:legacy:stack:azure-oidc-test:run_type:TRACKED:scope:read space:legacy:stack:azure-oidc-test:run_type:TRACKED:scope:write space:legacy:stack:azure-oidc-test:run_type:PROPOSED:scope:read space:legacy:stack:azure-oidc-test:run_type:TASK:scope:write space:legacy:stack:azure-oidc-test:run_type:DESTROY:scope:write And for a module called my-module in the development space you need to add the following: 1 2 space:development:stack:my-module:run_type:TESTING:scope:read space:development:stack:my-module:run_type:TESTING:scope:write After adding all the credentials for a stack, it should look something like this: Info Please see the Standard claims section for more information about the subject format. Configuring the Terraform Provider \u00bb Once workload identity federation is set up, the AzureRM provider can be configured without the need for any static credentials. To do this, enable the use_oidc feature of the provider, and use the oidc_token_file_path setting to tell the provider where to find the token: 1 2 3 4 5 provider \"azurerm\" { features {} use_oidc = true oidc_token_file_path = \"/mnt/workspace/spacelift.oidc\" } Next, add the following environment variables to your stack: ARM_CLIENT_ID - the client ID of the App registration created in the previous section. ARM_TENANT_ID - the tenant ID of the App registration created in the previous section. ARM_SUBSCRIPTION_ID - the ID of the Azure subscription you want to use. Info Note - before you can use your App registration to manage Azure resources, you need to assign the correct RBAC permissions to it. Vault \u00bb Configuring Spacelift as an Identity Provider \u00bb In order to enable Spacelift runs to access Vault, you need to set up Spacelift as a valid identity provider for your Vault instance. This is done using Vault's OIDC auth method . The set up process involves creating a role in Vault that tells Vault which Spacelift runs should be able to access which Vault secrets. This process can be completed via the Vault CLI or Terraform. For illustrative purposes we will use the Vault CLI. If you haven't enabled the JWT auth method in your Vault instance, you need to do so first. To do this, run the following command: 1 vault auth enable jwt In the next step, we need to add configuration for your Spacelift account as an identity provider. To do this, run the following command: 1 2 3 vault write auth/jwt/config \\ bound_issuer = \"https://demo.app.spacelift.io\" \\ oidc_discovery_url = \"https://demo.app.spacelift.io\" The bound_issuer parameter is the URL of your Spacelift account which is used as the issuer claim in the OIDC token you receive from Spacelift. The oidc_discovery_url parameter is the URL of the OIDC discovery endpoint for your Spacelift account, which is in this case identical to the bound_issuer parameter. Next, you will need to create a policy that will be used to determine which Spacelift runs can access which Vault secrets. For example, the following policy allows all Spacelift runs to read any secret in the secrets/preprod path: 1 2 3 4 5 vault policy write infra-preprod - <<EOF path \"secrets/preprod/*\" { capabilities = [\"read\"] } EOF Last but not least, you will need to create a role that binds the policy to the identity provider. The following command creates a role called infra-preprod that binds the infra-preprod policy to the JWT identity provider: 1 2 3 4 5 6 7 8 9 10 vault write auth/jwt/role/infra-preprod - <<EOF { \"role_type\": \"jwt\", \"user_claim\": \"actor\", \"bound_audiences\": \"demo.app.spacelift.io\", \"bound_claims\": { \"space\": \"preprod\" }, \"policies\": [\"infra-preprod\"], \"ttl\": \"10m\" } EOF The bound_audiences parameter is the hostname of your Spacelift account, which is used as the audience claim in the OIDC token you receive from Spacelift. The bound_claims parameter is a JSON object that contains the claims that the OIDC token must contain in order to be able to access the Vault secrets. How you scope this will very much depend on your use case. In the above example, only runs belonging to a stack or module in the space claim can assume \"infra-preprod\" Vault role. You can refer to this document to see the available standard and custom claims presented by the Spacelift OIDC token. Configuring the Terraform Provider \u00bb Once the Vault setup is complete, you need to configure the Terraform Vault provider to use the Spacelift OIDC JWT token to assume a particular role. To do this, you will provide the auth_login_jwt configuration block to the provider, and set the role parameter to the name of the role you created in the previous section: 1 2 3 4 5 6 7 provider \"vault\" # ... other configuration auth_login_jwt { role = \"infra-preprod\" } } Next, set the TERRAFORM_VAULT_AUTH_JWT environment variable to ${SPACELIFT_OIDC_TOKEN} , either directly on your stack, or on one of the attached contexts . This approach uses interpolation to dynamically set the value of the variable the provider is looking for to the value of the environment variable that Spacelift provides.","title":"OpenID Connect (OIDC)"},{"location":"integrations/cloud-providers/oidc.html#openid-connect-oidc","text":"Hint For this feature to work, the service you are integrating with needs to be able to access your Spacelift instance. For example, if you have deployed your Spacelift instance with an internal rather than public-facing load balancer, you will not be able to use OIDC Federation for AWS role assumption. OpenID Connect is a federated identity technology that allows you to exchange short-lived Spacelift credentials for temporary credentials valid for external service providers like AWS, GCP, Azure, HashiCorp Vault etc. This allows you to use Spacelift to manage your infrastructure on these cloud providers without the need of using static credentials. OIDC is also an attractive alternative to our native AWS integration in that it implements a common protocol, requires no additional configuration on the Spacelift side, supports a wider range of external service providers and empowers the user to construct more sophisticated access policies based on JWT claims. It is not the purpose of this document to explain the details of the OpenID Connect protocol. If you are not familiar with it, we recommend you read the OpenID Connect specification or GitHub's excellent introduction to security hardening with OpenID Connect .","title":"OpenID Connect (OIDC)"},{"location":"integrations/cloud-providers/oidc.html#about-the-spacelift-oidc-token","text":"The Spacelift OIDC token is a JSON Web Token that is signed by Spacelift and contains a set of claims that can be used to construct a set of temporary credentials for the external service provider. The token is valid for an hour and is available to every run in any paid Spacelift account. The token is available in the SPACELIFT_OIDC_TOKEN environment variable and in the /mnt/workspace/spacelift.oidc file.","title":"About the Spacelift OIDC token"},{"location":"integrations/cloud-providers/oidc.html#standard-claims","text":"The token contains the following standard claims: iss - the issuer of the token - the URL of your Spacelift account, for example https://demo.app.spacelift.io . This is unique for each Spacelift account; sub - the subject of the token - some information about the Spacelift run that generated this token. The subject claim is constructed as follows: space:<space_id>:(stack|module):<stack_id|module_id>:run_type:<run_type>:scope:<read|write> . For example, space:legacy:stack:infra:run_type:TRACKED:scope:write . Individual values are also available as separate custom claims - see below ; aud - the audience of the token - the hostname of your Spacelift account. For example, demo.app.spacelift.io . This is unique for each Spacelift account; exp - the expiration time of the token - the time at which the token will expire, in seconds since the Unix epoch. The token is valid for one hour; iat - the time at which the token was issued, in seconds since the Unix epoch; jti - the unique identifier of the token; nbf - the time before which the token is not valid, in seconds since the Unix epoch. This is always set to the same value as iat ;","title":"Standard claims"},{"location":"integrations/cloud-providers/oidc.html#custom-claims","text":"The token also contains the following custom claims: spaceId - the ID of the space in which the run that owns the token was executed; callerType - the type of the caller, ie. the entity that owns the run - either stack or module ; callerId - the ID of the caller, ie. the stack or module that generated the run; runType - the type of the run ( PROPOSED , TRACKED , TASK , TESTING or DESTROY ; runId - the ID of the run that owns the token; scope - the scope of the token - either read or write .","title":"Custom claims"},{"location":"integrations/cloud-providers/oidc.html#about-scopes","text":"Whether the token is given read or write scope depends on the type of the run that generated the token. Proposed runs get a read scope, while tracked , testing and destroy runs as well as tasks get a write scope. The only exception to that rule are tracked runs whose stack is not set to autodeploy . In that case, the token will have a read scope during the planning phase, and a write scope during the apply phase. This is because we know in advance that the tracked run requiring a manual approval should not perform write operations before human confirmation. Note that the scope claim, as well as other claims presented by the Spacelift token are merely advisory. It depends on you whether you want to control access to your external service provider based on the scope of the token or on some other claim like space, caller or run type. In other words, Spacelift just gives you the data and it's up to you to decide whether and how to use it.","title":"About scopes"},{"location":"integrations/cloud-providers/oidc.html#using-the-spacelift-oidc-token","text":"In this section we will show you how to use the Spacelift OIDC token to authenticate with AWS , GCP , Azure , and HashiCorp Vault . In particular, we will focus on setting up the integration and using it from these services' respective Terraform providers","title":"Using the Spacelift OIDC token"},{"location":"integrations/cloud-providers/oidc.html#aws","text":"Warning While the Terraform AWS provider supports authenticating with OIDC , the AWS S3 state backend does not support it yet. If you need to use the AWS S3 state backend, you can use the following workaround: Add the following command as a before_init hook (make sure to replace <ROLE ARN> with your IAM role ARN) 1 export $( printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" $( aws sts assume-role-with-web-identity --web-identity-token \" $( cat /mnt/workspace/spacelift.oidc ) \" --role-arn <ROLE ARN> --role-session-name spacelift-run- ${ TF_VAR_spacelift_run_id } --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" --output text )) Comment out the role_arn argument in the backend block Comment out the assume_role_with_web_identity section in the AWS provider block Alternatively, you can use the dedicated AWS Cloud Integration that uses AWS STS to obtain temporary credentials.","title":"AWS"},{"location":"integrations/cloud-providers/oidc.html#configuring-spacelift-as-an-identity-provider","text":"In order to be able to do that, you will need to set up Spacelift as a valid identity provider for your AWS account. This is done by creating an OpenID Connect identity provider . You can do it declaratively using any of the IaC providers, programmatically using the AWS CLI or simply use the console. For illustrative purposes, we will use the console: Go to the AWS console and select the IAM service; Click on the \"Identity providers\" link in the left-hand menu; Click on the \"Add provider\" button in the top bar Select \"OpenID Connect\" as the provider type Make sure to get the host thumbprint by clicking the \"Get thumbprint\" button. This is required by AWS and protects you from a certain class of MitM attacks. Once created, the identity provider will be listed in the \"Identity providers\" table. You can click on the provider name to see the details. From here, you will also be able to assign an IAM role to this new identity provider: A dialog will pop up, asking you to select whether you want to create a new role or use an existing one. Let's create a brand new role. The most important thing for us is to select the right trusted entity - the new Spacelift OIDC provider. Make sure you select the audience from the dropdown - there should be just one option to choose from: The rest of the process is the same as for any other role creation. You will be asked to select the policies that you want to attach to the role. You can also add tags and a description. Once you're done, click the \"Create role\" button. If you go to your new role's details page, in the Trust relationships section you will notice that it is now associated with the Spacelift OIDC provider: This trust relationship is very relaxed and will allow any stack or module in the demo Spacelift account to assume this role. If you want to be more restrictive, you will want to add more conditions. For example, we can restrict the role to be only assumable by stacks in the production space by adding the following condition: 1 2 3 \"StringLike\" : { \"demo.app.spacelift.io:sub\" : \"space:production:*\" } Hint You will need to replace demo.app.spacelift.io with the hostname of your Spacelift account. You can also restrict the role to be assumable only by a specific stack by matching on the stack ID: 1 2 3 \"StringLike\" : { \"demo.app.spacelift.io:sub\" : \"*:stack:oidc-is-awesome:*\" } You can mix and match these to get the exact constraints you need. It is not the purpose of this guide to go into the intricacies of AWS IAM conditions - you can learn all about these in the official doc . One important thing to remember though is that AWS does not seem to support custom claims so you will need to use the standard ones to do the matching - primarily sub , as shown above.","title":"Configuring Spacelift as an Identity Provider"},{"location":"integrations/cloud-providers/oidc.html#configuring-the-terraform-provider","text":"Once the Spacelift-AWS OIDC integration is set up, the provider can be configured without the need for any static credentials. The aws_role_arn variable should be set to the ARN of the role that you want to assume: 1 2 3 4 5 6 provider \"aws\" { assume_role_with_web_identity { role_arn = var.aws_role_arn web_identity_token_file = \"/mnt/workspace/spacelift.oidc\" } }","title":"Configuring the Terraform Provider"},{"location":"integrations/cloud-providers/oidc.html#gcp","text":"","title":"GCP"},{"location":"integrations/cloud-providers/oidc.html#configuring-workload-identity","text":"In order to enable Spacelift runs to access GCP resources, you need to set up Spacelift as a valid identity provider for your account. To do this you need to perform a number of steps within GCP: Create a workload identity pool and set up the Spacelift OIDC provider as an identity provider for it; Create a service account that will be used by Spacelift; Connect the service account to the workload identity pool; Let's go through these steps one by one. First, you will want to go to the GCP console and select the IAM service, then click on the \"Workload Identity Federation\" link in the left-hand menu: There, you will want to click on the Create pool button, which will take you to the pool creation form. First, give your new identity pool a name and optionally set a description. The next step is more interesting - you will need to set up an identity provider. The name is pretty much arbitrary but the rest of the fields are important to get right. The Issuer URL needs to be set to the URL of your Spacelift account (including the scheme). You will want to manually specify allowed audiences. There's just one you need - the hostname of your Spacelift account. Here is what a properly filled out form would look like: In the last step, you will need to configure a mapping between provider Spacelift token claims (assertions) and Google attributes. google.subject is a required mapping and should generally map to assertion.sub . Custom claims can be mapped to custom attributes, which need to start with the attribute. prefix. In the below example, we are also mapping Spacelift's spaceId claim to GCP's custom space attribute: To restrict which identities can authenticate using your workload identity pool you can specify extra conditions using Google's Common Expression Language . Last but not least, we will want to grant the workload identity pool the ability to impersonate the service account we will be using. Assuming we already have a service account, let's allow any token claiming to originate from the production space in our Spacelift account to impersonate it:","title":"Configuring Workload Identity"},{"location":"integrations/cloud-providers/oidc.html#configuring-the-terraform-provider_1","text":"Once the Spacelift-GCP OIDC integration is set up, the Google Cloud Terraform provider can be configured without the need for any static credentials. You will however want to provide a configuration file telling the provider how to authenticate. The configuration file can be created manually or generated by the gcloud utility and would look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"type\" : \"external_account\" , \"audience\" : \"//iam.googleapis.com/projects/${PROJECT_ID}/locations/global/workloadIdentityPools/${WORKER_POOL_ID}/providers/${IDENTITY_PROVIDER_ID}\" , \"subject_token_type\" : \"urn:ietf:params:oauth:token-type:jwt\" , \"token_url\" : \"https://sts.googleapis.com/v1/token\" , \"credential_source\" : { \"file\" : \"/mnt/workspace/spacelift.oidc\" }, \"service_account_impersonation_url\" : \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/${SERVICE_ACCOUNT_EMAIL}:generateAccessToken\" , \"service_account_impersonation\" : { \"token_lifetime_seconds\" : 3600 } } Your Spacelift run needs to have access to this file, so you can check it in (there's nothing secret here), mount it on a stack or mount it in a context that is then attached to the stack. Note that you will also need to tell the provider how to find this configuration file. This bit is nicely documented in the Google Cloud Terraform provider docs . And here is an example of us using a Spacelift context to mount the file and configure the provider to be attached to an arbitrary number of stacks:","title":"Configuring the Terraform Provider"},{"location":"integrations/cloud-providers/oidc.html#azure","text":"","title":"Azure"},{"location":"integrations/cloud-providers/oidc.html#configuring-workload-identity-federation","text":"In order to enable Spacelift runs to access Azure resources, you need to set up Spacelift as a valid identity provider for your account. This is done using workload identity federation . The set up process involves creating an App Registration, and then adding federated credentials that tell Azure which Spacelift runs should be able to use which App Registrations. This process can be completed via the Azure Portal, Azure CLI or Terraform. For illustrative purposes we will use the Azure Portal. The first step is to go to the Azure AD section of the Azure Portal, go to App registrations , and then click on the New registration button: Specify a name for your registration, select the Accounts in this organizational directory only option, and click on the Register button: On the overview page, take a note of the Application (client) ID and Directory (tenant) ID - you will need them later when configuring the Terraform provider. Next, go to the Certificates & secrets section, select the Federated credentials tab and click on the Add credential button: On the next screen, choose Other issuer as the Federated credential scenario : The next step is to configure the trust relationship between Spacelift and Azure. In order to do this, we need to fill out the following pieces of information: Issuer - the URL of your Spacelift account, for example https://myaccount.app.spacelift.io . Subject identifier - the subject that a token must contain to be able to get credentials for your App. This uses the format mentioned in the Standard claims section. Name - a name for this credential. Audience - the hostname of your Spacelift account, for example myaccount.app.spacelift.io . Take a look at the following screenshot for an example allowing a proposed run to use our App: Workload federation in Azure requires the subject claim of the OIDC token to exactly match the federated credential, and doesn't allow wildcards. Because of this you will need to repeat the same process and add a number of different federated credentials in order to support all the different types of runs for your Stack or module. For example for a stack called azure-oidc-test in the legacy space you need to add credentials for the following subjects: 1 2 3 4 5 space:legacy:stack:azure-oidc-test:run_type:TRACKED:scope:read space:legacy:stack:azure-oidc-test:run_type:TRACKED:scope:write space:legacy:stack:azure-oidc-test:run_type:PROPOSED:scope:read space:legacy:stack:azure-oidc-test:run_type:TASK:scope:write space:legacy:stack:azure-oidc-test:run_type:DESTROY:scope:write And for a module called my-module in the development space you need to add the following: 1 2 space:development:stack:my-module:run_type:TESTING:scope:read space:development:stack:my-module:run_type:TESTING:scope:write After adding all the credentials for a stack, it should look something like this: Info Please see the Standard claims section for more information about the subject format.","title":"Configuring workload identity federation"},{"location":"integrations/cloud-providers/oidc.html#configuring-the-terraform-provider_2","text":"Once workload identity federation is set up, the AzureRM provider can be configured without the need for any static credentials. To do this, enable the use_oidc feature of the provider, and use the oidc_token_file_path setting to tell the provider where to find the token: 1 2 3 4 5 provider \"azurerm\" { features {} use_oidc = true oidc_token_file_path = \"/mnt/workspace/spacelift.oidc\" } Next, add the following environment variables to your stack: ARM_CLIENT_ID - the client ID of the App registration created in the previous section. ARM_TENANT_ID - the tenant ID of the App registration created in the previous section. ARM_SUBSCRIPTION_ID - the ID of the Azure subscription you want to use. Info Note - before you can use your App registration to manage Azure resources, you need to assign the correct RBAC permissions to it.","title":"Configuring the Terraform Provider"},{"location":"integrations/cloud-providers/oidc.html#vault","text":"","title":"Vault"},{"location":"integrations/cloud-providers/oidc.html#configuring-spacelift-as-an-identity-provider_1","text":"In order to enable Spacelift runs to access Vault, you need to set up Spacelift as a valid identity provider for your Vault instance. This is done using Vault's OIDC auth method . The set up process involves creating a role in Vault that tells Vault which Spacelift runs should be able to access which Vault secrets. This process can be completed via the Vault CLI or Terraform. For illustrative purposes we will use the Vault CLI. If you haven't enabled the JWT auth method in your Vault instance, you need to do so first. To do this, run the following command: 1 vault auth enable jwt In the next step, we need to add configuration for your Spacelift account as an identity provider. To do this, run the following command: 1 2 3 vault write auth/jwt/config \\ bound_issuer = \"https://demo.app.spacelift.io\" \\ oidc_discovery_url = \"https://demo.app.spacelift.io\" The bound_issuer parameter is the URL of your Spacelift account which is used as the issuer claim in the OIDC token you receive from Spacelift. The oidc_discovery_url parameter is the URL of the OIDC discovery endpoint for your Spacelift account, which is in this case identical to the bound_issuer parameter. Next, you will need to create a policy that will be used to determine which Spacelift runs can access which Vault secrets. For example, the following policy allows all Spacelift runs to read any secret in the secrets/preprod path: 1 2 3 4 5 vault policy write infra-preprod - <<EOF path \"secrets/preprod/*\" { capabilities = [\"read\"] } EOF Last but not least, you will need to create a role that binds the policy to the identity provider. The following command creates a role called infra-preprod that binds the infra-preprod policy to the JWT identity provider: 1 2 3 4 5 6 7 8 9 10 vault write auth/jwt/role/infra-preprod - <<EOF { \"role_type\": \"jwt\", \"user_claim\": \"actor\", \"bound_audiences\": \"demo.app.spacelift.io\", \"bound_claims\": { \"space\": \"preprod\" }, \"policies\": [\"infra-preprod\"], \"ttl\": \"10m\" } EOF The bound_audiences parameter is the hostname of your Spacelift account, which is used as the audience claim in the OIDC token you receive from Spacelift. The bound_claims parameter is a JSON object that contains the claims that the OIDC token must contain in order to be able to access the Vault secrets. How you scope this will very much depend on your use case. In the above example, only runs belonging to a stack or module in the space claim can assume \"infra-preprod\" Vault role. You can refer to this document to see the available standard and custom claims presented by the Spacelift OIDC token.","title":"Configuring Spacelift as an Identity Provider"},{"location":"integrations/cloud-providers/oidc.html#configuring-the-terraform-provider_3","text":"Once the Vault setup is complete, you need to configure the Terraform Vault provider to use the Spacelift OIDC JWT token to assume a particular role. To do this, you will provide the auth_login_jwt configuration block to the provider, and set the role parameter to the name of the role you created in the previous section: 1 2 3 4 5 6 7 provider \"vault\" # ... other configuration auth_login_jwt { role = \"infra-preprod\" } } Next, set the TERRAFORM_VAULT_AUTH_JWT environment variable to ${SPACELIFT_OIDC_TOKEN} , either directly on your stack, or on one of the attached contexts . This approach uses interpolation to dynamically set the value of the variable the provider is looking for to the value of the environment variable that Spacelift provides.","title":"Configuring the Terraform Provider"},{"location":"integrations/observability/datadog.html","text":"Datadog integration \u00bb Spacelift can send data to Datadog to help you monitor your infrastructure and Spacelift stacks using Datadog's excellent monitoring and analytics tools. Our integration with Datadog focuses primarily on runs and lets you create dashboards and alerts to answer questions like: How many runs are failing? Which stacks see the most activity? How long does it take to plan a given stack? How long does it take to apply a stack? What is the load on my Spacelift private workers? How many resources am I changing? ...and many more! Here's a very simple dashboard we've created based on this integration that shows the performance of our continuous regression tests: Prerequisites \u00bb The Datadog integration is based on our notification policy feature, which requires at least an active Cloud tier subscription. While building a notification-based Datadog integration from scratch is possible, we've created a Terraform module that will set up all the necessary integration elements for you. This module will only create Spacelift assets: a notification policy that will send data to Datadog; a webhook endpoint that serve as a notification target for the policy; a webhook secret header that will securely authenticate the payload with Datadog; Setting up the integration \u00bb To set up the integration, you'll need to have a Datadog account and a Datadog API key . If you don't have a an administrative stack declaratively manage your Spacelift resources, we suggest you create one, and add module instantiation to it according to its usage instructions. We suggest that you pass the Datadog API key as a stack secret , or - even better - retrieve it from a remote secret store (eg. AWS Parameter Store) using Terraform. If you intend to monitor your entire account (our suggested approach), we suggest that the module is installed in the root space of your Spacelift account. If you only want to monitor a subset of your stacks, you can install the module in their respective space. Metrics \u00bb The following metrics are sent: spacelift.integration.run.count (counter) - a simple count of runs; spacelift.integration.run.timing (counter, nanoseconds) - the duration of different run states. In addition to common tags , this metric is also tagged with the state name, eg. state:planning , state:applying , etc.; spacelift.integration.run.resources (counter) - the resources affected by a run. In addition to common tags , this metric is also tagged with the change type, eg. change_type:added , change_type:changed , etc.; Common tags \u00bb Common tags for all metrics are the following: account (string) : name of the Spacelift account generating the metric; branch (string): name of the Git branch the run was triggered from; drift_detection (boolean): whether the run was triggered by drift detection; run_type (string): type of a run, eg. \"tracked\", \"proposed\", etc.; final_state (string): the terminal state of the run, eg. \"finished\", \"failed\", etc.; space (string): name of the Spacelift space the run belongs to; stack (string): name of the Spacelift stack the run belongs to; worker_pool (string): name of the Spacelift worker pool the run was executed on - for the public worker pool this value is always public ; Extending the integration \u00bb The benefit of building this integration on top of a notification policy is that you can easily extend it to send additional data to Datadog, change the naming of your metrics, change the tags, etc. To do so, you'll need to edit the policy body generated by the module. You can do so by editing the policy in the Spacelift UI, or by forking the module and editing the policy body in the module's source code. Note that this is an advanced feature, and we recommend that you only do so if you're already familiar with Spacelift's notification policy feature and Datadog's API, or are willing to learn about them.","title":"Datadog integration"},{"location":"integrations/observability/datadog.html#datadog-integration","text":"Spacelift can send data to Datadog to help you monitor your infrastructure and Spacelift stacks using Datadog's excellent monitoring and analytics tools. Our integration with Datadog focuses primarily on runs and lets you create dashboards and alerts to answer questions like: How many runs are failing? Which stacks see the most activity? How long does it take to plan a given stack? How long does it take to apply a stack? What is the load on my Spacelift private workers? How many resources am I changing? ...and many more! Here's a very simple dashboard we've created based on this integration that shows the performance of our continuous regression tests:","title":"Datadog integration"},{"location":"integrations/observability/datadog.html#prerequisites","text":"The Datadog integration is based on our notification policy feature, which requires at least an active Cloud tier subscription. While building a notification-based Datadog integration from scratch is possible, we've created a Terraform module that will set up all the necessary integration elements for you. This module will only create Spacelift assets: a notification policy that will send data to Datadog; a webhook endpoint that serve as a notification target for the policy; a webhook secret header that will securely authenticate the payload with Datadog;","title":"Prerequisites"},{"location":"integrations/observability/datadog.html#setting-up-the-integration","text":"To set up the integration, you'll need to have a Datadog account and a Datadog API key . If you don't have a an administrative stack declaratively manage your Spacelift resources, we suggest you create one, and add module instantiation to it according to its usage instructions. We suggest that you pass the Datadog API key as a stack secret , or - even better - retrieve it from a remote secret store (eg. AWS Parameter Store) using Terraform. If you intend to monitor your entire account (our suggested approach), we suggest that the module is installed in the root space of your Spacelift account. If you only want to monitor a subset of your stacks, you can install the module in their respective space.","title":"Setting up the integration"},{"location":"integrations/observability/datadog.html#metrics","text":"The following metrics are sent: spacelift.integration.run.count (counter) - a simple count of runs; spacelift.integration.run.timing (counter, nanoseconds) - the duration of different run states. In addition to common tags , this metric is also tagged with the state name, eg. state:planning , state:applying , etc.; spacelift.integration.run.resources (counter) - the resources affected by a run. In addition to common tags , this metric is also tagged with the change type, eg. change_type:added , change_type:changed , etc.;","title":"Metrics"},{"location":"integrations/observability/datadog.html#common-tags","text":"Common tags for all metrics are the following: account (string) : name of the Spacelift account generating the metric; branch (string): name of the Git branch the run was triggered from; drift_detection (boolean): whether the run was triggered by drift detection; run_type (string): type of a run, eg. \"tracked\", \"proposed\", etc.; final_state (string): the terminal state of the run, eg. \"finished\", \"failed\", etc.; space (string): name of the Spacelift space the run belongs to; stack (string): name of the Spacelift stack the run belongs to; worker_pool (string): name of the Spacelift worker pool the run was executed on - for the public worker pool this value is always public ;","title":"Common tags"},{"location":"integrations/observability/datadog.html#extending-the-integration","text":"The benefit of building this integration on top of a notification policy is that you can easily extend it to send additional data to Datadog, change the naming of your metrics, change the tags, etc. To do so, you'll need to edit the policy body generated by the module. You can do so by editing the policy in the Spacelift UI, or by forking the module and editing the policy body in the module's source code. Note that this is an advanced feature, and we recommend that you only do so if you're already familiar with Spacelift's notification policy feature and Datadog's API, or are willing to learn about them.","title":"Extending the integration"},{"location":"integrations/single-sign-on/index.html","text":"Single Sign-On \u00bb By default, Spacelift supports logging in using GitHub, GitLab, or Google. Some organizations however prefer a Single Sign-On approach, where access to resources is centralized. To accommodate this use-case, Spacelift supports Single Sign-On using SAML 2.0 or OIDC . Managed Identity Provider vs SSO \u00bb Tip The SSO integration can only be configured once the Spacelift account has been created using one of the supported Identity Providers . To create a Spacelift account, a user needs to choose one of the supported managed identity providers. That user then becomes the \"Managed IdP Admin\". If SSO is configured, the managed identity provider used to create the account and the associated admin are disabled and the first user to successfully log in becomes the \"SSO Admin\". Login policies are not evaluated for Managed IdP and SSO admins so that they cannot lock themselves out. As a side effect, there won\u2019t be any Login policy samples for them in the Policy Workbench . If SSO is disabled later, the managed identity provider and associated admin are re-enabled automatically. Backup Credentials \u00bb Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section. Managing integrations \u00bb In order to manage Single Sign-On integrations on your Spacelift account, please go to the Settings section of your account view. Next, navigate to the Single Sign-On tab. If SSO is not enabled for your account, all you're going to see is instructions on how to get started. The first steps are always taken in your identity provider (Google Workspace, Okta, Auth0, ActiveDirectory, etc.). Navigate to your identity provider and create a dedicated SSO application filled with appropriate URLs taken from the Spacelift settings page presented below. Setting up SAML \u00bb When setting up Spacelift on your identity provider, you may want to add three attribute mappings: FirstName is used to build human-friendly user name; LastName is used to build human-friendly user name; Teams can be used by login and stack access policies to determine the level access to the Spacelift account and/or individual Stacks; Depending on your identity provider and your use case, your mapping may be different. Especially with regards to Teams , some identity providers (eg. Okta ) will support an arbitrary list of memberships similar to GitHub teams out of the box, some will need extra customizations like (eg. Google Workspace ) and as a courtesy, we will flush your login history. Some identity providers (eg. Okta ) will allow you to provide a custom per-user SAML 2.0 Subject for SAML assertions. You could use this feature to map GitHub usernames to your identity provider users and thus get the exact same experience as when using GitHub as your identity provider. Warning When setting up SSO without this GitHub mapping, your future logins will appear as new users since Spacelift has no way of mapping those without your assistance. New users will count against your seat quota and you may run out of seats. If you run into this problem, you can contact us . Info Spacelift uses both HTTP-Redirect and HTTP-POST bindings for SAML 2.0. Most of the IdPs enable both by default, but if you run into any issues, please check your application settings. NameID format \u00bb The NameID format specifies the format that Spacelift requests user identifiers from your identity provider. The user identifier is used as the Spacelift login, and each unique identifier will count against your seat quota. Some identity providers allow you to configure this format, but certain providers (eg. Azure AD) do not. If your identity provider does not allow the NameID format to be configured at their end, you can choose from one of the following options: Transient - an opaque identifier that is not guaranteed to remain the same between logins. Email Address - an email address. Persistent - an opaque identifier that remains the same between logins. SAML Setup Guides \u00bb The following are links to example implementations you can use as a reference/guide for setting up your own SAML integration. AWS IAM Identity Center (formerly known as AWS SSO) If you can't find your SAML provider in the list above, don't worry - we do support all SAML 2.0 providers. Setting up OIDC \u00bb When setting up Spacelift on your identity provider, you must make sure it supports the email scope and returns the corresponding email Additional claims \u00bb Spacelift dynamically checks integrated Identity Provider's Well-Known OpenID configuration for a list of supported scopes and, optionally, asks for profile and groups scopes if those are available. Warning In order to populate the input.session.teams value in the Login Policies Spacelift tries to fetch the groups claim. For many Identity Providers, this claim has to be manually set and configured. Bear in mind that some providers such as Google Workspace do not support retrieving groups of given users. OIDC Setup Guides \u00bb The following are links to example implementations you can use as a reference/guide for setting up your own OIDC integration. GitLab Okta OneLogin Azure AD If you can't find your OIDC provider in the list above, don't worry - we do support all OIDC providers as long as they support the email scope and return the user's email. Fortunately, most OIDC providers do. IdP-initiated SSO \u00bb While certainly more convenient, IdP-initiated SSO lacks some of the protections awarded by SP-initiated SSO and is thus inherently less safe. Since Spacelift manages some of your most valuable resources, we decided against supporting this feature. If our server detects an IdP-initiated SSO session, it simply redirects the browser using 303 See other HTTP status code to the endpoint that triggers a regular SP-initiated SSO flow. As a result, you can still access Spacelift by clicking on the link in your IdP catalog, but are not exposed to the vulnerabilities of the IdP-initiated SSO. Disabling SSO \u00bb In order to disable SSO integration for your Spacelift account, or change the IdP provider, please click the Disable button to delete the integration. This change takes effect immediately for new logins, and will invalidate existing sessions. New sessions will be created using the new SSO identity provider or - if none is set up - Spacelift will utilize the default identity provider that was used to create the account originally. Warning Again, please note that new usernames will occupy new seats, even if they're the same users registered with a different identity provider.","title":"Single Sign-On"},{"location":"integrations/single-sign-on/index.html#single-sign-on","text":"By default, Spacelift supports logging in using GitHub, GitLab, or Google. Some organizations however prefer a Single Sign-On approach, where access to resources is centralized. To accommodate this use-case, Spacelift supports Single Sign-On using SAML 2.0 or OIDC .","title":"Single Sign-On"},{"location":"integrations/single-sign-on/index.html#managed-identity-provider-vs-sso","text":"Tip The SSO integration can only be configured once the Spacelift account has been created using one of the supported Identity Providers . To create a Spacelift account, a user needs to choose one of the supported managed identity providers. That user then becomes the \"Managed IdP Admin\". If SSO is configured, the managed identity provider used to create the account and the associated admin are disabled and the first user to successfully log in becomes the \"SSO Admin\". Login policies are not evaluated for Managed IdP and SSO admins so that they cannot lock themselves out. As a side effect, there won\u2019t be any Login policy samples for them in the Policy Workbench . If SSO is disabled later, the managed identity provider and associated admin are re-enabled automatically.","title":"Managed Identity Provider vs SSO"},{"location":"integrations/single-sign-on/index.html#backup-credentials","text":"Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.","title":"Backup Credentials"},{"location":"integrations/single-sign-on/index.html#managing-integrations","text":"In order to manage Single Sign-On integrations on your Spacelift account, please go to the Settings section of your account view. Next, navigate to the Single Sign-On tab. If SSO is not enabled for your account, all you're going to see is instructions on how to get started. The first steps are always taken in your identity provider (Google Workspace, Okta, Auth0, ActiveDirectory, etc.). Navigate to your identity provider and create a dedicated SSO application filled with appropriate URLs taken from the Spacelift settings page presented below.","title":"Managing integrations"},{"location":"integrations/single-sign-on/index.html#setting-up-saml","text":"When setting up Spacelift on your identity provider, you may want to add three attribute mappings: FirstName is used to build human-friendly user name; LastName is used to build human-friendly user name; Teams can be used by login and stack access policies to determine the level access to the Spacelift account and/or individual Stacks; Depending on your identity provider and your use case, your mapping may be different. Especially with regards to Teams , some identity providers (eg. Okta ) will support an arbitrary list of memberships similar to GitHub teams out of the box, some will need extra customizations like (eg. Google Workspace ) and as a courtesy, we will flush your login history. Some identity providers (eg. Okta ) will allow you to provide a custom per-user SAML 2.0 Subject for SAML assertions. You could use this feature to map GitHub usernames to your identity provider users and thus get the exact same experience as when using GitHub as your identity provider. Warning When setting up SSO without this GitHub mapping, your future logins will appear as new users since Spacelift has no way of mapping those without your assistance. New users will count against your seat quota and you may run out of seats. If you run into this problem, you can contact us . Info Spacelift uses both HTTP-Redirect and HTTP-POST bindings for SAML 2.0. Most of the IdPs enable both by default, but if you run into any issues, please check your application settings.","title":"Setting up SAML"},{"location":"integrations/single-sign-on/index.html#nameid-format","text":"The NameID format specifies the format that Spacelift requests user identifiers from your identity provider. The user identifier is used as the Spacelift login, and each unique identifier will count against your seat quota. Some identity providers allow you to configure this format, but certain providers (eg. Azure AD) do not. If your identity provider does not allow the NameID format to be configured at their end, you can choose from one of the following options: Transient - an opaque identifier that is not guaranteed to remain the same between logins. Email Address - an email address. Persistent - an opaque identifier that remains the same between logins.","title":"NameID format"},{"location":"integrations/single-sign-on/index.html#saml-setup-guides","text":"The following are links to example implementations you can use as a reference/guide for setting up your own SAML integration. AWS IAM Identity Center (formerly known as AWS SSO) If you can't find your SAML provider in the list above, don't worry - we do support all SAML 2.0 providers.","title":"SAML Setup Guides"},{"location":"integrations/single-sign-on/index.html#setting-up-oidc","text":"When setting up Spacelift on your identity provider, you must make sure it supports the email scope and returns the corresponding email","title":"Setting up OIDC"},{"location":"integrations/single-sign-on/index.html#additional-claims","text":"Spacelift dynamically checks integrated Identity Provider's Well-Known OpenID configuration for a list of supported scopes and, optionally, asks for profile and groups scopes if those are available. Warning In order to populate the input.session.teams value in the Login Policies Spacelift tries to fetch the groups claim. For many Identity Providers, this claim has to be manually set and configured. Bear in mind that some providers such as Google Workspace do not support retrieving groups of given users.","title":"Additional claims"},{"location":"integrations/single-sign-on/index.html#oidc-setup-guides","text":"The following are links to example implementations you can use as a reference/guide for setting up your own OIDC integration. GitLab Okta OneLogin Azure AD If you can't find your OIDC provider in the list above, don't worry - we do support all OIDC providers as long as they support the email scope and return the user's email. Fortunately, most OIDC providers do.","title":"OIDC Setup Guides"},{"location":"integrations/single-sign-on/index.html#idp-initiated-sso","text":"While certainly more convenient, IdP-initiated SSO lacks some of the protections awarded by SP-initiated SSO and is thus inherently less safe. Since Spacelift manages some of your most valuable resources, we decided against supporting this feature. If our server detects an IdP-initiated SSO session, it simply redirects the browser using 303 See other HTTP status code to the endpoint that triggers a regular SP-initiated SSO flow. As a result, you can still access Spacelift by clicking on the link in your IdP catalog, but are not exposed to the vulnerabilities of the IdP-initiated SSO.","title":"IdP-initiated SSO"},{"location":"integrations/single-sign-on/index.html#disabling-sso","text":"In order to disable SSO integration for your Spacelift account, or change the IdP provider, please click the Disable button to delete the integration. This change takes effect immediately for new logins, and will invalidate existing sessions. New sessions will be created using the new SSO identity provider or - if none is set up - Spacelift will utilize the default identity provider that was used to create the account originally. Warning Again, please note that new usernames will occupy new seats, even if they're the same users registered with a different identity provider.","title":"Disabling SSO"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html","text":"AWS IAM Identity Center SAML Setup Guide \u00bb If you'd like to set up the ability to sign in to your Spacelift account using a SAML 2.0 integration with AWS IAM Identity Center (formerly known as AWS SSO), you've come to the right place. This example will walk you through the steps to get this set up, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section. Pre-requisites \u00bb Spacelift account, with access to admin permissions. AWS account which is a member of an AWS Organization, with permission to create AWS IAM Identity applications. Configure the AWS IAM Identity application \u00bb Log into the AWS account, go to the IAM Identity Center home page and finally, click on the \"Applications\" link. On that screen, click on the \"Add a new application\" button. Finally, click on the \"Add a custom SAML 2.0 application\" button. Set the \"Display name\" field to \"Spacelift\". Then, copy the URL for the \"IAM Identity Center SAML metadata file\" and head to the settings in your Spacelift account. Configure Spacelift SAML integration \u00bb From the navigation side bar menu, select \"Settings.\" Next, you'll want to click the Set Up button underneath the \"SAML Settings\" section. In the SAML settings: Set the value for \"NameID Format\" to \"Persistent\". Enable the \"Dynamic configuration\". Paste the URL you just copied in AWS in the \"IdP metadata URL\" field. Danger Do not click on the \"Save\" button yet , otherwise Spacelift will try to activate SAML integration right away and we are not completely done with the setup yet. If you clicked on the button anyway, you will be presented with an AWS login page and you will likely be unable to log in at this time. Don't worry. Just open another tab in your browser and go to your Spacelift account. As an administrator, you will be able to log in via the Identity Provider your used to create the account. From there, you will be able to activate the SAML integration once you have completed all the remaining steps documented below. Configure the AWS IAM Identity application (Continued) \u00bb Go back to the AWS console. In the \"Application metadata\" section, click on the \"If you don't have a metadata file, you can manually type your metadata values.\" link. Copy/paste the values for \"Single Sign-On URL\" and \"Entity ID (audience)\" from Spacelift to \"Application ACS URL\" and \"Application SAML audience\", respectively. Finally, click on the \"Save changes\" button. Set the attribute mappings \u00bb Go to the \"Attribute mappings\" tab, set the values as described below and click on the \"Save changes\" button. User attribute in the application Maps to this string value or user attribute in IAM Identity Center Format Subject ${user:subject} persistent FirstName ${user:givenName} basic LastName ${user:familyName} basic Teams ${user:groups} basic Warning Please note that while available, ${user:groups} is not officially supported by AWS IAM Identity Center and it will return the group ID (GUUID) and not the group name. There is currently no way to get the group name. Assign users and groups to the application \u00bb Make sure to assign users and/or groups to the SAML application in the \"Assigned users\" tab. Activate the Spacelift SAML integration \u00bb Back to Spacelift for the final step. You can finally click on the \"Save\" button on the SAML integration page. The page will reload and the AWS login page will be displayed. Use the credentials for a user that has access to the SAML application and you should be able to log into Spacelift.","title":"AWS IAM Identity Center SAML Setup Guide"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#aws-iam-identity-center-saml-setup-guide","text":"If you'd like to set up the ability to sign in to your Spacelift account using a SAML 2.0 integration with AWS IAM Identity Center (formerly known as AWS SSO), you've come to the right place. This example will walk you through the steps to get this set up, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.","title":"AWS IAM Identity Center SAML Setup Guide"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#pre-requisites","text":"Spacelift account, with access to admin permissions. AWS account which is a member of an AWS Organization, with permission to create AWS IAM Identity applications.","title":"Pre-requisites"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#configure-the-aws-iam-identity-application","text":"Log into the AWS account, go to the IAM Identity Center home page and finally, click on the \"Applications\" link. On that screen, click on the \"Add a new application\" button. Finally, click on the \"Add a custom SAML 2.0 application\" button. Set the \"Display name\" field to \"Spacelift\". Then, copy the URL for the \"IAM Identity Center SAML metadata file\" and head to the settings in your Spacelift account.","title":"Configure the AWS IAM Identity application"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#configure-spacelift-saml-integration","text":"From the navigation side bar menu, select \"Settings.\" Next, you'll want to click the Set Up button underneath the \"SAML Settings\" section. In the SAML settings: Set the value for \"NameID Format\" to \"Persistent\". Enable the \"Dynamic configuration\". Paste the URL you just copied in AWS in the \"IdP metadata URL\" field. Danger Do not click on the \"Save\" button yet , otherwise Spacelift will try to activate SAML integration right away and we are not completely done with the setup yet. If you clicked on the button anyway, you will be presented with an AWS login page and you will likely be unable to log in at this time. Don't worry. Just open another tab in your browser and go to your Spacelift account. As an administrator, you will be able to log in via the Identity Provider your used to create the account. From there, you will be able to activate the SAML integration once you have completed all the remaining steps documented below.","title":"Configure Spacelift SAML integration"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#configure-the-aws-iam-identity-application-continued","text":"Go back to the AWS console. In the \"Application metadata\" section, click on the \"If you don't have a metadata file, you can manually type your metadata values.\" link. Copy/paste the values for \"Single Sign-On URL\" and \"Entity ID (audience)\" from Spacelift to \"Application ACS URL\" and \"Application SAML audience\", respectively. Finally, click on the \"Save changes\" button.","title":"Configure the AWS IAM Identity application (Continued)"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#set-the-attribute-mappings","text":"Go to the \"Attribute mappings\" tab, set the values as described below and click on the \"Save changes\" button. User attribute in the application Maps to this string value or user attribute in IAM Identity Center Format Subject ${user:subject} persistent FirstName ${user:givenName} basic LastName ${user:familyName} basic Teams ${user:groups} basic Warning Please note that while available, ${user:groups} is not officially supported by AWS IAM Identity Center and it will return the group ID (GUUID) and not the group name. There is currently no way to get the group name.","title":"Set the attribute mappings"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#assign-users-and-groups-to-the-application","text":"Make sure to assign users and/or groups to the SAML application in the \"Assigned users\" tab.","title":"Assign users and groups to the application"},{"location":"integrations/single-sign-on/aws-iam-identity-saml-setup-guide.html#activate-the-spacelift-saml-integration","text":"Back to Spacelift for the final step. You can finally click on the \"Save\" button on the SAML integration page. The page will reload and the AWS login page will be displayed. Use the credentials for a user that has access to the SAML application and you should be able to log into Spacelift.","title":"Activate the Spacelift SAML integration"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html","text":"Azure AD OIDC Setup Guide \u00bb If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with Azure AD, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section. Pre-requisites \u00bb Spacelift account, with access to admin permissions Azure account, with an existing Azure Active Directory You'll need permissions to create an App Registration within your Azure AD Info Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On. Configure Account Settings \u00bb You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\" Setup OIDC \u00bb Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from Azure. For now, copy the authorized redirect URL as we will need to provide Azure this URL when configuring our Azure App Registration within your Azure AD. Azure Portal: Navigate to Azure Active Directory \u00bb Within your Azure Account, navigate to your Azure Active Directory where you'd like to setup the OIDC integration for. In this guide, we are using a Default Directory for example purposes. Azure AD: Create an App Registration \u00bb While you are within your Active Directory's settings, click on App registrations from the navigation, and then select New registration . Azure AD: App Registration Configuration \u00bb Give your application a name - Spacelift sounds like a good one. Configure your supported account types as per your login requirements. In this example, we are allowing Accounts in this organizational directory access to Spacelift. Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Redirect URI input as shown. Make sure you select Web for the type. Click Register . Azure AD: Add UPN Claim \u00bb Start by navigating to the Token configuration section of your application. Click the Add optional claim button, choose the ID token type, and select the upn claim: Click the Add button, making sure to enable the Turn on the Microsoft Graph profile permission checkbox on the popup that appears: Azure AD: Configure App Credentials \u00bb Navigate to the Certificates & secrets section of your application. Click the New client secret button. Give your secret a Description . Define an Expires duration . Info In this example, we are using 6 months for Expires. This means you will need to generate a new client secret in 6 months for your OIDC integration. Click Add. Now that we have the Client secret setup for our application, we'll need to take the Value and copy this into our Spacelift OIDC settings within the Secret input. Value within Azure AD = Spacelift's Secret input. Info Don't click Save in Spacelift just yet, we still need to get the Client ID and Provider URL for your application. The best way we've found to obtain the Client ID and Provider URL is to perform the following steps: Click on Overview within your Azure App. On this page Application (client) ID. Copy this value to Spacelift as the Client ID Next, Click Endpoints which should expand a page with the endpoints for your App. Copy the portion of the OpenID Connect metadata document URL that is highlighted as shown in the screenshot, and paste the value into Spacelift as the Provider URL . In summary, here are the values that should be copied over to Spacelift: Application (client) ID within Azure AD => Client ID on Spacelift Secret Value you generated => Secret input on Spacelift OpenID Connect metadata document URL => Provider URL on Spacelift Click Save. Azure AD OIDC Setup Completed \u00bb That's it! Your OIDC integration with Azure AD should now be configured (as per this example). Feel free to make any changes to your liking within your Azure AD App Registration configuration for the app that you just created.","title":"Azure AD OIDC Setup Guide"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-oidc-setup-guide","text":"If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with Azure AD, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.","title":"Azure AD OIDC Setup Guide"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#pre-requisites","text":"Spacelift account, with access to admin permissions Azure account, with an existing Azure Active Directory You'll need permissions to create an App Registration within your Azure AD Info Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.","title":"Pre-requisites"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#configure-account-settings","text":"You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\"","title":"Configure Account Settings"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#setup-oidc","text":"Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from Azure. For now, copy the authorized redirect URL as we will need to provide Azure this URL when configuring our Azure App Registration within your Azure AD.","title":"Setup OIDC"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-portal-navigate-to-azure-active-directory","text":"Within your Azure Account, navigate to your Azure Active Directory where you'd like to setup the OIDC integration for. In this guide, we are using a Default Directory for example purposes.","title":"Azure Portal: Navigate to Azure Active Directory"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-create-an-app-registration","text":"While you are within your Active Directory's settings, click on App registrations from the navigation, and then select New registration .","title":"Azure AD: Create an App Registration"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-app-registration-configuration","text":"Give your application a name - Spacelift sounds like a good one. Configure your supported account types as per your login requirements. In this example, we are allowing Accounts in this organizational directory access to Spacelift. Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Redirect URI input as shown. Make sure you select Web for the type. Click Register .","title":"Azure AD: App Registration Configuration"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-add-upn-claim","text":"Start by navigating to the Token configuration section of your application. Click the Add optional claim button, choose the ID token type, and select the upn claim: Click the Add button, making sure to enable the Turn on the Microsoft Graph profile permission checkbox on the popup that appears:","title":"Azure AD: Add UPN Claim"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-configure-app-credentials","text":"Navigate to the Certificates & secrets section of your application. Click the New client secret button. Give your secret a Description . Define an Expires duration . Info In this example, we are using 6 months for Expires. This means you will need to generate a new client secret in 6 months for your OIDC integration. Click Add. Now that we have the Client secret setup for our application, we'll need to take the Value and copy this into our Spacelift OIDC settings within the Secret input. Value within Azure AD = Spacelift's Secret input. Info Don't click Save in Spacelift just yet, we still need to get the Client ID and Provider URL for your application. The best way we've found to obtain the Client ID and Provider URL is to perform the following steps: Click on Overview within your Azure App. On this page Application (client) ID. Copy this value to Spacelift as the Client ID Next, Click Endpoints which should expand a page with the endpoints for your App. Copy the portion of the OpenID Connect metadata document URL that is highlighted as shown in the screenshot, and paste the value into Spacelift as the Provider URL . In summary, here are the values that should be copied over to Spacelift: Application (client) ID within Azure AD => Client ID on Spacelift Secret Value you generated => Secret input on Spacelift OpenID Connect metadata document URL => Provider URL on Spacelift Click Save.","title":"Azure AD: Configure App Credentials"},{"location":"integrations/single-sign-on/azure-ad-oidc-setup-guide.html#azure-ad-oidc-setup-completed","text":"That's it! Your OIDC integration with Azure AD should now be configured (as per this example). Feel free to make any changes to your liking within your Azure AD App Registration configuration for the app that you just created.","title":"Azure AD OIDC Setup Completed"},{"location":"integrations/single-sign-on/backup-credentials.html","text":"Backup Credentials \u00bb Spacelift is SSO-first. This means that you can only create a Spacelift account using one of the supported identity providers, and can then further configure custom SAML or OIDC integrations. However, usually you'd also like to set up a set of backup credentials that can be safely stored, in case you ever need to access Spacelift without going through your SSO provider. This might be useful in situations such as accidentally misconfiguring the SSO provider and locking oneself out, or the SSO provider having an outage. Creating Backup Credentials \u00bb In order to create a set of backup credentials, just create an API Key with admin permissions, as described in Spacelift API Key > Token . Then, securely store the API Key ID, as well as the first secret token in the downloaded secret file, the one right after Please use the following API secret when communicating with Spacelift programmatically: . Logging in with Backup Credentials \u00bb In order to log in using an API Key open https://<your-account>.app.spacelift.io/apikeytoken . There, you'll be able to provide the previously stored API key ID and secret. After clicking EXCHANGE , you'll be logged into the Spacelift UI. Additional Use-Cases \u00bb Since API Keys go through the login policy and you can specify the teams an API Key should be on, you can use the above functionality with a non-admin key to easily debug how other users see your account, for the purposes of i.e. debugging your login policy.","title":"Backup Credentials"},{"location":"integrations/single-sign-on/backup-credentials.html#backup-credentials","text":"Spacelift is SSO-first. This means that you can only create a Spacelift account using one of the supported identity providers, and can then further configure custom SAML or OIDC integrations. However, usually you'd also like to set up a set of backup credentials that can be safely stored, in case you ever need to access Spacelift without going through your SSO provider. This might be useful in situations such as accidentally misconfiguring the SSO provider and locking oneself out, or the SSO provider having an outage.","title":"Backup Credentials"},{"location":"integrations/single-sign-on/backup-credentials.html#creating-backup-credentials","text":"In order to create a set of backup credentials, just create an API Key with admin permissions, as described in Spacelift API Key > Token . Then, securely store the API Key ID, as well as the first secret token in the downloaded secret file, the one right after Please use the following API secret when communicating with Spacelift programmatically: .","title":"Creating Backup Credentials"},{"location":"integrations/single-sign-on/backup-credentials.html#logging-in-with-backup-credentials","text":"In order to log in using an API Key open https://<your-account>.app.spacelift.io/apikeytoken . There, you'll be able to provide the previously stored API key ID and secret. After clicking EXCHANGE , you'll be logged into the Spacelift UI.","title":"Logging in with Backup Credentials"},{"location":"integrations/single-sign-on/backup-credentials.html#additional-use-cases","text":"Since API Keys go through the login policy and you can specify the teams an API Key should be on, you can use the above functionality with a non-admin key to easily debug how other users see your account, for the purposes of i.e. debugging your login policy.","title":"Additional Use-Cases"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html","text":"GitLab OIDC Setup Guide \u00bb If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with GitLab, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section. Pre-requisites \u00bb Spacelift account, with access to admin permissions GitLab account, with permission to create GitLab Applications Info Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On. Configure Account Settings \u00bb You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\" Setup OIDC \u00bb Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from GitLab. For now copy the authorized redirect URL as we will need to provide GitLab this URL when configuring our GitLab application. GitLab: Create GitLab Application \u00bb Within your GitLab account, visit the Applications section of your account. Create your GitLab Application as shown, the application's Name can be whatever you'd like. Spacelift sounds like a great name to use though. Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Redirect URI input as shown. Ensure that the openId , profile and email scopes are check'd. Click Save Application . Configure OIDC Settings \u00bb Now that we have the GitLab Application setup, we'll need to take the Application ID and Secret to configure the Spacelift OIDC Settings. Application ID = Spacelift's Client ID Secret = Spacelift's Secret In Spacelift, the Provider URL depends on where you are using GitLab, if you are using GitLab.com this value can be set as https://gitlab.com Info When setting your Provider URL within Spacelift, do not include a trailing slash \"/\" at the end of your URL or you may receive an error. GitLab OIDC Setup Completed \u00bb That's it! Your OIDC integration with GitLab should now be fully configured.","title":"GitLab OIDC Setup Guide"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#gitlab-oidc-setup-guide","text":"If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with GitLab, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.","title":"GitLab OIDC Setup Guide"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#pre-requisites","text":"Spacelift account, with access to admin permissions GitLab account, with permission to create GitLab Applications Info Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.","title":"Pre-requisites"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#configure-account-settings","text":"You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\"","title":"Configure Account Settings"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#setup-oidc","text":"Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from GitLab. For now copy the authorized redirect URL as we will need to provide GitLab this URL when configuring our GitLab application.","title":"Setup OIDC"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#gitlab-create-gitlab-application","text":"Within your GitLab account, visit the Applications section of your account. Create your GitLab Application as shown, the application's Name can be whatever you'd like. Spacelift sounds like a great name to use though. Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Redirect URI input as shown. Ensure that the openId , profile and email scopes are check'd. Click Save Application .","title":"GitLab: Create GitLab Application"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#configure-oidc-settings","text":"Now that we have the GitLab Application setup, we'll need to take the Application ID and Secret to configure the Spacelift OIDC Settings. Application ID = Spacelift's Client ID Secret = Spacelift's Secret In Spacelift, the Provider URL depends on where you are using GitLab, if you are using GitLab.com this value can be set as https://gitlab.com Info When setting your Provider URL within Spacelift, do not include a trailing slash \"/\" at the end of your URL or you may receive an error.","title":"Configure OIDC Settings"},{"location":"integrations/single-sign-on/gitlab-oidc-setup-guide.html#gitlab-oidc-setup-completed","text":"That's it! Your OIDC integration with GitLab should now be fully configured.","title":"GitLab OIDC Setup Completed"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html","text":"Okta OIDC Setup Guide \u00bb If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with Okta, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section. Pre-requisites \u00bb Spacelift account, with access to admin permissions Okta account, with permission to create Okta App Integrations Info Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On. Configure Account Settings \u00bb You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\" Setup OIDC \u00bb Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from Okta. For now copy the authorized redirect URL as we will need to provide Okta this URL when configuring our Okta App Integration. Okta: Select Applications \u00bb In a new browser tab, open your Okta account. Select the Applications link from the navigation. Okta: Create App Integration \u00bb Click the \"Create App Integration\" button. For the sign in type, ensure you select OIDC - for the application type, select Web Application. Okta: Configure App Integration \u00bb Give your app integration a name - Spacelift sounds like a good one. Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Sign-in redirect URIs input as shown. As far as the assignments for this app integration, that's up to you at the end of the day. This determines what users from your Okta account will be able to access Spacelift. Click Save . Okta: Configure Group Claim \u00bb Click on the Sign On tab within your newly created Okta App Integration, You'll need to edit the groups claim type to return groups you consider useful in Spacelift Login Policies. For testing purposes, you could set it to Matches regex with .* for the regex value. Configure OIDC Settings \u00bb Switch back to the General tab. Now that we have the Okta App Integration setup, we'll need to take the Client ID , Client Secret , and Okta domain , to configure the Spacelift OIDC Settings. Info The Okta Domain will be set as the \"Provider URL\" in your Spacelift OIDC settings. Ensure that you prefix this URL with https:// . Okta OIDC Setup Completed \u00bb That's it! Your OIDC integration with Okta should now be fully configured. Feel free to make any changes to your liking within your Okta App Integration configuration for the app that you just created.","title":"Okta OIDC Setup Guide"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-oidc-setup-guide","text":"If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with Okta, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.","title":"Okta OIDC Setup Guide"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#pre-requisites","text":"Spacelift account, with access to admin permissions Okta account, with permission to create Okta App Integrations Info Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.","title":"Pre-requisites"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#configure-account-settings","text":"You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\"","title":"Configure Account Settings"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#setup-oidc","text":"Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from Okta. For now copy the authorized redirect URL as we will need to provide Okta this URL when configuring our Okta App Integration.","title":"Setup OIDC"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-select-applications","text":"In a new browser tab, open your Okta account. Select the Applications link from the navigation.","title":"Okta: Select Applications"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-create-app-integration","text":"Click the \"Create App Integration\" button. For the sign in type, ensure you select OIDC - for the application type, select Web Application.","title":"Okta: Create App Integration"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-configure-app-integration","text":"Give your app integration a name - Spacelift sounds like a good one. Remember the authorized redirect URL we copied earlier from Spacelift? We'll need that in this step. You'll want to paste that URL into the Sign-in redirect URIs input as shown. As far as the assignments for this app integration, that's up to you at the end of the day. This determines what users from your Okta account will be able to access Spacelift. Click Save .","title":"Okta: Configure App Integration"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-configure-group-claim","text":"Click on the Sign On tab within your newly created Okta App Integration, You'll need to edit the groups claim type to return groups you consider useful in Spacelift Login Policies. For testing purposes, you could set it to Matches regex with .* for the regex value.","title":"Okta: Configure Group Claim"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#configure-oidc-settings","text":"Switch back to the General tab. Now that we have the Okta App Integration setup, we'll need to take the Client ID , Client Secret , and Okta domain , to configure the Spacelift OIDC Settings. Info The Okta Domain will be set as the \"Provider URL\" in your Spacelift OIDC settings. Ensure that you prefix this URL with https:// .","title":"Configure OIDC Settings"},{"location":"integrations/single-sign-on/okta-oidc-setup-guide.html#okta-oidc-setup-completed","text":"That's it! Your OIDC integration with Okta should now be fully configured. Feel free to make any changes to your liking within your Okta App Integration configuration for the app that you just created.","title":"Okta OIDC Setup Completed"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html","text":"OneLogin OIDC Setup Guide \u00bb If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with OneLogin, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section. Pre-requisites \u00bb Spacelift account, with access to admin permissions OneLogin account, with permission to create OneLogin Applications Info Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On. Configure Account Settings \u00bb You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\" Setup OIDC \u00bb Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from OneLogin. For now copy the authorized redirect URL as we will need to provide OneLogin this URL when configuring our OneLogin application. OneLogin: Select Applications \u00bb In a new browser tab, open your OneLogin account and visit the Administration page. Select the Applications link from the navigation. OneLogin: Add Application \u00bb Click the Add App button. Search for OpenId Connect and select the result as shown. Give your new OneLogin App a name, Spacelift sounds like a good one. In regards to \"Visible in portal\" this is a OneLogin configuration decision that's up to you. In this example, we are enabled this value. In the app navigation, navigate to the Configuration section. Input your Login URL for example \"https://AccountName.app.spacelift.io\" Make sure to replace AccountName with your actual account name. Next, paste the previously copied authorized redirect URL into the Redirect URI's input field. Once done, remember to click on the Save button. In the app navigation, click on the SSO section. Now that we have the OneLogin App setup, we'll need to take the Client ID , Client Secret , and Issuer URL , to configure the Spacelift OIDC Settings Info Important: You'll need to ensure your OneLogin user has access to the OneLogin App you just created, or else you will receive an unauthorized error when clicking save. OneLogin OIDC Setup Completed \u00bb That's it! OIDC integration with OneLogin should now be fully configured. Feel free to make any changes to your liking within your OneLogin App configuration. You'll of course need to configure any users/groups within your OneLogin account to have access to this newly created app.","title":"OneLogin OIDC Setup Guide"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-oidc-setup-guide","text":"If you'd like to set up the ability to sign in to your Spacelift account using an OIDC integration with OneLogin, you've come to the right place. This example will walk you through the steps to get this setup, and you'll have Single Sign-On running in no time! Warning Before setting up SSO, it's recommended to create backup credentials for your Spacelift account for use in case of SSO misconfiguration, or for other break-glass procedures. You can find more about this in the Backup Credentials section.","title":"OneLogin OIDC Setup Guide"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#pre-requisites","text":"Spacelift account, with access to admin permissions OneLogin account, with permission to create OneLogin Applications Info Please note you'll need to be an admin on the Spacelift account to access the account settings to configure Single Sign-On.","title":"Pre-requisites"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#configure-account-settings","text":"You'll need to visit the Spacelift account settings page to set up this integration, from the navigation side bar menu, select \"Settings.\"","title":"Configure Account Settings"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#setup-oidc","text":"Next, you'll want to click the Set Up box underneath the \"OIDC Settings\" section. This will expand some configuration we will need to fill out in a few minutes, which we will be obtaining from OneLogin. For now copy the authorized redirect URL as we will need to provide OneLogin this URL when configuring our OneLogin application.","title":"Setup OIDC"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-select-applications","text":"In a new browser tab, open your OneLogin account and visit the Administration page. Select the Applications link from the navigation.","title":"OneLogin: Select Applications"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-add-application","text":"Click the Add App button. Search for OpenId Connect and select the result as shown. Give your new OneLogin App a name, Spacelift sounds like a good one. In regards to \"Visible in portal\" this is a OneLogin configuration decision that's up to you. In this example, we are enabled this value. In the app navigation, navigate to the Configuration section. Input your Login URL for example \"https://AccountName.app.spacelift.io\" Make sure to replace AccountName with your actual account name. Next, paste the previously copied authorized redirect URL into the Redirect URI's input field. Once done, remember to click on the Save button. In the app navigation, click on the SSO section. Now that we have the OneLogin App setup, we'll need to take the Client ID , Client Secret , and Issuer URL , to configure the Spacelift OIDC Settings Info Important: You'll need to ensure your OneLogin user has access to the OneLogin App you just created, or else you will receive an unauthorized error when clicking save.","title":"OneLogin: Add Application"},{"location":"integrations/single-sign-on/onelogin-oidc-setup-guide.html#onelogin-oidc-setup-completed","text":"That's it! OIDC integration with OneLogin should now be fully configured. Feel free to make any changes to your liking within your OneLogin App configuration. You'll of course need to configure any users/groups within your OneLogin account to have access to this newly created app.","title":"OneLogin OIDC Setup Completed"},{"location":"integrations/source-control/index.html","text":"Source Control \u00bb Spacelift supports the following source control tools: GitHub GitLab Bitbucket Cloud Datacenter/Server Azure DevOps","title":"Source Control"},{"location":"integrations/source-control/index.html#source-control","text":"Spacelift supports the following source control tools: GitHub GitLab Bitbucket Cloud Datacenter/Server Azure DevOps","title":"Source Control"},{"location":"integrations/source-control/azure-devops.html","text":"Azure DevOps \u00bb Spacelift supports using Azure DevOps as the source of code for your stacks and modules . Setting up the integration \u00bb In order to set up the integration from the Spacelift side, please navigate to the VCS providers section of the admin Settings page, find the Azure DevOps integration and click the Set up button: This should open a form like this one: Finding your Organization URL \u00bb Now you'll have to fill in the Organization URL, which is the main URL of your Azure DevOps organization. Info The Azure DevOps Organization URL usually has the following format: https://dev.azure.com/{my-organization-name} Depending on when your Azure DevOps organization was created, it may use a different format, for example: https://{my-organization-name}.visualstudio.com You can find out more about Azure DevOps URLs here . Creating a Personal Access Token \u00bb In order to create a Personal access token you need to: 1. Go to User settings -> Personal access tokens (in the top right section of the Azure DevOps page) 2. On the Personal Access Tokens page click + New Token 3. Create a new personal access token. There, you will need to set the name of your token, expiration and scope. For Spacelift give it Code (Source code, repositories, pull requests and notifications) Read & write access. 4. Once the token is created, put it into the Personal access token field on the Spacelift Azure DevOps setup form. Creating the Integration \u00bb After doing all this you should have all fields filled in. If all the data is correct, after saving you should see two notifications in the bottom right part of Spacelift: New integration was created; and Connecting to Azure DevOps succeeded . Configuring Webhooks \u00bb In order for Spacelift to be notified of any changes made in your Azure DevOps repos, you need to setup webhooks in Azure DevOps. You can find your webhook endpoint and webhook password under the Azure DevOps VCS integration section: For each Azure DevOps project you want to use with Spacelift, you now have to go into its Project settings -> Service hooks -> Create subscription . Within the services list choose Web Hooks and click Next . You will need to create a Web Hooks Service Hook for the following events: Code pushed. Pull request created. Pull request merge attempted. Pull request updated. Pull request commented on. Let's first create the Code pushed webhook. Once on the Trigger page of New Service Hooks Subscription window, select Code pushed in the Trigger on this type of event dropdown and click Next . After clicking Next you should see the Action page. Under the Settings section fill in the Spacelift Webhook endpoint URL. Leave Basic authentication username empty and put the Webhook password under Basic authentication password and click Finish . Once done you should see the list of configured Service Hooks. Repeat the same process for the others. Afterwards you should see the configured webhooks on the Service Hooks settings page. Using the Integration \u00bb When creating a Stack, you will now be able to choose the Azure DevOps provider and a repository inside of it: Unlinking the Integration \u00bb If you no longer need the integration, you can remove it via the Unlink button on the VCS settings page. Please also remember to remove any Spacelift webhooks from your repositories.","title":"Azure DevOps"},{"location":"integrations/source-control/azure-devops.html#azure-devops","text":"Spacelift supports using Azure DevOps as the source of code for your stacks and modules .","title":"Azure DevOps"},{"location":"integrations/source-control/azure-devops.html#setting-up-the-integration","text":"In order to set up the integration from the Spacelift side, please navigate to the VCS providers section of the admin Settings page, find the Azure DevOps integration and click the Set up button: This should open a form like this one:","title":"Setting up the integration"},{"location":"integrations/source-control/azure-devops.html#finding-your-organization-url","text":"Now you'll have to fill in the Organization URL, which is the main URL of your Azure DevOps organization. Info The Azure DevOps Organization URL usually has the following format: https://dev.azure.com/{my-organization-name} Depending on when your Azure DevOps organization was created, it may use a different format, for example: https://{my-organization-name}.visualstudio.com You can find out more about Azure DevOps URLs here .","title":"Finding your Organization URL"},{"location":"integrations/source-control/azure-devops.html#creating-a-personal-access-token","text":"In order to create a Personal access token you need to: 1. Go to User settings -> Personal access tokens (in the top right section of the Azure DevOps page) 2. On the Personal Access Tokens page click + New Token 3. Create a new personal access token. There, you will need to set the name of your token, expiration and scope. For Spacelift give it Code (Source code, repositories, pull requests and notifications) Read & write access. 4. Once the token is created, put it into the Personal access token field on the Spacelift Azure DevOps setup form.","title":"Creating a Personal Access Token"},{"location":"integrations/source-control/azure-devops.html#creating-the-integration","text":"After doing all this you should have all fields filled in. If all the data is correct, after saving you should see two notifications in the bottom right part of Spacelift: New integration was created; and Connecting to Azure DevOps succeeded .","title":"Creating the Integration"},{"location":"integrations/source-control/azure-devops.html#configuring-webhooks","text":"In order for Spacelift to be notified of any changes made in your Azure DevOps repos, you need to setup webhooks in Azure DevOps. You can find your webhook endpoint and webhook password under the Azure DevOps VCS integration section: For each Azure DevOps project you want to use with Spacelift, you now have to go into its Project settings -> Service hooks -> Create subscription . Within the services list choose Web Hooks and click Next . You will need to create a Web Hooks Service Hook for the following events: Code pushed. Pull request created. Pull request merge attempted. Pull request updated. Pull request commented on. Let's first create the Code pushed webhook. Once on the Trigger page of New Service Hooks Subscription window, select Code pushed in the Trigger on this type of event dropdown and click Next . After clicking Next you should see the Action page. Under the Settings section fill in the Spacelift Webhook endpoint URL. Leave Basic authentication username empty and put the Webhook password under Basic authentication password and click Finish . Once done you should see the list of configured Service Hooks. Repeat the same process for the others. Afterwards you should see the configured webhooks on the Service Hooks settings page.","title":"Configuring Webhooks"},{"location":"integrations/source-control/azure-devops.html#using-the-integration","text":"When creating a Stack, you will now be able to choose the Azure DevOps provider and a repository inside of it:","title":"Using the Integration"},{"location":"integrations/source-control/azure-devops.html#unlinking-the-integration","text":"If you no longer need the integration, you can remove it via the Unlink button on the VCS settings page. Please also remember to remove any Spacelift webhooks from your repositories.","title":"Unlinking the Integration"},{"location":"integrations/source-control/bitbucket-cloud.html","text":"Bitbucket Cloud \u00bb Spacelift supports using Bitbucket Cloud as the source of code for your stacks and modules . Setting up the integration \u00bb In order to set up the integration from the Spacelift side, please navigate to the VCS providers section of the admin Settings page, find the Bitbucket Cloud integration and click the Set up button: This should open a form like this one: Now you'll have to fill in the Username, which is a username of your Bitbucket Cloud account. In order to get the App password you'll need to go to the Bitbucket Cloud site and navigate to Personal settings -> App passwords (it's under Access management) -> Create app password . There, you will need to give your new app password a label and give it read access to repositories and pull requests: This will give you an app password token which you can put into the App Password field in the integration configuration. After doing all this you should have all fields filled in. After saving, you'll receive your webhook endpoint: For each repository you want to use with Spacelift, you now have to go into its Repository settings -> Webhooks -> Add webhook , and configure the webhook accordingly, by activating the following events: Repository > Push Pull Request > Created Pull Request > Updated Pull Request > Approved Pull Request > Approval removed Pull Request > Merged Pull Request > Comment created It should look something like the following: The last step is to install the Pull Request Commit Links app to be able to use this API. This is done automatically when you go to the commit's details and then click \"Pull requests\" link. When creating a Stack, you will now be able to choose the Bitbucket Cloud provider and a repository inside of it: Unlinking the Integration \u00bb If you no-longer need the integration, you can remove it via the Unlink button on the VCS settings page. Please also remember to remove any Spacelift webhooks from your repositories.","title":"Bitbucket Cloud"},{"location":"integrations/source-control/bitbucket-cloud.html#bitbucket-cloud","text":"Spacelift supports using Bitbucket Cloud as the source of code for your stacks and modules .","title":"Bitbucket Cloud"},{"location":"integrations/source-control/bitbucket-cloud.html#setting-up-the-integration","text":"In order to set up the integration from the Spacelift side, please navigate to the VCS providers section of the admin Settings page, find the Bitbucket Cloud integration and click the Set up button: This should open a form like this one: Now you'll have to fill in the Username, which is a username of your Bitbucket Cloud account. In order to get the App password you'll need to go to the Bitbucket Cloud site and navigate to Personal settings -> App passwords (it's under Access management) -> Create app password . There, you will need to give your new app password a label and give it read access to repositories and pull requests: This will give you an app password token which you can put into the App Password field in the integration configuration. After doing all this you should have all fields filled in. After saving, you'll receive your webhook endpoint: For each repository you want to use with Spacelift, you now have to go into its Repository settings -> Webhooks -> Add webhook , and configure the webhook accordingly, by activating the following events: Repository > Push Pull Request > Created Pull Request > Updated Pull Request > Approved Pull Request > Approval removed Pull Request > Merged Pull Request > Comment created It should look something like the following: The last step is to install the Pull Request Commit Links app to be able to use this API. This is done automatically when you go to the commit's details and then click \"Pull requests\" link. When creating a Stack, you will now be able to choose the Bitbucket Cloud provider and a repository inside of it:","title":"Setting up the integration"},{"location":"integrations/source-control/bitbucket-cloud.html#unlinking-the-integration","text":"If you no-longer need the integration, you can remove it via the Unlink button on the VCS settings page. Please also remember to remove any Spacelift webhooks from your repositories.","title":"Unlinking the Integration"},{"location":"integrations/source-control/bitbucket-datacenter-server.html","text":"Bitbucket Datacenter/Server \u00bb Spacelift supports using an on-premises Bitbucket installation as the source of code for your stacks and modules . Setting up the integration \u00bb Creating the integration \u00bb In order to set up the integration from the Spacelift side, please navigate to the VCS Providers section of the admin Settings page and click the Set up button next to the Bitbucket Data Center integration: This should open a form like this one: Now you'll have to fill in the API host URL, which is the URL on which Spacelift will access the Bitbucket server. The user facing host URL is the address on which users of your Bitbucket instance access it. This could be an internal address inside of your company network, but could also be a public address if your Bitbucket instance is publicly available. Creating an access token \u00bb In order to use the integration, you need a user account for Spacelift to use, and you need to generate an access token for that account. The user account requires the following access: Read access to any projects Spacelift needs to be able to access. Write access to the repositories within those projects that Spacelift should have access to. Once you have a user account created, login as that user and go to Manage account -> Personal access tokens -> create . There, you will need to give your new access token a name and give it write access to repositories: This will give you an access token which you can put into the Access token field in the integration configuration. Saving the integration \u00bb Once you have your access token, enter it into Spacelift. At this point all the fields should be filled out: You can now save the integration. Configuring webhooks \u00bb Once you have saved your integration, you'll receive your webhook secret and endpoint: For each repository you want to use with Spacelift, you need to go into its Repository settings -> Webhooks -> Create webhook , and configure the webhooks accordingly, by activating the following events: Repository > Push Pull Request > Opened Pull Request > Source branch updated Pull Request > Modified Pull Request > Approved Pull Request > Unapproved Pull Request > Merged Pull Request > Comment added It should look something like this: Warning Don't forget to enter a secret when configuring your webhook. Bitbucket will allow you to create your webhook with no secret specified, but any webhook requests to Spacelift will fail without one configured. Using the integration \u00bb When creating a Stack, you will now be able to choose the Bitbucket Datacenter provider and a repository inside of it:","title":"Bitbucket Datacenter/Server"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#bitbucket-datacenterserver","text":"Spacelift supports using an on-premises Bitbucket installation as the source of code for your stacks and modules .","title":"Bitbucket Datacenter/Server"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#setting-up-the-integration","text":"","title":"Setting up the integration"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#creating-the-integration","text":"In order to set up the integration from the Spacelift side, please navigate to the VCS Providers section of the admin Settings page and click the Set up button next to the Bitbucket Data Center integration: This should open a form like this one: Now you'll have to fill in the API host URL, which is the URL on which Spacelift will access the Bitbucket server. The user facing host URL is the address on which users of your Bitbucket instance access it. This could be an internal address inside of your company network, but could also be a public address if your Bitbucket instance is publicly available.","title":"Creating the integration"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#creating-an-access-token","text":"In order to use the integration, you need a user account for Spacelift to use, and you need to generate an access token for that account. The user account requires the following access: Read access to any projects Spacelift needs to be able to access. Write access to the repositories within those projects that Spacelift should have access to. Once you have a user account created, login as that user and go to Manage account -> Personal access tokens -> create . There, you will need to give your new access token a name and give it write access to repositories: This will give you an access token which you can put into the Access token field in the integration configuration.","title":"Creating an access token"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#saving-the-integration","text":"Once you have your access token, enter it into Spacelift. At this point all the fields should be filled out: You can now save the integration.","title":"Saving the integration"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#configuring-webhooks","text":"Once you have saved your integration, you'll receive your webhook secret and endpoint: For each repository you want to use with Spacelift, you need to go into its Repository settings -> Webhooks -> Create webhook , and configure the webhooks accordingly, by activating the following events: Repository > Push Pull Request > Opened Pull Request > Source branch updated Pull Request > Modified Pull Request > Approved Pull Request > Unapproved Pull Request > Merged Pull Request > Comment added It should look something like this: Warning Don't forget to enter a secret when configuring your webhook. Bitbucket will allow you to create your webhook with no secret specified, but any webhook requests to Spacelift will fail without one configured.","title":"Configuring webhooks"},{"location":"integrations/source-control/bitbucket-datacenter-server.html#using-the-integration","text":"When creating a Stack, you will now be able to choose the Bitbucket Datacenter provider and a repository inside of it:","title":"Using the integration"},{"location":"integrations/source-control/github.html","text":"GitHub \u00bb One of the things we're most proud of at Spacelift is the deep integration with everyone's favorite version control system - GitHub. This integration is typically set up automatically for users who have selected GitHub as their login source, but if you are logging into Spacelift using a different identity provider (e.g. Google, GitLab, or another SSO provider), and do not have GitHub configured as a VCS Provider, see the following section on setting up the integration . This is also applicable for users who want to connect Spacelift to their GitHub Enterprise instance or to multiple GitHub accounts. Setting up the integration \u00bb Setting up the custom application \u00bb In some cases, using the Spacelift application from the Marketplace is not an option, or you might already have it installed and want to link another GitHub account with your Spacelift account. For these advanced uses cases, you can use the custom Spacelift application. Creating the custom application \u00bb In order to do so, navigate to the Source Code section of the Account Settings page in your Spacelift account, then click the Set Up button next to the GitHub (custom App) option: You will be presented with two options: Wizard Manual setup The easiest and recommended way to install the custom Spacelift application in your GitHub account is by using the wizard. Answer the questions and you should be set up in no time. You can create the custom Spacelift application for GitHub manually. Warning This should be used as a last resort when the other methods can not be used as it is more tedious and error-prone. After selecting the option to enter your details manually, you should see the following form: Before you can complete this step you need to create a GitHub App within GitHub. Start by navigating to the GitHub Apps page in the Developer Settings for your account/organization, and clicking on New GitHub App. You will need the Webhook endpoint and Webhook secret while creating your App, so take a note of them. You can either create the App in an individual user account or within an organization account: Give your app a name and homepage URL (these are only used for informational purposes within GitHub): Enter your Webhook URL and secret: Set the following Repository permissions: Permission Access Checks Read & write Contents Read-only Deployments Read & write Metadata Read-only Pull requests Read & write Webhooks Read & write Commit statuses Read & write Set the following Organization permissions: Permission Access Members Read-only Subscribe to the following events: Organization Pull request Pull request review Push Repository Finally, choose whether you want to allow the App to be installed on any account or only on the account it is being created in and click on Create GitHub App: Once your App has been created, make a note of the App ID in the About section: Now scroll down to the Private keys section of the page and click on Generate a private key: This will download a file onto your machine containing the private key for your GitHub app. The file will be named <app-name>.<date>.private-key.pem , for example spacelift.2021-05-11.private-key.pem . Now that your GitHub App has been created, go back to the integration configuration screen in Spacelift, and enter your API host URL (the URL to your GitHub server), the App ID , and paste the contents of your private key file into the Private key box: Info If you are using github.com set your API host URL as: https://api.github.com Click on the Save button to save your integration settings. Congratulations! You are almost done! \ud83c\udf89 The last step is to install the application you just created so that Spacelift can interact with GitHub. This is what the next section is about. Installing the custom application \u00bb Now that you've created a GitHub App and configured it in Spacelift, the last step is to install your App in one or more accounts or organizations you have access to. To do this, go back to GitHub, find your App in the GitHub Apps page in your account settings, and click on the Edit button next to it: Go to the Install App section, and click on the Install button next to the account your want Spacelift to access: Choose whether you want to allow Spacelift access to all the repositories in the account, or only certain ones: Congrats, you've just linked your GitHub account to Spacelift! Using GitHub with stacks and modules \u00bb If your Spacelift account is integrated with GitHub, the stack or module creation and editing forms will show a dropdown from which you can choose the VCS provider to use. GitHub will always come first, assuming that you've integrated it with Spacelift for a good reason: The rest of the process is exactly the same as with creating a GitHub-backed stack or module, so we won't go into further details. Team-based access \u00bb In order to spare you the need to separately manage access to Spacelift, you can reuse GitHub's native teams. If you're using GitHub as your identity provider (which is the default), upon login, Spacelift uses GitHub API to determine organization membership level and team membership within an organization and persists it in the session token which is valid for one hour. Based on that you can set up login policies to determine who can log in to your Spacelift account, and stack access policies that can grant an appropriate level of access to individual Stacks . Info The list of teams is empty for individual/private GitHub accounts. Notifications \u00bb Commit status notifications \u00bb Commit status notifications are triggered for proposed runs to provide feedback on the proposed changes to your stack - running a preview command (eg. terraform plan for Terraform) with the source code of a short-lived feature branch with the state and config of the stack that's pointing to another, long-lived branch. Here's what such a notification looks like: ...when the run is in progress ( initializing ): ...when it succeeds without changes : ...when it succeeds with changes : ...and when it fails: In each case, clicking on the Details link will take you to the GitHub check view showing more details about the run: The Check view provides high-level information about the changes introduced by the push, including the list of changing resources, including cost data if Infracost is set up. From this view you can also perform two types of Spacelift actions: Preview - execute a proposed run against the tested commit; Deploy - execute a tracked run against the tested commit; PR (Pre-merge) Deployments \u00bb The Deploy functionality has been introduced in response to customers used to the Atlantis approach, where the deployment happens from within a Pull Request itself rather than on merge, which we see as the default and most typical workflow. If you want to prevent users from deploying directly from GitHub, you can add a simple plan policy to that effect, based on the fact that the run trigger always indicates GitHub as the source (the exact format is github/$username ). 1 2 3 4 5 6 package spacelift deny [ \"Do not deploy from GitHub\" ] { input . spacelift . run . type == \"TRACKED\" startswith ( input . spacelift . run . triggered_by , \"github/\" ) } The effect is as follows: Using Spacelift checks to protect branches \u00bb You can use commit statuses to protect your branches tracked by Spacelift stacks by ensuring that proposed runs succeed before merging their Pull Requests: This is is an important part of our proposed workflow - please refer to this section for more details. Deployment status notifications \u00bb Deployments and their associated statuses are created by tracked runs to indicate that changes are being made to the Terraform state. A GitHub deployment is created and marked as Pending when the planning phase detects changes and a tracked run either transitions to Unconfirmed state or automatically starts applying the diff: If the user does not like the proposed changes during the manual review and discards the tracked run , its associated GitHub deployment is immediately marked as a Failure . Same happens when the user confirms the tracked run but the Applying phase fails: If the Applying phase succeeds (fingers crossed!), the deployment is marked as Active : The whole deployment history broken down by stack can be accessed from your repo's Environments section - a previously obscure feature that's recently getting more and more love from GitHub: That's what it looks like for our test repo, with just a singe stack pointing at it: GitHub deployment environment names are derived from their respective stack names. This can be customized by setting the ghenv: label on the stack. For example, if you have a stack named Production and you want to name the deployment environment I love bacon , you can set the ghenv:I love bacon label on the stack. You can also disable the creation of a GitHub deployments by setting the ghenv:- label on the stack. Info The Deployed links lead to their corresponding Spacelift tracked runs . Pull Requests \u00bb In order to help you keep track of all the pending changes to your infrastructure, Spacelift also has a PRs tab that lists all the active Pull Request against your tracked branch. Each of the entries shows the current status of the change as determined by Spacelift, and a link to the most recent Run responsible for determining that status: Note that this view is read-only - you can't change a Pull Request through here, but clicking on the name will take you to GitHub where you can make changes. Once a Pull Request is closed, whether with or merging or without merging, it disappears from this list. Proposed workflow \u00bb In this section, we'd like to propose a workflow that has worked for us and many other DevOps professionals working with infrastructure-as-code. Its simplest version is based on a single stack tracking a long-lived branch like main , and short-lived feature branches temporarily captured in Pull Requests. A more sophisticated version can involve multiple stacks and a process like GitFlow . Tip These are mere suggestions and Spacelift will fit pretty much any Git workflow, but feel free to experiment and find what works best for you. Single stack version \u00bb Let's say you have a single stack called Infra . Let's have it track the default master branch in the repository called... infra . Let's say you want to introduce some changes - define an Amazon S3 bucket, for example. What we suggest is opening a short-lived feature branch, making your change there, and opening a Pull Request from that branch to master . At this point, a proposed run is triggered by the push notification, and the result of running terraform plan with the new code but existing state and config is reported to the Pull Request. First, we should ensure that the Pull Request does not get merged to master without a successful run, so we'd protect the branch by requiring a successful status check from your stack. Second, we can decide whether we just need a tick from Spacelift, or we'd rather require a manual review . We generally believe that more eyes is always better, but sometimes that's not practicable. Still, it's possible to protect the tracked branch in a way that requires manual Pull Request approval before merging. We're almost there, but let's also consider a scenario where our coworkers are also busy modifying the same stack. One way of preventing snafus as a team and get meaningful feedback from Spacelift is to require that branches are up to date before merging . If the current feature branch is behind the PR target branch, it needs to be rebased, which triggers a fresh Spacelift run that will ultimately produce the newest and most relevant commit status. Multi-stack version \u00bb One frequent type of setup involves two similar or even identical environments - for example, staging and production . One approach would be to have them in a single repository but in different directories, setting project_root runtime configuration accordingly. This approach means changing the staging directory a lot and using as much or as little duplication as necessary to keep things moving, and a lot of commits will necessarily be no-ops for the production stack. This is a very flexible approach, and we generally like it, but it leaves Git history pretty messy and some people really don't like that. If you're in that group, you can create two long-lived Git branches, each linked to a different stack - the default staging branch linked to the staging stack, and a production branch linked to the production stack. Most development thus occurs on the staging branch and once the code is perfected there over a few iterations, a Pull Request can be opened from the staging to production branch, incorporating all the changes. That's essentially how we've seen most teams implement GitFlow . This approach keeps the history of the production branch clear and allows plenty of experimentation in the staging branch. With the above GitFlow-like setup, we propose protecting both staging and production branches in GitHub. To maximize flexibility, staging branch may require a green commit status from its associated stack but not necessarily a manual review. In the meantime, production branch should probably require both a manual approval and a green commit status from its associated stack. Webhook integrations \u00bb Below is the list of some of the GitHub webhooks we subscribe to with a brief explanation of what we do with those. Push events \u00bb Any time we receive a repository code push notification, we match it against Spacelift repositories and - if necessary - create runs . We'll also stop proposed runs that have been superseded by a newer commit on their branch. App installation created or deleted \u00bb When the Spacelift GitHub app is installed on an account, we create a corresponding Spacelift account. When the installation is deleted, we deleted the corresponding Spacelift account and all its data. Organization renamed \u00bb If a GitHub organization name is changed, we change the name of the corresponding account in Spacelift. Warning This is only applicable for accounts that were created using GitHub originally. Pull Request events \u00bb Whenever a Pull Request is opened or reopened, we generate a record in our database to show it on the Stack's PRs page. When it's closed, we delete that record. When it's synchronized (eg. new push) or renamed, we update the record accordingly. This way, what you see in Spacelift should be consistent with what you see in GitHub. Pull Request Review events \u00bb Whenever a review is added or dismissed from a Pull Request, we check whether a new run should be triggered based on any push policies attached to your stacks. This allows you to make decisions about whether or not to trigger runs based on the approval status of your Pull Request. Repository renamed \u00bb If a GitHub repository is renamed, we update its name in all the stacks pointing to it. GitHub Action \u00bb You can use the Setup Spacectl GitHub Action to install our spacectl CLI tool to easily interact with Spacelift. Unlinking GitHub and Spacelift \u00bb Uninstalling the Marketplace application Uninstalling the custom application If you wish to uninstall the Spacelift application you installed from the GitHub Marketplace, go to the GitHub account settings and select the Applications menu item. Click on the Configure button for the spacelift.io application. Finally, click on the Uninstall button. Go to the Developer settings of the GitHub account, then in the GitHub Apps section click on the Edit button for the Spacelift application. On the page for the Spacelift application, go to the Advanced section and click on the Delete GitHub App button. Confirm by typing the name of the application and it is gone. You can now remove the integration via the Unlink button on the Source Code page:","title":"GitHub"},{"location":"integrations/source-control/github.html#github","text":"One of the things we're most proud of at Spacelift is the deep integration with everyone's favorite version control system - GitHub. This integration is typically set up automatically for users who have selected GitHub as their login source, but if you are logging into Spacelift using a different identity provider (e.g. Google, GitLab, or another SSO provider), and do not have GitHub configured as a VCS Provider, see the following section on setting up the integration . This is also applicable for users who want to connect Spacelift to their GitHub Enterprise instance or to multiple GitHub accounts.","title":"GitHub"},{"location":"integrations/source-control/github.html#setting-up-the-integration","text":"","title":"Setting up the integration"},{"location":"integrations/source-control/github.html#setting-up-the-custom-application","text":"In some cases, using the Spacelift application from the Marketplace is not an option, or you might already have it installed and want to link another GitHub account with your Spacelift account. For these advanced uses cases, you can use the custom Spacelift application.","title":"Setting up the custom application"},{"location":"integrations/source-control/github.html#creating-the-custom-application","text":"In order to do so, navigate to the Source Code section of the Account Settings page in your Spacelift account, then click the Set Up button next to the GitHub (custom App) option: You will be presented with two options: Wizard Manual setup The easiest and recommended way to install the custom Spacelift application in your GitHub account is by using the wizard. Answer the questions and you should be set up in no time. You can create the custom Spacelift application for GitHub manually. Warning This should be used as a last resort when the other methods can not be used as it is more tedious and error-prone. After selecting the option to enter your details manually, you should see the following form: Before you can complete this step you need to create a GitHub App within GitHub. Start by navigating to the GitHub Apps page in the Developer Settings for your account/organization, and clicking on New GitHub App. You will need the Webhook endpoint and Webhook secret while creating your App, so take a note of them. You can either create the App in an individual user account or within an organization account: Give your app a name and homepage URL (these are only used for informational purposes within GitHub): Enter your Webhook URL and secret: Set the following Repository permissions: Permission Access Checks Read & write Contents Read-only Deployments Read & write Metadata Read-only Pull requests Read & write Webhooks Read & write Commit statuses Read & write Set the following Organization permissions: Permission Access Members Read-only Subscribe to the following events: Organization Pull request Pull request review Push Repository Finally, choose whether you want to allow the App to be installed on any account or only on the account it is being created in and click on Create GitHub App: Once your App has been created, make a note of the App ID in the About section: Now scroll down to the Private keys section of the page and click on Generate a private key: This will download a file onto your machine containing the private key for your GitHub app. The file will be named <app-name>.<date>.private-key.pem , for example spacelift.2021-05-11.private-key.pem . Now that your GitHub App has been created, go back to the integration configuration screen in Spacelift, and enter your API host URL (the URL to your GitHub server), the App ID , and paste the contents of your private key file into the Private key box: Info If you are using github.com set your API host URL as: https://api.github.com Click on the Save button to save your integration settings. Congratulations! You are almost done! \ud83c\udf89 The last step is to install the application you just created so that Spacelift can interact with GitHub. This is what the next section is about.","title":"Creating the custom application"},{"location":"integrations/source-control/github.html#installing-the-custom-application","text":"Now that you've created a GitHub App and configured it in Spacelift, the last step is to install your App in one or more accounts or organizations you have access to. To do this, go back to GitHub, find your App in the GitHub Apps page in your account settings, and click on the Edit button next to it: Go to the Install App section, and click on the Install button next to the account your want Spacelift to access: Choose whether you want to allow Spacelift access to all the repositories in the account, or only certain ones: Congrats, you've just linked your GitHub account to Spacelift!","title":"Installing the custom application"},{"location":"integrations/source-control/github.html#using-github-with-stacks-and-modules","text":"If your Spacelift account is integrated with GitHub, the stack or module creation and editing forms will show a dropdown from which you can choose the VCS provider to use. GitHub will always come first, assuming that you've integrated it with Spacelift for a good reason: The rest of the process is exactly the same as with creating a GitHub-backed stack or module, so we won't go into further details.","title":"Using GitHub with stacks and modules"},{"location":"integrations/source-control/github.html#team-based-access","text":"In order to spare you the need to separately manage access to Spacelift, you can reuse GitHub's native teams. If you're using GitHub as your identity provider (which is the default), upon login, Spacelift uses GitHub API to determine organization membership level and team membership within an organization and persists it in the session token which is valid for one hour. Based on that you can set up login policies to determine who can log in to your Spacelift account, and stack access policies that can grant an appropriate level of access to individual Stacks . Info The list of teams is empty for individual/private GitHub accounts.","title":"Team-based access"},{"location":"integrations/source-control/github.html#notifications","text":"","title":"Notifications"},{"location":"integrations/source-control/github.html#commit-status-notifications","text":"Commit status notifications are triggered for proposed runs to provide feedback on the proposed changes to your stack - running a preview command (eg. terraform plan for Terraform) with the source code of a short-lived feature branch with the state and config of the stack that's pointing to another, long-lived branch. Here's what such a notification looks like: ...when the run is in progress ( initializing ): ...when it succeeds without changes : ...when it succeeds with changes : ...and when it fails: In each case, clicking on the Details link will take you to the GitHub check view showing more details about the run: The Check view provides high-level information about the changes introduced by the push, including the list of changing resources, including cost data if Infracost is set up. From this view you can also perform two types of Spacelift actions: Preview - execute a proposed run against the tested commit; Deploy - execute a tracked run against the tested commit;","title":"Commit status notifications"},{"location":"integrations/source-control/github.html#pr-pre-merge-deployments","text":"The Deploy functionality has been introduced in response to customers used to the Atlantis approach, where the deployment happens from within a Pull Request itself rather than on merge, which we see as the default and most typical workflow. If you want to prevent users from deploying directly from GitHub, you can add a simple plan policy to that effect, based on the fact that the run trigger always indicates GitHub as the source (the exact format is github/$username ). 1 2 3 4 5 6 package spacelift deny [ \"Do not deploy from GitHub\" ] { input . spacelift . run . type == \"TRACKED\" startswith ( input . spacelift . run . triggered_by , \"github/\" ) } The effect is as follows:","title":"PR (Pre-merge) Deployments"},{"location":"integrations/source-control/github.html#using-spacelift-checks-to-protect-branches","text":"You can use commit statuses to protect your branches tracked by Spacelift stacks by ensuring that proposed runs succeed before merging their Pull Requests: This is is an important part of our proposed workflow - please refer to this section for more details.","title":"Using Spacelift checks to protect branches"},{"location":"integrations/source-control/github.html#deployment-status-notifications","text":"Deployments and their associated statuses are created by tracked runs to indicate that changes are being made to the Terraform state. A GitHub deployment is created and marked as Pending when the planning phase detects changes and a tracked run either transitions to Unconfirmed state or automatically starts applying the diff: If the user does not like the proposed changes during the manual review and discards the tracked run , its associated GitHub deployment is immediately marked as a Failure . Same happens when the user confirms the tracked run but the Applying phase fails: If the Applying phase succeeds (fingers crossed!), the deployment is marked as Active : The whole deployment history broken down by stack can be accessed from your repo's Environments section - a previously obscure feature that's recently getting more and more love from GitHub: That's what it looks like for our test repo, with just a singe stack pointing at it: GitHub deployment environment names are derived from their respective stack names. This can be customized by setting the ghenv: label on the stack. For example, if you have a stack named Production and you want to name the deployment environment I love bacon , you can set the ghenv:I love bacon label on the stack. You can also disable the creation of a GitHub deployments by setting the ghenv:- label on the stack. Info The Deployed links lead to their corresponding Spacelift tracked runs .","title":"Deployment status notifications"},{"location":"integrations/source-control/github.html#pull-requests","text":"In order to help you keep track of all the pending changes to your infrastructure, Spacelift also has a PRs tab that lists all the active Pull Request against your tracked branch. Each of the entries shows the current status of the change as determined by Spacelift, and a link to the most recent Run responsible for determining that status: Note that this view is read-only - you can't change a Pull Request through here, but clicking on the name will take you to GitHub where you can make changes. Once a Pull Request is closed, whether with or merging or without merging, it disappears from this list.","title":"Pull Requests"},{"location":"integrations/source-control/github.html#proposed-workflow","text":"In this section, we'd like to propose a workflow that has worked for us and many other DevOps professionals working with infrastructure-as-code. Its simplest version is based on a single stack tracking a long-lived branch like main , and short-lived feature branches temporarily captured in Pull Requests. A more sophisticated version can involve multiple stacks and a process like GitFlow . Tip These are mere suggestions and Spacelift will fit pretty much any Git workflow, but feel free to experiment and find what works best for you.","title":"Proposed workflow"},{"location":"integrations/source-control/github.html#single-stack-version","text":"Let's say you have a single stack called Infra . Let's have it track the default master branch in the repository called... infra . Let's say you want to introduce some changes - define an Amazon S3 bucket, for example. What we suggest is opening a short-lived feature branch, making your change there, and opening a Pull Request from that branch to master . At this point, a proposed run is triggered by the push notification, and the result of running terraform plan with the new code but existing state and config is reported to the Pull Request. First, we should ensure that the Pull Request does not get merged to master without a successful run, so we'd protect the branch by requiring a successful status check from your stack. Second, we can decide whether we just need a tick from Spacelift, or we'd rather require a manual review . We generally believe that more eyes is always better, but sometimes that's not practicable. Still, it's possible to protect the tracked branch in a way that requires manual Pull Request approval before merging. We're almost there, but let's also consider a scenario where our coworkers are also busy modifying the same stack. One way of preventing snafus as a team and get meaningful feedback from Spacelift is to require that branches are up to date before merging . If the current feature branch is behind the PR target branch, it needs to be rebased, which triggers a fresh Spacelift run that will ultimately produce the newest and most relevant commit status.","title":"Single stack version"},{"location":"integrations/source-control/github.html#multi-stack-version","text":"One frequent type of setup involves two similar or even identical environments - for example, staging and production . One approach would be to have them in a single repository but in different directories, setting project_root runtime configuration accordingly. This approach means changing the staging directory a lot and using as much or as little duplication as necessary to keep things moving, and a lot of commits will necessarily be no-ops for the production stack. This is a very flexible approach, and we generally like it, but it leaves Git history pretty messy and some people really don't like that. If you're in that group, you can create two long-lived Git branches, each linked to a different stack - the default staging branch linked to the staging stack, and a production branch linked to the production stack. Most development thus occurs on the staging branch and once the code is perfected there over a few iterations, a Pull Request can be opened from the staging to production branch, incorporating all the changes. That's essentially how we've seen most teams implement GitFlow . This approach keeps the history of the production branch clear and allows plenty of experimentation in the staging branch. With the above GitFlow-like setup, we propose protecting both staging and production branches in GitHub. To maximize flexibility, staging branch may require a green commit status from its associated stack but not necessarily a manual review. In the meantime, production branch should probably require both a manual approval and a green commit status from its associated stack.","title":"Multi-stack version"},{"location":"integrations/source-control/github.html#webhook-integrations","text":"Below is the list of some of the GitHub webhooks we subscribe to with a brief explanation of what we do with those.","title":"Webhook integrations"},{"location":"integrations/source-control/github.html#push-events","text":"Any time we receive a repository code push notification, we match it against Spacelift repositories and - if necessary - create runs . We'll also stop proposed runs that have been superseded by a newer commit on their branch.","title":"Push events"},{"location":"integrations/source-control/github.html#app-installation-created-or-deleted","text":"When the Spacelift GitHub app is installed on an account, we create a corresponding Spacelift account. When the installation is deleted, we deleted the corresponding Spacelift account and all its data.","title":"App installation created or deleted"},{"location":"integrations/source-control/github.html#organization-renamed","text":"If a GitHub organization name is changed, we change the name of the corresponding account in Spacelift. Warning This is only applicable for accounts that were created using GitHub originally.","title":"Organization renamed"},{"location":"integrations/source-control/github.html#pull-request-events","text":"Whenever a Pull Request is opened or reopened, we generate a record in our database to show it on the Stack's PRs page. When it's closed, we delete that record. When it's synchronized (eg. new push) or renamed, we update the record accordingly. This way, what you see in Spacelift should be consistent with what you see in GitHub.","title":"Pull Request events"},{"location":"integrations/source-control/github.html#pull-request-review-events","text":"Whenever a review is added or dismissed from a Pull Request, we check whether a new run should be triggered based on any push policies attached to your stacks. This allows you to make decisions about whether or not to trigger runs based on the approval status of your Pull Request.","title":"Pull Request Review events"},{"location":"integrations/source-control/github.html#repository-renamed","text":"If a GitHub repository is renamed, we update its name in all the stacks pointing to it.","title":"Repository renamed"},{"location":"integrations/source-control/github.html#github-action","text":"You can use the Setup Spacectl GitHub Action to install our spacectl CLI tool to easily interact with Spacelift.","title":"GitHub Action"},{"location":"integrations/source-control/github.html#unlinking-github-and-spacelift","text":"Uninstalling the Marketplace application Uninstalling the custom application If you wish to uninstall the Spacelift application you installed from the GitHub Marketplace, go to the GitHub account settings and select the Applications menu item. Click on the Configure button for the spacelift.io application. Finally, click on the Uninstall button. Go to the Developer settings of the GitHub account, then in the GitHub Apps section click on the Edit button for the Spacelift application. On the page for the Spacelift application, go to the Advanced section and click on the Delete GitHub App button. Confirm by typing the name of the application and it is gone. You can now remove the integration via the Unlink button on the Source Code page:","title":"Unlinking GitHub and Spacelift"},{"location":"integrations/source-control/gitlab.html","text":"GitLab \u00bb Spacelift supports using GitLab as the source of code for your stacks and modules . While we support both managed ( gitlab.com ) and self-hosted GitLab installations just the same, only one GitLab server and its associated token can be used by a single Spacelift account. Setup Guide \u00bb In order to set up the GitLab integration from the Spacelift side, please navigate to the Settings page of Spacelift, select source code and click the Set up button next to the GitLab: This should open a form like this one: In this step you will need to provide the API host URL of your GitLab server, the User facing host URL, and an API token generated for Spacelift to communicate with the GitLab API. Let's assume we don't have token handy, so let's navigate to our GitLab server (we'll just use gitlab.com ) to create one from the Access Tokens section of your User Settings page: Please give the personal access token a descriptive name and grant it api scope. Note that while we will only write commit statuses, merge request comments and environment deployments, GitLab's permissions are coarse enough to require us to take write on the whole thing. Warning Please note, when creating tokens bound to a GitLab user, the user is required to have \"Maintainer\" level access to any projects you require Spacelift to be able to access. Once you've created your personal API token, please pass it - along with the server API host - to the integration form in Spacelift and click the Save button: Congrats, you've just linked your GitLab account to Spacelift. You should be taken to the integration settings page where you can retrieve the webhook data - secret and endpoint - which you will need to integrate Spacelift stacks with GitLab projects . Don't worry, this data will be accessible again to Spacelift admins, so there's no need to persist it separately: Warning Unlike GitHub credentials which are specific to an organization rather than an individual, the GitLab integration uses personal credentials, which makes it more fragile in situations where an individual leaves the organization and deletes the access token. Thus, it may be a good idea to create \"virtual\" (machine) users in GitLab as a source of more stable credentials. Note however that this is a general concern, not one specific to Spacelift. Using GitLab with stacks and modules \u00bb If your Spacelift account is integrated with GitLab, the stack or module creation and editing forms will show a dropdown from which you can choose the VCS provider to use. GitLab will always come first, assuming that you've integrated it with Spacelift for a good reason: The rest of the process is exactly the same as with creating a GitHub-backed stack or module, so we won't be going into further details. An important thing though is that for every GitLab project that's being used by a Spacelift project (stack or module), you will need to set up a webhook to notify Spacelift about the project changes. That's where you will use the webhooks data from the previous step - the URL and webhook secret. Spacelift is interested in pushes, tags and merge requests, so make sure you add triggers for all these types of events: If that sounds like hassle (it sure does to us), you can do the same thing automatically using GitLab's Terraform provider . Warning Note that you only need to set it up one hook for each repo used by Spacelift, regardless of how many stacks it is used by. Setting up multiple hooks for a single repo may lead to unintended behavior. Regardless of whether you've created it manually or programmatically, once your project webhook is set up, your GitLab-powered stack or module is ready to use. Namespaces \u00bb When utilizing the Terraform provider to provision Spacelift Stacks for GitLab, you are required to specify a namespace . The namespace value should be set to the the grouping mechanism that your project (repository) is within. For example, if you are simply referencing a project (repository) within your GitLab account, that is not within any group, then the namespace value should be set to your GitLab username. If your project lives within a group, then the namespace should be set to the group slug that the project is within. For example, if you have project-a within group-1 the namespace would be group-1 . When using subgroups, you will also need to include these within your namespace references. GitLab provides a Namespaces API which you can use to find information about your project's namespace. The full_url attribute value is what you'll want to reference as this namespace for a given project. Spacelift in GitLab \u00bb Spacelift provides feedback to GitLab in a number of ways. Commits and merge requests \u00bb When a webhook containing a push or tag event is received by Spacelift, it may trigger a test run . Test runs provide feedback though GitLab's pipeline functionality. When viewed from a merge request, the pipeline looks like this: You can see all the Spacelift jobs executed as part of this pipeline by clicking through to its dedicated view: As you can see, the test job passed and gave some brief information about the proposed change - that - if applied - it would add a single resource. Also, for every merge request affected by the commit there will be a comment showing the exact change: Environments \u00bb Each Spacelift stack creates an Environment in GitLab where we report the status of each tracked run : For example, this successful run: ...is thus reflected in its respective GitLab environment: This functionality allows you to track Spacelift history directly from GitLab.","title":"GitLab"},{"location":"integrations/source-control/gitlab.html#gitlab","text":"Spacelift supports using GitLab as the source of code for your stacks and modules . While we support both managed ( gitlab.com ) and self-hosted GitLab installations just the same, only one GitLab server and its associated token can be used by a single Spacelift account.","title":"GitLab"},{"location":"integrations/source-control/gitlab.html#setup-guide","text":"In order to set up the GitLab integration from the Spacelift side, please navigate to the Settings page of Spacelift, select source code and click the Set up button next to the GitLab: This should open a form like this one: In this step you will need to provide the API host URL of your GitLab server, the User facing host URL, and an API token generated for Spacelift to communicate with the GitLab API. Let's assume we don't have token handy, so let's navigate to our GitLab server (we'll just use gitlab.com ) to create one from the Access Tokens section of your User Settings page: Please give the personal access token a descriptive name and grant it api scope. Note that while we will only write commit statuses, merge request comments and environment deployments, GitLab's permissions are coarse enough to require us to take write on the whole thing. Warning Please note, when creating tokens bound to a GitLab user, the user is required to have \"Maintainer\" level access to any projects you require Spacelift to be able to access. Once you've created your personal API token, please pass it - along with the server API host - to the integration form in Spacelift and click the Save button: Congrats, you've just linked your GitLab account to Spacelift. You should be taken to the integration settings page where you can retrieve the webhook data - secret and endpoint - which you will need to integrate Spacelift stacks with GitLab projects . Don't worry, this data will be accessible again to Spacelift admins, so there's no need to persist it separately: Warning Unlike GitHub credentials which are specific to an organization rather than an individual, the GitLab integration uses personal credentials, which makes it more fragile in situations where an individual leaves the organization and deletes the access token. Thus, it may be a good idea to create \"virtual\" (machine) users in GitLab as a source of more stable credentials. Note however that this is a general concern, not one specific to Spacelift.","title":"Setup Guide"},{"location":"integrations/source-control/gitlab.html#using-gitlab-with-stacks-and-modules","text":"If your Spacelift account is integrated with GitLab, the stack or module creation and editing forms will show a dropdown from which you can choose the VCS provider to use. GitLab will always come first, assuming that you've integrated it with Spacelift for a good reason: The rest of the process is exactly the same as with creating a GitHub-backed stack or module, so we won't be going into further details. An important thing though is that for every GitLab project that's being used by a Spacelift project (stack or module), you will need to set up a webhook to notify Spacelift about the project changes. That's where you will use the webhooks data from the previous step - the URL and webhook secret. Spacelift is interested in pushes, tags and merge requests, so make sure you add triggers for all these types of events: If that sounds like hassle (it sure does to us), you can do the same thing automatically using GitLab's Terraform provider . Warning Note that you only need to set it up one hook for each repo used by Spacelift, regardless of how many stacks it is used by. Setting up multiple hooks for a single repo may lead to unintended behavior. Regardless of whether you've created it manually or programmatically, once your project webhook is set up, your GitLab-powered stack or module is ready to use.","title":"Using GitLab with stacks and modules"},{"location":"integrations/source-control/gitlab.html#namespaces","text":"When utilizing the Terraform provider to provision Spacelift Stacks for GitLab, you are required to specify a namespace . The namespace value should be set to the the grouping mechanism that your project (repository) is within. For example, if you are simply referencing a project (repository) within your GitLab account, that is not within any group, then the namespace value should be set to your GitLab username. If your project lives within a group, then the namespace should be set to the group slug that the project is within. For example, if you have project-a within group-1 the namespace would be group-1 . When using subgroups, you will also need to include these within your namespace references. GitLab provides a Namespaces API which you can use to find information about your project's namespace. The full_url attribute value is what you'll want to reference as this namespace for a given project.","title":"Namespaces"},{"location":"integrations/source-control/gitlab.html#spacelift-in-gitlab","text":"Spacelift provides feedback to GitLab in a number of ways.","title":"Spacelift in GitLab"},{"location":"integrations/source-control/gitlab.html#commits-and-merge-requests","text":"When a webhook containing a push or tag event is received by Spacelift, it may trigger a test run . Test runs provide feedback though GitLab's pipeline functionality. When viewed from a merge request, the pipeline looks like this: You can see all the Spacelift jobs executed as part of this pipeline by clicking through to its dedicated view: As you can see, the test job passed and gave some brief information about the proposed change - that - if applied - it would add a single resource. Also, for every merge request affected by the commit there will be a comment showing the exact change:","title":"Commits and merge requests"},{"location":"integrations/source-control/gitlab.html#environments","text":"Each Spacelift stack creates an Environment in GitLab where we report the status of each tracked run : For example, this successful run: ...is thus reflected in its respective GitLab environment: This functionality allows you to track Spacelift history directly from GitLab.","title":"Environments"},{"location":"legal/cookie-policy.html","text":"Cookie Policy \u00bb Last updated: June 10, 2022 This Cookies Policy explains what Cookies are and how We use them. You should read this policy so You can understand what type of cookies We use, the information We collect using Cookies, and how that information is used. Cookies do not typically contain any information that personally identifies a user, but personal information that we store about You may be linked to the information stored in and obtained from Cookies. For further information on how We use, store and keep your personal data secure, see our Privacy Policy. We do not store sensitive personal information, such as mailing addresses, account passwords, etc., in the Cookies We use. Interpretation and Definitions \u00bb Interpretation \u00bb The words in which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in the plural. Definitions \u00bb For the purposes of this Cookies Policy: Company (referred to as either \"the Company\", \"We\", \"Us\", or \"Our\" in this Cookies Policy) refers to SPACELIFT, INC., 541 Jefferson Ave. Suite 100, Redwood City, CA 94063. Cookies are small files that are placed on Your computer, mobile device, or any other device by a website, containing details of your browsing history on that website among its many uses. Website refers to Spacelift.io, accessible from https://spacelift.io You mean the individual accessing or using the Website, or a company, or any legal entity on behalf of which such individual is accessing or using the Website, as applicable. The use of the Cookies \u00bb Type of Cookies We Use \u00bb Cookies can be \"Persistent\" or \"Session\" Cookies. Persistent Cookies remain on your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close your web browser. We use both session, and persistent Cookies for the purposes set out below: Necessary / Essential Cookies Type: Session Cookies Administered by: Us Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services. Cookies Policy / Notice Acceptance Cookies Type: Persistent Cookies Administered by: Us Purpose: These Cookies identify if users have accepted the use of cookies on the Website. Functionality Cookies Type: Persistent Cookies Administered by: Us Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website. Tracking and Performance Cookies Type: Persistent Cookies Administered by: Third-Parties Purpose: These Cookies are used to track information about traffic to the Website and how users use the Website. The information gathered via these Cookies may directly or indirectly identify you as an individual visitor. This is because the information collected is typically linked to a pseudonymous identifier associated with the device you use to access the Website. We may also use these Cookies to test new advertisements, pages, features or new functionality of the Website to see how our users react to them. Targeting and Advertising Cookies Type: Persistent Cookies Administered by: Third-Parties Purpose: These Cookies track your browsing habits to enable Us to show advertising which is more likely to be of interest to You. These Cookies use information about your browsing history to group You with other users who have similar interests. Based on that information, and with Our permission, third party advertisers can place Cookies to enable them to show adverts that We think will be relevant to your interests while You are on third-party websites. Social Media Cookies Type: Persistent Cookies Administered by: Third-Parties Purpose: In addition to Our own Cookies, We may also use various third parties Cookies to report usage statistics of the Website, deliver advertisements on and through the Website, and so on. These Cookies may be used when You share information using a social media networking website such as Facebook, Instagram, Twitter, or Google+. Your Choices Regarding Cookies \u00bb If You prefer to avoid using Cookies on the Website, first, You must disable the use of Cookies in your browser and then delete the Cookies saved in your browser associated with this website. You may use this option to prevent Cookies use at any time. If You do not accept Our Cookies, You may experience some inconvenience in your use of the Website, and some features may not function properly. If You'd like to delete Cookies or instruct your web browser to delete or refuse Cookies, please visit the help pages of your web browser. For the Chrome web browser, please visit this page from Google: https://support.google.com/accounts/answer/32050 For the Internet Explorer web browser, please visit this page from Microsoft: http://support.microsoft.com/kb/278835 For the Firefox web browser, please visit this page from Mozilla: https://support.mozilla.org/en-US/kb/delete-cookies-remove-info-websites-stored For the Safari web browser, please visit this page from Apple: https://support.apple.com/guide/safari/manage-cookies-and-website-data-sfri11471/mac For any other web browser, please visit your web browser's official web pages. More Information about Cookies \u00bb You can learn more about cookies: Cookies: What Do They Do? . Contact Us \u00bb If you have any questions about this Cookies Policy, You can contact us by email at privacy@spacelift.io .","title":"Cookie Policy"},{"location":"legal/cookie-policy.html#cookie-policy","text":"Last updated: June 10, 2022 This Cookies Policy explains what Cookies are and how We use them. You should read this policy so You can understand what type of cookies We use, the information We collect using Cookies, and how that information is used. Cookies do not typically contain any information that personally identifies a user, but personal information that we store about You may be linked to the information stored in and obtained from Cookies. For further information on how We use, store and keep your personal data secure, see our Privacy Policy. We do not store sensitive personal information, such as mailing addresses, account passwords, etc., in the Cookies We use.","title":"Cookie Policy"},{"location":"legal/cookie-policy.html#interpretation-and-definitions","text":"","title":"Interpretation and Definitions"},{"location":"legal/cookie-policy.html#interpretation","text":"The words in which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in the plural.","title":"Interpretation"},{"location":"legal/cookie-policy.html#definitions","text":"For the purposes of this Cookies Policy: Company (referred to as either \"the Company\", \"We\", \"Us\", or \"Our\" in this Cookies Policy) refers to SPACELIFT, INC., 541 Jefferson Ave. Suite 100, Redwood City, CA 94063. Cookies are small files that are placed on Your computer, mobile device, or any other device by a website, containing details of your browsing history on that website among its many uses. Website refers to Spacelift.io, accessible from https://spacelift.io You mean the individual accessing or using the Website, or a company, or any legal entity on behalf of which such individual is accessing or using the Website, as applicable.","title":"Definitions"},{"location":"legal/cookie-policy.html#the-use-of-the-cookies","text":"","title":"The use of the Cookies"},{"location":"legal/cookie-policy.html#type-of-cookies-we-use","text":"Cookies can be \"Persistent\" or \"Session\" Cookies. Persistent Cookies remain on your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close your web browser. We use both session, and persistent Cookies for the purposes set out below: Necessary / Essential Cookies Type: Session Cookies Administered by: Us Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services. Cookies Policy / Notice Acceptance Cookies Type: Persistent Cookies Administered by: Us Purpose: These Cookies identify if users have accepted the use of cookies on the Website. Functionality Cookies Type: Persistent Cookies Administered by: Us Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website. Tracking and Performance Cookies Type: Persistent Cookies Administered by: Third-Parties Purpose: These Cookies are used to track information about traffic to the Website and how users use the Website. The information gathered via these Cookies may directly or indirectly identify you as an individual visitor. This is because the information collected is typically linked to a pseudonymous identifier associated with the device you use to access the Website. We may also use these Cookies to test new advertisements, pages, features or new functionality of the Website to see how our users react to them. Targeting and Advertising Cookies Type: Persistent Cookies Administered by: Third-Parties Purpose: These Cookies track your browsing habits to enable Us to show advertising which is more likely to be of interest to You. These Cookies use information about your browsing history to group You with other users who have similar interests. Based on that information, and with Our permission, third party advertisers can place Cookies to enable them to show adverts that We think will be relevant to your interests while You are on third-party websites. Social Media Cookies Type: Persistent Cookies Administered by: Third-Parties Purpose: In addition to Our own Cookies, We may also use various third parties Cookies to report usage statistics of the Website, deliver advertisements on and through the Website, and so on. These Cookies may be used when You share information using a social media networking website such as Facebook, Instagram, Twitter, or Google+.","title":"Type of Cookies We Use"},{"location":"legal/cookie-policy.html#your-choices-regarding-cookies","text":"If You prefer to avoid using Cookies on the Website, first, You must disable the use of Cookies in your browser and then delete the Cookies saved in your browser associated with this website. You may use this option to prevent Cookies use at any time. If You do not accept Our Cookies, You may experience some inconvenience in your use of the Website, and some features may not function properly. If You'd like to delete Cookies or instruct your web browser to delete or refuse Cookies, please visit the help pages of your web browser. For the Chrome web browser, please visit this page from Google: https://support.google.com/accounts/answer/32050 For the Internet Explorer web browser, please visit this page from Microsoft: http://support.microsoft.com/kb/278835 For the Firefox web browser, please visit this page from Mozilla: https://support.mozilla.org/en-US/kb/delete-cookies-remove-info-websites-stored For the Safari web browser, please visit this page from Apple: https://support.apple.com/guide/safari/manage-cookies-and-website-data-sfri11471/mac For any other web browser, please visit your web browser's official web pages.","title":"Your Choices Regarding Cookies"},{"location":"legal/cookie-policy.html#more-information-about-cookies","text":"You can learn more about cookies: Cookies: What Do They Do? .","title":"More Information about Cookies"},{"location":"legal/cookie-policy.html#contact-us","text":"If you have any questions about this Cookies Policy, You can contact us by email at privacy@spacelift.io .","title":"Contact Us"},{"location":"legal/privacy.html","text":"Privacy \u00bb Thank you for choosing to be part of our community at Spacelift (\u201ccompany\u201d, \u201cwe\u201d, \u201cus\u201d, or \u201cour\u201d). We are committed to protecting your personal information and your right to privacy. If you have any questions or concerns about our policy, or our practices with regards to your personal information, please contact us at privacy@spacelift.io . When you visit our and use our services, you trust us with your personal information. We take your privacy very seriously. In this privacy notice, we describe our privacy policy. We seek to explain to you in the clearest way possible what information we collect, how we use it and what rights you have in relation to it. We hope you take some time to read through it carefully, as it is important. If there are any terms in this privacy policy that you do not agree with, please discontinue use of our and our services. This privacy policy applies to all information collected through our and/or any related services, sales, marketing or events (we refer to them collectively in this privacy policy as the \"Sites\"). Please read this privacy policy carefully as it will help you make informed decisions about sharing your personal information with us. 1. What information do we collect? \u00bb Personal information you disclose to us \u00bb We collect personal information that you voluntarily provide to us when registering at the expressing an interest in obtaining information about us or our products and services, when participating in activities on the or otherwise contacting us. The personal information that we collect depends on the context of your interactions with us and the site, the choices you make and the products and features you use. The personal information we collect can include the following: Name and Contact Data. We may collect your first and last name, email address, and other similar contact data from contact forms. Social Media Login Data. We provide you with the option to register using social media account details, like your GitHub. If you choose to register in this way, we will collect the Information described in the relevant section below. All personal information that you provide to us must be true, complete and accurate, and you must notify us of any changes to such personal information. Information automatically collected \u00bb We automatically collect certain information when you visit, use or navigate the site. This information does not reveal your specific identity (like your name or contact information) but may include device and usage information, such as your IP address, browser and device characteristics, operating system, language preferences, referring URLs, device name, country, location, information about how and when you use our and other technical information. This information is primarily needed to maintain the security and operation of our site, and for our internal analytics and reporting purposes. Like many businesses, we also collect information through cookies and similar technologies. 2. Will your information be shared with anyone? \u00bb We may process or share data based on the following legal basis: Consent: We may process your data if you have given us specific consent to use your personal information in a specific purpose. Legitimate Interests: We may process your data when it is reasonably necessary to achieve our legitimate business interests. Performance of a Contract: Where we have entered into a contract with you, we may process your personal information to fulfil the terms of our contract. Legal Obligations: We may disclose your information where we are legally required to do so in order to comply with applicable law, governmental requests, a judicial proceeding, court order, or legal process, such as in response to a court order or a subpoena (including in response to public authorities to meet national security or law enforcement requirements). Vital Interests: We may disclose your information where we believe it is necessary to investigate, prevent, or take action regarding potential violations of our policies, suspected fraud, situations involving potential threats to the safety of any person and illegal activities, or as evidence in litigation in which we are involved. More specifically, we may need to process your data or share your personal information in the following situations: Vendors, Consultants and Other Third-Party Service Providers. We may share your data with third party vendors, service providers, contractors or agents who perform services for us or on our behalf and require access to such information to do that work. Examples include: payment processing, data analysis, email delivery, hosting services, customer service and marketing efforts. We may allow selected third parties to use tracking technology on the , which will enable them to collect data about how you interact with the over time. This information may be used to, among other things, analyze and track data, determine the popularity of certain content and better understand online activity. Unless described in this Policy, we do not share, sell, rent or trade any of your information with third parties for their promotional purposes. Business Transfers. We may share or transfer your information in connection with, or during negotiations of, any merger, sale of company assets, financing, or acquisition of all or a portion of our business to another company. 3. Do we use cookies and other tracking technologies? \u00bb We may use cookies and similar tracking technologies (like web beacons and pixels) to access or store information. Specific information about how we use such technologies and how you can refuse certain cookies is set out in our Cookie Policy . 4. How do we handle your social logins? \u00bb Our offer you the ability to register and login using your third party social media account details (like your GitHub login). Where you choose to do this, we will receive certain profile information about you from your social media provider. The profile Information we receive may vary depending on the social media provider concerned, but will often include your name, e-mail address, organization membership, profile picture as well as other information you choose to make public. We will use the information we receive only for the purposes that are described in this privacy policy or that are otherwise made clear to you on the site. Please note that we do not control, and are not responsible for, other uses of your personal information by your third party social media provider. We recommend that you review their privacy policy to understand how they collect, use and share your personal information, and how you can set your privacy preferences on their sites and apps. 5. Is your information transferred internationally? \u00bb Our servers are located in Ireland. If you are accessing our site from outside, please be aware that your information may be transferred to, stored, and processed by us in our facilities and by those third parties with whom we may share your personal information , in and other countries. If you are a resident in the European Economic Area, then these countries may not have data protection or other laws as comprehensive as those in your country. We will however take all necessary measures to protect your personal information in accordance with this privacy policy and applicable law. 6. How long do we keep your information? \u00bb We will only keep your personal information for as long as it is necessary for the purposes set out in this privacy policy, unless a longer retention period is required or permitted by law (such as tax, accounting or other legal requirements). No purpose in this policy will require us keeping your personal information for longer than 30 days. When we have no ongoing legitimate business need to process your personal information, we will either delete or anonymize it, or, if this is not possible (for example, because your personal information has been stored in backup archives), then we will securely store your personal information and isolate it from any further processing until deletion is possible. 7. How do we keep your information safe? \u00bb We have implemented appropriate technical and organisational security measures designed to protect the security of any personal information we process. However, please also remember that we cannot guarantee that the internet itself is 100% secure. Although we will do our best to protect your personal information, transmission of personal information to and from our is at your own risk. You should only access the services within a secure environment. 8. Do we collect information from minors? \u00bb We do not knowingly solicit data from or market to children under 13 years of age. By using the , you represent that you are at least 13 or that you are the parent or guardian of such a minor and consent to such minor dependent\u2019s use of the site. If we learn that personal information from users less than 18 years of age has been collected, we will deactivate the account and take reasonable measures to promptly delete such data from our records. If you become aware of any data we have collected from children under age 13, please contact us at privacy@spacelift.io . 9. What are your privacy rights? \u00bb In some regions (like the European Economic Area), you have certain rights under applicable data protection laws. These may include the right (i) to request access and obtain a copy of your personal information, (ii) to request rectification or erasure; (iii) to restrict the processing of your personal information; and (iv) if applicable, to data portability. In certain circumstances, you may also have the right to object to the processing of your personal information. To make such a request, please use the contact details provided below. We will consider and act upon any request in accordance with applicable data protection laws. If we are relying on your consent to process your personal information, you have the right to withdraw your consent at any time. Please note however that this will not affect the lawfulness of the processing before its withdrawal. If you are resident in the European Economic Area and you believe we are unlawfully processing your personal information, you also have the right to complain to your local data protection supervisory authority. You can find their contact details here: http://ec.europa.eu/justice/data-protection/bodies/authorities/index_en.htm Account Information \u00bb If you would at any time like to review or change the information in your account or terminate your account, you can: Upon your request to terminate your account, we will deactivate or delete your account and information from our active databases. However, some information may be retained in our files to prevent fraud, troubleshoot problems, assist with any investigations, enforce our Terms of Use and/or comply with legal requirements. 10. Do California residents have specific privacy rights? \u00bb California Civil Code Section 1798.83, also known as the \u201cShine The Light\u201d law, permits our users who are California residents to request and obtain from us, once a year and free of charge, information about categories of personal information (if any) we disclosed to third parties for direct marketing purposes and the names and addresses of all third parties with which we shared personal information in the immediately preceding calendar year. If you are a California resident and would like to make such a request, please submit your request in writing to us using the contact information provided below. If you are under 18 years of age, reside in California, and have a registered account with the site, you have the right to request removal of unwanted data that you publicly post on the site. To request removal of such data, please contact us using the contact information provided below, and include the email address associated with your account and a statement that you reside in California. We will make sure the data is not publicly displayed on the , but please be aware that the data may not be completely or comprehensively removed from our systems. 11. How can you contact us about this policy? \u00bb If you have questions or comments about this policy, you may email us at privacy@spacelift.io or by post to: Spacelift, Inc. 541 Jefferson Ave. Suite 100 Redwood City CA 94063 For GDPR related: Spacelift Lanciego 12/20 02-792 Warszawa Poland 12. How can you review, update, or delete the data we collect from you? \u00bb Based on the laws of some countries, you may have the right to request access to the personal information we collect from you, change that information, or delete it in some circumstances. To request to review, update, or delete your personal information, please email us at privacy@spacelift.io. We will respond to your request within 30 days.","title":"Privacy"},{"location":"legal/privacy.html#privacy","text":"Thank you for choosing to be part of our community at Spacelift (\u201ccompany\u201d, \u201cwe\u201d, \u201cus\u201d, or \u201cour\u201d). We are committed to protecting your personal information and your right to privacy. If you have any questions or concerns about our policy, or our practices with regards to your personal information, please contact us at privacy@spacelift.io . When you visit our and use our services, you trust us with your personal information. We take your privacy very seriously. In this privacy notice, we describe our privacy policy. We seek to explain to you in the clearest way possible what information we collect, how we use it and what rights you have in relation to it. We hope you take some time to read through it carefully, as it is important. If there are any terms in this privacy policy that you do not agree with, please discontinue use of our and our services. This privacy policy applies to all information collected through our and/or any related services, sales, marketing or events (we refer to them collectively in this privacy policy as the \"Sites\"). Please read this privacy policy carefully as it will help you make informed decisions about sharing your personal information with us.","title":"Privacy"},{"location":"legal/privacy.html#1-what-information-do-we-collect","text":"","title":"1. What information do we collect?"},{"location":"legal/privacy.html#personal-information-you-disclose-to-us","text":"We collect personal information that you voluntarily provide to us when registering at the expressing an interest in obtaining information about us or our products and services, when participating in activities on the or otherwise contacting us. The personal information that we collect depends on the context of your interactions with us and the site, the choices you make and the products and features you use. The personal information we collect can include the following: Name and Contact Data. We may collect your first and last name, email address, and other similar contact data from contact forms. Social Media Login Data. We provide you with the option to register using social media account details, like your GitHub. If you choose to register in this way, we will collect the Information described in the relevant section below. All personal information that you provide to us must be true, complete and accurate, and you must notify us of any changes to such personal information.","title":"Personal information you disclose to us"},{"location":"legal/privacy.html#information-automatically-collected","text":"We automatically collect certain information when you visit, use or navigate the site. This information does not reveal your specific identity (like your name or contact information) but may include device and usage information, such as your IP address, browser and device characteristics, operating system, language preferences, referring URLs, device name, country, location, information about how and when you use our and other technical information. This information is primarily needed to maintain the security and operation of our site, and for our internal analytics and reporting purposes. Like many businesses, we also collect information through cookies and similar technologies.","title":"Information automatically collected"},{"location":"legal/privacy.html#2-will-your-information-be-shared-with-anyone","text":"We may process or share data based on the following legal basis: Consent: We may process your data if you have given us specific consent to use your personal information in a specific purpose. Legitimate Interests: We may process your data when it is reasonably necessary to achieve our legitimate business interests. Performance of a Contract: Where we have entered into a contract with you, we may process your personal information to fulfil the terms of our contract. Legal Obligations: We may disclose your information where we are legally required to do so in order to comply with applicable law, governmental requests, a judicial proceeding, court order, or legal process, such as in response to a court order or a subpoena (including in response to public authorities to meet national security or law enforcement requirements). Vital Interests: We may disclose your information where we believe it is necessary to investigate, prevent, or take action regarding potential violations of our policies, suspected fraud, situations involving potential threats to the safety of any person and illegal activities, or as evidence in litigation in which we are involved. More specifically, we may need to process your data or share your personal information in the following situations: Vendors, Consultants and Other Third-Party Service Providers. We may share your data with third party vendors, service providers, contractors or agents who perform services for us or on our behalf and require access to such information to do that work. Examples include: payment processing, data analysis, email delivery, hosting services, customer service and marketing efforts. We may allow selected third parties to use tracking technology on the , which will enable them to collect data about how you interact with the over time. This information may be used to, among other things, analyze and track data, determine the popularity of certain content and better understand online activity. Unless described in this Policy, we do not share, sell, rent or trade any of your information with third parties for their promotional purposes. Business Transfers. We may share or transfer your information in connection with, or during negotiations of, any merger, sale of company assets, financing, or acquisition of all or a portion of our business to another company.","title":"2. Will your information be shared with anyone?"},{"location":"legal/privacy.html#3-do-we-use-cookies-and-other-tracking-technologies","text":"We may use cookies and similar tracking technologies (like web beacons and pixels) to access or store information. Specific information about how we use such technologies and how you can refuse certain cookies is set out in our Cookie Policy .","title":"3. Do we use cookies and other tracking technologies?"},{"location":"legal/privacy.html#4-how-do-we-handle-your-social-logins","text":"Our offer you the ability to register and login using your third party social media account details (like your GitHub login). Where you choose to do this, we will receive certain profile information about you from your social media provider. The profile Information we receive may vary depending on the social media provider concerned, but will often include your name, e-mail address, organization membership, profile picture as well as other information you choose to make public. We will use the information we receive only for the purposes that are described in this privacy policy or that are otherwise made clear to you on the site. Please note that we do not control, and are not responsible for, other uses of your personal information by your third party social media provider. We recommend that you review their privacy policy to understand how they collect, use and share your personal information, and how you can set your privacy preferences on their sites and apps.","title":"4. How do we handle your social logins?"},{"location":"legal/privacy.html#5-is-your-information-transferred-internationally","text":"Our servers are located in Ireland. If you are accessing our site from outside, please be aware that your information may be transferred to, stored, and processed by us in our facilities and by those third parties with whom we may share your personal information , in and other countries. If you are a resident in the European Economic Area, then these countries may not have data protection or other laws as comprehensive as those in your country. We will however take all necessary measures to protect your personal information in accordance with this privacy policy and applicable law.","title":"5. Is your information transferred internationally?"},{"location":"legal/privacy.html#6-how-long-do-we-keep-your-information","text":"We will only keep your personal information for as long as it is necessary for the purposes set out in this privacy policy, unless a longer retention period is required or permitted by law (such as tax, accounting or other legal requirements). No purpose in this policy will require us keeping your personal information for longer than 30 days. When we have no ongoing legitimate business need to process your personal information, we will either delete or anonymize it, or, if this is not possible (for example, because your personal information has been stored in backup archives), then we will securely store your personal information and isolate it from any further processing until deletion is possible.","title":"6. How long do we keep your information?"},{"location":"legal/privacy.html#7-how-do-we-keep-your-information-safe","text":"We have implemented appropriate technical and organisational security measures designed to protect the security of any personal information we process. However, please also remember that we cannot guarantee that the internet itself is 100% secure. Although we will do our best to protect your personal information, transmission of personal information to and from our is at your own risk. You should only access the services within a secure environment.","title":"7. How do we keep your information safe?"},{"location":"legal/privacy.html#8-do-we-collect-information-from-minors","text":"We do not knowingly solicit data from or market to children under 13 years of age. By using the , you represent that you are at least 13 or that you are the parent or guardian of such a minor and consent to such minor dependent\u2019s use of the site. If we learn that personal information from users less than 18 years of age has been collected, we will deactivate the account and take reasonable measures to promptly delete such data from our records. If you become aware of any data we have collected from children under age 13, please contact us at privacy@spacelift.io .","title":"8. Do we collect information from minors?"},{"location":"legal/privacy.html#9-what-are-your-privacy-rights","text":"In some regions (like the European Economic Area), you have certain rights under applicable data protection laws. These may include the right (i) to request access and obtain a copy of your personal information, (ii) to request rectification or erasure; (iii) to restrict the processing of your personal information; and (iv) if applicable, to data portability. In certain circumstances, you may also have the right to object to the processing of your personal information. To make such a request, please use the contact details provided below. We will consider and act upon any request in accordance with applicable data protection laws. If we are relying on your consent to process your personal information, you have the right to withdraw your consent at any time. Please note however that this will not affect the lawfulness of the processing before its withdrawal. If you are resident in the European Economic Area and you believe we are unlawfully processing your personal information, you also have the right to complain to your local data protection supervisory authority. You can find their contact details here: http://ec.europa.eu/justice/data-protection/bodies/authorities/index_en.htm","title":"9. What are your privacy rights?"},{"location":"legal/privacy.html#account-information","text":"If you would at any time like to review or change the information in your account or terminate your account, you can: Upon your request to terminate your account, we will deactivate or delete your account and information from our active databases. However, some information may be retained in our files to prevent fraud, troubleshoot problems, assist with any investigations, enforce our Terms of Use and/or comply with legal requirements.","title":"Account Information"},{"location":"legal/privacy.html#10-do-california-residents-have-specific-privacy-rights","text":"California Civil Code Section 1798.83, also known as the \u201cShine The Light\u201d law, permits our users who are California residents to request and obtain from us, once a year and free of charge, information about categories of personal information (if any) we disclosed to third parties for direct marketing purposes and the names and addresses of all third parties with which we shared personal information in the immediately preceding calendar year. If you are a California resident and would like to make such a request, please submit your request in writing to us using the contact information provided below. If you are under 18 years of age, reside in California, and have a registered account with the site, you have the right to request removal of unwanted data that you publicly post on the site. To request removal of such data, please contact us using the contact information provided below, and include the email address associated with your account and a statement that you reside in California. We will make sure the data is not publicly displayed on the , but please be aware that the data may not be completely or comprehensively removed from our systems.","title":"10. Do California residents have specific privacy rights?"},{"location":"legal/privacy.html#11-how-can-you-contact-us-about-this-policy","text":"If you have questions or comments about this policy, you may email us at privacy@spacelift.io or by post to: Spacelift, Inc. 541 Jefferson Ave. Suite 100 Redwood City CA 94063 For GDPR related: Spacelift Lanciego 12/20 02-792 Warszawa Poland","title":"11. How can you contact us about this policy?"},{"location":"legal/privacy.html#12-how-can-you-review-update-or-delete-the-data-we-collect-from-you","text":"Based on the laws of some countries, you may have the right to request access to the personal information we collect from you, change that information, or delete it in some circumstances. To request to review, update, or delete your personal information, please email us at privacy@spacelift.io. We will respond to your request within 30 days.","title":"12. How can you review, update, or delete the data we collect from you?"},{"location":"legal/refund-policy.html","text":"Refund Policy \u00bb GENERAL PROVISIONS \u00bb Refunds will be made on the terms and conditions set out in this Policy in the event that the Customer withdraws from the Software purchase Agreement. DEFINITIONS \u00bb The following definitions are set out for the purposes of this Policy: Policy - these regulations for making refunds. Company \u2013 Spacelift, Inc., a Delaware corporation with offices located at 541 Jefferson Ave. Suite 100, Redwood City , CA 94063. Customer - an entity who purchases software from the Company on the basis of the Agreement. Software - software offered by the Company, purchased by the Customer, which is the subject of the Agreement. Agreement - the agreement between the Company and the Customer concerning the purchase of the Software. Funds - funds paid by the Customer to the Company under the Agreement for the purchase of the Software. REFUND PROCESS \u00bb The Customer may withdraw from the Agreement and claim a refund of funds within 14 days after the completion of the purchase of the Software provided that the purchased Software has not been activated during that period. Upon receipt of notification of withdrawal, access to the Software will be blocked and verification of compliance with the conditions described in Section 3 above will begin. The verification process may take up to 10 days. In the case of positive verification, referred to in Section 4 above, the procedure of returning the Funds will be initiated. The Funds will be returned using the original payment method. The time taken to return the Funds may be affected by the operation of the payment provider. FINAL PROVISIONS \u00bb The current version of the Policy is available here This Policy is valid from 1st November 2021. In matters not regulated by this Policy, the Terms and Conditions shall apply.","title":"Refund Policy"},{"location":"legal/refund-policy.html#refund-policy","text":"","title":"Refund Policy"},{"location":"legal/refund-policy.html#general-provisions","text":"Refunds will be made on the terms and conditions set out in this Policy in the event that the Customer withdraws from the Software purchase Agreement.","title":"GENERAL PROVISIONS"},{"location":"legal/refund-policy.html#definitions","text":"The following definitions are set out for the purposes of this Policy: Policy - these regulations for making refunds. Company \u2013 Spacelift, Inc., a Delaware corporation with offices located at 541 Jefferson Ave. Suite 100, Redwood City , CA 94063. Customer - an entity who purchases software from the Company on the basis of the Agreement. Software - software offered by the Company, purchased by the Customer, which is the subject of the Agreement. Agreement - the agreement between the Company and the Customer concerning the purchase of the Software. Funds - funds paid by the Customer to the Company under the Agreement for the purchase of the Software.","title":"DEFINITIONS"},{"location":"legal/refund-policy.html#refund-process","text":"The Customer may withdraw from the Agreement and claim a refund of funds within 14 days after the completion of the purchase of the Software provided that the purchased Software has not been activated during that period. Upon receipt of notification of withdrawal, access to the Software will be blocked and verification of compliance with the conditions described in Section 3 above will begin. The verification process may take up to 10 days. In the case of positive verification, referred to in Section 4 above, the procedure of returning the Funds will be initiated. The Funds will be returned using the original payment method. The time taken to return the Funds may be affected by the operation of the payment provider.","title":"REFUND PROCESS"},{"location":"legal/refund-policy.html#final-provisions","text":"The current version of the Policy is available here This Policy is valid from 1st November 2021. In matters not regulated by this Policy, the Terms and Conditions shall apply.","title":"FINAL PROVISIONS"},{"location":"legal/terms.html","text":"Terms and Conditions \u00bb Last updated: March 7, 2023 This Terms and Conditions is between the entity you represent, or, if you do not indicate an entity in connection with the Services, you individually (\u201c Client \u201d, \u201c you \u201d or \u201c your \u201d), and Spacelift, Inc. with its principal office at 541 Jefferson Ave. Suite 100, Redwood City CA 94063, United States of America (\u201c Spacelift \u201d, \u201c the Company \u201d \u201c we \u201d, \u201c us \u201d, or \u201c our \u201d). It consists of the terms and conditions below, the Privacy Policy, the Refund Policy and the Cookie Policy (together, the \u201c Agreement \u201d). The Agreement does not govern the use of: the Services under Enterprise Plan provided that a separate and mutually agreed contract has been executed, the Services purchased via AWS Marketplace, which are subject to Standard Contract for AWS Marketplace and any amendments, any self-hosted services provided by Spacelift, Inc. which are subject to a separate agreement. 1. DEFINITIONS \u00bb 1.1. \u201c Authorized User \u201d means Client\u2019s employees, consultants, contractors, agents, and Workers (a) who Client authorizes to access and use the Services under the rights granted to Client under this Agreement; and (b) for whom access to the Services has been purchased hereunder. 1.2. \u201c Client Data \u201d means information, data, and other content, in any form or medium, that is collected, downloaded, or otherwise received from the Client or an Authorized User by or through the Services. For the avoidance of doubt, Client Data does not include Resultant Data or any other information reflecting the access or use of the Services by or on behalf of Client or any Authorized User. 1.3. \u201c Confidential Information \u201d means all nonpublic information, including, but not limited to, source code, software, trade secrets, know-how, technical drawings, algorithms, ideas, inventions, other technical, business or sales information, negotiations or proposals, disclosed by us in whatever form and which is known by the Client or its Authorized User to be confidential or under circumstances by which the receiving party should reasonably understand such information is to be treated as confidential, whether or not marked as \u201cConfidential\u201d. 1.4. \u201c Documentation \u201d means any manuals, instructions, including technical specifications, or other documents or materials describing the features and functionality of the Services, which are located on Website or provided to Clients, and may be updated from time to time. 1.5. \u201c Intellectual Property Rights \u201d or \u201c IPR \u201d means any registered and unregistered rights granted, applied for, or otherwise now or hereafter in existence under or related to any patent, copyright, trademark, trade secret, database protection, or other intellectual property rights laws in any part of the world. 1.6. \u201c Metrics \u201d - means the measurements used for quantifying the Services usage with the following meaning: \u200b1.6.1. \u201c Private Minute(s) \u201d means minute(s) of Private Workers\u2019 Services usage in a given month; 1.6.2. \u201c Public Minutes \u201d means minutes of Public Worker\u2019s Services usage in a given month; 16.3. \u201c Seat(s) \u201d means an Authorized User(s) who actively logged in to the Services in a given month; 1.6.4. \u201c Worker(s) \u201d means a predefined set(s) of computing resources that are specifically optimized for the development and provisioning and deployment of cloud-based infrastructures based on IaC; a Worker can be either a self-hosted agent performing infrastructure management in a Client-controlled environment (\u201c Private Worker \u201d) or any other software agent, provided and managed by Spacelift in a common secure worker pool (\u201c Public Worker \u201d). 1.7. \u201c Services \u201d means the Spacelift\u2019s specialized, continuous integration and deployment (CI/CD) platform for infra-as-code available at https://spacelift.io as SaaS; 1.8. \u201c Subscription \u201d means enrollment for Services for a Subscription Plan on Subscription Terms as defined in the Agreement; 1.9. \u201c Subscription Plan(s) \u201d means available Subscription offer(s) for use of the Services as described on the Website, including Trial, Free, Cloud, and Enterprise; 1.10. \u201c Subscription Term(s) \u201d means the conditions under which a Subscription is made under the Agreement, including the Subscription Period, Metrics, and Subscription Fees, as described in Section 5; 1.11. \u201c Website \u201d means https://spacelift.io website managed by Spacelift. 2. GENERAL \u00bb 2.1. Execution of the Agreement . Accepting this Agreement is a condition of using the Services provided by Spacelift. \u200b\u200bBY COMPLETING THE REGISTRATION PROCESS, ACCESSING OR USING THE SERVICES, YOU ACKNOWLEDGE AND AGREE THAT (I) YOU HAVE READ, UNDERSTOOD, AND ACCEPTED THIS AGREEMENT, AND (II) YOU HEREBY REPRESENT AND WARRANT THAT YOU ARE AUTHORIZED TO ENTER OR ACT ON BEHALF OF THE CLIENT, AND BIND TO THIS AGREEMENT. If you do not have the legal authority to enter this Agreement, do not understand this Agreement, or do not agree to the Agreement, you should not accept the Terms and Conditions, or use the Services. 2.2. Conflict of Provisions . In the event of any inconsistencies or conflict between the documents that make up this Agreement, the documents will prevail in the following order: (a) any written amendment agreed upon by the parties (such as order forms); (b) Privacy Policy; (c) these Terms and Conditions and (d) the Refund Policy. 2.3. Compliance . You are responsible for (a) compliance with the provisions of the Agreement by you and your Authorized Users and for any and all acts and omissions of Authorized Users connected with their use and access to the Services and for any breach of this Agreement by Authorized Users; and (b) any delay or failure of performance caused in whole or in part by your delay in performing, or failure to perform, any of your obligations under this Agreement. Without limiting the foregoing, you are solely responsible for ensuring that your use of the Services is compliant with all applicable laws and regulations, as well as any and all privacy policies, agreements, or other obligations you may maintain or enter into. 2.4. Amendments . Any individual amendment to this Agreement must be made in writing (expressly stating that it is amending this Agreement) and signed by both parties. 3. LICENSE, INTELLECTUAL PROPERTY RIGHTS, AND OWNERSHIP \u00bb 3.1. Ownership . The Services, Documentation, and Website, all copies and portions thereof, and all IPR therein, including, but not limited to source code, databases, functionality, software, website designs, audio, video, text, photographs, graphics, or derivative works therefrom, are owned by us or licensed to us. You are not authorized to use (and will not permit any third party to use) the Services, Website, Documentation, or any portion thereof except as expressly authorized by this Agreement. Specifically, no part of the Services, Documentation, or Website may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, modified, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission. 3.2. Confidential Information . All our Confidential Information and derivations thereof will remain our sole and exclusive property. You should not disclose, use or publish Confidential Information without our prior written consent. You must hold all our Confidential Information in strict confidence and safeguard the Confidential Information from unauthorized use, access, or disclosure using at least the degree of care you use to protect your similarly sensitive information and in no event less than a reasonable degree of care. 3.3. License . Spacelift makes the Services available to you during the Subscription Period, subject to the terms and conditions of this Agreement and Subscription Terms. Spacelift grants a limited, non-exclusive, non-sublicensable, non-transferable right to access and use the Services and its Documentation during the Subscription Period, solely for your internal business purposes or your personal use. 3.4. Restrictions . You will not, and will not permit any other person to, access or use the Services except as expressly permitted by this Agreement. For purposes of clarity and without limiting the generality of the foregoing, you will not, except as this Agreement expressly permits: (a) copy, modify, or create derivative works or improvements of the Services or Documentation; (b) rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer, or otherwise make available any Services or Documentation to any person; (c) reverse engineer, disassemble, decompile, decode, adapt, or otherwise attempt to derive or gain access to the source code of the Services, in whole or in part; (d) bypass or breach any security device or protection used by the Services or access or use the Services other than by an Authorized User through the use of his or her own then valid access credentials; (e) input, upload, transmit, or otherwise provide to or through the Services or Documentation, any information or materials that are unlawful or injurious, or contain, transmit, or activate any harmful code; (f) damage, destroy, disrupt, disable, impair, interfere with, or otherwise impede or harm in any manner the Services or Documentation, or our provision of services to any third party, in whole or in part; (g) remove, delete, alter, or obscure any trademarks, Documentation, warranties, or disclaimers, or IPR notices from any Services; (h) access or use the Services or Documentation in any manner or for any purpose that infringes, misappropriates, or otherwise violates any IPR or other right of any third party or that violates any applicable law; or (i) access or use the Services or Documentation for purposes of competitive analysis of the Services, the development, provision, or use of a competing software service or product. 3.5. Client Data . You are and will remain the sole and exclusive owner of all rights, title, and interest in and to all Client Data, including all IPR, subject to the rights and permissions granted in the Agreement. You have exclusive control and responsibility for determining what data you submit to the Services, for obtaining all necessary consents and permissions for the submission of Client Data, and for the accuracy, quality, and legality of Client Data. 3.6. Consent to Use Client Data . You irrevocably grant all such rights and permissions in or relating to Client Data as are reasonably necessary or useful to us to enforce this Agreement and exercise our rights and perform our obligations hereunder. 3.7. Use of Resultant Data . We may collect data and information related to your use of the Services that is used by us in an aggregate manner, including to compile statistical and performance information related to the provision and operation of the Services (\u201c Resultant Data \u201d). You unconditionally and irrevocably grant to us an assignment of all rights, title, and interest in and to the Resultant Data, including all IPR relating thereto, if any. 4. PROVISION OF SERVICES \u00bb 4.1. Metrics . Use of the Services is subject to usage limits reflected in Metrics, as set forth in the Subscription Plan. We will monitor your use of the Services in order to verify whether you comply with the presented limits. 4.2. Services Modifications . We reserve the right to make any changes to the Services or Documentation that we deem necessary or useful to: (a) maintain or enhance: (i) the quality or delivery of Services to you and other clients; (ii) the competitive strength of or market for Services; or (iii) the Services' cost efficiency or performance; or (b) to comply with applicable law. 4.3. Privacy . When applicable, we will comply with all applicable laws, regulations, and government orders relating to personally identifiable information and data privacy with respect to any such Client Data that we receive or have access to under the Agreement or in connection with the performance of the Services. In particular, regulations for the protection of personally identifiable information are indicated in the Privacy Policy incorporated herein by reference. 4.4. Access and Security . You will employ all physical, administrative, and technical controls, screening, security procedures, and other safeguards necessary to: (a) securely administer the distribution and use of all access credentials and protect against any unauthorized access to or use of the Services; and (b) control the content and use of Client Data, including the uploading or other provision of Client Data for processing by the Services. 4.5. Security . We maintain industry-standard security and privacy certification, such as a SOC II certification. We will use appropriate technical and organizational measures designed to prevent unauthorized access, use, alteration, or disclosure of the Services or Client Data. 4.6. Incidents . We will notify you in case of any security incident as soon as possible, provided that you have indicated your contact data in the Services under the address: https://*.app.spacelift.io/settings/security (* being the domain name chosen by you to access Services). 4.7. Downtimes . We will use commercially reasonable efforts to give you at least 24 hours prior notice of all scheduled outages of the Services. You can check the current Services\u2019 availability status at https://spacelift.statuspage.io/ 4.8. Services and Website Management . We reserve the right at our sole discretion, to (a) monitor the Services for breaches of the Agreement; (b) take appropriate legal action against anyone in breach of applicable laws or the Agreement; (c) refuse, restrict access to, or availability of, or disable (to the extent technologically feasible) any of Client Data; (d) remove from the Services or Website or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (e) otherwise manage the Website and Services in a manner designed to protect our rights and property and to facilitate the proper functioning of the Website and Services. 4.9. Third-Party Materials . The Website and/or Services may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third-party websites or applications or the third-party operator. We are not responsible for and do not endorse any third-party websites or applications or their availability or content. 4.10. Access by Third-Party Accounts . You may register and login to the Services using your third-party service providers account details, like a Google or GitHub account (\u201c Third-Party Account \u201d). When you do so, we will receive certain profile information varying on the identity provider and the information you decided to include in your Third-Party Account. You will have the ability to disable the connection between your Services account and your Third Party Accounts at any time. Please note that your relationship with the third-party service providers associated with your Third-Party Accounts is governed solely by your agreement(s) with such third-party service providers. If a Third Party Account or associated service becomes unavailable or the access to such Third Party Account is terminated by the third-party service provider, then your access using such Third Party Account may no longer be available. 4.11. Support Services . During the Subscription Period, we will provide support services depending on the Subscription Plan, as described in https://docs.spacelift.io/product/support/ 4.12. Client Systems and Cooperation . You will at all times during the period of Subscription: (a) set up, maintain, and operate in good repair and in accordance with the Documentation all your systems (meaning information technology infrastructure, including computers, software, databases, electronic systems, database management systems, and networks) on or through which the Services are accessed or used; (b) provide us with such access to your data or systems as is necessary for us to perform the Services in accordance with the Agreement and Documentation; (c) use reasonable measures to prevent and promptly notify us of any unauthorized access to Authorized User accounts of which you become aware of, and (d) provide all cooperation and assistance as we may reasonably request to enable us to exercise our rights and perform our obligations under and in connection with this Agreement. 4.13. Quality of Services . You are aware that the quality of the Services and your use of the Services might be affected by a number of factors outside our control, including but not limited to force majeure, technical conditions, hardware or software (including third-party software and network) issues. Any delay or default affecting the availability, functionality, correctness, or timely performance of the Services caused by such circumstances will not constitute a breach of the Agreement. 4.14. No Data Backup . We do not store or backup any Client Data. The Services do not replace the need for you to maintain regular data backups or redundant data archives. We have no obligation or liability for any loss, alteration, destruction, damage, corruption, or recovery of Client Data. 4.15. Disclaimer . The content on the Website is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Website. 5. SUBSCRIPTION PLANS AND TERMS \u00bb 5.1. Effective Date and Term . This Agreement commences on the effective date being the day of your registration, access to, or use of the Services, whichever happens first (\u201c Effective Date \u201d). Unless earlier terminated pursuant to the terms of this Section, the Agreement will continue through the Subscription Period of a chosen Subscription Plan. 5.2. Subscription Plans . The Services are available under the following Subscription Plans with the relevant Subscription Terms: Plan Subscription Period Termination Subscription Fee Trial 14 days Any time None Free Non-definite term Any time None Cloud Default: 1 month, monthly renewal. Individual arrangements may include an annual or a multi-year Subscription Period. Any time during the Subscription Period, having its effect on the last day of the given Subscription Period. Current fees are set forth in https://spacelift.io/pricing Enterprise As agreed by the parties in a separate agreement or order form As agreed by the parties in a separate agreement or order form As agreed by the parties in a separate agreement or order form 5.3. The Trial Plan . The Subscription Terms for the Trial Plan are as follows: 5.3.1. Scope and Metrics . The Trial Plan offers access to Services to get to know Services before starting the Free, Cloud, or Enterprise Plan. Any usage limitations are indicated on https://spacelift.io/pricing . 5.3.2. Subscription Period . The Trial Plan expires 14 days after your registration to the Services. 5.3.3. Termination . You may cancel the Trial Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io . 5.3.4. Next Steps . Upon the end of Trial Plan, you may: (a) stop using the Services and delete your account thus terminating the Agreement, (b) continue to use the Services under Free Plan, (c) subscribe to Cloud Plan if you provide payment details to make a Subscription Fee according to the currently effective rates presented on https://spacelift.io/pricing , or (d) contact sales@spacelift.io to discuss the Enterprise Plan which is subject to separate agreement. 5.4. The Free Plan . The Subscription Terms for the Free Plan are as follows: 5.4.1. Scope and Metrics . The Free Plan offers access to the Services with usage limitations indicated on https://spacelift.io/pricing . In order to expand the usage limitations, upgrade to Cloud Plan or Enterprise Plan. 5.4.2. Subscription Period and Termination . The Free Plan is available for an indefinite period of time and might be terminated at any time. You may cancel the Free Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io . 5.5. The Cloud Plan . The Subscription Terms for the Cloud Plan are as follows: 5.5.1. Scope and Metrics . The Cloud Plan offers access to the Services with usage limitations indicated on https://spacelift.io/pricing . This plan can be supplemented with additional Seats and/or Workers as required, at an additional cost calculated on the basis of current rates. 5.5.2. Subscription Period and Billing . If you activate the Cloud Plan, you authorize Spacelift to periodically charge, on a going-forward basis and until cancellation of either the recurring payments or your account, all accrued sums on or before the payment due date for the accrued sums. The \" Subscription Billing Date \" is the date when you purchase your first Subscription to the Services. Your account will be charged automatically on the Subscription Billing Date for all applicable fees for the next Subscription Period. The subscription will continue unless and until you cancel your Subscription or we terminate it. You must cancel your Subscription before it renews in order to avoid billing the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide to us during registration (or to a different payment method if you change your payment information). 5.5.3. Termination . You may cancel the Cloud Plan at any time by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io . The termination will be effective on the last day of the given Subscription Period. 5.6. Withdrawal and Refund . You may withdraw from the Agreement and claim a refund of funds within 14 days after its execution provided that the Services have not been activated during that period. You can find all the details regarding the refund in our Refund Policy. 5.7. Plan Adjustments and Upgrades . The Subscription Terms for each plan, including Subscription Period, Fees, and Metrics may be adjusted by written agreement of the parties. If you wish to upgrade your Subscription Plan, contact sales@spacelift.io to discuss the available options. 5.8. The Enterprise Plan . In most cases, Subscription Terms of the Enterprise Plan are individually discussed by the parties and bind the parties on the basis of a separately executed agreement. In case a separate agreement is not executed between the parties, the written arrangements (such as order forms) regarding Subscription Terms apply and in the remaining scope terms for the use of the Services will be subject to conditions set forth in this Agreement. 5.9. Services Usage during Negotiations . If you wish to actively use the Services in the course of negotiating the separate agreement, the parties may agree on the temporary terms of use of the Services, including the relevant Metrics, period, and fees, and in the remaining scope terms for the use of the Services will be subject to conditions set forth in this Agreement. 6. SUBSCRIPTION FEES \u00bb 6.1. Terms of Payment . Unless otherwise agreed by the parties, Subscription Fees will be payable in USD via a credit card on a going-forward basis and will be subject to this Section 6. 6.2. Taxes . All Subscription Fees and other amounts payable by you under this Agreement are exclusive of taxes and similar assessments. Without limiting the foregoing, you are responsible for all sales, use, excise taxes, and any other similar taxes, duties, and charges of any kind, other than any taxes imposed on our income. 6.3. Late Payment . If you fail to make any payment when due then, in addition to all other remedies that may be available: (a) we may charge interest on the past due amount at the rate of 1.5% per month calculated daily and compounded monthly or, if lower, the highest rate permitted under applicable law; (b) you will reimburse us for all reasonable costs incurred by us in collecting any late payments or interest, including attorneys' fees, court costs, and collection agency fees; and (c) if such failure continues for fourteen (14) days following written notice, we may suspend performance of the Services until all past due amounts and interest have been paid, without incurring any obligation or liability to you or any other person by reason of such suspension. 6.4. Subscription Fees Increases . Separately from any changes in Subscription Fees due to the upgrade of the relevant Metrics, we may increase Fees for any Subscription Period before its start by providing you a notice prior to the commencement of the next Subscription Period. Your continued use of the Services constitutes your acceptance of such changed Subscription Fees. 7. SUSPENSION AND TERMINATION \u00bb 7.1. Suspension . Without limiting any other provision of the Agreement, we reserve the right to, in our sole discretion and without notice or liability, deny access to and use of the Services (including blocking certain IP addresses), to any person for any reason including but not limited to (a) proven or suspected breach of any representation, warranty or covenant contained in the Agreement or of any applicable law or regulation; (b) your use of the Services poses a risk to the Services, our other customers, or us (including our infrastructure, security, and third-party relationships); (c) your use of the Services could subject us to liability or (d) you are past due in the payment of Subscription Fee. We will provide you with prompt notice of any suspension. 7.2. Effect of Suspension . If we suspend your access to the Services for any reason set out in the Agreement, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress. 7.3. Termination for Cause . Notwithstanding the termination for convenience as described in Section 5, Either Party may terminate this Agreement, effective on written notice to the other party, if the other party (a) materially breaches this Agreement, and such breach: (i) is incapable of cure; or (ii) being capable of cure, remains uncured 30 days after the non-breaching party provides the breaching party with written notice of such breach; (b) becomes insolvent or is generally unable to pay, or fails to pay, its debts as they become due; (c) files, or has filed against it, a petition for voluntary or involuntary bankruptcy or otherwise becomes subject, voluntarily or involuntarily, to any proceeding under any domestic or foreign bankruptcy or insolvency law; (d) makes or seeks to make a general assignment for the benefit of its creditors; or (e) applies for or has appointed a receiver, trustee, custodian, or similar agent appointed by order of any court of competent jurisdiction to take charge of or sell any material portion of its property or business. 7.4. Effect of Termination . Upon any termination of this Agreement, except as expressly otherwise provided in this Agreement: (a) all rights, licenses, consents, and authorizations granted by either party to the other will immediately terminate; (b) we will immediately cease all use of any Client Data and at your request destroy, all documents and tangible materials containing or based on Client Data and erase all Client Data from all our systems, provided that, for clarity, our obligations under this Section 7.4 do not apply to any Resultant Data or other data that is required to establish proof of a right or a contract, which will be stored for the duration provided by enforceable law; (c) you will immediately cease all use of any Services and within sixty (60) days destroy, all documents and tangible materials containing or based on any our materials, including Documentation and erase all our materials from the systems you directly or indirectly control. You acknowledge and agree that you are responsible to retrieve Client Data from the Services prior to the termination of this Agreement. 7.5. Surviving Terms . The provisions set forth in the following sections, and any other right or obligation of the parties in this Agreement that, by its nature, should survive termination or expiration of this Agreement, will survive any expiration or termination of this Agreement: 3.1, 3.2, 3.4, 3.7, 7.4, 7.5, 8.4, 9, 10, 12. 8. REPRESENTATIONS AND WARRANTIES \u00bb 8.1. Mutual Representations and Warranties . Each party represents and warrants to the other party that it has the full right, power, and authority to enter into and perform its obligations and grant the rights, licenses, consents, and authorizations it grants or is required to grant under this Agreement. 8.2. Additional Spacelift Representations, Warranties, and Covenants . We represent, warrant, and covenant to you that (a) we will perform the Services using personnel of required skill, experience, and qualifications and in a professional and workmanlike manner in accordance with generally recognized industry standards and will devote adequate resources to meet its obligations under this Agreement; (b) the Services will be performed materially in accordance with the applicable Documentation; (c) to the best of our knowledge, the Services is free from any viruses, worms, malware, or other malicious source code. 8.3. Additional Client Representations, Warranties, and Covenants . You represent, warrant, and covenant to us that (a) you own or otherwise have and will have the necessary rights and consents in and relating to the Client Data so that, as received by us and processed in accordance with this Agreement, they do not and will not infringe, misappropriate, or otherwise violate any IPR, or any privacy or other rights of any third party or violate any applicable law; (b) all registration information you submit will be true, accurate, current, and complete and relate to you and not a third party; (c) you will maintain the accuracy of such information and promptly update such information as necessary; (d) you will keep your access credentials confidential and will be responsible for all use of your access credentials; (e) you are aware that you may not access or use the Services for any purpose other than that for which we make the Services available and (f) you are at least eighteen years of age. 8.4. DISCLAIMER OF WARRANTIES . EXCEPT FOR THE EXPRESS WARRANTIES SET FORTH IN THIS SECTION 8, ALL SERVICES, DOCUMENTATION, AND WEBSITE ARE PROVIDED \"AS IS.\" WE SPECIFICALLY DISCLAIM ALL IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT, AND ALL WARRANTIES ARISING FROM THE COURSE OF DEALING, USAGE, OR TRADE PRACTICE. WITHOUT LIMITING THE FOREGOING, WE MAKE NO WARRANTY OF ANY KIND THAT THE SERVICES, DOCUMENTATION OR WEBSITE, OR ANY PRODUCTS OR RESULTS OF THE USE THEREOF, WILL MEET YOUR OR ANY OTHER PERSON'S REQUIREMENTS, OPERATE WITHOUT INTERRUPTION, ACHIEVE ANY INTENDED RESULT, BE COMPATIBLE OR WORK WITH ANY SOFTWARE, SYSTEM, OR OTHER SERVICES, OR BE SECURE, ACCURATE, COMPLETE, FREE OF HARMFUL CODE, OR ERROR-FREE. ALL THIRD-PARTY MATERIALS ARE PROVIDED \"AS IS\" AND ANY REPRESENTATION OR WARRANTY OF OR CONCERNING ANY THIRD-PARTY MATERIALS IS STRICTLY BETWEEN THE CLIENT AND THE THIRD-PARTY OWNER OR DISTRIBUTOR OF THE THIRD-PARTY MATERIALS. 9. INDEMNIFICATION \u00bb 9.1. Spacelift Indemnification . Subject to the remainder of this Section 9 and the liability limitations set forth in Section 10, we will indemnify, defend, and hold you harmless from and against any and all losses incurred by you resulting from any action by a third party that your use of the Services (excluding Client Data and any third-party materials) in accordance with this Agreement infringes or misappropriates IPR. The foregoing obligation does not apply to the extent that the alleged infringement arises from (a) any third-party materials or Client Data; (b) access to or use of the Services in combination with any hardware, system, software, network, or other materials or service not provided by us; (c) failure to timely implement any modifications, upgrades, replacements, or enhancements made available to you by us or on our behalf; or (d) use of the Services other than in accordance with the terms and conditions of this Agreement and the Documentation. THIS SECTION 9 SETS FORTH THE CLIENT\u2019S SOLE REMEDIES AND SPACELIFT\u2019S SOLE LIABILITY AND OBLIGATION FOR ANY ACTUAL, THREATENED, OR ALLEGED CLAIMS THAT THE SERVICES AND ANY OTHER PROVIDER MATERIALS OR ANY SUBJECT MATTER OF THIS AGREEMENT INFRINGES, MISAPPROPRIATES OR OTHERWISE VIOLATES ANY INTELLECTUAL PROPERTY RIGHTS OF ANY THIRD PARTY. 9.2. Mitigation . If the Services or any of the other Spacelift\u2019s materials are, or in our opinion are likely to be, claimed to infringe, misappropriate, or otherwise violate any third-party IPR, or if your or your Authorized User's use of the Services or other Spacelift\u2019s materials is enjoined or threatened to be enjoined, we may, at our option and sole cost and expense: (a) obtain the right for you to continue to use the Services and said materials materially as contemplated by this Agreement, or (b) modify or replace the Services and said materials. 9.3. Client Indemnification . You will indemnify, defend, and hold us harmless from and against any and all losses incurred by us resulting from any action by a third party to the extent that such losses arise out of or result from, or are alleged to arise out of or result from: (a) your use of the Services; (b) Client Data, including any processing of Client Data by us or on our behalf in accordance with this Agreement; (c) any other materials or information (including any documents, data, or technology) provided by you or on your behalf, (d) your breach of any of its representations, warranties, covenants, or obligations under this Agreement; or (e) negligence or more culpable act or omission (including recklessness or willful misconduct) by you, any Authorized User, or any third party acting on your behalf or any Authorized User, in connection with this Agreement, provided, that Client will have no obligation under this Section 9.3 to the extent the applicable claim arises from Spacelift\u2019s breach of this Agreement. 9.4. Indemnification Procedure . Each party\u2019s indemnification obligations are conditioned on the indemnified party: (a) promptly giving written notice of the claim to the indemnifying party; (b) giving the indemnifying party sole control of the defense and settlement of the claim; and (c) providing to the indemnifying party all available information and assistance in connection with the claim, at the indemnifying party\u2019s request and expense. The indemnified party may participate in the defense of the claim, at the indemnified party\u2019s sole expense (not subject to reimbursement). Neither party may admit liability for or consent to any judgment or concede or settle or compromise any claim unless such admission or concession or settlement or compromise includes a full and unconditional release of the other Party from all liabilities in respect of the such claim. 10. LIABILITY \u00bb 10.1. Exclusion of Liability In no event will Spacelift have any obligation or liability arising from (a) use or inability to use any Services if modified or combined with materials not provided by us; (b) statements or conduct of any third party on or in the Services, (c) any Client Data, (d) any failure by Client to comply with the Agreement; and (e) damages suffered by the Client or Authorized Users, or any other person having arisen due to the third-party claims (other than described in Section 9.1), suspension or termination of the Services, or for other reasons arising from the Client\u2019s fault. 10.2. EXCLUSION OF DAMAGES . EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4 AND TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL SPACELIFT OR ANY OF ITS LICENSORS OR SERVICE PROVIDERS BE LIABLE UNDER OR IN CONNECTION WITH THIS AGREEMENT OR ITS SUBJECT MATTER UNDER ANY LEGAL OR EQUITABLE THEORY, INCLUDING BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, AND OTHERWISE, FOR ANY: (a) LOSS OF PRODUCTION, USE, BUSINESS, REVENUE, OR PROFIT OR DIMINUTION IN VALUE; (b) IMPAIRMENT, INABILITY TO USE OR LOSS, INTERRUPTION, OR DELAY OF THE SERVICES; (c) LOSS, DAMAGE, CORRUPTION, OR RECOVERY OF DATA, OR BREACH OF DATA OR SYSTEM SECURITY; (d) COST OF REPLACEMENT GOODS OR SERVICES; (e) LOSS OF GOODWILL OR REPUTATION; OR (f) CONSEQUENTIAL, INCIDENTAL, INDIRECT, EXEMPLARY, SPECIAL, ENHANCED, OR PUNITIVE DAMAGES, REGARDLESS OF WHETHER SUCH PERSONS WERE ADVISED OF THE POSSIBILITY OF SUCH LOSSES OR DAMAGES OR SUCH LOSSES OR DAMAGES WERE OTHERWISE FORESEEABLE, AND NOTWITHSTANDING THE FAILURE OF ANY AGREED OR OTHER REMEDY OF ITS ESSENTIAL PURPOSE. 10.3. CAP ON MONETARY LIABILITY . SPACELIFT WILL ONLY BE LIABLE FOR DIRECT DAMAGES EXCLUDING ANY SITUATION FOR WHICH WE ARE NOT RESPONSIBLE OR WHICH ARE CAUSED BY EVENTS OUTSIDE OUR REASONABLE CONTROL. HOWEVER, EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4, IN NO EVENT WILL THE COLLECTIVE AGGREGATE LIABILITY OF SPACELIFT ARISING OUT OF OR RELATED TO THIS AGREEMENT, WHETHER ARISING UNDER OR RELATED TO BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, OR ANY OTHER LEGAL OR EQUITABLE THEORY, EXCEED THE GREATER OF (a) THE TOTAL AMOUNTS PAID TO SPACELIFT UNDER THIS AGREEMENT IN THE 6 MONTH PERIOD PRECEDING THE EVENT GIVING RISE TO THE CLAIM OR (b) THE AMOUNT OF 5000 USD. THE FOREGOING LIMITATIONS APPLY EVEN IF ANY REMEDY FAILS OF ITS ESSENTIAL PURPOSE. 10.4. Exceptions . NOTHING IN THIS SECTION 10 WILL BE DEEMED TO LIMIT EITHER PARTY\u2019S LIABILITY FOR WILLFUL MISCONDUCT, GROSS NEGLIGENCE, FRAUD, OR INFRINGEMENT BY ONE PARTY OF THE OTHER\u2019S INTELLECTUAL PROPERTY RIGHTS. 11. PROVISIONS RELATING TO CONSUMERS \u00bb 11.1. Right to Withdraw . If you are a natural person and have your habitual residence within a Member State of the European Union or the European Economic Area and are entering into the Agreement as a consumer (i.e. for purposes which are outside your trade, business, craft or profession), you have the right to withdraw from the contract as described below. 11.2. Withdrawal Period . You have the right to withdraw from this Agreement (concluded under any Subscription Plan) within 14 days without giving any reason. The withdrawal right will expire after 14 days from the day of the conclusion of the Agreement. 11.3. Exercise of the Right to Withdraw . To exercise the right of withdrawal, you must inform us, Spacelift, Inc, of your decision to withdraw from this Agreement by an unequivocal statement (e.g. an e-mail sent to legal@spacelift.io ). To meet the withdrawal deadline, it is sufficient for you to send your communication concerning your exercise of the right of withdrawal before the withdrawal period expires. 11.4. Model Withdrawal Form . To exercise your right of withdrawal, you may use the model withdrawal form, included in Appendix No. 2 to the Act on Consumer Rights of May 20, 2014, but this is not obligatory. 11.5. Effect of the Withdrawal . If you withdraw from this Agreement, we will reimburse you all payments received from you, without undue delay and in any event not later than 14 days from the day on which we are informed about your decision to withdraw from this Agreement. We will carry out such reimbursement using the same means of payment as you used for the initial transaction unless you have expressly agreed otherwise; in any event, you will not incur any fees as a result of such reimbursement. 11.6. Consumer Rights . Nothing in the Agreement will affect your legal rights as a consumer. If any provision of the Agreement does not comply with the relevant law for you as a consumer, the relevant law will apply instead of this provision. The severability clause equally applies. In case of any concerns, questions, or doubts, contact us at legal@spacelift.io . 11.7. Complaints . If you have a complaint about Services, you should contact us at contact@spacelift.io , providing as much detail as possible about the complaint, together with your name, date of execution of the Agreement, and expected means of settling a complaint. We will respond by confirming receipt and will investigate the matter. Upon receiving the complaint, we will investigate the complaint internally, taking into account the importance and complexity of the issue raised, and get back to you no later than 30 days from the receipt of the complaint. 11.8. ADR . If you are a consumer, you may consider Alternative Dispute Resolution means in the event of a dispute with us, including referring to the trade inspection, a consumer ombudsman, or an organization whose statutory tasks include consumer protection. 12. FINAL PROVISIONS \u00bb 12.1. Current Version of Agreement . Usage of the Services is subject to the then-current version of the Agreement posted on the Website and we advise you to periodically review the latest currently effective Agreement. We reserve the right to update the provisions of the Agreement from time to time at our sole discretion. The updated Agreement version supersedes all prior versions, as well as is effective and binding immediately after posting on the Website. Your continued use of the Services on or after the date of the updated version of the Agreement is effective and constitutes your acceptance of such updated terms. If you do not agree to our updated Agreement, you can terminate the Subscription in accordance with Section 5. 12.2. Applicable Law and Jurisdiction . This Agreement is governed by and construed in accordance with the Applicable Law without giving effect to any choice or conflict of law provision of any jurisdiction. Any legal suit, action, or proceeding arising out of or related to this Agreement will be subject to the exclusive jurisdiction of the Applicable Jurisdiction as provided in the following table: Client Applicable Law Applicable Jurisdiction Consumers residing in the Member State of the European Union or the European Economic Area Poland Warsaw, Poland Other Clients State of Delaware, US County New Castle, Delaware, US Each party irrevocably submits to the exclusive jurisdiction of such courts in any such suit, action, or proceeding. 12.3. Contact details . In order to resolve a complaint regarding the Services, receive further information regarding the use of the Services, or send any notice to Spacelift, please contact us by email at contact@spacelift.io . 12.4. Notices . Except as otherwise expressly set forth in this Agreement, any notice, request, consent, claim, demand, waiver, or other communications under this Agreement have legal effect and will be deemed effectively given: (a) when received, if delivered by hand or with signed confirmation of receipt; (b) when received, if sent by a nationally recognized overnight courier or by certified or registered mail, signature required; or (c) when sent, if by email, if sent during the addressee's normal business hours, and on the next business day, if sent after the addressee's normal business hours. 12.5. Feedback . If you provide us with any suggestions, comments, recommendations, opinions, or other information relating to the Services or Website (\u201c Feedback \u201d), you grant us a royalty-free, non-exclusive, irrevocable, perpetual, worldwide right and license to use the Feedback on our websites or in marketing materials. We reserve the right to remove any Feedback posted on the Website if, in our opinion, such Feedback does not comply with the Agreement or applicable law. 12.6. Logo usage . You grant us the right to use your name and other indicia, such as logo or trademark in our list of current or former clients in promotional materials and on our websites. Any other announcement, statement, press release, or other publicity or marketing materials relating to your use of Services will be subject to your consent. 12.7. Export Laws . Each Party will comply with the export laws and regulations of the United States and other applicable jurisdictions in providing and using the Services. Without limiting the generality of the foregoing, Client represents that it is not named on any U.S. government denied-party list and will not make the Services available to any user or entity that is located in a country that is subject to a U.S. government embargo, or is listed on any U.S. government list of prohibited or restricted parties. 12.8. Non-waiver . Our failure to exercise or enforce any right or provision of the Agreement will not operate as a waiver of such right or provision. 12.9. Assignment . We may assign any or all of our rights and obligations to others at any time. We will notify you of any assignment. 12.0. Severability . If any term or provision of this Agreement is invalid, illegal, or unenforceable in any jurisdiction, such invalidity, illegality, or unenforceability will not affect any other term or provision of this Agreement or invalidate or render unenforceable such term or provision in any other jurisdiction. 12.11. No relationship . There is no joint venture, partnership, employment, or agency relationship created between you and us as a result of the Agreement or use of the Services.","title":"Terms and Conditions"},{"location":"legal/terms.html#terms-and-conditions","text":"Last updated: March 7, 2023 This Terms and Conditions is between the entity you represent, or, if you do not indicate an entity in connection with the Services, you individually (\u201c Client \u201d, \u201c you \u201d or \u201c your \u201d), and Spacelift, Inc. with its principal office at 541 Jefferson Ave. Suite 100, Redwood City CA 94063, United States of America (\u201c Spacelift \u201d, \u201c the Company \u201d \u201c we \u201d, \u201c us \u201d, or \u201c our \u201d). It consists of the terms and conditions below, the Privacy Policy, the Refund Policy and the Cookie Policy (together, the \u201c Agreement \u201d). The Agreement does not govern the use of: the Services under Enterprise Plan provided that a separate and mutually agreed contract has been executed, the Services purchased via AWS Marketplace, which are subject to Standard Contract for AWS Marketplace and any amendments, any self-hosted services provided by Spacelift, Inc. which are subject to a separate agreement.","title":"Terms and Conditions"},{"location":"legal/terms.html#1-definitions","text":"1.1. \u201c Authorized User \u201d means Client\u2019s employees, consultants, contractors, agents, and Workers (a) who Client authorizes to access and use the Services under the rights granted to Client under this Agreement; and (b) for whom access to the Services has been purchased hereunder. 1.2. \u201c Client Data \u201d means information, data, and other content, in any form or medium, that is collected, downloaded, or otherwise received from the Client or an Authorized User by or through the Services. For the avoidance of doubt, Client Data does not include Resultant Data or any other information reflecting the access or use of the Services by or on behalf of Client or any Authorized User. 1.3. \u201c Confidential Information \u201d means all nonpublic information, including, but not limited to, source code, software, trade secrets, know-how, technical drawings, algorithms, ideas, inventions, other technical, business or sales information, negotiations or proposals, disclosed by us in whatever form and which is known by the Client or its Authorized User to be confidential or under circumstances by which the receiving party should reasonably understand such information is to be treated as confidential, whether or not marked as \u201cConfidential\u201d. 1.4. \u201c Documentation \u201d means any manuals, instructions, including technical specifications, or other documents or materials describing the features and functionality of the Services, which are located on Website or provided to Clients, and may be updated from time to time. 1.5. \u201c Intellectual Property Rights \u201d or \u201c IPR \u201d means any registered and unregistered rights granted, applied for, or otherwise now or hereafter in existence under or related to any patent, copyright, trademark, trade secret, database protection, or other intellectual property rights laws in any part of the world. 1.6. \u201c Metrics \u201d - means the measurements used for quantifying the Services usage with the following meaning: \u200b1.6.1. \u201c Private Minute(s) \u201d means minute(s) of Private Workers\u2019 Services usage in a given month; 1.6.2. \u201c Public Minutes \u201d means minutes of Public Worker\u2019s Services usage in a given month; 16.3. \u201c Seat(s) \u201d means an Authorized User(s) who actively logged in to the Services in a given month; 1.6.4. \u201c Worker(s) \u201d means a predefined set(s) of computing resources that are specifically optimized for the development and provisioning and deployment of cloud-based infrastructures based on IaC; a Worker can be either a self-hosted agent performing infrastructure management in a Client-controlled environment (\u201c Private Worker \u201d) or any other software agent, provided and managed by Spacelift in a common secure worker pool (\u201c Public Worker \u201d). 1.7. \u201c Services \u201d means the Spacelift\u2019s specialized, continuous integration and deployment (CI/CD) platform for infra-as-code available at https://spacelift.io as SaaS; 1.8. \u201c Subscription \u201d means enrollment for Services for a Subscription Plan on Subscription Terms as defined in the Agreement; 1.9. \u201c Subscription Plan(s) \u201d means available Subscription offer(s) for use of the Services as described on the Website, including Trial, Free, Cloud, and Enterprise; 1.10. \u201c Subscription Term(s) \u201d means the conditions under which a Subscription is made under the Agreement, including the Subscription Period, Metrics, and Subscription Fees, as described in Section 5; 1.11. \u201c Website \u201d means https://spacelift.io website managed by Spacelift.","title":"1. DEFINITIONS"},{"location":"legal/terms.html#2-general","text":"2.1. Execution of the Agreement . Accepting this Agreement is a condition of using the Services provided by Spacelift. \u200b\u200bBY COMPLETING THE REGISTRATION PROCESS, ACCESSING OR USING THE SERVICES, YOU ACKNOWLEDGE AND AGREE THAT (I) YOU HAVE READ, UNDERSTOOD, AND ACCEPTED THIS AGREEMENT, AND (II) YOU HEREBY REPRESENT AND WARRANT THAT YOU ARE AUTHORIZED TO ENTER OR ACT ON BEHALF OF THE CLIENT, AND BIND TO THIS AGREEMENT. If you do not have the legal authority to enter this Agreement, do not understand this Agreement, or do not agree to the Agreement, you should not accept the Terms and Conditions, or use the Services. 2.2. Conflict of Provisions . In the event of any inconsistencies or conflict between the documents that make up this Agreement, the documents will prevail in the following order: (a) any written amendment agreed upon by the parties (such as order forms); (b) Privacy Policy; (c) these Terms and Conditions and (d) the Refund Policy. 2.3. Compliance . You are responsible for (a) compliance with the provisions of the Agreement by you and your Authorized Users and for any and all acts and omissions of Authorized Users connected with their use and access to the Services and for any breach of this Agreement by Authorized Users; and (b) any delay or failure of performance caused in whole or in part by your delay in performing, or failure to perform, any of your obligations under this Agreement. Without limiting the foregoing, you are solely responsible for ensuring that your use of the Services is compliant with all applicable laws and regulations, as well as any and all privacy policies, agreements, or other obligations you may maintain or enter into. 2.4. Amendments . Any individual amendment to this Agreement must be made in writing (expressly stating that it is amending this Agreement) and signed by both parties.","title":"2. GENERAL"},{"location":"legal/terms.html#3-license-intellectual-property-rights-and-ownership","text":"3.1. Ownership . The Services, Documentation, and Website, all copies and portions thereof, and all IPR therein, including, but not limited to source code, databases, functionality, software, website designs, audio, video, text, photographs, graphics, or derivative works therefrom, are owned by us or licensed to us. You are not authorized to use (and will not permit any third party to use) the Services, Website, Documentation, or any portion thereof except as expressly authorized by this Agreement. Specifically, no part of the Services, Documentation, or Website may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, modified, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission. 3.2. Confidential Information . All our Confidential Information and derivations thereof will remain our sole and exclusive property. You should not disclose, use or publish Confidential Information without our prior written consent. You must hold all our Confidential Information in strict confidence and safeguard the Confidential Information from unauthorized use, access, or disclosure using at least the degree of care you use to protect your similarly sensitive information and in no event less than a reasonable degree of care. 3.3. License . Spacelift makes the Services available to you during the Subscription Period, subject to the terms and conditions of this Agreement and Subscription Terms. Spacelift grants a limited, non-exclusive, non-sublicensable, non-transferable right to access and use the Services and its Documentation during the Subscription Period, solely for your internal business purposes or your personal use. 3.4. Restrictions . You will not, and will not permit any other person to, access or use the Services except as expressly permitted by this Agreement. For purposes of clarity and without limiting the generality of the foregoing, you will not, except as this Agreement expressly permits: (a) copy, modify, or create derivative works or improvements of the Services or Documentation; (b) rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer, or otherwise make available any Services or Documentation to any person; (c) reverse engineer, disassemble, decompile, decode, adapt, or otherwise attempt to derive or gain access to the source code of the Services, in whole or in part; (d) bypass or breach any security device or protection used by the Services or access or use the Services other than by an Authorized User through the use of his or her own then valid access credentials; (e) input, upload, transmit, or otherwise provide to or through the Services or Documentation, any information or materials that are unlawful or injurious, or contain, transmit, or activate any harmful code; (f) damage, destroy, disrupt, disable, impair, interfere with, or otherwise impede or harm in any manner the Services or Documentation, or our provision of services to any third party, in whole or in part; (g) remove, delete, alter, or obscure any trademarks, Documentation, warranties, or disclaimers, or IPR notices from any Services; (h) access or use the Services or Documentation in any manner or for any purpose that infringes, misappropriates, or otherwise violates any IPR or other right of any third party or that violates any applicable law; or (i) access or use the Services or Documentation for purposes of competitive analysis of the Services, the development, provision, or use of a competing software service or product. 3.5. Client Data . You are and will remain the sole and exclusive owner of all rights, title, and interest in and to all Client Data, including all IPR, subject to the rights and permissions granted in the Agreement. You have exclusive control and responsibility for determining what data you submit to the Services, for obtaining all necessary consents and permissions for the submission of Client Data, and for the accuracy, quality, and legality of Client Data. 3.6. Consent to Use Client Data . You irrevocably grant all such rights and permissions in or relating to Client Data as are reasonably necessary or useful to us to enforce this Agreement and exercise our rights and perform our obligations hereunder. 3.7. Use of Resultant Data . We may collect data and information related to your use of the Services that is used by us in an aggregate manner, including to compile statistical and performance information related to the provision and operation of the Services (\u201c Resultant Data \u201d). You unconditionally and irrevocably grant to us an assignment of all rights, title, and interest in and to the Resultant Data, including all IPR relating thereto, if any.","title":"3. LICENSE, INTELLECTUAL PROPERTY RIGHTS, AND OWNERSHIP"},{"location":"legal/terms.html#4-provision-of-services","text":"4.1. Metrics . Use of the Services is subject to usage limits reflected in Metrics, as set forth in the Subscription Plan. We will monitor your use of the Services in order to verify whether you comply with the presented limits. 4.2. Services Modifications . We reserve the right to make any changes to the Services or Documentation that we deem necessary or useful to: (a) maintain or enhance: (i) the quality or delivery of Services to you and other clients; (ii) the competitive strength of or market for Services; or (iii) the Services' cost efficiency or performance; or (b) to comply with applicable law. 4.3. Privacy . When applicable, we will comply with all applicable laws, regulations, and government orders relating to personally identifiable information and data privacy with respect to any such Client Data that we receive or have access to under the Agreement or in connection with the performance of the Services. In particular, regulations for the protection of personally identifiable information are indicated in the Privacy Policy incorporated herein by reference. 4.4. Access and Security . You will employ all physical, administrative, and technical controls, screening, security procedures, and other safeguards necessary to: (a) securely administer the distribution and use of all access credentials and protect against any unauthorized access to or use of the Services; and (b) control the content and use of Client Data, including the uploading or other provision of Client Data for processing by the Services. 4.5. Security . We maintain industry-standard security and privacy certification, such as a SOC II certification. We will use appropriate technical and organizational measures designed to prevent unauthorized access, use, alteration, or disclosure of the Services or Client Data. 4.6. Incidents . We will notify you in case of any security incident as soon as possible, provided that you have indicated your contact data in the Services under the address: https://*.app.spacelift.io/settings/security (* being the domain name chosen by you to access Services). 4.7. Downtimes . We will use commercially reasonable efforts to give you at least 24 hours prior notice of all scheduled outages of the Services. You can check the current Services\u2019 availability status at https://spacelift.statuspage.io/ 4.8. Services and Website Management . We reserve the right at our sole discretion, to (a) monitor the Services for breaches of the Agreement; (b) take appropriate legal action against anyone in breach of applicable laws or the Agreement; (c) refuse, restrict access to, or availability of, or disable (to the extent technologically feasible) any of Client Data; (d) remove from the Services or Website or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (e) otherwise manage the Website and Services in a manner designed to protect our rights and property and to facilitate the proper functioning of the Website and Services. 4.9. Third-Party Materials . The Website and/or Services may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third-party websites or applications or the third-party operator. We are not responsible for and do not endorse any third-party websites or applications or their availability or content. 4.10. Access by Third-Party Accounts . You may register and login to the Services using your third-party service providers account details, like a Google or GitHub account (\u201c Third-Party Account \u201d). When you do so, we will receive certain profile information varying on the identity provider and the information you decided to include in your Third-Party Account. You will have the ability to disable the connection between your Services account and your Third Party Accounts at any time. Please note that your relationship with the third-party service providers associated with your Third-Party Accounts is governed solely by your agreement(s) with such third-party service providers. If a Third Party Account or associated service becomes unavailable or the access to such Third Party Account is terminated by the third-party service provider, then your access using such Third Party Account may no longer be available. 4.11. Support Services . During the Subscription Period, we will provide support services depending on the Subscription Plan, as described in https://docs.spacelift.io/product/support/ 4.12. Client Systems and Cooperation . You will at all times during the period of Subscription: (a) set up, maintain, and operate in good repair and in accordance with the Documentation all your systems (meaning information technology infrastructure, including computers, software, databases, electronic systems, database management systems, and networks) on or through which the Services are accessed or used; (b) provide us with such access to your data or systems as is necessary for us to perform the Services in accordance with the Agreement and Documentation; (c) use reasonable measures to prevent and promptly notify us of any unauthorized access to Authorized User accounts of which you become aware of, and (d) provide all cooperation and assistance as we may reasonably request to enable us to exercise our rights and perform our obligations under and in connection with this Agreement. 4.13. Quality of Services . You are aware that the quality of the Services and your use of the Services might be affected by a number of factors outside our control, including but not limited to force majeure, technical conditions, hardware or software (including third-party software and network) issues. Any delay or default affecting the availability, functionality, correctness, or timely performance of the Services caused by such circumstances will not constitute a breach of the Agreement. 4.14. No Data Backup . We do not store or backup any Client Data. The Services do not replace the need for you to maintain regular data backups or redundant data archives. We have no obligation or liability for any loss, alteration, destruction, damage, corruption, or recovery of Client Data. 4.15. Disclaimer . The content on the Website is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Website.","title":"4. PROVISION OF SERVICES"},{"location":"legal/terms.html#5-subscription-plans-and-terms","text":"5.1. Effective Date and Term . This Agreement commences on the effective date being the day of your registration, access to, or use of the Services, whichever happens first (\u201c Effective Date \u201d). Unless earlier terminated pursuant to the terms of this Section, the Agreement will continue through the Subscription Period of a chosen Subscription Plan. 5.2. Subscription Plans . The Services are available under the following Subscription Plans with the relevant Subscription Terms: Plan Subscription Period Termination Subscription Fee Trial 14 days Any time None Free Non-definite term Any time None Cloud Default: 1 month, monthly renewal. Individual arrangements may include an annual or a multi-year Subscription Period. Any time during the Subscription Period, having its effect on the last day of the given Subscription Period. Current fees are set forth in https://spacelift.io/pricing Enterprise As agreed by the parties in a separate agreement or order form As agreed by the parties in a separate agreement or order form As agreed by the parties in a separate agreement or order form 5.3. The Trial Plan . The Subscription Terms for the Trial Plan are as follows: 5.3.1. Scope and Metrics . The Trial Plan offers access to Services to get to know Services before starting the Free, Cloud, or Enterprise Plan. Any usage limitations are indicated on https://spacelift.io/pricing . 5.3.2. Subscription Period . The Trial Plan expires 14 days after your registration to the Services. 5.3.3. Termination . You may cancel the Trial Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io . 5.3.4. Next Steps . Upon the end of Trial Plan, you may: (a) stop using the Services and delete your account thus terminating the Agreement, (b) continue to use the Services under Free Plan, (c) subscribe to Cloud Plan if you provide payment details to make a Subscription Fee according to the currently effective rates presented on https://spacelift.io/pricing , or (d) contact sales@spacelift.io to discuss the Enterprise Plan which is subject to separate agreement. 5.4. The Free Plan . The Subscription Terms for the Free Plan are as follows: 5.4.1. Scope and Metrics . The Free Plan offers access to the Services with usage limitations indicated on https://spacelift.io/pricing . In order to expand the usage limitations, upgrade to Cloud Plan or Enterprise Plan. 5.4.2. Subscription Period and Termination . The Free Plan is available for an indefinite period of time and might be terminated at any time. You may cancel the Free Plan by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io . 5.5. The Cloud Plan . The Subscription Terms for the Cloud Plan are as follows: 5.5.1. Scope and Metrics . The Cloud Plan offers access to the Services with usage limitations indicated on https://spacelift.io/pricing . This plan can be supplemented with additional Seats and/or Workers as required, at an additional cost calculated on the basis of current rates. 5.5.2. Subscription Period and Billing . If you activate the Cloud Plan, you authorize Spacelift to periodically charge, on a going-forward basis and until cancellation of either the recurring payments or your account, all accrued sums on or before the payment due date for the accrued sums. The \" Subscription Billing Date \" is the date when you purchase your first Subscription to the Services. Your account will be charged automatically on the Subscription Billing Date for all applicable fees for the next Subscription Period. The subscription will continue unless and until you cancel your Subscription or we terminate it. You must cancel your Subscription before it renews in order to avoid billing the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide to us during registration (or to a different payment method if you change your payment information). 5.5.3. Termination . You may cancel the Cloud Plan at any time by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io . The termination will be effective on the last day of the given Subscription Period. 5.6. Withdrawal and Refund . You may withdraw from the Agreement and claim a refund of funds within 14 days after its execution provided that the Services have not been activated during that period. You can find all the details regarding the refund in our Refund Policy. 5.7. Plan Adjustments and Upgrades . The Subscription Terms for each plan, including Subscription Period, Fees, and Metrics may be adjusted by written agreement of the parties. If you wish to upgrade your Subscription Plan, contact sales@spacelift.io to discuss the available options. 5.8. The Enterprise Plan . In most cases, Subscription Terms of the Enterprise Plan are individually discussed by the parties and bind the parties on the basis of a separately executed agreement. In case a separate agreement is not executed between the parties, the written arrangements (such as order forms) regarding Subscription Terms apply and in the remaining scope terms for the use of the Services will be subject to conditions set forth in this Agreement. 5.9. Services Usage during Negotiations . If you wish to actively use the Services in the course of negotiating the separate agreement, the parties may agree on the temporary terms of use of the Services, including the relevant Metrics, period, and fees, and in the remaining scope terms for the use of the Services will be subject to conditions set forth in this Agreement.","title":"5. SUBSCRIPTION PLANS AND TERMS"},{"location":"legal/terms.html#6-subscription-fees","text":"6.1. Terms of Payment . Unless otherwise agreed by the parties, Subscription Fees will be payable in USD via a credit card on a going-forward basis and will be subject to this Section 6. 6.2. Taxes . All Subscription Fees and other amounts payable by you under this Agreement are exclusive of taxes and similar assessments. Without limiting the foregoing, you are responsible for all sales, use, excise taxes, and any other similar taxes, duties, and charges of any kind, other than any taxes imposed on our income. 6.3. Late Payment . If you fail to make any payment when due then, in addition to all other remedies that may be available: (a) we may charge interest on the past due amount at the rate of 1.5% per month calculated daily and compounded monthly or, if lower, the highest rate permitted under applicable law; (b) you will reimburse us for all reasonable costs incurred by us in collecting any late payments or interest, including attorneys' fees, court costs, and collection agency fees; and (c) if such failure continues for fourteen (14) days following written notice, we may suspend performance of the Services until all past due amounts and interest have been paid, without incurring any obligation or liability to you or any other person by reason of such suspension. 6.4. Subscription Fees Increases . Separately from any changes in Subscription Fees due to the upgrade of the relevant Metrics, we may increase Fees for any Subscription Period before its start by providing you a notice prior to the commencement of the next Subscription Period. Your continued use of the Services constitutes your acceptance of such changed Subscription Fees.","title":"6. SUBSCRIPTION FEES"},{"location":"legal/terms.html#7-suspension-and-termination","text":"7.1. Suspension . Without limiting any other provision of the Agreement, we reserve the right to, in our sole discretion and without notice or liability, deny access to and use of the Services (including blocking certain IP addresses), to any person for any reason including but not limited to (a) proven or suspected breach of any representation, warranty or covenant contained in the Agreement or of any applicable law or regulation; (b) your use of the Services poses a risk to the Services, our other customers, or us (including our infrastructure, security, and third-party relationships); (c) your use of the Services could subject us to liability or (d) you are past due in the payment of Subscription Fee. We will provide you with prompt notice of any suspension. 7.2. Effect of Suspension . If we suspend your access to the Services for any reason set out in the Agreement, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress. 7.3. Termination for Cause . Notwithstanding the termination for convenience as described in Section 5, Either Party may terminate this Agreement, effective on written notice to the other party, if the other party (a) materially breaches this Agreement, and such breach: (i) is incapable of cure; or (ii) being capable of cure, remains uncured 30 days after the non-breaching party provides the breaching party with written notice of such breach; (b) becomes insolvent or is generally unable to pay, or fails to pay, its debts as they become due; (c) files, or has filed against it, a petition for voluntary or involuntary bankruptcy or otherwise becomes subject, voluntarily or involuntarily, to any proceeding under any domestic or foreign bankruptcy or insolvency law; (d) makes or seeks to make a general assignment for the benefit of its creditors; or (e) applies for or has appointed a receiver, trustee, custodian, or similar agent appointed by order of any court of competent jurisdiction to take charge of or sell any material portion of its property or business. 7.4. Effect of Termination . Upon any termination of this Agreement, except as expressly otherwise provided in this Agreement: (a) all rights, licenses, consents, and authorizations granted by either party to the other will immediately terminate; (b) we will immediately cease all use of any Client Data and at your request destroy, all documents and tangible materials containing or based on Client Data and erase all Client Data from all our systems, provided that, for clarity, our obligations under this Section 7.4 do not apply to any Resultant Data or other data that is required to establish proof of a right or a contract, which will be stored for the duration provided by enforceable law; (c) you will immediately cease all use of any Services and within sixty (60) days destroy, all documents and tangible materials containing or based on any our materials, including Documentation and erase all our materials from the systems you directly or indirectly control. You acknowledge and agree that you are responsible to retrieve Client Data from the Services prior to the termination of this Agreement. 7.5. Surviving Terms . The provisions set forth in the following sections, and any other right or obligation of the parties in this Agreement that, by its nature, should survive termination or expiration of this Agreement, will survive any expiration or termination of this Agreement: 3.1, 3.2, 3.4, 3.7, 7.4, 7.5, 8.4, 9, 10, 12.","title":"7. SUSPENSION AND TERMINATION"},{"location":"legal/terms.html#8-representations-and-warranties","text":"8.1. Mutual Representations and Warranties . Each party represents and warrants to the other party that it has the full right, power, and authority to enter into and perform its obligations and grant the rights, licenses, consents, and authorizations it grants or is required to grant under this Agreement. 8.2. Additional Spacelift Representations, Warranties, and Covenants . We represent, warrant, and covenant to you that (a) we will perform the Services using personnel of required skill, experience, and qualifications and in a professional and workmanlike manner in accordance with generally recognized industry standards and will devote adequate resources to meet its obligations under this Agreement; (b) the Services will be performed materially in accordance with the applicable Documentation; (c) to the best of our knowledge, the Services is free from any viruses, worms, malware, or other malicious source code. 8.3. Additional Client Representations, Warranties, and Covenants . You represent, warrant, and covenant to us that (a) you own or otherwise have and will have the necessary rights and consents in and relating to the Client Data so that, as received by us and processed in accordance with this Agreement, they do not and will not infringe, misappropriate, or otherwise violate any IPR, or any privacy or other rights of any third party or violate any applicable law; (b) all registration information you submit will be true, accurate, current, and complete and relate to you and not a third party; (c) you will maintain the accuracy of such information and promptly update such information as necessary; (d) you will keep your access credentials confidential and will be responsible for all use of your access credentials; (e) you are aware that you may not access or use the Services for any purpose other than that for which we make the Services available and (f) you are at least eighteen years of age. 8.4. DISCLAIMER OF WARRANTIES . EXCEPT FOR THE EXPRESS WARRANTIES SET FORTH IN THIS SECTION 8, ALL SERVICES, DOCUMENTATION, AND WEBSITE ARE PROVIDED \"AS IS.\" WE SPECIFICALLY DISCLAIM ALL IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT, AND ALL WARRANTIES ARISING FROM THE COURSE OF DEALING, USAGE, OR TRADE PRACTICE. WITHOUT LIMITING THE FOREGOING, WE MAKE NO WARRANTY OF ANY KIND THAT THE SERVICES, DOCUMENTATION OR WEBSITE, OR ANY PRODUCTS OR RESULTS OF THE USE THEREOF, WILL MEET YOUR OR ANY OTHER PERSON'S REQUIREMENTS, OPERATE WITHOUT INTERRUPTION, ACHIEVE ANY INTENDED RESULT, BE COMPATIBLE OR WORK WITH ANY SOFTWARE, SYSTEM, OR OTHER SERVICES, OR BE SECURE, ACCURATE, COMPLETE, FREE OF HARMFUL CODE, OR ERROR-FREE. ALL THIRD-PARTY MATERIALS ARE PROVIDED \"AS IS\" AND ANY REPRESENTATION OR WARRANTY OF OR CONCERNING ANY THIRD-PARTY MATERIALS IS STRICTLY BETWEEN THE CLIENT AND THE THIRD-PARTY OWNER OR DISTRIBUTOR OF THE THIRD-PARTY MATERIALS.","title":"8. REPRESENTATIONS AND WARRANTIES"},{"location":"legal/terms.html#9-indemnification","text":"9.1. Spacelift Indemnification . Subject to the remainder of this Section 9 and the liability limitations set forth in Section 10, we will indemnify, defend, and hold you harmless from and against any and all losses incurred by you resulting from any action by a third party that your use of the Services (excluding Client Data and any third-party materials) in accordance with this Agreement infringes or misappropriates IPR. The foregoing obligation does not apply to the extent that the alleged infringement arises from (a) any third-party materials or Client Data; (b) access to or use of the Services in combination with any hardware, system, software, network, or other materials or service not provided by us; (c) failure to timely implement any modifications, upgrades, replacements, or enhancements made available to you by us or on our behalf; or (d) use of the Services other than in accordance with the terms and conditions of this Agreement and the Documentation. THIS SECTION 9 SETS FORTH THE CLIENT\u2019S SOLE REMEDIES AND SPACELIFT\u2019S SOLE LIABILITY AND OBLIGATION FOR ANY ACTUAL, THREATENED, OR ALLEGED CLAIMS THAT THE SERVICES AND ANY OTHER PROVIDER MATERIALS OR ANY SUBJECT MATTER OF THIS AGREEMENT INFRINGES, MISAPPROPRIATES OR OTHERWISE VIOLATES ANY INTELLECTUAL PROPERTY RIGHTS OF ANY THIRD PARTY. 9.2. Mitigation . If the Services or any of the other Spacelift\u2019s materials are, or in our opinion are likely to be, claimed to infringe, misappropriate, or otherwise violate any third-party IPR, or if your or your Authorized User's use of the Services or other Spacelift\u2019s materials is enjoined or threatened to be enjoined, we may, at our option and sole cost and expense: (a) obtain the right for you to continue to use the Services and said materials materially as contemplated by this Agreement, or (b) modify or replace the Services and said materials. 9.3. Client Indemnification . You will indemnify, defend, and hold us harmless from and against any and all losses incurred by us resulting from any action by a third party to the extent that such losses arise out of or result from, or are alleged to arise out of or result from: (a) your use of the Services; (b) Client Data, including any processing of Client Data by us or on our behalf in accordance with this Agreement; (c) any other materials or information (including any documents, data, or technology) provided by you or on your behalf, (d) your breach of any of its representations, warranties, covenants, or obligations under this Agreement; or (e) negligence or more culpable act or omission (including recklessness or willful misconduct) by you, any Authorized User, or any third party acting on your behalf or any Authorized User, in connection with this Agreement, provided, that Client will have no obligation under this Section 9.3 to the extent the applicable claim arises from Spacelift\u2019s breach of this Agreement. 9.4. Indemnification Procedure . Each party\u2019s indemnification obligations are conditioned on the indemnified party: (a) promptly giving written notice of the claim to the indemnifying party; (b) giving the indemnifying party sole control of the defense and settlement of the claim; and (c) providing to the indemnifying party all available information and assistance in connection with the claim, at the indemnifying party\u2019s request and expense. The indemnified party may participate in the defense of the claim, at the indemnified party\u2019s sole expense (not subject to reimbursement). Neither party may admit liability for or consent to any judgment or concede or settle or compromise any claim unless such admission or concession or settlement or compromise includes a full and unconditional release of the other Party from all liabilities in respect of the such claim.","title":"9. INDEMNIFICATION"},{"location":"legal/terms.html#10-liability","text":"10.1. Exclusion of Liability In no event will Spacelift have any obligation or liability arising from (a) use or inability to use any Services if modified or combined with materials not provided by us; (b) statements or conduct of any third party on or in the Services, (c) any Client Data, (d) any failure by Client to comply with the Agreement; and (e) damages suffered by the Client or Authorized Users, or any other person having arisen due to the third-party claims (other than described in Section 9.1), suspension or termination of the Services, or for other reasons arising from the Client\u2019s fault. 10.2. EXCLUSION OF DAMAGES . EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4 AND TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL SPACELIFT OR ANY OF ITS LICENSORS OR SERVICE PROVIDERS BE LIABLE UNDER OR IN CONNECTION WITH THIS AGREEMENT OR ITS SUBJECT MATTER UNDER ANY LEGAL OR EQUITABLE THEORY, INCLUDING BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, AND OTHERWISE, FOR ANY: (a) LOSS OF PRODUCTION, USE, BUSINESS, REVENUE, OR PROFIT OR DIMINUTION IN VALUE; (b) IMPAIRMENT, INABILITY TO USE OR LOSS, INTERRUPTION, OR DELAY OF THE SERVICES; (c) LOSS, DAMAGE, CORRUPTION, OR RECOVERY OF DATA, OR BREACH OF DATA OR SYSTEM SECURITY; (d) COST OF REPLACEMENT GOODS OR SERVICES; (e) LOSS OF GOODWILL OR REPUTATION; OR (f) CONSEQUENTIAL, INCIDENTAL, INDIRECT, EXEMPLARY, SPECIAL, ENHANCED, OR PUNITIVE DAMAGES, REGARDLESS OF WHETHER SUCH PERSONS WERE ADVISED OF THE POSSIBILITY OF SUCH LOSSES OR DAMAGES OR SUCH LOSSES OR DAMAGES WERE OTHERWISE FORESEEABLE, AND NOTWITHSTANDING THE FAILURE OF ANY AGREED OR OTHER REMEDY OF ITS ESSENTIAL PURPOSE. 10.3. CAP ON MONETARY LIABILITY . SPACELIFT WILL ONLY BE LIABLE FOR DIRECT DAMAGES EXCLUDING ANY SITUATION FOR WHICH WE ARE NOT RESPONSIBLE OR WHICH ARE CAUSED BY EVENTS OUTSIDE OUR REASONABLE CONTROL. HOWEVER, EXCEPT AS OTHERWISE PROVIDED IN SECTION 10.4, IN NO EVENT WILL THE COLLECTIVE AGGREGATE LIABILITY OF SPACELIFT ARISING OUT OF OR RELATED TO THIS AGREEMENT, WHETHER ARISING UNDER OR RELATED TO BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY, OR ANY OTHER LEGAL OR EQUITABLE THEORY, EXCEED THE GREATER OF (a) THE TOTAL AMOUNTS PAID TO SPACELIFT UNDER THIS AGREEMENT IN THE 6 MONTH PERIOD PRECEDING THE EVENT GIVING RISE TO THE CLAIM OR (b) THE AMOUNT OF 5000 USD. THE FOREGOING LIMITATIONS APPLY EVEN IF ANY REMEDY FAILS OF ITS ESSENTIAL PURPOSE. 10.4. Exceptions . NOTHING IN THIS SECTION 10 WILL BE DEEMED TO LIMIT EITHER PARTY\u2019S LIABILITY FOR WILLFUL MISCONDUCT, GROSS NEGLIGENCE, FRAUD, OR INFRINGEMENT BY ONE PARTY OF THE OTHER\u2019S INTELLECTUAL PROPERTY RIGHTS.","title":"10. LIABILITY"},{"location":"legal/terms.html#11-provisions-relating-to-consumers","text":"11.1. Right to Withdraw . If you are a natural person and have your habitual residence within a Member State of the European Union or the European Economic Area and are entering into the Agreement as a consumer (i.e. for purposes which are outside your trade, business, craft or profession), you have the right to withdraw from the contract as described below. 11.2. Withdrawal Period . You have the right to withdraw from this Agreement (concluded under any Subscription Plan) within 14 days without giving any reason. The withdrawal right will expire after 14 days from the day of the conclusion of the Agreement. 11.3. Exercise of the Right to Withdraw . To exercise the right of withdrawal, you must inform us, Spacelift, Inc, of your decision to withdraw from this Agreement by an unequivocal statement (e.g. an e-mail sent to legal@spacelift.io ). To meet the withdrawal deadline, it is sufficient for you to send your communication concerning your exercise of the right of withdrawal before the withdrawal period expires. 11.4. Model Withdrawal Form . To exercise your right of withdrawal, you may use the model withdrawal form, included in Appendix No. 2 to the Act on Consumer Rights of May 20, 2014, but this is not obligatory. 11.5. Effect of the Withdrawal . If you withdraw from this Agreement, we will reimburse you all payments received from you, without undue delay and in any event not later than 14 days from the day on which we are informed about your decision to withdraw from this Agreement. We will carry out such reimbursement using the same means of payment as you used for the initial transaction unless you have expressly agreed otherwise; in any event, you will not incur any fees as a result of such reimbursement. 11.6. Consumer Rights . Nothing in the Agreement will affect your legal rights as a consumer. If any provision of the Agreement does not comply with the relevant law for you as a consumer, the relevant law will apply instead of this provision. The severability clause equally applies. In case of any concerns, questions, or doubts, contact us at legal@spacelift.io . 11.7. Complaints . If you have a complaint about Services, you should contact us at contact@spacelift.io , providing as much detail as possible about the complaint, together with your name, date of execution of the Agreement, and expected means of settling a complaint. We will respond by confirming receipt and will investigate the matter. Upon receiving the complaint, we will investigate the complaint internally, taking into account the importance and complexity of the issue raised, and get back to you no later than 30 days from the receipt of the complaint. 11.8. ADR . If you are a consumer, you may consider Alternative Dispute Resolution means in the event of a dispute with us, including referring to the trade inspection, a consumer ombudsman, or an organization whose statutory tasks include consumer protection.","title":"11. PROVISIONS RELATING TO CONSUMERS"},{"location":"legal/terms.html#12-final-provisions","text":"12.1. Current Version of Agreement . Usage of the Services is subject to the then-current version of the Agreement posted on the Website and we advise you to periodically review the latest currently effective Agreement. We reserve the right to update the provisions of the Agreement from time to time at our sole discretion. The updated Agreement version supersedes all prior versions, as well as is effective and binding immediately after posting on the Website. Your continued use of the Services on or after the date of the updated version of the Agreement is effective and constitutes your acceptance of such updated terms. If you do not agree to our updated Agreement, you can terminate the Subscription in accordance with Section 5. 12.2. Applicable Law and Jurisdiction . This Agreement is governed by and construed in accordance with the Applicable Law without giving effect to any choice or conflict of law provision of any jurisdiction. Any legal suit, action, or proceeding arising out of or related to this Agreement will be subject to the exclusive jurisdiction of the Applicable Jurisdiction as provided in the following table: Client Applicable Law Applicable Jurisdiction Consumers residing in the Member State of the European Union or the European Economic Area Poland Warsaw, Poland Other Clients State of Delaware, US County New Castle, Delaware, US Each party irrevocably submits to the exclusive jurisdiction of such courts in any such suit, action, or proceeding. 12.3. Contact details . In order to resolve a complaint regarding the Services, receive further information regarding the use of the Services, or send any notice to Spacelift, please contact us by email at contact@spacelift.io . 12.4. Notices . Except as otherwise expressly set forth in this Agreement, any notice, request, consent, claim, demand, waiver, or other communications under this Agreement have legal effect and will be deemed effectively given: (a) when received, if delivered by hand or with signed confirmation of receipt; (b) when received, if sent by a nationally recognized overnight courier or by certified or registered mail, signature required; or (c) when sent, if by email, if sent during the addressee's normal business hours, and on the next business day, if sent after the addressee's normal business hours. 12.5. Feedback . If you provide us with any suggestions, comments, recommendations, opinions, or other information relating to the Services or Website (\u201c Feedback \u201d), you grant us a royalty-free, non-exclusive, irrevocable, perpetual, worldwide right and license to use the Feedback on our websites or in marketing materials. We reserve the right to remove any Feedback posted on the Website if, in our opinion, such Feedback does not comply with the Agreement or applicable law. 12.6. Logo usage . You grant us the right to use your name and other indicia, such as logo or trademark in our list of current or former clients in promotional materials and on our websites. Any other announcement, statement, press release, or other publicity or marketing materials relating to your use of Services will be subject to your consent. 12.7. Export Laws . Each Party will comply with the export laws and regulations of the United States and other applicable jurisdictions in providing and using the Services. Without limiting the generality of the foregoing, Client represents that it is not named on any U.S. government denied-party list and will not make the Services available to any user or entity that is located in a country that is subject to a U.S. government embargo, or is listed on any U.S. government list of prohibited or restricted parties. 12.8. Non-waiver . Our failure to exercise or enforce any right or provision of the Agreement will not operate as a waiver of such right or provision. 12.9. Assignment . We may assign any or all of our rights and obligations to others at any time. We will notify you of any assignment. 12.0. Severability . If any term or provision of this Agreement is invalid, illegal, or unenforceable in any jurisdiction, such invalidity, illegality, or unenforceability will not affect any other term or provision of this Agreement or invalidate or render unenforceable such term or provision in any other jurisdiction. 12.11. No relationship . There is no joint venture, partnership, employment, or agency relationship created between you and us as a result of the Agreement or use of the Services.","title":"12. FINAL PROVISIONS"},{"location":"legal/archive/terms.html","text":"Terms and conditions \u00bb Effective until March 6, 2023 1. Agreement to Terms \u00bb 1.1 These Terms and Conditions constitute a legally binding agreement made between you, whether personally or on behalf of an entity ( you ), and Spacelift Inc., registered at 651 N. Broad Street, Suite 206B Middletown, County of New Castle DE 19709 United States, doing business as Spacelift ( we, us ), concerning your access to and use of the Spacelift ( https://spacelift.io ) website as well as any related applications (the Site). The Site provides the following services: a specialized, Terraform-compatible continuous integration and deployment (CI/CD) platform for infra-as-code (Services). You agree that by accessing the Site and/or Services, you have read, understood, and agree to be bound by all of these Terms and Conditions. If you do not agree with all of these Terms and Conditions, then you are prohibited from using the Site and Services and you must discontinue use immediately. We recommend that you print a copy of these Terms and Conditions for future reference. 1.2 The supplemental policies set out in Section 1.7 below, as well as any supplemental terms and conditions or documents that may be posted on the Site from time to time, are expressly incorporated by reference. 1.3 We may make changes to these Terms and Conditions at any time. The updated version of these Terms and Conditions will be indicated by an updated \u201cRevised\u201d date and the updated version will be effective as soon as it is accessible. You are responsible for reviewing these Terms and Conditions to stay informed of updates. Your continued use of the Site represents that you have accepted such changes. 1.4 We may update or change the Site from time to time to reflect changes to our products, our users' needs and/or our business priorities. 1.5 The information provided on the Site is not intended for distribution to or use by any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject us to any registration requirement within such jurisdiction or country. 1.6 The Site is intended for users who are at least 18 years old. If you are under the age of 18, you are not permitted to register for the Site or use the Services without parental permission. 1.7 Subscription The Services may include automatically recurring payments for periodic charges (\"Subscription Service\"). If you activate a Subscription Service, you authorize Spacelift to periodically charge, on a going-forward basis and until cancellation of either the recurring payments or your account, all accrued sums on or before the payment due date for the accrued sums. The \"Subscription Billing Date\" is the date when you purchase your first subscription to the Service. Your account will be charged automatically on the Subscription Billing Date all applicable fees for the next subscription period. The subscription will continue unless and until you cancel your subscription or we terminate it. You must cancel your subscription before it renews in order to avoid billing of the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide to us during registration (or to a different payment method if you change your payment information). You may cancel the Subscription Service by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io . In Subscription Service: Seat means each user who actively logged in to the Site in the last month. Private worker is a single worker installed on-premise that allows you to execute Spacelift workflows on your end. You can read more about it here . 1.8 Additional policies which also apply to your use of the Site include: Our Privacy Notice , which sets out the terms on which we process any personal data we collect from you, or that you provide to us. By using the Site, you consent to such processing and you warrant that all data provided by you is accurate. Our Cookie Policy , which sets out information about the cookies on the Site. 2. Acceptable Use \u00bb 2.1 You may not access or use the Site for any purpose other than that for which we make the site and our services available. The Site may not be used in connection with any commercial endeavors except those that are specifically endorsed or approved by us. 2.2 As a user of this Site, you agree not to: Systematically retrieve data or other content from the Site to a compile database or directory without written permission from us; Make any unauthorized use of the Site, including collecting usernames and/or email addresses of users to send unsolicited email or creating user accounts under false pretenses; Use the Site to advertise or sell goods and services; Circumvent, disable, or otherwise interfere with security-related features of the Site, including features that prevent or restrict the use or copying of any content or enforce limitations on the use; Trick, defraud, or mislead us and other users, especially in any attempt to learn sensitive account information such as user passwords; Make improper use of our support services, or submit false reports of abuse or misconduct; Interfere with, disrupt, or create an undue burden on the Site or the networks and services connected to the Site; Attempt to impersonate another user or person, or use the username of another user; Sell or otherwise transfer your profile; Use any information obtained from the Site in order to harass, abuse, or harm another person; Harass, annoy, intimidate, or threaten any of our employees, agents, or other users; Delete the copyright or other proprietary rights notice from any of the content; Upload or transmit (or attempt to upload or to transmit) viruses, Trojan horses, or other material that interferes with any party\u2019s uninterrupted use and enjoyment of the Site, or any material that acts as a passive or active information collection or transmission mechanism; Use the Site in a manner inconsistent with any applicable laws or regulations; Falsely imply a relationship with us or another company with whom you do not have a relationship; 3. Information you provide to us \u00bb 3.1 You represent and warrant that: (a) all registration information you submit will be true, accurate, current, and complete and relate to you and not a third party; (b) you will maintain the accuracy of such information and promptly update such information as necessary; (c) you will keep your password confidential and will be responsible for all use of your password and account; (d) you have the legal capacity and you agree to comply with these Terms and Conditions; and (e) you are not a minor in the jurisdiction in which you reside, or if a minor, you have received parental permission to use the Site. If you know or suspect that anyone other than you knows your user information (such as an identification code or user name) and/or password you must promptly notify us at contact@spacelift.io . 3.2 If you provide any information that is untrue, inaccurate, not current or incomplete, we may suspend or terminate your account. We may remove or change a user name you select if we determine that such user name is inappropriate. 3.3 As part of the functionality of the Site, you may link your account with online accounts you may have with third party service providers (each such account, a Third Party Account) by either: (a) providing your Third Party Account login information through the Site; or (b) allowing us to access your Third Party Account, as is permitted under the applicable terms and conditions that govern your use of each Third Party Account. You represent that you are entitled to disclose your Third Party Account login information to us and/or grant us access to your Third Party Account without breach by you of any of the terms and conditions that govern your use of the applicable Third Party Account and without obligating us to pay any fees or making us subject to any usage limitations imposed by such third party service providers. 3.4 By granting us access to any Third Party Accounts, you understand that (a) we may access, make available and store (if applicable) any content that you have provided to and stored in your Third Party Account (the \u201cSocial Network Content\u201d) so that it is available on and through the Site via your account; and (b) we may submit and receive additional information to your Third Party Account to the extent you are notified when you link your account with the Third Party Account. Depending on the Third Party Accounts you choose and subject to the privacy settings that you have set in such Third Party Accounts, personally identifiable information that you post to your Third Party Accounts may be available on and through your account on the Site. Please note that if a Third Party Account or associated service becomes unavailable or our access to such Third Party Account is terminated by the third party service provider, then Social Network Content may no longer be available on and through the Site. You will have the ability to disable the connection between your account on the Site and your Third Party Accounts at any time. Please note that your relationship with the third party service providers associated with your third party accounts is governed solely by your agreement(s) with such third party service providers. We make no effort to review any Social Network Content for any purpose, including but not limited to, for accuracy, legality or non-infringement, and we are not responsible for any Social Network Content. 4. Content you provide to us \u00bb 4.1 There may be opportunities for you to post content to the Site or send feedback to us (User Content). You understand and agree that your User Content may be viewed by other users on the Site, and that they may be able to see who has posted that User Content. 4.2 In posting User Content, including reviews or making contact with other users of the Site you shall comply with our Acceptable Use Policy. 4.3 You warrant that any User Content does comply with our Acceptable Use Policy, and you will be liable to us and indemnify us for any breach of that warranty. This means you will be responsible for any loss or damage we suffer as a result of your breach of this warranty. 4.4 We have the right to remove any User Content you put on the Site if, in our opinion, such User Content does not comply with the Acceptable Use Policy. 4.5 We are not responsible and accept no liability for any User Content including any such content that contains incorrect information or is defamatory or loss of User Content. We accept no obligation to screen, edit or monitor any User Content but we reserve the right to remove, screen and/or edit any User Content without notice and at any time. User Content has not been verified or approved by us and the views expressed by other users on the Site do not represent our views or values. 4.6 If you wish to complain about User Content uploaded by other users please contact us at contact@spacelift.io . 5. Our content \u00bb 5.1 Unless otherwise indicated, the Site and Services including source code, databases, functionality, software, website designs, audio, video, text, photographs, and graphics on the Site (Our Content) are owned or licensed to us, and are protected by copyright and trade mark laws. 5.2 Except as expressly provided in these Terms and Conditions, no part of the Site, Services or Our Content may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission. 5.3 Provided that you are eligible to use the Site, you are granted a limited licence to access and use the Site and Our Content and to download or print a copy of any portion of the Content to which you have properly gained access solely for your personal, non-commercial use. 5.4 You shall not (a) try to gain unauthorised access to the Site or any networks, servers or computer systems connected to the Site; and/or (b) make for any purpose including error correction, any modifications, adaptations, additions or enhancements to the Site or Our Content, including the modification of the paper or digital copies you may have downloaded. 5.5 We shall (a) prepare the Site and Our Content with reasonable skill and care; and (b) use industry standard virus detection software to try to block the uploading of content to the Site that contains viruses. 5.6 The content on the Site is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Site. 5.7 Although we make reasonable efforts to update the information on our site, we make no representations, warranties or guarantees, whether express or implied, that Our Content on the Site is accurate, complete or up to date. 6. Link to third party content \u00bb 6.1 The Site may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third party websites or applications or the third party operator. We are not responsible for and do not endorse any third party websites or applications or their availability or content. 6.2 We accept no responsibility for adverts contained within the Site. If you agree to purchase goods and/or services from any third party who advertises in the Site, you do so at your own risk. The advertiser, and not us, is responsible for such goods and/or services and if you have any questions or complaints in relation to them, you should contact the advertiser. 7. Site Management \u00bb 7.1 We reserve the right at our sole discretion, to (1) monitor the Site for breaches of these Terms and Conditions; (2) take appropriate legal action against anyone in breach of applicable laws or these Terms and Conditions; (3) refuse, restrict access to or availability of, or disable (to the extent technologically feasible) any of your Contributions; (4) remove from the Site or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (5) otherwise manage the Site in a manner designed to protect our rights and property and to facilitate the proper functioning of the Site and Services. 7.2 We do not guarantee that the Site will be secure or free from bugs or viruses. 7.3 You are responsible for configuring your information technology, computer programs and platform to access the Site and you should use your own virus protection software. 8. Modifications to and availability of the Site \u00bb 8.1 We reserve the right to change, modify, or remove the contents of the Site at any time or for any reason at our sole discretion without notice. We also reserve the right to modify or discontinue all or part of the Services without notice at any time. 8.2 We cannot guarantee the Site and Services will be available at all times. We may experience hardware, software, or other problems or need to perform maintenance related to the Site, resulting in interruptions, delays, or errors. You agree that we have no liability whatsoever for any loss, damage, or inconvenience caused by your inability to access or use the Site or Services during any downtime or discontinuance of the Site or Services. We are not obliged to maintain and support the Site or Services or to supply any corrections, updates, or releases. 8.3 There may be information on the Site that contains typographical errors, inaccuracies, or omissions that may relate to the Services, including descriptions, pricing, availability, and various other information. We reserve the right to correct any errors, inaccuracies, or omissions and to change or update the information at any time, without prior notice. 9. Disclaimer/Limitation of Liability \u00bb 9.1 The Site and Services are provided on an as-is and as-available basis. You agree that your use of the Site and/or Services will be at your sole risk except as expressly set out in these Terms and Conditions. All warranties, terms, conditions and undertakings, express or implied (including by statute, custom or usage, a course of dealing, or common law) in connection with the Site and Services and your use thereof including, without limitation, the implied warranties of satisfactory quality, fitness for a particular purpose and non-infringement are excluded to the fullest extent permitted by applicable law. We make no warranties or representations about the accuracy or completeness of the Site\u2019s content and are not liable for any (1) errors or omissions in content: (2) any unauthorized access to or use of our servers and/or any and all personal information and/or financial information stored on our server; (3) any interruption or cessation of transmission to or from the site or services; and/or (4) any bugs, viruses, trojan horses, or the like which may be transmitted to or through the site by any third party. We will not be responsible for any delay or failure to comply with our obligations under these Terms and Conditions if such delay or failure is caused by an event beyond our reasonable control. 9.2 Our responsibility for loss or damage suffered by you: Whether you are a consumer or a business user: We do not exclude or limit in any way our liability to you where it would be unlawful to do so. This includes liability for death or personal injury caused by our negligence or the negligence of our employees, agents or subcontractors and for fraud or fraudulent misrepresentation; If we fail to comply with these Terms and Conditions, we will be responsible for loss or damage you suffer that is a foreseeable result of our breach of these Terms and Conditions, but we would not be responsible for any loss or damage that were not foreseeable at the time you started using the Site/Services; Notwithstanding anything to the contrary contained in the Disclaimer/Limitation of Liability section, our liability to you for any cause whatsoever and regardless of the form of the action, will at all times be limited to a total aggregate amount equal to the greater of (a) the sum of PLN 5000 or (b) the amount paid, if any, by you to us for the Services/Site during the six (6) month period prior to any cause of action arising. If you are a business user: We will not be liable to you for any loss or damage, whether in contract, tort (including negligence), breach of statutory duty, or otherwise, even if foreseeable, arising under or in connection with: use of, or inability to use, our Site/Services; or use of or reliance on any content displayed on our Site. In particular, we will not be liable for: loss of profits, sales, business, or revenue; business interruption; loss of anticipated savings; loss of business opportunity, goodwill or reputation; or any indirect or consequential loss or damage. If you are a consumer user: Please note that we only provide our Site for domestic and private use. You agree not to use our Site for any commercial or business purposes, and we have no liability to you for any loss of profit, loss of business, business interruption, or loss of business opportunity; If defective digital content that we have supplied, damages a device or digital content belonging to you and this is caused by our failure to use reasonable care and skill, we will either repair the damage or pay you compensation. You have legal rights in relation to goods that are faulty or not as described. Advice about your legal rights is available from your local Citizens' Advice Bureau or Trading Standards office. Nothing in these Terms and Conditions will affect these legal rights. 10. Term and Termination \u00bb 10.1 These Terms and Conditions shall remain in full force and effect while you use the Site or Services or are otherwise a user of the Site, as applicable. You may terminate your use or participation at any time, for any reason, by following the instructions for terminating user accounts, if available, or by contacting us at contact@spacelift.io . 10.2 Without limiting any other provision of these Terms and Conditions, we reserve the right to, in our sole discretion and without notice or liability, deny access to and use of the Site and the Services (including blocking certain IP addresses), to any person for any reason including without limitation for breach of any representation, warranty or covenant contained in these Terms and Conditions or of any applicable law or regulation. If we determine, in our sole discretion, that your use of the Site/Services is in breach of these Terms and Conditions or of any applicable law or regulation, we may terminate your use or participation in the Site and the Services or delete your profile and any content or information that you posted at any time, without warning, in our sole discretion. 10.3 If we terminate or suspend your account for any reason set out in this Section 9, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to terminating or suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress. 11. General \u00bb 11.1 Visiting the Site, sending us emails, and completing online forms constitute electronic communications. You consent to receive electronic communications and you agree that all agreements, notices, disclosures, and other communications we provide to you electronically, via email and on the Site, satisfy any legal requirement that such communication be in writing. You hereby agree to the use of electronic signatures, contracts, orders and other records and to electronic delivery of notices, policies and records of transactions initiated or completed by us or via the Site. You hereby waive any rights or requirements under any statutes, regulations, rules, ordinances or other laws in any jurisdiction which require an original signature or delivery or retention of non-electronic records, or to payments or the granting of credits by other than electronic means. 11.2 These Terms and Conditions and any policies or operating rules posted by us on the Site or in respect to the Services constitute the entire agreement and understanding between you and us. 11.3 Our failure to exercise or enforce any right or provision of these Terms and Conditions shall not operate as a waiver of such right or provision. 11.4 We may assign any or all of our rights and obligations to others at any time. 11.5 We shall not be responsible or liable for any loss, damage, delay or failure to act caused by any cause beyond our reasonable control. 11.6 If any provision or part of a provision of these Terms and Conditions is unlawful, void or unenforceable, that provision or part of the provision is deemed severable from these Terms and Conditions and does not affect the validity and enforceability of any remaining provisions. 11.7 There is no joint venture, partnership, employment or agency relationship created between you and us as a result of these Terms and Conditions or use of the Site or Services. 11.8 For consumers only - Please note that these Terms and Conditions, their subject matter and their formation, are governed by Polish law. You and we both agree that the courts of Poland will have exclusive jurisdiction. If you have any complaint or wish to raise a dispute under these Terms and Conditions or otherwise in relation to the Site please follow this link . 11.9 For business users only - If you are a business user, these Terms and Conditions, their subject matter and their formation (and any non-contractual disputes or claims) are governed by Polish Law. We both agree to the exclusive jurisdiction of the courts of Poland. 11.10 A person who is not a party to these Terms and Conditions shall have no right to enforce any term of these Terms and Conditions. 11.11 In order to resolve a complaint regarding the Services or to receive further information regarding use of the Services, please contact us by email at contact@spacelift.io or by post to: Spacelift Inc. 651 N. Broad Street, Suite 206B Middletown, County of New Castle DE 19709, United States","title":"Terms and conditions"},{"location":"legal/archive/terms.html#terms-and-conditions","text":"Effective until March 6, 2023","title":"Terms and conditions"},{"location":"legal/archive/terms.html#1-agreement-to-terms","text":"1.1 These Terms and Conditions constitute a legally binding agreement made between you, whether personally or on behalf of an entity ( you ), and Spacelift Inc., registered at 651 N. Broad Street, Suite 206B Middletown, County of New Castle DE 19709 United States, doing business as Spacelift ( we, us ), concerning your access to and use of the Spacelift ( https://spacelift.io ) website as well as any related applications (the Site). The Site provides the following services: a specialized, Terraform-compatible continuous integration and deployment (CI/CD) platform for infra-as-code (Services). You agree that by accessing the Site and/or Services, you have read, understood, and agree to be bound by all of these Terms and Conditions. If you do not agree with all of these Terms and Conditions, then you are prohibited from using the Site and Services and you must discontinue use immediately. We recommend that you print a copy of these Terms and Conditions for future reference. 1.2 The supplemental policies set out in Section 1.7 below, as well as any supplemental terms and conditions or documents that may be posted on the Site from time to time, are expressly incorporated by reference. 1.3 We may make changes to these Terms and Conditions at any time. The updated version of these Terms and Conditions will be indicated by an updated \u201cRevised\u201d date and the updated version will be effective as soon as it is accessible. You are responsible for reviewing these Terms and Conditions to stay informed of updates. Your continued use of the Site represents that you have accepted such changes. 1.4 We may update or change the Site from time to time to reflect changes to our products, our users' needs and/or our business priorities. 1.5 The information provided on the Site is not intended for distribution to or use by any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject us to any registration requirement within such jurisdiction or country. 1.6 The Site is intended for users who are at least 18 years old. If you are under the age of 18, you are not permitted to register for the Site or use the Services without parental permission. 1.7 Subscription The Services may include automatically recurring payments for periodic charges (\"Subscription Service\"). If you activate a Subscription Service, you authorize Spacelift to periodically charge, on a going-forward basis and until cancellation of either the recurring payments or your account, all accrued sums on or before the payment due date for the accrued sums. The \"Subscription Billing Date\" is the date when you purchase your first subscription to the Service. Your account will be charged automatically on the Subscription Billing Date all applicable fees for the next subscription period. The subscription will continue unless and until you cancel your subscription or we terminate it. You must cancel your subscription before it renews in order to avoid billing of the next periodic Subscription Fee to your account. We will bill the periodic Subscription Fee to the payment method you provide to us during registration (or to a different payment method if you change your payment information). You may cancel the Subscription Service by accessing your account settings and clicking on the \"Cancel Plan\" option or by contacting us at: contact@spacelift.io . In Subscription Service: Seat means each user who actively logged in to the Site in the last month. Private worker is a single worker installed on-premise that allows you to execute Spacelift workflows on your end. You can read more about it here . 1.8 Additional policies which also apply to your use of the Site include: Our Privacy Notice , which sets out the terms on which we process any personal data we collect from you, or that you provide to us. By using the Site, you consent to such processing and you warrant that all data provided by you is accurate. Our Cookie Policy , which sets out information about the cookies on the Site.","title":"1. Agreement to Terms"},{"location":"legal/archive/terms.html#2-acceptable-use","text":"2.1 You may not access or use the Site for any purpose other than that for which we make the site and our services available. The Site may not be used in connection with any commercial endeavors except those that are specifically endorsed or approved by us. 2.2 As a user of this Site, you agree not to: Systematically retrieve data or other content from the Site to a compile database or directory without written permission from us; Make any unauthorized use of the Site, including collecting usernames and/or email addresses of users to send unsolicited email or creating user accounts under false pretenses; Use the Site to advertise or sell goods and services; Circumvent, disable, or otherwise interfere with security-related features of the Site, including features that prevent or restrict the use or copying of any content or enforce limitations on the use; Trick, defraud, or mislead us and other users, especially in any attempt to learn sensitive account information such as user passwords; Make improper use of our support services, or submit false reports of abuse or misconduct; Interfere with, disrupt, or create an undue burden on the Site or the networks and services connected to the Site; Attempt to impersonate another user or person, or use the username of another user; Sell or otherwise transfer your profile; Use any information obtained from the Site in order to harass, abuse, or harm another person; Harass, annoy, intimidate, or threaten any of our employees, agents, or other users; Delete the copyright or other proprietary rights notice from any of the content; Upload or transmit (or attempt to upload or to transmit) viruses, Trojan horses, or other material that interferes with any party\u2019s uninterrupted use and enjoyment of the Site, or any material that acts as a passive or active information collection or transmission mechanism; Use the Site in a manner inconsistent with any applicable laws or regulations; Falsely imply a relationship with us or another company with whom you do not have a relationship;","title":"2. Acceptable Use"},{"location":"legal/archive/terms.html#3-information-you-provide-to-us","text":"3.1 You represent and warrant that: (a) all registration information you submit will be true, accurate, current, and complete and relate to you and not a third party; (b) you will maintain the accuracy of such information and promptly update such information as necessary; (c) you will keep your password confidential and will be responsible for all use of your password and account; (d) you have the legal capacity and you agree to comply with these Terms and Conditions; and (e) you are not a minor in the jurisdiction in which you reside, or if a minor, you have received parental permission to use the Site. If you know or suspect that anyone other than you knows your user information (such as an identification code or user name) and/or password you must promptly notify us at contact@spacelift.io . 3.2 If you provide any information that is untrue, inaccurate, not current or incomplete, we may suspend or terminate your account. We may remove or change a user name you select if we determine that such user name is inappropriate. 3.3 As part of the functionality of the Site, you may link your account with online accounts you may have with third party service providers (each such account, a Third Party Account) by either: (a) providing your Third Party Account login information through the Site; or (b) allowing us to access your Third Party Account, as is permitted under the applicable terms and conditions that govern your use of each Third Party Account. You represent that you are entitled to disclose your Third Party Account login information to us and/or grant us access to your Third Party Account without breach by you of any of the terms and conditions that govern your use of the applicable Third Party Account and without obligating us to pay any fees or making us subject to any usage limitations imposed by such third party service providers. 3.4 By granting us access to any Third Party Accounts, you understand that (a) we may access, make available and store (if applicable) any content that you have provided to and stored in your Third Party Account (the \u201cSocial Network Content\u201d) so that it is available on and through the Site via your account; and (b) we may submit and receive additional information to your Third Party Account to the extent you are notified when you link your account with the Third Party Account. Depending on the Third Party Accounts you choose and subject to the privacy settings that you have set in such Third Party Accounts, personally identifiable information that you post to your Third Party Accounts may be available on and through your account on the Site. Please note that if a Third Party Account or associated service becomes unavailable or our access to such Third Party Account is terminated by the third party service provider, then Social Network Content may no longer be available on and through the Site. You will have the ability to disable the connection between your account on the Site and your Third Party Accounts at any time. Please note that your relationship with the third party service providers associated with your third party accounts is governed solely by your agreement(s) with such third party service providers. We make no effort to review any Social Network Content for any purpose, including but not limited to, for accuracy, legality or non-infringement, and we are not responsible for any Social Network Content.","title":"3. Information you provide to us"},{"location":"legal/archive/terms.html#4-content-you-provide-to-us","text":"4.1 There may be opportunities for you to post content to the Site or send feedback to us (User Content). You understand and agree that your User Content may be viewed by other users on the Site, and that they may be able to see who has posted that User Content. 4.2 In posting User Content, including reviews or making contact with other users of the Site you shall comply with our Acceptable Use Policy. 4.3 You warrant that any User Content does comply with our Acceptable Use Policy, and you will be liable to us and indemnify us for any breach of that warranty. This means you will be responsible for any loss or damage we suffer as a result of your breach of this warranty. 4.4 We have the right to remove any User Content you put on the Site if, in our opinion, such User Content does not comply with the Acceptable Use Policy. 4.5 We are not responsible and accept no liability for any User Content including any such content that contains incorrect information or is defamatory or loss of User Content. We accept no obligation to screen, edit or monitor any User Content but we reserve the right to remove, screen and/or edit any User Content without notice and at any time. User Content has not been verified or approved by us and the views expressed by other users on the Site do not represent our views or values. 4.6 If you wish to complain about User Content uploaded by other users please contact us at contact@spacelift.io .","title":"4. Content you provide to us"},{"location":"legal/archive/terms.html#5-our-content","text":"5.1 Unless otherwise indicated, the Site and Services including source code, databases, functionality, software, website designs, audio, video, text, photographs, and graphics on the Site (Our Content) are owned or licensed to us, and are protected by copyright and trade mark laws. 5.2 Except as expressly provided in these Terms and Conditions, no part of the Site, Services or Our Content may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission. 5.3 Provided that you are eligible to use the Site, you are granted a limited licence to access and use the Site and Our Content and to download or print a copy of any portion of the Content to which you have properly gained access solely for your personal, non-commercial use. 5.4 You shall not (a) try to gain unauthorised access to the Site or any networks, servers or computer systems connected to the Site; and/or (b) make for any purpose including error correction, any modifications, adaptations, additions or enhancements to the Site or Our Content, including the modification of the paper or digital copies you may have downloaded. 5.5 We shall (a) prepare the Site and Our Content with reasonable skill and care; and (b) use industry standard virus detection software to try to block the uploading of content to the Site that contains viruses. 5.6 The content on the Site is provided for general information only. It is not intended to amount to advice on which you should rely. You must obtain professional or specialist advice before taking, or refraining from taking, any action on the basis of the content on the Site. 5.7 Although we make reasonable efforts to update the information on our site, we make no representations, warranties or guarantees, whether express or implied, that Our Content on the Site is accurate, complete or up to date.","title":"5. Our content"},{"location":"legal/archive/terms.html#6-link-to-third-party-content","text":"6.1 The Site may contain links to websites or applications operated by third parties. We do not have any influence or control over any such third party websites or applications or the third party operator. We are not responsible for and do not endorse any third party websites or applications or their availability or content. 6.2 We accept no responsibility for adverts contained within the Site. If you agree to purchase goods and/or services from any third party who advertises in the Site, you do so at your own risk. The advertiser, and not us, is responsible for such goods and/or services and if you have any questions or complaints in relation to them, you should contact the advertiser.","title":"6. Link to third party content"},{"location":"legal/archive/terms.html#7-site-management","text":"7.1 We reserve the right at our sole discretion, to (1) monitor the Site for breaches of these Terms and Conditions; (2) take appropriate legal action against anyone in breach of applicable laws or these Terms and Conditions; (3) refuse, restrict access to or availability of, or disable (to the extent technologically feasible) any of your Contributions; (4) remove from the Site or otherwise disable all files and content that are excessive in size or are in any way a burden to our systems; and (5) otherwise manage the Site in a manner designed to protect our rights and property and to facilitate the proper functioning of the Site and Services. 7.2 We do not guarantee that the Site will be secure or free from bugs or viruses. 7.3 You are responsible for configuring your information technology, computer programs and platform to access the Site and you should use your own virus protection software.","title":"7. Site Management"},{"location":"legal/archive/terms.html#8-modifications-to-and-availability-of-the-site","text":"8.1 We reserve the right to change, modify, or remove the contents of the Site at any time or for any reason at our sole discretion without notice. We also reserve the right to modify or discontinue all or part of the Services without notice at any time. 8.2 We cannot guarantee the Site and Services will be available at all times. We may experience hardware, software, or other problems or need to perform maintenance related to the Site, resulting in interruptions, delays, or errors. You agree that we have no liability whatsoever for any loss, damage, or inconvenience caused by your inability to access or use the Site or Services during any downtime or discontinuance of the Site or Services. We are not obliged to maintain and support the Site or Services or to supply any corrections, updates, or releases. 8.3 There may be information on the Site that contains typographical errors, inaccuracies, or omissions that may relate to the Services, including descriptions, pricing, availability, and various other information. We reserve the right to correct any errors, inaccuracies, or omissions and to change or update the information at any time, without prior notice.","title":"8. Modifications to and availability of the Site"},{"location":"legal/archive/terms.html#9-disclaimerlimitation-of-liability","text":"9.1 The Site and Services are provided on an as-is and as-available basis. You agree that your use of the Site and/or Services will be at your sole risk except as expressly set out in these Terms and Conditions. All warranties, terms, conditions and undertakings, express or implied (including by statute, custom or usage, a course of dealing, or common law) in connection with the Site and Services and your use thereof including, without limitation, the implied warranties of satisfactory quality, fitness for a particular purpose and non-infringement are excluded to the fullest extent permitted by applicable law. We make no warranties or representations about the accuracy or completeness of the Site\u2019s content and are not liable for any (1) errors or omissions in content: (2) any unauthorized access to or use of our servers and/or any and all personal information and/or financial information stored on our server; (3) any interruption or cessation of transmission to or from the site or services; and/or (4) any bugs, viruses, trojan horses, or the like which may be transmitted to or through the site by any third party. We will not be responsible for any delay or failure to comply with our obligations under these Terms and Conditions if such delay or failure is caused by an event beyond our reasonable control. 9.2 Our responsibility for loss or damage suffered by you: Whether you are a consumer or a business user: We do not exclude or limit in any way our liability to you where it would be unlawful to do so. This includes liability for death or personal injury caused by our negligence or the negligence of our employees, agents or subcontractors and for fraud or fraudulent misrepresentation; If we fail to comply with these Terms and Conditions, we will be responsible for loss or damage you suffer that is a foreseeable result of our breach of these Terms and Conditions, but we would not be responsible for any loss or damage that were not foreseeable at the time you started using the Site/Services; Notwithstanding anything to the contrary contained in the Disclaimer/Limitation of Liability section, our liability to you for any cause whatsoever and regardless of the form of the action, will at all times be limited to a total aggregate amount equal to the greater of (a) the sum of PLN 5000 or (b) the amount paid, if any, by you to us for the Services/Site during the six (6) month period prior to any cause of action arising. If you are a business user: We will not be liable to you for any loss or damage, whether in contract, tort (including negligence), breach of statutory duty, or otherwise, even if foreseeable, arising under or in connection with: use of, or inability to use, our Site/Services; or use of or reliance on any content displayed on our Site. In particular, we will not be liable for: loss of profits, sales, business, or revenue; business interruption; loss of anticipated savings; loss of business opportunity, goodwill or reputation; or any indirect or consequential loss or damage. If you are a consumer user: Please note that we only provide our Site for domestic and private use. You agree not to use our Site for any commercial or business purposes, and we have no liability to you for any loss of profit, loss of business, business interruption, or loss of business opportunity; If defective digital content that we have supplied, damages a device or digital content belonging to you and this is caused by our failure to use reasonable care and skill, we will either repair the damage or pay you compensation. You have legal rights in relation to goods that are faulty or not as described. Advice about your legal rights is available from your local Citizens' Advice Bureau or Trading Standards office. Nothing in these Terms and Conditions will affect these legal rights.","title":"9. Disclaimer/Limitation of Liability"},{"location":"legal/archive/terms.html#10-term-and-termination","text":"10.1 These Terms and Conditions shall remain in full force and effect while you use the Site or Services or are otherwise a user of the Site, as applicable. You may terminate your use or participation at any time, for any reason, by following the instructions for terminating user accounts, if available, or by contacting us at contact@spacelift.io . 10.2 Without limiting any other provision of these Terms and Conditions, we reserve the right to, in our sole discretion and without notice or liability, deny access to and use of the Site and the Services (including blocking certain IP addresses), to any person for any reason including without limitation for breach of any representation, warranty or covenant contained in these Terms and Conditions or of any applicable law or regulation. If we determine, in our sole discretion, that your use of the Site/Services is in breach of these Terms and Conditions or of any applicable law or regulation, we may terminate your use or participation in the Site and the Services or delete your profile and any content or information that you posted at any time, without warning, in our sole discretion. 10.3 If we terminate or suspend your account for any reason set out in this Section 9, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to terminating or suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.","title":"10. Term and Termination"},{"location":"legal/archive/terms.html#11-general","text":"11.1 Visiting the Site, sending us emails, and completing online forms constitute electronic communications. You consent to receive electronic communications and you agree that all agreements, notices, disclosures, and other communications we provide to you electronically, via email and on the Site, satisfy any legal requirement that such communication be in writing. You hereby agree to the use of electronic signatures, contracts, orders and other records and to electronic delivery of notices, policies and records of transactions initiated or completed by us or via the Site. You hereby waive any rights or requirements under any statutes, regulations, rules, ordinances or other laws in any jurisdiction which require an original signature or delivery or retention of non-electronic records, or to payments or the granting of credits by other than electronic means. 11.2 These Terms and Conditions and any policies or operating rules posted by us on the Site or in respect to the Services constitute the entire agreement and understanding between you and us. 11.3 Our failure to exercise or enforce any right or provision of these Terms and Conditions shall not operate as a waiver of such right or provision. 11.4 We may assign any or all of our rights and obligations to others at any time. 11.5 We shall not be responsible or liable for any loss, damage, delay or failure to act caused by any cause beyond our reasonable control. 11.6 If any provision or part of a provision of these Terms and Conditions is unlawful, void or unenforceable, that provision or part of the provision is deemed severable from these Terms and Conditions and does not affect the validity and enforceability of any remaining provisions. 11.7 There is no joint venture, partnership, employment or agency relationship created between you and us as a result of these Terms and Conditions or use of the Site or Services. 11.8 For consumers only - Please note that these Terms and Conditions, their subject matter and their formation, are governed by Polish law. You and we both agree that the courts of Poland will have exclusive jurisdiction. If you have any complaint or wish to raise a dispute under these Terms and Conditions or otherwise in relation to the Site please follow this link . 11.9 For business users only - If you are a business user, these Terms and Conditions, their subject matter and their formation (and any non-contractual disputes or claims) are governed by Polish Law. We both agree to the exclusive jurisdiction of the courts of Poland. 11.10 A person who is not a party to these Terms and Conditions shall have no right to enforce any term of these Terms and Conditions. 11.11 In order to resolve a complaint regarding the Services or to receive further information regarding use of the Services, please contact us by email at contact@spacelift.io or by post to: Spacelift Inc. 651 N. Broad Street, Suite 206B Middletown, County of New Castle DE 19709, United States","title":"11. General"},{"location":"product/disaster-continuity.html","text":"Disaster Continuity \u00bb Preparation \u00bb The following preparation items are recommended to be followed to ensure that you are able to continue with your infrastructure as code deployments in the event of a Spacelift outage. State Management \u00bb Here is a list of best practices that should be followed in regards to managing the state of your infrastructure. Store your state externally (e.g. Amazon S3 Bucket) Enable versioning on your state file to keep a record of changes Replicate your state file across regions Enable MFA on deletion to prevent accidental loss of your state file Deployment Roles \u00bb Within your Spacelift configuration, each Spacelift stack utilizes a given Role for deployment purposes. We will be referring to this Role as the deployment role . In the event of a disaster, Spacelift will presumably not be accessible or usable. You should ensure that you have appropriate access to your deployment role, to provide yourself the ability to assume it for deployment purposes, or have plans to use another role for deployment purposes. Keep a record of all Roles used by your Spacelift Stacks that are used for deployment purposes Ensure that you've done one of the following: Provided yourself access to the deployment role that Spacelift is using Have a plan to create, or have already created a break-glass role that you can use for disaster purposes Terraform Break Glass Example Procedure \u00bb Pre-requisites \u00bb Access to assume your deployment role(s) Terraform installed locally Managing your state externally (not Spacelift-managed state) Assume deployment role locally \u00bb Using your favorite cloud provider, generate temporary credentials for your deployment role. With Amazon Web Services for example, this would be done using the following command: 1 2 3 aws sts assume-role \\ --role-arn <your-deployment-role-arn> \\ --role-session-name local-infra-deployment Using the output from the assume-role command, set your credentials in your shell. 1 2 3 export AWS_ACCESS_KEY_ID = <value for access key id> export AWS_SECRET_ACCESS_KEY = <value for secret access key> export AWS_SESSION_TOKEN = <value for session toknen> Run deployment commands as required \u00bb Initialize your code locally: 1 terraform init To preview changes to be deployed: 1 terraform plan To deploy changes: 1 terraform apply","title":"Disaster Continuity"},{"location":"product/disaster-continuity.html#disaster-continuity","text":"","title":"Disaster Continuity"},{"location":"product/disaster-continuity.html#preparation","text":"The following preparation items are recommended to be followed to ensure that you are able to continue with your infrastructure as code deployments in the event of a Spacelift outage.","title":"Preparation"},{"location":"product/disaster-continuity.html#state-management","text":"Here is a list of best practices that should be followed in regards to managing the state of your infrastructure. Store your state externally (e.g. Amazon S3 Bucket) Enable versioning on your state file to keep a record of changes Replicate your state file across regions Enable MFA on deletion to prevent accidental loss of your state file","title":"State Management"},{"location":"product/disaster-continuity.html#deployment-roles","text":"Within your Spacelift configuration, each Spacelift stack utilizes a given Role for deployment purposes. We will be referring to this Role as the deployment role . In the event of a disaster, Spacelift will presumably not be accessible or usable. You should ensure that you have appropriate access to your deployment role, to provide yourself the ability to assume it for deployment purposes, or have plans to use another role for deployment purposes. Keep a record of all Roles used by your Spacelift Stacks that are used for deployment purposes Ensure that you've done one of the following: Provided yourself access to the deployment role that Spacelift is using Have a plan to create, or have already created a break-glass role that you can use for disaster purposes","title":"Deployment Roles"},{"location":"product/disaster-continuity.html#terraform-break-glass-example-procedure","text":"","title":"Terraform Break Glass Example Procedure"},{"location":"product/disaster-continuity.html#pre-requisites","text":"Access to assume your deployment role(s) Terraform installed locally Managing your state externally (not Spacelift-managed state)","title":"Pre-requisites"},{"location":"product/disaster-continuity.html#assume-deployment-role-locally","text":"Using your favorite cloud provider, generate temporary credentials for your deployment role. With Amazon Web Services for example, this would be done using the following command: 1 2 3 aws sts assume-role \\ --role-arn <your-deployment-role-arn> \\ --role-session-name local-infra-deployment Using the output from the assume-role command, set your credentials in your shell. 1 2 3 export AWS_ACCESS_KEY_ID = <value for access key id> export AWS_SECRET_ACCESS_KEY = <value for secret access key> export AWS_SESSION_TOKEN = <value for session toknen>","title":"Assume deployment role locally"},{"location":"product/disaster-continuity.html#run-deployment-commands-as-required","text":"Initialize your code locally: 1 terraform init To preview changes to be deployed: 1 terraform plan To deploy changes: 1 terraform apply","title":"Run deployment commands as required"},{"location":"product/migrating-to-spacelift.html","text":"Migrating to Spacelift \u00bb Migrating from one Infrastructure as Code CI/CD provider to another can feel daunting. This is why we created a migration kit that takes care of the heavy lifting. Edit a few settings, and let it do the hard work. Then review, and possibly tweak, the generated code, and finally have your Spacelift entities created. There is no one-size-fits-all for this kind of migration. This is why we designed this tool to be flexible and easy to hack to meet your specific needs. Feel free to reach out to our support team if you need any help or guidance. Overview \u00bb The migration process is as follows: Export the definition for your resources at your current vendor. Generate the Terraform code to recreate similar resources at Spacelift using the Terraform provider . Review and possibly edit the generated Terraform code. Commit the Terraform code to a repository. Create a manager Spacelift stack that points to the repository with the Terraform code. Tip Currently, only Terraform Cloud and Terraform Enterprise are supported as sources. The instructions below apply to both. Prerequisites \u00bb Terraform Instructions \u00bb Preparation \u00bb Clone the Spacelift Migration Kit repository locally . Use the terraform login spacelift.io command to ensure that Terraform can interact with your Spacelift account. Depending on the exporter used, you may need additional steps: Terraform Cloud/Enterprise : Use the terraform login command to ensure that Terraform can interact with your Terraform Cloud/Enterprise account. Pre-Migration Cleanup \u00bb In order to start fresh, clean up files and folders from previous runs. 1 rm -rf ./out ./ { exporters/tfc,generator,manager-stack } /.terraform ./ { exporters/tfc,generator,manager-stack } /.terraform.lock.hcl ./ { exporters/tfc,generator,manager-stack } /terraform.tfstate ./ { exporters/tfc,generator,manager-stack } /terraform.tfstate.backup Export the resource definitions and Terraform state \u00bb Choose an exporter and copy the example .tfvars file for it into exporter.tfvars . Edit that file to match your context. Run the following commands: 1 2 3 cd exporters/<EXPORTER> terraform init terraform apply -auto-approve -var-file = ../../exporter.tfvars A new out folder should have been created. The data.json files contains the mapping of your vendor resources to the equivalent Spacelift resources, and the state-files folder contains the files for the Terraform state of your stacks, if the state export was enabled. Please note that once exported the Terraform state files can be imported into Spacelift or to any backend supported by Terraform. Generate the Terraform code \u00bb If you want to customize the template that generates the Terraform code, run cp ../../generator/generator.tftpl ../generator.tftpl , and edit the generator.tftpl file at the root of the repository. If present, it will be used automatically. Run the following commands: 1 2 3 cd ../../generator terraform init terraform apply -auto-approve -var-file = ../out/data.json Review and edit the generated Terraform code \u00bb A main.tf should have been generated in the out folder. It contains all the Terraform code for your Spacelift resources. Mapping resources from a vendor to Spacelift resources is not an exact science. There are gaps in functionality and caveats in the mapping process. Please carefully review the generated Terraform code and make sure that it looks fine. If it does not, repeat the process with a different configuration or edit the Terraform code. Commit the Terraform code \u00bb When the Terraform code is ready, commit it to a repository. Create a manager Spacelift stack \u00bb It is now time to create a Spacelift stack that will point to the commited Terraform code that manages your Spacelift resources. Copy the example manager-stack.example.tfvars file into manager-stack.tfvars . Edit that file to match your context. Run the following commands: 1 2 3 cd ../manager-stack terraform init terraform apply -auto-approve -var-file = ../manager-stack.tfvars After the stack has been created, a tracked run will be triggered automatically. That run will create the defined Spacelift resources. Post-Migration Cleanup \u00bb Before you can use Spacelift to manage your infrastructure, you may need to make changes to the Terraform code for your infrastructure, depending on the Terraform state is managed. If the Terraform state is managed by Spacelift,perform the following actions, otherwise you can skip this section: Remove any backend / cloud block from the Terraform code that manages your infrastructure to avoid a conflict with Spacelift's backend. Delete the import_state_file arguments from the Terraform code that manages your Spacelift resources. After the manager stack has successfully run, the mounted Terraform state files are not needed anymore and can be deleted by setting the import_state argument to false in the manager-stack.tfvars file and run terraform apply -auto-approve -var-file=../manager-stack.tfvars in the manager-stack folder. Sources \u00bb Terraform Cloud/Enterprise \u00bb Known Limitations \u00bb The limitations listed below come from the original provider. We are actively looking for workarounds. The variable sets are not exposed so they cannot be listed and exported. The name of the Version Control System (VCS) provider for a stack is not returned so it has to be set in the exporter configuration file. When the branch for the stack is the repository default branch, the value is empty. You can set the value for the default branch in the exporter configuration file, or edit the generated Terraform code. Glossary \u00bb Terraform Cloud/Enterprise Spacelift Agent Pool Worker Pool Organization Account Policy Policy Project Space Variable Environment Variable Variable Set Context Workspace Stack","title":"Migrating to Spacelift"},{"location":"product/migrating-to-spacelift.html#migrating-to-spacelift","text":"Migrating from one Infrastructure as Code CI/CD provider to another can feel daunting. This is why we created a migration kit that takes care of the heavy lifting. Edit a few settings, and let it do the hard work. Then review, and possibly tweak, the generated code, and finally have your Spacelift entities created. There is no one-size-fits-all for this kind of migration. This is why we designed this tool to be flexible and easy to hack to meet your specific needs. Feel free to reach out to our support team if you need any help or guidance.","title":"Migrating to Spacelift"},{"location":"product/migrating-to-spacelift.html#overview","text":"The migration process is as follows: Export the definition for your resources at your current vendor. Generate the Terraform code to recreate similar resources at Spacelift using the Terraform provider . Review and possibly edit the generated Terraform code. Commit the Terraform code to a repository. Create a manager Spacelift stack that points to the repository with the Terraform code. Tip Currently, only Terraform Cloud and Terraform Enterprise are supported as sources. The instructions below apply to both.","title":"Overview"},{"location":"product/migrating-to-spacelift.html#prerequisites","text":"Terraform","title":"Prerequisites"},{"location":"product/migrating-to-spacelift.html#instructions","text":"","title":"Instructions"},{"location":"product/migrating-to-spacelift.html#preparation","text":"Clone the Spacelift Migration Kit repository locally . Use the terraform login spacelift.io command to ensure that Terraform can interact with your Spacelift account. Depending on the exporter used, you may need additional steps: Terraform Cloud/Enterprise : Use the terraform login command to ensure that Terraform can interact with your Terraform Cloud/Enterprise account.","title":"Preparation"},{"location":"product/migrating-to-spacelift.html#pre-migration-cleanup","text":"In order to start fresh, clean up files and folders from previous runs. 1 rm -rf ./out ./ { exporters/tfc,generator,manager-stack } /.terraform ./ { exporters/tfc,generator,manager-stack } /.terraform.lock.hcl ./ { exporters/tfc,generator,manager-stack } /terraform.tfstate ./ { exporters/tfc,generator,manager-stack } /terraform.tfstate.backup","title":"Pre-Migration Cleanup"},{"location":"product/migrating-to-spacelift.html#export-the-resource-definitions-and-terraform-state","text":"Choose an exporter and copy the example .tfvars file for it into exporter.tfvars . Edit that file to match your context. Run the following commands: 1 2 3 cd exporters/<EXPORTER> terraform init terraform apply -auto-approve -var-file = ../../exporter.tfvars A new out folder should have been created. The data.json files contains the mapping of your vendor resources to the equivalent Spacelift resources, and the state-files folder contains the files for the Terraform state of your stacks, if the state export was enabled. Please note that once exported the Terraform state files can be imported into Spacelift or to any backend supported by Terraform.","title":"Export the resource definitions and Terraform state"},{"location":"product/migrating-to-spacelift.html#generate-the-terraform-code","text":"If you want to customize the template that generates the Terraform code, run cp ../../generator/generator.tftpl ../generator.tftpl , and edit the generator.tftpl file at the root of the repository. If present, it will be used automatically. Run the following commands: 1 2 3 cd ../../generator terraform init terraform apply -auto-approve -var-file = ../out/data.json","title":"Generate the Terraform code"},{"location":"product/migrating-to-spacelift.html#review-and-edit-the-generated-terraform-code","text":"A main.tf should have been generated in the out folder. It contains all the Terraform code for your Spacelift resources. Mapping resources from a vendor to Spacelift resources is not an exact science. There are gaps in functionality and caveats in the mapping process. Please carefully review the generated Terraform code and make sure that it looks fine. If it does not, repeat the process with a different configuration or edit the Terraform code.","title":"Review and edit the generated Terraform code"},{"location":"product/migrating-to-spacelift.html#commit-the-terraform-code","text":"When the Terraform code is ready, commit it to a repository.","title":"Commit the Terraform code"},{"location":"product/migrating-to-spacelift.html#create-a-manager-spacelift-stack","text":"It is now time to create a Spacelift stack that will point to the commited Terraform code that manages your Spacelift resources. Copy the example manager-stack.example.tfvars file into manager-stack.tfvars . Edit that file to match your context. Run the following commands: 1 2 3 cd ../manager-stack terraform init terraform apply -auto-approve -var-file = ../manager-stack.tfvars After the stack has been created, a tracked run will be triggered automatically. That run will create the defined Spacelift resources.","title":"Create a manager Spacelift stack"},{"location":"product/migrating-to-spacelift.html#post-migration-cleanup","text":"Before you can use Spacelift to manage your infrastructure, you may need to make changes to the Terraform code for your infrastructure, depending on the Terraform state is managed. If the Terraform state is managed by Spacelift,perform the following actions, otherwise you can skip this section: Remove any backend / cloud block from the Terraform code that manages your infrastructure to avoid a conflict with Spacelift's backend. Delete the import_state_file arguments from the Terraform code that manages your Spacelift resources. After the manager stack has successfully run, the mounted Terraform state files are not needed anymore and can be deleted by setting the import_state argument to false in the manager-stack.tfvars file and run terraform apply -auto-approve -var-file=../manager-stack.tfvars in the manager-stack folder.","title":"Post-Migration Cleanup"},{"location":"product/migrating-to-spacelift.html#sources","text":"","title":"Sources"},{"location":"product/migrating-to-spacelift.html#terraform-cloudenterprise","text":"","title":"Terraform Cloud/Enterprise"},{"location":"product/migrating-to-spacelift.html#known-limitations","text":"The limitations listed below come from the original provider. We are actively looking for workarounds. The variable sets are not exposed so they cannot be listed and exported. The name of the Version Control System (VCS) provider for a stack is not returned so it has to be set in the exporter configuration file. When the branch for the stack is the repository default branch, the value is empty. You can set the value for the default branch in the exporter configuration file, or edit the generated Terraform code.","title":"Known Limitations"},{"location":"product/migrating-to-spacelift.html#glossary","text":"Terraform Cloud/Enterprise Spacelift Agent Pool Worker Pool Organization Account Policy Policy Project Space Variable Environment Variable Variable Set Context Workspace Stack","title":"Glossary"},{"location":"product/notifications.html","text":"Notifications \u00bb As nicely stated by Murphy's law : \"Anything that can go wrong will go wrong.\". Some issues will blow in your face and be obvious, and others will be sneakier. In the background, Spacelift interacts with different systems (VCS providers, cloud providers, Slack, etc.), which can fail in various ways. The Notification Inbox section gives you visibility into issues arising from those interactions. Visibility \u00bb Notifications are only available to admins. They can be checked either at the account level, which includes all the stacks. Or, they can be checked for a specific stack. Available Categories \u00bb Currently, the Notifications only include VCS provider issues, but more categories will be added soon. Retention \u00bb Notifications are kept for 14 days.","title":"Notifications"},{"location":"product/notifications.html#notifications","text":"As nicely stated by Murphy's law : \"Anything that can go wrong will go wrong.\". Some issues will blow in your face and be obvious, and others will be sneakier. In the background, Spacelift interacts with different systems (VCS providers, cloud providers, Slack, etc.), which can fail in various ways. The Notification Inbox section gives you visibility into issues arising from those interactions.","title":"Notifications"},{"location":"product/notifications.html#visibility","text":"Notifications are only available to admins. They can be checked either at the account level, which includes all the stacks. Or, they can be checked for a specific stack.","title":"Visibility"},{"location":"product/notifications.html#available-categories","text":"Currently, the Notifications only include VCS provider issues, but more categories will be added soon.","title":"Available Categories"},{"location":"product/notifications.html#retention","text":"Notifications are kept for 14 days.","title":"Retention"},{"location":"product/security.html","text":"Security \u00bb At Spacelift, your security is our first and foremost priority. We're aware of the utmost importance of security in our service and we're grateful for your trust. Here's what we're doing to earn and maintain this trust, and to keep Spacelift secure by design. Certifications \u00bb SOC2 Type II Certified Certification performed by an independent external auditor, who confirms the effectiveness of internal controls in terms of Spacelift Security: Confidentiality, Integrity, Availability, and Privacy of customer data. Security Audits \u00bb Spacelift regularly engages with external security firms to perform audits and penetration testing at least once per year. Additionally, the Spacelift Security Team conducts internal security audits regularly in combination with automated security tooling. Encryption \u00bb All of our data is encrypted at rest and in transit. With the exception of intra-VPC traffic between the web server and the load balancer protected by a restrictive AWS security group , all other traffic is handled using secure transport protocols. All the data sources (Amazon S3, database, Amazon SNS topics and Amazon SQS queues) are encrypted at rest using AWS KMS keys with restricted and audited access. Customer secrets are extra encrypted at rest in a way that should withstand even an internal attacker. Security Features \u00bb Single Sign-On (SSO) \u00bb In addition to the default login providers (currently GitHub, GitLab, and Google), Spacelift also supports the ability to configure Single Sign-On (SSO) via SAML or OIDC using your favorite identity provider. Using SSO, Spacelift can be configured in a password-less approach, helping your company follow a zero-trust approach. As long as your Identity Provider supports SAML or OIDC, and passing the email scope, you're good to go! You can learn more about our Single Sign-On support here . Environment Variables \u00bb Spacelift allows for granular control of environment variables on your Stacks either by setting environment variables on a per-stack basis, or creating collections of variables as a Context . These environment variables can be created in two types: plain or secret . Policies \u00bb Spacelift policies provide a way to express rules as code to manage your infrastructure as a code environment. Users can build policies to control Spacelift login permissions, access controls, deployment workflows, and even govern the infrastructure itself to be deployed. Policies are based on the Open Policy Agent project and can be defined using its rule language Rego . You can learn more about policies here . Responsible disclosure \u00bb If you discover a vulnerability, we would like to know about it so we can take steps to address it as quickly as possible. We would like to ask you to help us better protect our clients and our systems. Please do the following: email your findings to security@spacelift.io ; do not take advantage of the vulnerability or problem you have discovered, for example by downloading more data than necessary to demonstrate the vulnerability or deleting or modifying other people's data; do not reveal the problem to others until it has been resolved; do not use attacks on physical security, social engineering, distributed denial of service, spam or applications of third parties, and; do provide sufficient information to reproduce the problem, so we will be able to resolve it as quickly as possible; What we promise: we will respond to your report within 3 business days with our evaluation of the report and an expected resolution date; if you have followed the instructions above, we will not take any legal action against you in regard to the report; we will handle your report with strict confidentiality, and not pass on your personal details to third parties without your permission; we will keep you informed of the progress towards resolving the problem; in the public information concerning the problem reported, we will give your name as the discoverer of the problem (unless you desire otherwise), and; as a token of our gratitude for your assistance, we offer a reward for every report of a security problem that was not yet known to us. The amount of the reward will be determined based on the severity of the leak and the quality of the report; We strive to resolve all problems as quickly as possible, and we would like to play an active role in the ultimate publication on the problem after it is resolved.","title":"Security"},{"location":"product/security.html#security","text":"At Spacelift, your security is our first and foremost priority. We're aware of the utmost importance of security in our service and we're grateful for your trust. Here's what we're doing to earn and maintain this trust, and to keep Spacelift secure by design.","title":"Security"},{"location":"product/security.html#certifications","text":"SOC2 Type II Certified Certification performed by an independent external auditor, who confirms the effectiveness of internal controls in terms of Spacelift Security: Confidentiality, Integrity, Availability, and Privacy of customer data.","title":"Certifications"},{"location":"product/security.html#security-audits","text":"Spacelift regularly engages with external security firms to perform audits and penetration testing at least once per year. Additionally, the Spacelift Security Team conducts internal security audits regularly in combination with automated security tooling.","title":"Security Audits"},{"location":"product/security.html#encryption","text":"All of our data is encrypted at rest and in transit. With the exception of intra-VPC traffic between the web server and the load balancer protected by a restrictive AWS security group , all other traffic is handled using secure transport protocols. All the data sources (Amazon S3, database, Amazon SNS topics and Amazon SQS queues) are encrypted at rest using AWS KMS keys with restricted and audited access. Customer secrets are extra encrypted at rest in a way that should withstand even an internal attacker.","title":"Encryption"},{"location":"product/security.html#security-features","text":"","title":"Security Features"},{"location":"product/security.html#single-sign-on-sso","text":"In addition to the default login providers (currently GitHub, GitLab, and Google), Spacelift also supports the ability to configure Single Sign-On (SSO) via SAML or OIDC using your favorite identity provider. Using SSO, Spacelift can be configured in a password-less approach, helping your company follow a zero-trust approach. As long as your Identity Provider supports SAML or OIDC, and passing the email scope, you're good to go! You can learn more about our Single Sign-On support here .","title":"Single Sign-On (SSO)"},{"location":"product/security.html#environment-variables","text":"Spacelift allows for granular control of environment variables on your Stacks either by setting environment variables on a per-stack basis, or creating collections of variables as a Context . These environment variables can be created in two types: plain or secret .","title":"Environment Variables"},{"location":"product/security.html#policies","text":"Spacelift policies provide a way to express rules as code to manage your infrastructure as a code environment. Users can build policies to control Spacelift login permissions, access controls, deployment workflows, and even govern the infrastructure itself to be deployed. Policies are based on the Open Policy Agent project and can be defined using its rule language Rego . You can learn more about policies here .","title":"Policies"},{"location":"product/security.html#responsible-disclosure","text":"If you discover a vulnerability, we would like to know about it so we can take steps to address it as quickly as possible. We would like to ask you to help us better protect our clients and our systems. Please do the following: email your findings to security@spacelift.io ; do not take advantage of the vulnerability or problem you have discovered, for example by downloading more data than necessary to demonstrate the vulnerability or deleting or modifying other people's data; do not reveal the problem to others until it has been resolved; do not use attacks on physical security, social engineering, distributed denial of service, spam or applications of third parties, and; do provide sufficient information to reproduce the problem, so we will be able to resolve it as quickly as possible; What we promise: we will respond to your report within 3 business days with our evaluation of the report and an expected resolution date; if you have followed the instructions above, we will not take any legal action against you in regard to the report; we will handle your report with strict confidentiality, and not pass on your personal details to third parties without your permission; we will keep you informed of the progress towards resolving the problem; in the public information concerning the problem reported, we will give your name as the discoverer of the problem (unless you desire otherwise), and; as a token of our gratitude for your assistance, we offer a reward for every report of a security problem that was not yet known to us. The amount of the reward will be determined based on the severity of the leak and the quality of the report; We strive to resolve all problems as quickly as possible, and we would like to play an active role in the ultimate publication on the problem after it is resolved.","title":"Responsible disclosure"},{"location":"product/administration/advanced-installations.html","text":"Advanced Installations \u00bb Custom VPC \u00bb In certain situations you may want to have full control of the network that Spacelift runs in, and the default VPC and security groups created by Spacelift don't fit your circumstances. In these situations, you can create your own networking components and supply them during the installation process. If you choose to do this, you will need to create the following resources: A VPC. A set of private subnets. A set of public subnets (the same subnets IDs can be used if you don't need separate private and public subnets). Security groups for the various Spacelift components. The following sections explain the requirements for each component. Also, see the section on HTTP Proxies if you need to use a proxy to allow the Spacelift components to make HTTP requests. VPC \u00bb The VPC needs to have a CIDR block large enough to run the Spacelift server and drain instances along with the database and a few other networking components. We would recommend using a minimum network prefix of /27 . Private Subnets \u00bb The number of private subnets depends on the number of availability zones you want to deploy Spacelift to. Public Subnets \u00bb The public subnets are used to place the load balancer for the Spacelift server. If you don't need separate public and private subnets you can use the same subnets for both. Just use the same subnet IDs to populate both the private and public subnet configuration options. Security Groups \u00bb Security groups need to be created for the following components: Drain. Server. Load Balancer. Installation Task. Database. The next sections explain the requirements of each group. Drain \u00bb Needs to be able to access the following components: Your VCS system (e.g. GitHub, GitLab, etc). Various AWS APIs. The Spacelift database. Our default CloudFormation template for the drain security group looks like the following, and allows unrestricted egress (for accessing VCS systems): 1 2 3 4 5 6 7 8 9 10 11 12 DrainSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"drain_sg\" GroupDescription : \"The security group for the Spacelift async-processing service\" SecurityGroupEgress : - Description : \"Unrestricted egress\" FromPort : 0 ToPort : 0 IpProtocol : \"-1\" CidrIp : \"0.0.0.0/0\" VpcId : { Ref : VPC } Server \u00bb Needs to be able to access the following components: Your VCS system (e.g. GitHub, GitLab, etc). Your identity provider for SSO. Various AWS APIs. The Spacelift database. The server also needs to allow ingress from its load balancer. Our default CloudFormation template for the server security group looks like the following, and allows unrestricted egress along with ingress via the load balancer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ServerSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"server_sg\" GroupDescription : \"The security group for the Spacelift HTTP server\" SecurityGroupEgress : - Description : \"Unrestricted egress\" FromPort : 0 ToPort : 0 IpProtocol : \"-1\" CidrIp : \"0.0.0.0/0\" SecurityGroupIngress : - Description : \"Only accept HTTP connections on port 1983 from the load balancer\" FromPort : 1983 ToPort : 1983 IpProtocol : \"tcp\" SourceSecurityGroupId : { Ref : LoadBalancerSecurityGroup } VpcId : { Ref : VPC } Load Balancer \u00bb The load balancer needs to be able to accept traffic from any clients (e.g. users logging into Spacelift via a browser or using spacectl ), and also needs to accept incoming webhooks from your VCS system. In addition it needs to be able to access the server container. Our default CloudFormation template for the load balancer security group looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 LoadBalancerSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"load_balancer_sg\" GroupDescription : \"The security group for the load balancer sitting in front of the Spacelift HTTP server\" SecurityGroupIngress : - Description : \"Accept HTTPS connections on port 443\" FromPort : 443 ToPort : 443 IpProtocol : \"tcp\" CidrIp : \"0.0.0.0/0\" VpcId : { Ref : VPC } LoadBalancerToServerEgress : Type : AWS::EC2::SecurityGroupEgress Properties : Description : \"Allow the server load balancer to access server app containers\" DestinationSecurityGroupId : { Ref : ServerSecurityGroup } FromPort : 1983 ToPort : 1983 IpProtocol : \"tcp\" GroupId : { Ref : LoadBalancerSecurityGroup } Installation Task \u00bb The installation task security group allows one-off tasks that are run during the installation process to access the Spacelift database. Our default CloudFormation template looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 InstallationTaskSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"installation_task_sg\" GroupDescription : \"The security group for tasks that run as part of the installation process\" SecurityGroupEgress : - Description : \"Unrestricted egress\" FromPort : 0 ToPort : 0 IpProtocol : \"-1\" CidrIp : \"0.0.0.0/0\" VpcId : { Ref : VPC } Database \u00bb The database security group needs to allow inbound access from the server, the drain and installation tasks. Our default CloudFormation template looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 DatabaseSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"database_sg\" GroupDescription : \"The security group defining what services can access the Spacelift database\" SecurityGroupIngress : - Description : \"Only accept TCP connections on appropriate port from the drain\" FromPort : 5432 ToPort : 5432 IpProtocol : \"tcp\" SourceSecurityGroupId : { Ref : DrainSecurityGroup } - Description : \"Only accept TCP connections on appropriate port from the server\" FromPort : 5432 ToPort : 5432 IpProtocol : \"tcp\" SourceSecurityGroupId : { Ref : ServerSecurityGroup } - Description : \"Only accept TCP connections on appropriate port from the installation tasks\" FromPort : 5432 ToPort : 5432 IpProtocol : \"tcp\" SourceSecurityGroupId : { Ref : InstallationTaskSecurityGroup } VpcId : { Ref : VPC } Performing a custom VPC installation \u00bb To install Spacelift into a custom VPC, create your VPC along with all the other required components like security groups, then edit the vpc_config section of your config.json file, making sure to set use_custom_vpc to true . A correctly populated vpc_config will look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"vpc_config\" : { \"use_custom_vpc\" : true , \"vpc_id\" : \"vpc-091e6f4d35908e7c1\" , \"private_subnet_ids\" : \"subnet-01a25f47c5a7e94fc,subnet-035169be40fbfbbbf,subnet-09c72e8ab5499eed1\" , \"public_subnet_ids\" : \"subnet-08aa756ab626d690f,subnet-0d03bb49f32922d93,subnet-0d85c4a80db226099\" , \"drain_security_group_id\" : \"sg-045061f7120343acd\" , \"load_balancer_security_group_id\" : \"sg-086a38a75894c4fc5\" , \"server_security_group_id\" : \"sg-0cb943fd285fc5c85\" , \"installation_task_security_group_id\" : \"sg-03b9b0e17cce91d3a\" , \"database_security_group_id\" : \"sg-0b67dd8ad00e237fd\" , \"availability_zones\" : \"eu-west-1a,eu-west-1b,eu-west-1c\" } } Once you have populated your configuration, just run the installer as described in the installation guide . HTTP Proxies \u00bb If you need to use an HTTP proxy to allow the Spacelift components to access the internet, you can specify this via the config.json file: 1 2 3 4 5 6 7 { \"proxy_config\" : { \"http_proxy\" : \"\" , \"https_proxy\" : \"\" , \"no_proxy\" : \"\" } } These three settings correspond to the HTTP_PROXY , HTTPS_PROXY and NO_PROXY environment variables, respectively. These environment variables are automatically added to the Spacelift ECS containers during installation when values are specified in the configuration file. Any variable that isn't populated will not be added. For example, if you use the following configuration all three environment variables will be added to the containers: 1 2 3 4 5 6 7 { \"proxy_config\" : { \"http_proxy\" : \"http://my.http.proxy\" , \"https_proxy\" : \"https://my.https.proxy\" , \"no_proxy\" : \"safe.domain\" } } However if you only need to specify the HTTPS_PROXY environment variable you can use the following configuration: 1 2 3 4 5 6 7 { \"proxy_config\" : { \"http_proxy\" : \"\" , \"https_proxy\" : \"https://my.https.proxy\" , \"no_proxy\" : \"\" } } NOTE: you must include the protocol with your proxy URL (e.g. http:// or https:// ), otherwise the proxy configuration can fail to parse and prevent the Spacelift ECS services from starting correctly. Using a TLS connection between the Application Load Balancer and Spacelift Server \u00bb The Spacelift server is served over TLS by default. This is achieved by using an AWS Application Load Balancer (ALB) to terminate the TLS connection and forward the request to the Spacelift server running in an ECS container. That is through HTTP. 1 client -> (https) -> Load Balancer -> (http) -> ECS If you want the server to use HTTPS as well, you can do so by specifying the TLS certificate in config.json . tls_config.server_certificate_secrets_manager_arn is the ARN of the SecretsManager secret that holds both the private and public keys of your TLS certificate in the following format: 1 { \"privateKey\" : \"<base64-encoded-private-key>\" , \"publicKey\" : \"<base64-encoded-public-key>\" } Example config.json : 1 2 3 4 5 6 { \"tls_config\" : { \"server_certificate_secrets_manager_arn\" : \"arn:aws:secretsmanager:eu-west-1:123456789012:secret:spacelift-server-tls-cert-123456\" , \"ca_certificates\" : [] } } Using custom CA certificates \u00bb If you use a custom certificate authority to issue TLS certs for components that Spacelift will communicate with, for example your VCS system, you need to provide your custom CA certificates. You can provide these via the config.json file via the tls_config.ca_certificates property: 1 2 3 4 5 6 7 8 9 { \"tls_config\" : { \"server_certificate_secrets_manager_arn\" : \"\" , \"ca_certificates\" : [ \"<base64-encoded certificate 1>\" , \"<base64-encoded certificate 2>\" ] } } Please note that each certificate should be base64-encoded and formatted onto a single line. For example, if we had the following certificate: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 -----BEGIN CERTIFICATE----- MIIFsTCCA5mgAwIBAgIUDD/4VBfLx5K/tAY+SckH05TJ8i8wDQYJKoZIhvcNAQEL BQAwaDELMAkGA1UEBhMCR0IxETAPBgNVBAgMCFNjb3RsYW5kMRAwDgYDVQQHDAdH bGFzZ293MRkwFwYDVQQKDBBBZGFtIEMgUm9vdCBDQSAxMRkwFwYDVQQDDBBBZGFt IEMgUm9vdCBDQSAxMB4XDTIzMDMxMzExMzYxMVoXDTI1MTIzMTExMzYxMVowaDEL MAkGA1UEBhMCR0IxETAPBgNVBAgMCFNjb3RsYW5kMRAwDgYDVQQHDAdHbGFzZ293 MRkwFwYDVQQKDBBBZGFtIEMgUm9vdCBDQSAxMRkwFwYDVQQDDBBBZGFtIEMgUm9v dCBDQSAxMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAxjv/+sInXiQ+ 2Fb+itF8ndlpmmYUoZwYN4dx+2wrcbOVngTvy4sE+33nGBzH4vt4pOhKTWwaYXFI 0CzqoIoazi8Zl0medyrwtIUDZ1pNcVugb4KAFb9Jbq40Ik3xG6t16maxQJGTiAG2 /xVtsuYdhnBGx//61SEbEwSpR145/Qf1cba8RlRQMz4QUWNe8XXo3SYaX2kxiw2V 1Op+fQxg2jf1AyzQXX1ch1jyG5RLESPUMFkBiQwi7LOSCaavfJEUzwqeoORgd7Ti uyMV+4Gsb1XAnK7KXYwisGeP5/QNFPAByfAdPjR20rMYYHfxqEDth4Najjmu/iyF PGk4CobRhitTtJXT/QxWcvtrRu1BCVnedyESMyiya4Q9dn27rFjjg3ZARqWOZhyq OTWHo2mO2FzEJuxhvYNe2iYVp2s8wMTB02nP3wpWoYwje2yDwcjkIl8uXKzEZ9Gf FATJaCLoO8o5J2HXsgOIqXlpzU9tUtEew/xTzZqX5A34o8/+NgUtm0F7joWa5mDC QB7L8cKfACydfpekJx/gFUGSy/5vdfBzOczc6Bmh66yHPBRDcgyDFnnx34m/XVQa rBwwIDDbqu3sscdOgm9v8csCJd0YlXGb/x4oAA61IITnsNd9NCw0GJIquSEcYiCE A0YrQTKVfRAXuhSZ1VPIuxXiF2K3XTMCAwEAAaNTMFEwHQYDVR0OBBYEFD55R4mt 0hNOJUgPL0JBKZB1jybSMB8GA1UdIwQYMBaAFD55R4mt0hNOJUgPL0JBKZB1jybS MA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggIBAHecVjMklTkS2Py5 XNpJ9cdzG66GuPDw8aQZIunrxqYud74CA1Y0K26kyDJkLnWzVa7nT+F0d8Qn3tov vFwI3xy5l+4upmuZ3u1jFEMiSk8C2FPohLDnDo3rwEUCGvJ6a4Gas7YyHPGL3DrJ 0dcu9wsX9cYB2YJ27QosZ5s6zmmUvBGTI30JNvPnSoC7kzqD3ArxvTEW9WaUqoJt 88lsMnn6+ps9A6exb/fK909ZWaEJWRd9cdMET0fna7EhhkO+Cqz415RgMxlK7ggT 97CvkjvvLNeFT5naHbzUANqfMVRRcUaP3PjTC9z5cDo9CaPaFjV/+Uxax2mAlARk fqYyWoqvZH90czpvFG1jUo6P4NpyxZS8layJwD24qX+EON43WYApLsl/jE2A/JmQ MdgWNhOy4HP8U8+aANr0Ev7gWWNi6VcR8T6PT/rbAGjnPmVmoZ4rc7CdoS8ZQZJh K8ELA17+pnMTgo7wxfARqL+p+mqgtUxRbiWitev8F2hUVB/SwP8hpcGrdhTEN7td pSW1ykPeGJFKSBo5QHanqqPFCzqtFeoL9DhYx5/xE6FpKMLg3vVcFsHu6glS8iMV 4Hvb2fXuhXxLTBCbD1+5lLP/bHXogQKmp2H6Oj0e6WBmQ0xqGou4Il6bavsZCx2v ADWvlue5jXdNu5xPZdsNVNAluAne -----END CERTIFICATE----- We would base64-encode it and then use a tls_config.json file that looks something like the following: 1 2 3 4 5 6 { \"ServerCertificateSecretsManagerArn\" : \"\" , \"CACertificates\" : [ \"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\" ] } Installing into a Private VPC \u00bb If you want to install Spacelift into a private VPC that does not have any internet access, you need to setup VPC endpoints for the following AWS services: ECR and ECR Docker endpoint. IoT. KMS. License manager. Logs. Monitoring. S3. Secrets manager. SQS. Xray. In addition, if you want to deploy a worker pool into a VPC with no internet access and are using our CloudFormation template to deploy the pool, you need to create a VPC endpoint for the following service: EC2 autoscaling. The following sections contain example CloudFormation definitions describing each endpoint that needs to be created. Proxy Configuration \u00bb As well as setting up VPC endpoints, you will also need to configure an HTTP Proxy . This is required because AWS does not currently provide an endpoint for the IoT control plane, meaning that these requests cannot use a private VPC endpoint. When using VPC endpoints, you should include the following in your NO_PROXY environment variable to ensure that requests to the required AWS services are routed via your VPC endpoints rather than via the proxy (making sure to substitute <region> with your install region): 1 s3.<region>.amazonaws.com,license-manager.<region>.amazonaws.com,a3mducvsqca9re-ats.iot.<region>.amazonaws.com,logs.<region>.amazonaws.com,monitoring.<region>.amazonaws.com,sqs.<region>.amazonaws.com,xray.<region>.amazonaws.com,secretsmanager.<region>.amazonaws.com,kms.<region>.amazonaws.com,ecr.<region>.amazonaws.com,api.ecr.<region>.amazonaws.com VPC Endpoint Security Group \u00bb You need to provide a security group for the VPC endpoints to use. This security group should allow inbound access on port 443. For example you could use something like the following: 1 2 3 4 5 6 7 8 9 10 11 12 VPCEndpointSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"vpc_endpoint_sg\" GroupDescription : \"The sg to use for VPC endpoints\" SecurityGroupIngress : - Description : \"Allow inbound HTTPS access to VPC endpoints from VPC\" FromPort : 443 ToPort : 443 IpProtocol : \"tcp\" CidrIp : \"<replace-with-your-own-address-range>\" VpcId : { Ref : VPC } ECR and ECR Docker endpoint \u00bb The following VPC endpoints need to be created: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ECRInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.ecr.api\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true ECRDockerInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.ecr.dkr\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true IoT \u00bb The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 IoTInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.iot.data\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : false Note that the PrivateDnsEnabled option is set to false. For the IoT data endpoint you need to manually create the correct DNS entry to allow Spacelift workers to connect to the IoT broker for your account. First, run the following command to find the correct IoT endpoint for your region: 1 aws iot describe-endpoint --endpoint-type iot:Data-ATS --region <region> --no-cli-pager --output json This should output something like the following: 1 2 3 { \"endpointAddress\" : \"b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com\" } Next, go to the Route53 console, and create a private hosted zone for your endpoint address. In the example above, this would be b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com . Finally, create an A record for your endpoint address (for example b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com ), and use an alias to point it at your IoT VPC endpoint. NOTE: make sure that you create your private hosted zone for your full endpoint address, and not for iot.<region>.amazonaws.com . If you create a hosted zone for iot.<region>.amazonaws.com it will prevent the Spacelift server and drain processes from being able to access the IoT control plane. KMS \u00bb The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 KMSInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.kms\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true License manager \u00bb The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 LicenseManagerInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.license-manager\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true Logs \u00bb The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 LogsInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.logs\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true Monitoring \u00bb The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 MonitoringInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.monitoring\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true S3 \u00bb The following VPC endpoint needs to be created. Also note that each of your Spacelift subnets will also need a route table attached that can be referenced in the endpoint definition: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 S3GatewayEndpoint : Type : AWS::EC2::VPCEndpoint Properties : ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.s3\" } VpcEndpointType : Gateway VpcId : { Ref : VPC } RouteTableIds : # Attach a route table corresponding to each of the subnets being used for Spacelift - { Ref : PrivateSubnet1RouteTable } - { Ref : PrivateSubnet2RouteTable } - { Ref : PrivateSubnet3RouteTable } PolicyDocument : Version : 2012-10-17 Statement : - Effect : Allow Principal : '*' Action : '*' Resource : '*' Secrets manager \u00bb The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 SecretsManagerInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.secretsmanager\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true SQS \u00bb The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 SQSInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.sqs\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true Xray \u00bb The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 XrayInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.xray\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true EC2 Autoscaling \u00bb The following optional VPC endpoint can be created if using our CloudFormation worker pool template: 1 2 3 4 5 6 7 8 9 10 11 AutoscalingInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.autoscaling\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"Advanced Installations"},{"location":"product/administration/advanced-installations.html#advanced-installations","text":"","title":"Advanced Installations"},{"location":"product/administration/advanced-installations.html#custom-vpc","text":"In certain situations you may want to have full control of the network that Spacelift runs in, and the default VPC and security groups created by Spacelift don't fit your circumstances. In these situations, you can create your own networking components and supply them during the installation process. If you choose to do this, you will need to create the following resources: A VPC. A set of private subnets. A set of public subnets (the same subnets IDs can be used if you don't need separate private and public subnets). Security groups for the various Spacelift components. The following sections explain the requirements for each component. Also, see the section on HTTP Proxies if you need to use a proxy to allow the Spacelift components to make HTTP requests.","title":"Custom VPC"},{"location":"product/administration/advanced-installations.html#vpc","text":"The VPC needs to have a CIDR block large enough to run the Spacelift server and drain instances along with the database and a few other networking components. We would recommend using a minimum network prefix of /27 .","title":"VPC"},{"location":"product/administration/advanced-installations.html#private-subnets","text":"The number of private subnets depends on the number of availability zones you want to deploy Spacelift to.","title":"Private Subnets"},{"location":"product/administration/advanced-installations.html#public-subnets","text":"The public subnets are used to place the load balancer for the Spacelift server. If you don't need separate public and private subnets you can use the same subnets for both. Just use the same subnet IDs to populate both the private and public subnet configuration options.","title":"Public Subnets"},{"location":"product/administration/advanced-installations.html#security-groups","text":"Security groups need to be created for the following components: Drain. Server. Load Balancer. Installation Task. Database. The next sections explain the requirements of each group.","title":"Security Groups"},{"location":"product/administration/advanced-installations.html#drain","text":"Needs to be able to access the following components: Your VCS system (e.g. GitHub, GitLab, etc). Various AWS APIs. The Spacelift database. Our default CloudFormation template for the drain security group looks like the following, and allows unrestricted egress (for accessing VCS systems): 1 2 3 4 5 6 7 8 9 10 11 12 DrainSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"drain_sg\" GroupDescription : \"The security group for the Spacelift async-processing service\" SecurityGroupEgress : - Description : \"Unrestricted egress\" FromPort : 0 ToPort : 0 IpProtocol : \"-1\" CidrIp : \"0.0.0.0/0\" VpcId : { Ref : VPC }","title":"Drain"},{"location":"product/administration/advanced-installations.html#server","text":"Needs to be able to access the following components: Your VCS system (e.g. GitHub, GitLab, etc). Your identity provider for SSO. Various AWS APIs. The Spacelift database. The server also needs to allow ingress from its load balancer. Our default CloudFormation template for the server security group looks like the following, and allows unrestricted egress along with ingress via the load balancer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ServerSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"server_sg\" GroupDescription : \"The security group for the Spacelift HTTP server\" SecurityGroupEgress : - Description : \"Unrestricted egress\" FromPort : 0 ToPort : 0 IpProtocol : \"-1\" CidrIp : \"0.0.0.0/0\" SecurityGroupIngress : - Description : \"Only accept HTTP connections on port 1983 from the load balancer\" FromPort : 1983 ToPort : 1983 IpProtocol : \"tcp\" SourceSecurityGroupId : { Ref : LoadBalancerSecurityGroup } VpcId : { Ref : VPC }","title":"Server"},{"location":"product/administration/advanced-installations.html#load-balancer","text":"The load balancer needs to be able to accept traffic from any clients (e.g. users logging into Spacelift via a browser or using spacectl ), and also needs to accept incoming webhooks from your VCS system. In addition it needs to be able to access the server container. Our default CloudFormation template for the load balancer security group looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 LoadBalancerSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"load_balancer_sg\" GroupDescription : \"The security group for the load balancer sitting in front of the Spacelift HTTP server\" SecurityGroupIngress : - Description : \"Accept HTTPS connections on port 443\" FromPort : 443 ToPort : 443 IpProtocol : \"tcp\" CidrIp : \"0.0.0.0/0\" VpcId : { Ref : VPC } LoadBalancerToServerEgress : Type : AWS::EC2::SecurityGroupEgress Properties : Description : \"Allow the server load balancer to access server app containers\" DestinationSecurityGroupId : { Ref : ServerSecurityGroup } FromPort : 1983 ToPort : 1983 IpProtocol : \"tcp\" GroupId : { Ref : LoadBalancerSecurityGroup }","title":"Load Balancer"},{"location":"product/administration/advanced-installations.html#installation-task","text":"The installation task security group allows one-off tasks that are run during the installation process to access the Spacelift database. Our default CloudFormation template looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 InstallationTaskSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"installation_task_sg\" GroupDescription : \"The security group for tasks that run as part of the installation process\" SecurityGroupEgress : - Description : \"Unrestricted egress\" FromPort : 0 ToPort : 0 IpProtocol : \"-1\" CidrIp : \"0.0.0.0/0\" VpcId : { Ref : VPC }","title":"Installation Task"},{"location":"product/administration/advanced-installations.html#database","text":"The database security group needs to allow inbound access from the server, the drain and installation tasks. Our default CloudFormation template looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 DatabaseSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"database_sg\" GroupDescription : \"The security group defining what services can access the Spacelift database\" SecurityGroupIngress : - Description : \"Only accept TCP connections on appropriate port from the drain\" FromPort : 5432 ToPort : 5432 IpProtocol : \"tcp\" SourceSecurityGroupId : { Ref : DrainSecurityGroup } - Description : \"Only accept TCP connections on appropriate port from the server\" FromPort : 5432 ToPort : 5432 IpProtocol : \"tcp\" SourceSecurityGroupId : { Ref : ServerSecurityGroup } - Description : \"Only accept TCP connections on appropriate port from the installation tasks\" FromPort : 5432 ToPort : 5432 IpProtocol : \"tcp\" SourceSecurityGroupId : { Ref : InstallationTaskSecurityGroup } VpcId : { Ref : VPC }","title":"Database"},{"location":"product/administration/advanced-installations.html#performing-a-custom-vpc-installation","text":"To install Spacelift into a custom VPC, create your VPC along with all the other required components like security groups, then edit the vpc_config section of your config.json file, making sure to set use_custom_vpc to true . A correctly populated vpc_config will look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"vpc_config\" : { \"use_custom_vpc\" : true , \"vpc_id\" : \"vpc-091e6f4d35908e7c1\" , \"private_subnet_ids\" : \"subnet-01a25f47c5a7e94fc,subnet-035169be40fbfbbbf,subnet-09c72e8ab5499eed1\" , \"public_subnet_ids\" : \"subnet-08aa756ab626d690f,subnet-0d03bb49f32922d93,subnet-0d85c4a80db226099\" , \"drain_security_group_id\" : \"sg-045061f7120343acd\" , \"load_balancer_security_group_id\" : \"sg-086a38a75894c4fc5\" , \"server_security_group_id\" : \"sg-0cb943fd285fc5c85\" , \"installation_task_security_group_id\" : \"sg-03b9b0e17cce91d3a\" , \"database_security_group_id\" : \"sg-0b67dd8ad00e237fd\" , \"availability_zones\" : \"eu-west-1a,eu-west-1b,eu-west-1c\" } } Once you have populated your configuration, just run the installer as described in the installation guide .","title":"Performing a custom VPC installation"},{"location":"product/administration/advanced-installations.html#http-proxies","text":"If you need to use an HTTP proxy to allow the Spacelift components to access the internet, you can specify this via the config.json file: 1 2 3 4 5 6 7 { \"proxy_config\" : { \"http_proxy\" : \"\" , \"https_proxy\" : \"\" , \"no_proxy\" : \"\" } } These three settings correspond to the HTTP_PROXY , HTTPS_PROXY and NO_PROXY environment variables, respectively. These environment variables are automatically added to the Spacelift ECS containers during installation when values are specified in the configuration file. Any variable that isn't populated will not be added. For example, if you use the following configuration all three environment variables will be added to the containers: 1 2 3 4 5 6 7 { \"proxy_config\" : { \"http_proxy\" : \"http://my.http.proxy\" , \"https_proxy\" : \"https://my.https.proxy\" , \"no_proxy\" : \"safe.domain\" } } However if you only need to specify the HTTPS_PROXY environment variable you can use the following configuration: 1 2 3 4 5 6 7 { \"proxy_config\" : { \"http_proxy\" : \"\" , \"https_proxy\" : \"https://my.https.proxy\" , \"no_proxy\" : \"\" } } NOTE: you must include the protocol with your proxy URL (e.g. http:// or https:// ), otherwise the proxy configuration can fail to parse and prevent the Spacelift ECS services from starting correctly.","title":"HTTP Proxies"},{"location":"product/administration/advanced-installations.html#using-a-tls-connection-between-the-application-load-balancer-and-spacelift-server","text":"The Spacelift server is served over TLS by default. This is achieved by using an AWS Application Load Balancer (ALB) to terminate the TLS connection and forward the request to the Spacelift server running in an ECS container. That is through HTTP. 1 client -> (https) -> Load Balancer -> (http) -> ECS If you want the server to use HTTPS as well, you can do so by specifying the TLS certificate in config.json . tls_config.server_certificate_secrets_manager_arn is the ARN of the SecretsManager secret that holds both the private and public keys of your TLS certificate in the following format: 1 { \"privateKey\" : \"<base64-encoded-private-key>\" , \"publicKey\" : \"<base64-encoded-public-key>\" } Example config.json : 1 2 3 4 5 6 { \"tls_config\" : { \"server_certificate_secrets_manager_arn\" : \"arn:aws:secretsmanager:eu-west-1:123456789012:secret:spacelift-server-tls-cert-123456\" , \"ca_certificates\" : [] } }","title":"Using a TLS connection between the Application Load Balancer and Spacelift Server"},{"location":"product/administration/advanced-installations.html#using-custom-ca-certificates","text":"If you use a custom certificate authority to issue TLS certs for components that Spacelift will communicate with, for example your VCS system, you need to provide your custom CA certificates. You can provide these via the config.json file via the tls_config.ca_certificates property: 1 2 3 4 5 6 7 8 9 { \"tls_config\" : { \"server_certificate_secrets_manager_arn\" : \"\" , \"ca_certificates\" : [ \"<base64-encoded certificate 1>\" , \"<base64-encoded certificate 2>\" ] } } Please note that each certificate should be base64-encoded and formatted onto a single line. For example, if we had the following certificate: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 -----BEGIN CERTIFICATE----- MIIFsTCCA5mgAwIBAgIUDD/4VBfLx5K/tAY+SckH05TJ8i8wDQYJKoZIhvcNAQEL BQAwaDELMAkGA1UEBhMCR0IxETAPBgNVBAgMCFNjb3RsYW5kMRAwDgYDVQQHDAdH bGFzZ293MRkwFwYDVQQKDBBBZGFtIEMgUm9vdCBDQSAxMRkwFwYDVQQDDBBBZGFt IEMgUm9vdCBDQSAxMB4XDTIzMDMxMzExMzYxMVoXDTI1MTIzMTExMzYxMVowaDEL MAkGA1UEBhMCR0IxETAPBgNVBAgMCFNjb3RsYW5kMRAwDgYDVQQHDAdHbGFzZ293 MRkwFwYDVQQKDBBBZGFtIEMgUm9vdCBDQSAxMRkwFwYDVQQDDBBBZGFtIEMgUm9v dCBDQSAxMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAxjv/+sInXiQ+ 2Fb+itF8ndlpmmYUoZwYN4dx+2wrcbOVngTvy4sE+33nGBzH4vt4pOhKTWwaYXFI 0CzqoIoazi8Zl0medyrwtIUDZ1pNcVugb4KAFb9Jbq40Ik3xG6t16maxQJGTiAG2 /xVtsuYdhnBGx//61SEbEwSpR145/Qf1cba8RlRQMz4QUWNe8XXo3SYaX2kxiw2V 1Op+fQxg2jf1AyzQXX1ch1jyG5RLESPUMFkBiQwi7LOSCaavfJEUzwqeoORgd7Ti uyMV+4Gsb1XAnK7KXYwisGeP5/QNFPAByfAdPjR20rMYYHfxqEDth4Najjmu/iyF PGk4CobRhitTtJXT/QxWcvtrRu1BCVnedyESMyiya4Q9dn27rFjjg3ZARqWOZhyq OTWHo2mO2FzEJuxhvYNe2iYVp2s8wMTB02nP3wpWoYwje2yDwcjkIl8uXKzEZ9Gf FATJaCLoO8o5J2HXsgOIqXlpzU9tUtEew/xTzZqX5A34o8/+NgUtm0F7joWa5mDC QB7L8cKfACydfpekJx/gFUGSy/5vdfBzOczc6Bmh66yHPBRDcgyDFnnx34m/XVQa rBwwIDDbqu3sscdOgm9v8csCJd0YlXGb/x4oAA61IITnsNd9NCw0GJIquSEcYiCE A0YrQTKVfRAXuhSZ1VPIuxXiF2K3XTMCAwEAAaNTMFEwHQYDVR0OBBYEFD55R4mt 0hNOJUgPL0JBKZB1jybSMB8GA1UdIwQYMBaAFD55R4mt0hNOJUgPL0JBKZB1jybS MA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggIBAHecVjMklTkS2Py5 XNpJ9cdzG66GuPDw8aQZIunrxqYud74CA1Y0K26kyDJkLnWzVa7nT+F0d8Qn3tov vFwI3xy5l+4upmuZ3u1jFEMiSk8C2FPohLDnDo3rwEUCGvJ6a4Gas7YyHPGL3DrJ 0dcu9wsX9cYB2YJ27QosZ5s6zmmUvBGTI30JNvPnSoC7kzqD3ArxvTEW9WaUqoJt 88lsMnn6+ps9A6exb/fK909ZWaEJWRd9cdMET0fna7EhhkO+Cqz415RgMxlK7ggT 97CvkjvvLNeFT5naHbzUANqfMVRRcUaP3PjTC9z5cDo9CaPaFjV/+Uxax2mAlARk fqYyWoqvZH90czpvFG1jUo6P4NpyxZS8layJwD24qX+EON43WYApLsl/jE2A/JmQ MdgWNhOy4HP8U8+aANr0Ev7gWWNi6VcR8T6PT/rbAGjnPmVmoZ4rc7CdoS8ZQZJh K8ELA17+pnMTgo7wxfARqL+p+mqgtUxRbiWitev8F2hUVB/SwP8hpcGrdhTEN7td pSW1ykPeGJFKSBo5QHanqqPFCzqtFeoL9DhYx5/xE6FpKMLg3vVcFsHu6glS8iMV 4Hvb2fXuhXxLTBCbD1+5lLP/bHXogQKmp2H6Oj0e6WBmQ0xqGou4Il6bavsZCx2v ADWvlue5jXdNu5xPZdsNVNAluAne -----END CERTIFICATE----- We would base64-encode it and then use a tls_config.json file that looks something like the following: 1 2 3 4 5 6 { \"ServerCertificateSecretsManagerArn\" : \"\" , \"CACertificates\" : [ \"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\" ] }","title":"Using custom CA certificates"},{"location":"product/administration/advanced-installations.html#installing-into-a-private-vpc","text":"If you want to install Spacelift into a private VPC that does not have any internet access, you need to setup VPC endpoints for the following AWS services: ECR and ECR Docker endpoint. IoT. KMS. License manager. Logs. Monitoring. S3. Secrets manager. SQS. Xray. In addition, if you want to deploy a worker pool into a VPC with no internet access and are using our CloudFormation template to deploy the pool, you need to create a VPC endpoint for the following service: EC2 autoscaling. The following sections contain example CloudFormation definitions describing each endpoint that needs to be created.","title":"Installing into a Private VPC"},{"location":"product/administration/advanced-installations.html#proxy-configuration","text":"As well as setting up VPC endpoints, you will also need to configure an HTTP Proxy . This is required because AWS does not currently provide an endpoint for the IoT control plane, meaning that these requests cannot use a private VPC endpoint. When using VPC endpoints, you should include the following in your NO_PROXY environment variable to ensure that requests to the required AWS services are routed via your VPC endpoints rather than via the proxy (making sure to substitute <region> with your install region): 1 s3.<region>.amazonaws.com,license-manager.<region>.amazonaws.com,a3mducvsqca9re-ats.iot.<region>.amazonaws.com,logs.<region>.amazonaws.com,monitoring.<region>.amazonaws.com,sqs.<region>.amazonaws.com,xray.<region>.amazonaws.com,secretsmanager.<region>.amazonaws.com,kms.<region>.amazonaws.com,ecr.<region>.amazonaws.com,api.ecr.<region>.amazonaws.com","title":"Proxy Configuration"},{"location":"product/administration/advanced-installations.html#vpc-endpoint-security-group","text":"You need to provide a security group for the VPC endpoints to use. This security group should allow inbound access on port 443. For example you could use something like the following: 1 2 3 4 5 6 7 8 9 10 11 12 VPCEndpointSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : \"vpc_endpoint_sg\" GroupDescription : \"The sg to use for VPC endpoints\" SecurityGroupIngress : - Description : \"Allow inbound HTTPS access to VPC endpoints from VPC\" FromPort : 443 ToPort : 443 IpProtocol : \"tcp\" CidrIp : \"<replace-with-your-own-address-range>\" VpcId : { Ref : VPC }","title":"VPC Endpoint Security Group"},{"location":"product/administration/advanced-installations.html#ecr-and-ecr-docker-endpoint","text":"The following VPC endpoints need to be created: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ECRInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.ecr.api\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true ECRDockerInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.ecr.dkr\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"ECR and ECR Docker endpoint"},{"location":"product/administration/advanced-installations.html#iot","text":"The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 IoTInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.iot.data\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : false Note that the PrivateDnsEnabled option is set to false. For the IoT data endpoint you need to manually create the correct DNS entry to allow Spacelift workers to connect to the IoT broker for your account. First, run the following command to find the correct IoT endpoint for your region: 1 aws iot describe-endpoint --endpoint-type iot:Data-ATS --region <region> --no-cli-pager --output json This should output something like the following: 1 2 3 { \"endpointAddress\" : \"b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com\" } Next, go to the Route53 console, and create a private hosted zone for your endpoint address. In the example above, this would be b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com . Finally, create an A record for your endpoint address (for example b2mdsfpsxca6rx-ats.iot.us-east-1.amazonaws.com ), and use an alias to point it at your IoT VPC endpoint. NOTE: make sure that you create your private hosted zone for your full endpoint address, and not for iot.<region>.amazonaws.com . If you create a hosted zone for iot.<region>.amazonaws.com it will prevent the Spacelift server and drain processes from being able to access the IoT control plane.","title":"IoT"},{"location":"product/administration/advanced-installations.html#kms","text":"The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 KMSInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.kms\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"KMS"},{"location":"product/administration/advanced-installations.html#license-manager","text":"The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 LicenseManagerInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.license-manager\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"License manager"},{"location":"product/administration/advanced-installations.html#logs","text":"The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 LogsInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.logs\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"Logs"},{"location":"product/administration/advanced-installations.html#monitoring","text":"The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 MonitoringInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.monitoring\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"Monitoring"},{"location":"product/administration/advanced-installations.html#s3","text":"The following VPC endpoint needs to be created. Also note that each of your Spacelift subnets will also need a route table attached that can be referenced in the endpoint definition: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 S3GatewayEndpoint : Type : AWS::EC2::VPCEndpoint Properties : ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.s3\" } VpcEndpointType : Gateway VpcId : { Ref : VPC } RouteTableIds : # Attach a route table corresponding to each of the subnets being used for Spacelift - { Ref : PrivateSubnet1RouteTable } - { Ref : PrivateSubnet2RouteTable } - { Ref : PrivateSubnet3RouteTable } PolicyDocument : Version : 2012-10-17 Statement : - Effect : Allow Principal : '*' Action : '*' Resource : '*'","title":"S3"},{"location":"product/administration/advanced-installations.html#secrets-manager","text":"The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 SecretsManagerInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.secretsmanager\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"Secrets manager"},{"location":"product/administration/advanced-installations.html#sqs","text":"The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 SQSInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.sqs\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"SQS"},{"location":"product/administration/advanced-installations.html#xray","text":"The following VPC endpoint needs to be created: 1 2 3 4 5 6 7 8 9 10 11 XrayInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.xray\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"Xray"},{"location":"product/administration/advanced-installations.html#ec2-autoscaling","text":"The following optional VPC endpoint can be created if using our CloudFormation worker pool template: 1 2 3 4 5 6 7 8 9 10 11 AutoscalingInterfaceEndpoint : Type : AWS::EC2::VPCEndpoint Properties : VpcEndpointType : Interface ServiceName : { Fn :: Sub : \"com.amazonaws.${AWS::Region}.autoscaling\" } VpcId : { Ref : VPC } SubnetIds : - { Ref : PrivateSubnet1 } # Replace this with at least one of your subnets SecurityGroupIds : - { Ref : VPCEndpointSecurityGroup } PrivateDnsEnabled : true","title":"EC2 Autoscaling"},{"location":"product/administration/install.html","text":"Installation Guide \u00bb This guide contains instructions on installing a self-hosted copy of Spacelift in an AWS account you control. Pre-requisites \u00bb Before proceeding with the installation, you need to satisfy the following pre-requisites: You need access to an AWS account you wish to install Spacelift into. You need to choose a hostname that you wish to use for your Spacelift installation, for example spacelift.example.com . This needs to be on a domain that you control and can add DNS records to. You need to create an ACM certificate for your chosen domain in the same account that you want to install Spacelift in. Requirements \u00bb The installation process requires the following tools: A Mac or Linux machine to run the installation script from. A copy of the AWS CLI v2 , configured to access the account you wish to install Spacelift into. jq version 1.6. Standard unix utilities including bash, base64, cat, read, openssl. Docker . Spacelift infrastructure \u00bb Server and drain \u00bb The Spacelift infrastructure has two core parts: the server and the drain. The server is a web application that provides a GraphQL API that serves frontend and every HTTPS request in general. The drain is a worker that processes all asynchronous tasks such as VCS webhooks, run scheduling, worker handling, etc. The drain consumes messages from several SQS queues. The server serves requests through a load balancer. As of today, both the server and the drain are ECS services running on Fargate. We have autoscaling set up for the server so ideally you don't need to worry about scaling it. Worker pool infrastructure \u00bb A worker pool is a group of workers that can be used to run workloads. During the startup the worker will attempt to connect to the regional AWS IoT Core broker endpoint and register itself. The drain and the server will then be able to communicate with the worker via AWS IoT Core, specifically via MQTT. Running Costs \u00bb We estimate the baseline running costs of a Spacelift Self-Hosted instance to be around $15 per day (roughly $450 per month). Note that this is with no activity in your account, so your costs may be higher after factoring in things like bandwidth. These baseline costs include all of the resources deployed as part of your Spacelift install, for example Aurora RDS, Fargate cluster, and KMS keys. Installation \u00bb This section explains the installation process for Spacelift. You may also be interested in the following pages that explain how to configure the Slack integration as well as advanced installations: Slack integration setup - explains how to configure the Slack integration for your Spacelift instance. Advanced installations - explains how to configure advanced options like providing a custom VPC configuration, or specifying HTTP proxy settings. AWS Requirements \u00bb Before you start the installation process, make sure the following requirements are met: The region that you wish to install self-hosting in has at least 3 EIPs available. The default quota per region in an AWS account only allows 5 EIPs to be created, so you may need to choose another region or ask for an increase. These EIPs are used as part of NAT Gateways to allow outbound traffic from Spacelift. If you want full control over your networking setup, please see advanced installations . Accepting your License \u00bb When you sign up for self-hosting, a license will be issued to your AWS account via AWS License Manager. Before you can use your license, you need to accept it. Navigate to AWS License Manager in your AWS Console, and go to Granted licenses : Note: if this is your first time accessing License Manager, you may need to grant permissions to AWS before you can use it. If this is the case you will automatically be prompted to grant permission. Click on your license ID, and then choose the Accept & activate license option on the license page: Follow the instructions on the popup that appears to activate your license: That\u2019s it - you\u2019re now ready to proceed with the rest of the installation! Release Archive \u00bb Spacelift self-hosted is distributed as an archive containing everything needed to install Spacelift into your AWS account. The archive has the following structure: config.json - the configuration file containing all necessary configuration options. Some options come prepopulated with default values. bin - contains binaries including a copy of the launcher built for self-hosting. cloudformation - contains CloudFormation templates used to deploy Spacelift. container-images - contains container images for running the Spacelift backend as well as a launcher image. install.sh - the installation script. uninstall.sh - the uninstallation script. scripts - contains other scripts called by the installation script. version - contains the version number. Signature Validation \u00bb Along with the release archive, we also provide a SHA-256 checksum of the archive as well as a GPG signature. The fingerprint of our GPG key is 380BD7699053035B71D027B173EBA0CF3B3F4A46 , and you can import it using the following command: 1 gpg --recv-keys 380BD7699053035B71D027B173EBA0CF3B3F4A46 You can verify the integrity of the release archive using the following command: 1 sha256sum -c self-hosted-<version>.tar.gz_SHA256SUMS And you can verify the authenticity using the following command: 1 gpg --verify self-hosted-<version>.tar.gz_SHA256SUMS.sig Extraction \u00bb First, extract the release artifacts and move to the extracted directory (replacing <version> with the version you are installing): 1 2 tar -zxf self-hosted-<version>.tar.gz cd self-hosted-<version> Configuration \u00bb The included config.json file provides an easy way to provide additional and required configuration for the resources created during the deployment. The mandatory fields are: account_name - the name of your Spacelift account. Note: the URL of your Spacelift installation doesn't neccessarily need to match this name but the account name affects the URL of the module registry . aws_region - the AWS region you wish to install Spacelift into. load_balancer.certificate_arn - the ARN of the ACM certificate you created in the pre-requisites. spacelift_hostname - the hostname you wish to use for your Spacelift installation without the protocol or trailing slash, for example spacelift.mycorp.com . sso_config.admin_login - the email address of the user you wish to use as the initial admin user for your Spacelift installation. sso_config.sso_type - the type of SSO you wish to use. Valid values are OIDC and SAML . sso_config.oidc_args - if sso_type is OIDC , all fields are mandatory: client_id - the OIDC client ID. client_credentials - the OIDC client secret. identity_provider_host - the OIDC identity provider host with protocol. Example: \"https://mycorp.okta.com\" . sso_config.saml_args - if sso_type is SAML , dynamic and metadata fields are mandatory: dynamic - true or false . if true then metadata must be a URL to the SAML IDP metadata. If false then metadata must be the SAML IDP metadata. metadata - either the full SAML IDP metadata or the URL to the SAML IDP metadata. name_id_format - SAML name identifier format , can be left empty. Valid values are TRANSIENT , EMAIL_ADDRESS or PERMANENT . Defaults to TRANSIENT . NOTE: we recommend that you store your config.json file so that you can reuse it when upgrading to newer Spacelift versions. Optional configuration options \u00bb Load balancer \u00bb Load balancer configuration has a mix of required and optional fields alongside some which should already be prefilled. The object itself looks like this: 1 2 3 4 5 6 7 8 9 \"load_balancer\" : { \"certificate_arn\" : \"\" , \"scheme\" : \"internet-facing\" , \"ssl_policy\" : \"ELBSecurityPolicy-TLS-1-2-2017-01\" , \"tag\" : { \"key\" : \"\" , \"value\" : \"\" } } The prefilled fields are valid defaults and can be left unchanged, while the certificate_arn field is required and must be set, and the tag object is optional and can be used to set a custom tag against the load balancer resource. The scheme can be either internet-facing or internal and defaults to internet-facing : internal load balancers can only route requests from clients with access to the VPC for the load balancer, while internet-facing load balancers can route requests from clients over the internet. ssl_policy is the name of the security policy that defines ciphers and protocols. The default value is ELBSecurityPolicy-TLS-1-2-2017-01 which is the most recent security policy that supports TLS 1.2. For more information, see Security policies . Database \u00bb You can configure the following options for the Spacelift Postgres database: database.delete_protection_enabled - whether to enable deletion protection for the database (defaults to true ). Note: uninstall.sh script will disable this option before deleting the database. database.instance_class - the instance class of the database (defaults to db.t4g.large ). Monitoring & Alerting \u00bb As part of the self-hosting installation, we deploy a monitoring dashboard to help you monitor the health of your Spacelift installation. It is accessible via the CloudWatch UI. Additionally, we can also create a few preconfigured alerts for you. You can configure the following options for the CloudWatch alarms: 1 2 3 \"alerting\" : { \"sns_topic_arn\" : \"\" } If an SNS topic ARN is configured, we'll create an SNS subscription for each alarm. If left empty, we won't create any alarms, only the monitoring dashboard. Important! Your SNS topic's access policy must allow the cloudwatch.amazonaws.com service principal to publish to the topic. An example access policy: 1 2 3 4 5 6 7 8 9 { \"Sid\" : \"Allow_Publish_Alarms\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"cloudwatch.amazonaws.com\" }, \"Action\" : \"sns:Publish\" , \"Resource\" : \"<sns-topic-arn>\" } Global tags \u00bb You can add additional tags to all the resources created by the installer by adding your desired tags to the global_resources_tags array in the config.json : 1 2 3 4 5 6 \"global_resource_tags\" : [ { \"key\" : \"selfhost\" , \"value\" : \"spacelift\" } ] Identity Provider \u00bb URLs \u00bb You may need certain URLs when configuring an application in your identity provider. For SAML, use the following URLs: Single Sign-On URL: https://<spacelift-hostname>/saml/acs Entity ID (audience): https://<spacelift-hostname>/saml/metadata For OIDC, use the following URL: Authorized redirect URL: https://<spacelift-hostname>/oidc/exchange NOTE: please make sure to substitute <spacelift-hostname> with the hostname you plan to use for your Spacelift install. SAML Metadata \u00bb If you are using non-dynamic SAML metadata rather than using a dynamic metadata URL, you need to ensure that the metadata provided is a valid JSON-escaped string. One way to do this is to use jq to escape your metadata. For example, if your metadata was contained in a file called metadata.xml , you could run the following command: 1 cat metadata.xml | jq -R -s '.' You would then enter the escaped string into your config.json file: 1 2 3 4 5 \"saml_args\" : { \"metadata\" : \"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><EntityDescriptor ID=\\\"_90756ab2...\" , \"dynamic\" : false , \"name_id_format\" : \"EMAIL_ADDRESS\" } Running the installer \u00bb This section covers simple installations using a Spacelift-created VPC and a public facing HTTP load balancer. For information about using an existing VPC please see advanced installations . To run the installer, pass in the path of the configuration file: 1 ./install.sh [ -c \"<configuration file>\" ] The -c flag is optional, it defaults to config.json if not specified. When the installer starts, it will check it can connect to your AWS account, and will ask for confirmation to continue. Please check the AWS account ID is correct, and if so enter yes : 1 2 3 4 ./install.sh [ 2023 -01-24T12:17:52+0000 ] INFO: installing version v0.0.6 of Spacelift into AWS account 123456789012 Are you sure you want to continue ? Only 'yes' will be accepted: yes Please note, the installation process can take between 10 and 20 minutes to create all of the required infrastructure for Spacelift. Troubleshooting \u00bb If you see the following error message indicating that the account could not be found, please check the credentials for your AWS CLI are correctly configured: 1 ERROR: could not find AWS account ID. Cannot continue with Spacelift install. This error message can also be displayed if your AWS credentials are connected to a GovCloud account but a non-GovCloud region has been specified in the aws_region configuration option. Setting up DNS entries \u00bb Once the installer has completed, you should see output similar to the following, providing you with the DNS address of the load balancer along with the Launcher container image URL: 1 2 3 4 5 6 Installation info: * Load balancer DNS: spacelift-server-1234567890.eu-west-1.elb.amazonaws.com * Launcher container image: 123456789012 .dkr.ecr.eu-west-1.amazonaws.com/spacelift-launcher:v0.0.6 [ 2023 -01-24T11:30:59+0000 ] INFO: Spacelift version v0.0.6 has been successfully installed! Please use the Load balancer DNS to setup a CNAME entry or an A record using an alias if using a hosted zone in the same AWS account as your Spacelift installation. This entry should point from the hostname you want to use for Spacelift (e.g. spacelift.saturnhead.io ) to the Spacelift Load Balancer. Once your DNS changes propagate, you should be able to login to your instance by navigating to its hostname (for example https://spacelift.saturnhead.io ). Assuming everything has been successful, you should see a welcome screen similar to the following: Creating your first worker pool \u00bb Before you can create stacks or trigger any runs, you need a worker pool. For more information please see our worker pools page. Updating existing SSO configuration \u00bb If you already have an existing SSO configuration and want to update it, you need to update the sso_config section of the configuration file and run the update script: 1 ./scripts/update-sso-settings.sh [ -c \"<configuration file>\" ] It will run the ECS task that will update the SSO configuration. Upgrading \u00bb To upgrade to the latest version of self-hosting, follow these steps: Make sure your config.json file is fully configured to match your existing installation. Ideally you should use the config file from your previous installation. Run ./install.sh . Restart any workers connected to your Spacelift installation to make sure they're running the latest version. Uninstalling \u00bb If you want to completely uninstall Spacelift, you can use the uninstall.sh script. By default, the script will retain S3 buckets, database and KMS keys so that they can be restored later. To run the uninstall script, use the following command, specifying your AWS region: 1 ./uninstall.sh [ -c <config-file> ] [ -f | -n | -h ] Flags: -c <config-file> : path to the config file (default: config.json ) -f : force uninstallation, do not prompt for confirmation -n : do not retain S3 buckets, database or KMS keys. complete uninstallation. -h : show help For example: 1 ./uninstall.sh","title":"Installation Guide"},{"location":"product/administration/install.html#installation-guide","text":"This guide contains instructions on installing a self-hosted copy of Spacelift in an AWS account you control.","title":"Installation Guide"},{"location":"product/administration/install.html#pre-requisites","text":"Before proceeding with the installation, you need to satisfy the following pre-requisites: You need access to an AWS account you wish to install Spacelift into. You need to choose a hostname that you wish to use for your Spacelift installation, for example spacelift.example.com . This needs to be on a domain that you control and can add DNS records to. You need to create an ACM certificate for your chosen domain in the same account that you want to install Spacelift in.","title":"Pre-requisites"},{"location":"product/administration/install.html#requirements","text":"The installation process requires the following tools: A Mac or Linux machine to run the installation script from. A copy of the AWS CLI v2 , configured to access the account you wish to install Spacelift into. jq version 1.6. Standard unix utilities including bash, base64, cat, read, openssl. Docker .","title":"Requirements"},{"location":"product/administration/install.html#spacelift-infrastructure","text":"","title":"Spacelift infrastructure"},{"location":"product/administration/install.html#server-and-drain","text":"The Spacelift infrastructure has two core parts: the server and the drain. The server is a web application that provides a GraphQL API that serves frontend and every HTTPS request in general. The drain is a worker that processes all asynchronous tasks such as VCS webhooks, run scheduling, worker handling, etc. The drain consumes messages from several SQS queues. The server serves requests through a load balancer. As of today, both the server and the drain are ECS services running on Fargate. We have autoscaling set up for the server so ideally you don't need to worry about scaling it.","title":"Server and drain"},{"location":"product/administration/install.html#worker-pool-infrastructure","text":"A worker pool is a group of workers that can be used to run workloads. During the startup the worker will attempt to connect to the regional AWS IoT Core broker endpoint and register itself. The drain and the server will then be able to communicate with the worker via AWS IoT Core, specifically via MQTT.","title":"Worker pool infrastructure"},{"location":"product/administration/install.html#running-costs","text":"We estimate the baseline running costs of a Spacelift Self-Hosted instance to be around $15 per day (roughly $450 per month). Note that this is with no activity in your account, so your costs may be higher after factoring in things like bandwidth. These baseline costs include all of the resources deployed as part of your Spacelift install, for example Aurora RDS, Fargate cluster, and KMS keys.","title":"Running Costs"},{"location":"product/administration/install.html#installation","text":"This section explains the installation process for Spacelift. You may also be interested in the following pages that explain how to configure the Slack integration as well as advanced installations: Slack integration setup - explains how to configure the Slack integration for your Spacelift instance. Advanced installations - explains how to configure advanced options like providing a custom VPC configuration, or specifying HTTP proxy settings.","title":"Installation"},{"location":"product/administration/install.html#aws-requirements","text":"Before you start the installation process, make sure the following requirements are met: The region that you wish to install self-hosting in has at least 3 EIPs available. The default quota per region in an AWS account only allows 5 EIPs to be created, so you may need to choose another region or ask for an increase. These EIPs are used as part of NAT Gateways to allow outbound traffic from Spacelift. If you want full control over your networking setup, please see advanced installations .","title":"AWS Requirements"},{"location":"product/administration/install.html#accepting-your-license","text":"When you sign up for self-hosting, a license will be issued to your AWS account via AWS License Manager. Before you can use your license, you need to accept it. Navigate to AWS License Manager in your AWS Console, and go to Granted licenses : Note: if this is your first time accessing License Manager, you may need to grant permissions to AWS before you can use it. If this is the case you will automatically be prompted to grant permission. Click on your license ID, and then choose the Accept & activate license option on the license page: Follow the instructions on the popup that appears to activate your license: That\u2019s it - you\u2019re now ready to proceed with the rest of the installation!","title":"Accepting your License"},{"location":"product/administration/install.html#release-archive","text":"Spacelift self-hosted is distributed as an archive containing everything needed to install Spacelift into your AWS account. The archive has the following structure: config.json - the configuration file containing all necessary configuration options. Some options come prepopulated with default values. bin - contains binaries including a copy of the launcher built for self-hosting. cloudformation - contains CloudFormation templates used to deploy Spacelift. container-images - contains container images for running the Spacelift backend as well as a launcher image. install.sh - the installation script. uninstall.sh - the uninstallation script. scripts - contains other scripts called by the installation script. version - contains the version number.","title":"Release Archive"},{"location":"product/administration/install.html#signature-validation","text":"Along with the release archive, we also provide a SHA-256 checksum of the archive as well as a GPG signature. The fingerprint of our GPG key is 380BD7699053035B71D027B173EBA0CF3B3F4A46 , and you can import it using the following command: 1 gpg --recv-keys 380BD7699053035B71D027B173EBA0CF3B3F4A46 You can verify the integrity of the release archive using the following command: 1 sha256sum -c self-hosted-<version>.tar.gz_SHA256SUMS And you can verify the authenticity using the following command: 1 gpg --verify self-hosted-<version>.tar.gz_SHA256SUMS.sig","title":"Signature Validation"},{"location":"product/administration/install.html#extraction","text":"First, extract the release artifacts and move to the extracted directory (replacing <version> with the version you are installing): 1 2 tar -zxf self-hosted-<version>.tar.gz cd self-hosted-<version>","title":"Extraction"},{"location":"product/administration/install.html#configuration","text":"The included config.json file provides an easy way to provide additional and required configuration for the resources created during the deployment. The mandatory fields are: account_name - the name of your Spacelift account. Note: the URL of your Spacelift installation doesn't neccessarily need to match this name but the account name affects the URL of the module registry . aws_region - the AWS region you wish to install Spacelift into. load_balancer.certificate_arn - the ARN of the ACM certificate you created in the pre-requisites. spacelift_hostname - the hostname you wish to use for your Spacelift installation without the protocol or trailing slash, for example spacelift.mycorp.com . sso_config.admin_login - the email address of the user you wish to use as the initial admin user for your Spacelift installation. sso_config.sso_type - the type of SSO you wish to use. Valid values are OIDC and SAML . sso_config.oidc_args - if sso_type is OIDC , all fields are mandatory: client_id - the OIDC client ID. client_credentials - the OIDC client secret. identity_provider_host - the OIDC identity provider host with protocol. Example: \"https://mycorp.okta.com\" . sso_config.saml_args - if sso_type is SAML , dynamic and metadata fields are mandatory: dynamic - true or false . if true then metadata must be a URL to the SAML IDP metadata. If false then metadata must be the SAML IDP metadata. metadata - either the full SAML IDP metadata or the URL to the SAML IDP metadata. name_id_format - SAML name identifier format , can be left empty. Valid values are TRANSIENT , EMAIL_ADDRESS or PERMANENT . Defaults to TRANSIENT . NOTE: we recommend that you store your config.json file so that you can reuse it when upgrading to newer Spacelift versions.","title":"Configuration"},{"location":"product/administration/install.html#optional-configuration-options","text":"","title":"Optional configuration options"},{"location":"product/administration/install.html#load-balancer","text":"Load balancer configuration has a mix of required and optional fields alongside some which should already be prefilled. The object itself looks like this: 1 2 3 4 5 6 7 8 9 \"load_balancer\" : { \"certificate_arn\" : \"\" , \"scheme\" : \"internet-facing\" , \"ssl_policy\" : \"ELBSecurityPolicy-TLS-1-2-2017-01\" , \"tag\" : { \"key\" : \"\" , \"value\" : \"\" } } The prefilled fields are valid defaults and can be left unchanged, while the certificate_arn field is required and must be set, and the tag object is optional and can be used to set a custom tag against the load balancer resource. The scheme can be either internet-facing or internal and defaults to internet-facing : internal load balancers can only route requests from clients with access to the VPC for the load balancer, while internet-facing load balancers can route requests from clients over the internet. ssl_policy is the name of the security policy that defines ciphers and protocols. The default value is ELBSecurityPolicy-TLS-1-2-2017-01 which is the most recent security policy that supports TLS 1.2. For more information, see Security policies .","title":"Load balancer"},{"location":"product/administration/install.html#database","text":"You can configure the following options for the Spacelift Postgres database: database.delete_protection_enabled - whether to enable deletion protection for the database (defaults to true ). Note: uninstall.sh script will disable this option before deleting the database. database.instance_class - the instance class of the database (defaults to db.t4g.large ).","title":"Database"},{"location":"product/administration/install.html#monitoring-alerting","text":"As part of the self-hosting installation, we deploy a monitoring dashboard to help you monitor the health of your Spacelift installation. It is accessible via the CloudWatch UI. Additionally, we can also create a few preconfigured alerts for you. You can configure the following options for the CloudWatch alarms: 1 2 3 \"alerting\" : { \"sns_topic_arn\" : \"\" } If an SNS topic ARN is configured, we'll create an SNS subscription for each alarm. If left empty, we won't create any alarms, only the monitoring dashboard. Important! Your SNS topic's access policy must allow the cloudwatch.amazonaws.com service principal to publish to the topic. An example access policy: 1 2 3 4 5 6 7 8 9 { \"Sid\" : \"Allow_Publish_Alarms\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"cloudwatch.amazonaws.com\" }, \"Action\" : \"sns:Publish\" , \"Resource\" : \"<sns-topic-arn>\" }","title":"Monitoring &amp; Alerting"},{"location":"product/administration/install.html#global-tags","text":"You can add additional tags to all the resources created by the installer by adding your desired tags to the global_resources_tags array in the config.json : 1 2 3 4 5 6 \"global_resource_tags\" : [ { \"key\" : \"selfhost\" , \"value\" : \"spacelift\" } ]","title":"Global tags"},{"location":"product/administration/install.html#identity-provider","text":"","title":"Identity Provider"},{"location":"product/administration/install.html#urls","text":"You may need certain URLs when configuring an application in your identity provider. For SAML, use the following URLs: Single Sign-On URL: https://<spacelift-hostname>/saml/acs Entity ID (audience): https://<spacelift-hostname>/saml/metadata For OIDC, use the following URL: Authorized redirect URL: https://<spacelift-hostname>/oidc/exchange NOTE: please make sure to substitute <spacelift-hostname> with the hostname you plan to use for your Spacelift install.","title":"URLs"},{"location":"product/administration/install.html#saml-metadata","text":"If you are using non-dynamic SAML metadata rather than using a dynamic metadata URL, you need to ensure that the metadata provided is a valid JSON-escaped string. One way to do this is to use jq to escape your metadata. For example, if your metadata was contained in a file called metadata.xml , you could run the following command: 1 cat metadata.xml | jq -R -s '.' You would then enter the escaped string into your config.json file: 1 2 3 4 5 \"saml_args\" : { \"metadata\" : \"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><EntityDescriptor ID=\\\"_90756ab2...\" , \"dynamic\" : false , \"name_id_format\" : \"EMAIL_ADDRESS\" }","title":"SAML Metadata"},{"location":"product/administration/install.html#running-the-installer","text":"This section covers simple installations using a Spacelift-created VPC and a public facing HTTP load balancer. For information about using an existing VPC please see advanced installations . To run the installer, pass in the path of the configuration file: 1 ./install.sh [ -c \"<configuration file>\" ] The -c flag is optional, it defaults to config.json if not specified. When the installer starts, it will check it can connect to your AWS account, and will ask for confirmation to continue. Please check the AWS account ID is correct, and if so enter yes : 1 2 3 4 ./install.sh [ 2023 -01-24T12:17:52+0000 ] INFO: installing version v0.0.6 of Spacelift into AWS account 123456789012 Are you sure you want to continue ? Only 'yes' will be accepted: yes Please note, the installation process can take between 10 and 20 minutes to create all of the required infrastructure for Spacelift.","title":"Running the installer"},{"location":"product/administration/install.html#troubleshooting","text":"If you see the following error message indicating that the account could not be found, please check the credentials for your AWS CLI are correctly configured: 1 ERROR: could not find AWS account ID. Cannot continue with Spacelift install. This error message can also be displayed if your AWS credentials are connected to a GovCloud account but a non-GovCloud region has been specified in the aws_region configuration option.","title":"Troubleshooting"},{"location":"product/administration/install.html#setting-up-dns-entries","text":"Once the installer has completed, you should see output similar to the following, providing you with the DNS address of the load balancer along with the Launcher container image URL: 1 2 3 4 5 6 Installation info: * Load balancer DNS: spacelift-server-1234567890.eu-west-1.elb.amazonaws.com * Launcher container image: 123456789012 .dkr.ecr.eu-west-1.amazonaws.com/spacelift-launcher:v0.0.6 [ 2023 -01-24T11:30:59+0000 ] INFO: Spacelift version v0.0.6 has been successfully installed! Please use the Load balancer DNS to setup a CNAME entry or an A record using an alias if using a hosted zone in the same AWS account as your Spacelift installation. This entry should point from the hostname you want to use for Spacelift (e.g. spacelift.saturnhead.io ) to the Spacelift Load Balancer. Once your DNS changes propagate, you should be able to login to your instance by navigating to its hostname (for example https://spacelift.saturnhead.io ). Assuming everything has been successful, you should see a welcome screen similar to the following:","title":"Setting up DNS entries"},{"location":"product/administration/install.html#creating-your-first-worker-pool","text":"Before you can create stacks or trigger any runs, you need a worker pool. For more information please see our worker pools page.","title":"Creating your first worker pool"},{"location":"product/administration/install.html#updating-existing-sso-configuration","text":"If you already have an existing SSO configuration and want to update it, you need to update the sso_config section of the configuration file and run the update script: 1 ./scripts/update-sso-settings.sh [ -c \"<configuration file>\" ] It will run the ECS task that will update the SSO configuration.","title":"Updating existing SSO configuration"},{"location":"product/administration/install.html#upgrading","text":"To upgrade to the latest version of self-hosting, follow these steps: Make sure your config.json file is fully configured to match your existing installation. Ideally you should use the config file from your previous installation. Run ./install.sh . Restart any workers connected to your Spacelift installation to make sure they're running the latest version.","title":"Upgrading"},{"location":"product/administration/install.html#uninstalling","text":"If you want to completely uninstall Spacelift, you can use the uninstall.sh script. By default, the script will retain S3 buckets, database and KMS keys so that they can be restored later. To run the uninstall script, use the following command, specifying your AWS region: 1 ./uninstall.sh [ -c <config-file> ] [ -f | -n | -h ] Flags: -c <config-file> : path to the config file (default: config.json ) -f : force uninstallation, do not prompt for confirmation -n : do not retain S3 buckets, database or KMS keys. complete uninstallation. -h : show help For example: 1 ./uninstall.sh","title":"Uninstalling"},{"location":"product/administration/slack-integration-setup.html","text":"Slack integration setup \u00bb If you want to use the Slack integration in your self-hosting instance, you need to create your own Slack app and add its details to the config.json file before running the self-hosting installer. This section explains how to do that, along with describing limitations of the Slack integration in self-hosting. Known Limitations \u00bb The Slack integration relies on Slack being able to communicate with Spacelift in order to provide support for Slash commands . This means that Slack must be able to access your Spacelift load balancer in order for it to be able to make requests to https://<your-spacelift-domain>/webhooks/slack . This means that if you are using an internal load balancer for Spacelift, Slack will not be able to access this endpoint. Creating your Slack app \u00bb First, create a new Slack app in your workspace by navigating to https://api.slack.com/apps and following these instructions: Click \"Create New App\" Choose \"From an app manifest\" Select your workspace Paste following manifest, replacing <your-domain> with the domain you want to host the self-hosted Spacelift instance on. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 { \"display_information\" : { \"name\" : \"Spacelift\" , \"description\" : \"Taking your infra-as-code to the next level\" , \"background_color\" : \"#131417\" , \"long_description\" : \"Spacelift is a sophisticated and compliant infrastructure delivery platform for Terraform (including Terragrunt), Pulumi, CloudFormation, Ansible, and Kubernetes.\\r\\n\\r\\n\u2022 No lock-in. Under the hood, Spacelift uses your choice of Infrastructure as Code providers: open-source projects with vibrant ecosystems and a multitude of existing providers, modules, and tutorials.\\r\\n\\r\\n\u2022 Works with your Git flow. Spacelift integrates with GitHub (and other VCSes) to provide feedback on commits and Pull Requests, allowing you and your team to preview the changes before they are applied.\\r\\n\\r\\n\u2022 Drift detection. Spacelift natively detects drift, and can optionally revert it, to provide visibility and awareness to those \\\"changes\\\" that will inevitably happen.\\r\\n\\r\\n\u2022 Policy as a Code. With Open Policy Agent (OPA) Rego, you can programmatically define policies, approval flows, and various decision points within your Infrastructure as Code flow.\\r\\n\\r\\n\u2022 Customize your runtime. Spacelift uses Docker to run its workflows, which allows you to fully control your execution environment.\\r\\n\\r\\n\u2022 Share config using contexts. Spacelift contexts are collections of configuration files and environment variables that can be attached to multiple stacks.\\r\\n\\r\\n\u2022 Look ma, no credentials. Spacelift integrates with identity management systems from major cloud providers; AWS, Azure, and Google Cloud, allowing you to set up limited temporary access to your resources without the need to supply powerful static credentials.\\r\\n\\r\\n\u2022 Manage programmatically. With the Terraform provider, you can manage Spacelift resources as code.\\r\\n\\r\\n\u2022 Protect your state. Spacelift supports a sophisticated state backend and can optionally manage the state on your behalf.\" }, \"features\" : { \"bot_user\" : { \"display_name\" : \"Spacelift\" , \"always_online\" : true }, \"slash_commands\" : [ { \"command\" : \"/spacelift\" , \"url\" : \"https://<your-domain>/webhooks/slack\" , \"description\" : \"Get notified about Spacelift events\" , \"usage_hint\" : \"subscribe, unsubscribe or help\" , \"should_escape\" : false } ] }, \"oauth_config\" : { \"redirect_urls\" : [ \"https://<your-domain>/slack_oauth\" ], \"scopes\" : { \"bot\" : [ \"channels:read\" , \"chat:write\" , \"chat:write.public\" , \"commands\" , \"links:write\" , \"team:read\" , \"users:read\" ] } }, \"settings\" : { \"event_subscriptions\" : { \"request_url\" : \"https://<your-domain>/webhooks/slack\" , \"bot_events\" : [ \"app_uninstalled\" ] }, \"interactivity\" : { \"is_enabled\" : true , \"request_url\" : \"https://<your-domain>/webhooks/slack\" }, \"org_deploy_enabled\" : false , \"socket_mode_enabled\" : false , \"token_rotation_enabled\" : false } } Configuring the Spacelift installer \u00bb Once it's done, you just need to copy the relevant information from the \"Basic Information\" tab of your Slack App to the configuration file: 1 2 3 4 5 6 7 8 { \"slack_config\" : { \"enabled\" : true , \"client_id\" : \"<client id here>\" , \"client_secret\" : \"<client secret here>\" , \"signing_secret\" : \"<signing secret here>\" } } Once you have populated your configuration, just run the installer as described in the installation guide . After the installation script finishes. You can now go to your selfhosted Spacelift instance, to Settings -> Slack and install the Slack app into your workspace.","title":"Slack integration setup"},{"location":"product/administration/slack-integration-setup.html#slack-integration-setup","text":"If you want to use the Slack integration in your self-hosting instance, you need to create your own Slack app and add its details to the config.json file before running the self-hosting installer. This section explains how to do that, along with describing limitations of the Slack integration in self-hosting.","title":"Slack integration setup"},{"location":"product/administration/slack-integration-setup.html#known-limitations","text":"The Slack integration relies on Slack being able to communicate with Spacelift in order to provide support for Slash commands . This means that Slack must be able to access your Spacelift load balancer in order for it to be able to make requests to https://<your-spacelift-domain>/webhooks/slack . This means that if you are using an internal load balancer for Spacelift, Slack will not be able to access this endpoint.","title":"Known Limitations"},{"location":"product/administration/slack-integration-setup.html#creating-your-slack-app","text":"First, create a new Slack app in your workspace by navigating to https://api.slack.com/apps and following these instructions: Click \"Create New App\" Choose \"From an app manifest\" Select your workspace Paste following manifest, replacing <your-domain> with the domain you want to host the self-hosted Spacelift instance on. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 { \"display_information\" : { \"name\" : \"Spacelift\" , \"description\" : \"Taking your infra-as-code to the next level\" , \"background_color\" : \"#131417\" , \"long_description\" : \"Spacelift is a sophisticated and compliant infrastructure delivery platform for Terraform (including Terragrunt), Pulumi, CloudFormation, Ansible, and Kubernetes.\\r\\n\\r\\n\u2022 No lock-in. Under the hood, Spacelift uses your choice of Infrastructure as Code providers: open-source projects with vibrant ecosystems and a multitude of existing providers, modules, and tutorials.\\r\\n\\r\\n\u2022 Works with your Git flow. Spacelift integrates with GitHub (and other VCSes) to provide feedback on commits and Pull Requests, allowing you and your team to preview the changes before they are applied.\\r\\n\\r\\n\u2022 Drift detection. Spacelift natively detects drift, and can optionally revert it, to provide visibility and awareness to those \\\"changes\\\" that will inevitably happen.\\r\\n\\r\\n\u2022 Policy as a Code. With Open Policy Agent (OPA) Rego, you can programmatically define policies, approval flows, and various decision points within your Infrastructure as Code flow.\\r\\n\\r\\n\u2022 Customize your runtime. Spacelift uses Docker to run its workflows, which allows you to fully control your execution environment.\\r\\n\\r\\n\u2022 Share config using contexts. Spacelift contexts are collections of configuration files and environment variables that can be attached to multiple stacks.\\r\\n\\r\\n\u2022 Look ma, no credentials. Spacelift integrates with identity management systems from major cloud providers; AWS, Azure, and Google Cloud, allowing you to set up limited temporary access to your resources without the need to supply powerful static credentials.\\r\\n\\r\\n\u2022 Manage programmatically. With the Terraform provider, you can manage Spacelift resources as code.\\r\\n\\r\\n\u2022 Protect your state. Spacelift supports a sophisticated state backend and can optionally manage the state on your behalf.\" }, \"features\" : { \"bot_user\" : { \"display_name\" : \"Spacelift\" , \"always_online\" : true }, \"slash_commands\" : [ { \"command\" : \"/spacelift\" , \"url\" : \"https://<your-domain>/webhooks/slack\" , \"description\" : \"Get notified about Spacelift events\" , \"usage_hint\" : \"subscribe, unsubscribe or help\" , \"should_escape\" : false } ] }, \"oauth_config\" : { \"redirect_urls\" : [ \"https://<your-domain>/slack_oauth\" ], \"scopes\" : { \"bot\" : [ \"channels:read\" , \"chat:write\" , \"chat:write.public\" , \"commands\" , \"links:write\" , \"team:read\" , \"users:read\" ] } }, \"settings\" : { \"event_subscriptions\" : { \"request_url\" : \"https://<your-domain>/webhooks/slack\" , \"bot_events\" : [ \"app_uninstalled\" ] }, \"interactivity\" : { \"is_enabled\" : true , \"request_url\" : \"https://<your-domain>/webhooks/slack\" }, \"org_deploy_enabled\" : false , \"socket_mode_enabled\" : false , \"token_rotation_enabled\" : false } }","title":"Creating your Slack app"},{"location":"product/administration/slack-integration-setup.html#configuring-the-spacelift-installer","text":"Once it's done, you just need to copy the relevant information from the \"Basic Information\" tab of your Slack App to the configuration file: 1 2 3 4 5 6 7 8 { \"slack_config\" : { \"enabled\" : true , \"client_id\" : \"<client id here>\" , \"client_secret\" : \"<client secret here>\" , \"signing_secret\" : \"<signing secret here>\" } } Once you have populated your configuration, just run the installer as described in the installation guide . After the installation script finishes. You can now go to your selfhosted Spacelift instance, to Settings -> Slack and install the Slack app into your workspace.","title":"Configuring the Spacelift installer"},{"location":"product/support/index.html","text":"Support \u00bb Spacelift offers a variety of support options depending on your needs. You should be able to find help using the resources linked below, regardless of how you use Spacelift. Have you tried\u2026 \u00bb Before reaching out for support, have you tried: Searching our documentation . Most answers can be found there. Reviewing the Scope of Support section to understand what is within the scope of Spacelift support. Contacting Support \u00bb For technical questions related to the Spacelift product, open a support ticket in the shared Slack channel (preferred, when available), or email support@spacelift.io if using Slack is not possible. Questions related to billing, purchasing, or subscriptions should be sent to ar@spacelift.io . Support SLA \u00bb The SLA times listed below are the timeframes in which you can expect the first response. Spacelift will make the best effort to resolve any issues to your satisfaction as quickly as possible. However, the SLA times are not to be considered as an expected time-to-resolution. Severity First Response Time Working Hours Critical 1 hour 24x7 Major 8 hours 4 am - 8 pm (ET), Mon - Fri Minor 48 hours 4 am - 8 pm (ET), Mon - Fri General Guidance 72 hours 4 am - 8 pm (ET), Mon - Fri Spacelift has support engineers in Europe and in the US. They observe local holidays, so the working hours might change on those days. Definitions of Severity Level \u00bb Severity 1 - Critical : A critical incident with very high impact (e.g., A customer-facing service is down for all customers). Severity 2 - Major : A major incident with significant impact. (e.g., A customer-facing service is down for a sub-set of customers). Severity 3 - Minor : A minor incident with low impact: Spacelift use has a minor loss of operational functionality, regardless of the environment or usage (e.g., A system bug creates a minor inconvenience to customers). Important Spacelift features are unavailable or somewhat slowed, but a workaround is available. Severity 4 - General Guidance : Implementation or production use of Spacelift is continuing, and work is not impeded (e.g., Information, an enhancement, or documentation clarification is requested, but there is no impact on the operation of Spacelift). Severity is assessed by Spacelift engineers based on the information at their disposal. Make sure to clearly and thoroughly communicate the extent and impact of an incident when reaching out to support to ensure it gets assigned the appropriate severity. Scope of Support \u00bb The scope of support, in the simplest terms, is what we support and what we do not. Ideally, we would support everything. However, without reducing the quality of our support or increasing the price of our product, this would be impossible. These \"limitations\" help create a more consistent and efficient support experience. Please understand that any support that might be offered beyond the scope defined here is done at the discretion of the Support Engineer and is provided as a courtesy. Spacelift Features and Adjacent Technologies \u00bb Of course, we provide support for all Spacelift features, but also for adjacent parts of the third parties we integrate with. Here are some examples of what is in scope and what is not for some of the technologies we support: Technology In Scope Out of Scope Cloud Provider Helping configure the permissions used with a Cloud Integration Helping architecture a cloud account IaC Tool Helping troubleshoot a failed deployment Helping architecture your source code VCS Provider Helping troubleshoot events not triggering Spacelift runs Advising how to best configure a VCS provider repository Requirements \u00bb Spacelift cannot provide training on the use of the underlying technologies that Spacelift integrates with. Spacelift is a product aimed at technical users, and we expect our users to be versed in the basic usage of the technologies related to features that they seek support for. For example, a customer looking for help with a Kubernetes integration should understand Kubernetes to the extent that they can retrieve log files or perform other essential tasks without in-depth instruction. Feature Preview \u00bb Alpha Features \u00bb Alpha features are not yet thoroughly tested for quality and stability, may contain bugs or errors, and be prone to see breaking changes in the future. You should not depend on them, and the functionality is subject to change. As such, support is provided on a best-effort basis. Beta Features \u00bb We provide support for Beta features on a commercially-reasonable effort basis. Because they are not yet thoroughly tested for quality and stability, we may not yet have identified all the corner cases and may be prone to see breaking changes in the future. Also, troubleshooting might require more time and assistance from the Engineering team.","title":"Support"},{"location":"product/support/index.html#support","text":"Spacelift offers a variety of support options depending on your needs. You should be able to find help using the resources linked below, regardless of how you use Spacelift.","title":"Support"},{"location":"product/support/index.html#have-you-tried","text":"Before reaching out for support, have you tried: Searching our documentation . Most answers can be found there. Reviewing the Scope of Support section to understand what is within the scope of Spacelift support.","title":"Have you tried\u2026"},{"location":"product/support/index.html#contacting-support","text":"For technical questions related to the Spacelift product, open a support ticket in the shared Slack channel (preferred, when available), or email support@spacelift.io if using Slack is not possible. Questions related to billing, purchasing, or subscriptions should be sent to ar@spacelift.io .","title":"Contacting Support"},{"location":"product/support/index.html#support-sla","text":"The SLA times listed below are the timeframes in which you can expect the first response. Spacelift will make the best effort to resolve any issues to your satisfaction as quickly as possible. However, the SLA times are not to be considered as an expected time-to-resolution. Severity First Response Time Working Hours Critical 1 hour 24x7 Major 8 hours 4 am - 8 pm (ET), Mon - Fri Minor 48 hours 4 am - 8 pm (ET), Mon - Fri General Guidance 72 hours 4 am - 8 pm (ET), Mon - Fri Spacelift has support engineers in Europe and in the US. They observe local holidays, so the working hours might change on those days.","title":"Support SLA"},{"location":"product/support/index.html#definitions-of-severity-level","text":"Severity 1 - Critical : A critical incident with very high impact (e.g., A customer-facing service is down for all customers). Severity 2 - Major : A major incident with significant impact. (e.g., A customer-facing service is down for a sub-set of customers). Severity 3 - Minor : A minor incident with low impact: Spacelift use has a minor loss of operational functionality, regardless of the environment or usage (e.g., A system bug creates a minor inconvenience to customers). Important Spacelift features are unavailable or somewhat slowed, but a workaround is available. Severity 4 - General Guidance : Implementation or production use of Spacelift is continuing, and work is not impeded (e.g., Information, an enhancement, or documentation clarification is requested, but there is no impact on the operation of Spacelift). Severity is assessed by Spacelift engineers based on the information at their disposal. Make sure to clearly and thoroughly communicate the extent and impact of an incident when reaching out to support to ensure it gets assigned the appropriate severity.","title":"Definitions of Severity Level"},{"location":"product/support/index.html#scope-of-support","text":"The scope of support, in the simplest terms, is what we support and what we do not. Ideally, we would support everything. However, without reducing the quality of our support or increasing the price of our product, this would be impossible. These \"limitations\" help create a more consistent and efficient support experience. Please understand that any support that might be offered beyond the scope defined here is done at the discretion of the Support Engineer and is provided as a courtesy.","title":"Scope of Support"},{"location":"product/support/index.html#spacelift-features-and-adjacent-technologies","text":"Of course, we provide support for all Spacelift features, but also for adjacent parts of the third parties we integrate with. Here are some examples of what is in scope and what is not for some of the technologies we support: Technology In Scope Out of Scope Cloud Provider Helping configure the permissions used with a Cloud Integration Helping architecture a cloud account IaC Tool Helping troubleshoot a failed deployment Helping architecture your source code VCS Provider Helping troubleshoot events not triggering Spacelift runs Advising how to best configure a VCS provider repository","title":"Spacelift Features and Adjacent Technologies"},{"location":"product/support/index.html#requirements","text":"Spacelift cannot provide training on the use of the underlying technologies that Spacelift integrates with. Spacelift is a product aimed at technical users, and we expect our users to be versed in the basic usage of the technologies related to features that they seek support for. For example, a customer looking for help with a Kubernetes integration should understand Kubernetes to the extent that they can retrieve log files or perform other essential tasks without in-depth instruction.","title":"Requirements"},{"location":"product/support/index.html#feature-preview","text":"","title":"Feature Preview"},{"location":"product/support/index.html#alpha-features","text":"Alpha features are not yet thoroughly tested for quality and stability, may contain bugs or errors, and be prone to see breaking changes in the future. You should not depend on them, and the functionality is subject to change. As such, support is provided on a best-effort basis.","title":"Alpha Features"},{"location":"product/support/index.html#beta-features","text":"We provide support for Beta features on a commercially-reasonable effort basis. Because they are not yet thoroughly tested for quality and stability, we may not yet have identified all the corner cases and may be prone to see breaking changes in the future. Also, troubleshooting might require more time and assistance from the Engineering team.","title":"Beta Features"},{"location":"vendors/ansible/index.html","text":"Ansible \u00bb You can find more details in the subpages: Getting Started Reference Using Policies with Ansible stacks Ansible Galaxy Why use Ansible? \u00bb Ansible is a versatile and battle-tested infrastructure configuration tool. It can do anything from software provisioning to configuration management and application deployment. You can tap into the wealth of roles, playbooks, and collections available online to get started in no time. Why use Spacelift with Ansible? \u00bb Spacelift helps you manage the complexities and compliance challenges of using Ansible. It brings with it a GitOps flow, so your infrastructure repository is synced with your Ansible Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies , which lets you automate compliance checks and build complex multi-stack workflows . You can also use Spacelift to mix and match Terraform, Pulumi, AWS CloudFormation, Kubernetes, and Ansible Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like a set of AWS EC2 instances with all their dependencies) and then connect that to an Ansible Stack which then transactionally configures these EC2 instances using trigger policies .","title":"Ansible"},{"location":"vendors/ansible/index.html#ansible","text":"You can find more details in the subpages: Getting Started Reference Using Policies with Ansible stacks Ansible Galaxy","title":"Ansible"},{"location":"vendors/ansible/index.html#why-use-ansible","text":"Ansible is a versatile and battle-tested infrastructure configuration tool. It can do anything from software provisioning to configuration management and application deployment. You can tap into the wealth of roles, playbooks, and collections available online to get started in no time.","title":"Why use Ansible?"},{"location":"vendors/ansible/index.html#why-use-spacelift-with-ansible","text":"Spacelift helps you manage the complexities and compliance challenges of using Ansible. It brings with it a GitOps flow, so your infrastructure repository is synced with your Ansible Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies , which lets you automate compliance checks and build complex multi-stack workflows . You can also use Spacelift to mix and match Terraform, Pulumi, AWS CloudFormation, Kubernetes, and Ansible Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like a set of AWS EC2 instances with all their dependencies) and then connect that to an Ansible Stack which then transactionally configures these EC2 instances using trigger policies .","title":"Why use Spacelift with Ansible?"},{"location":"vendors/ansible/ansible-galaxy.html","text":"Ansible Galaxy \u00bb If you followed previous examples in our Ansible documentation, you might have noticed that we do not do much in the Initialization phase. If it comes to Ansible stacks, during that phase we try to auto-detect the requirements.yml file that will be used to install dependencies. We will look for it in the following locations: requirements.yml in the root directory roles/requirements.yml for roles requirements collections/requirements.yml for collections requirements Tip We also check for the alternative .yaml extension for the paths listed above. As an example, try using an example requirements.yml file. Example requirements.yml file 1 2 3 4 --- collections : - name : community.grafana version : 1.3.1 After our Initialization phase detects this file, it will use Ansible Galaxy to install those dependencies.","title":"Ansible Galaxy"},{"location":"vendors/ansible/ansible-galaxy.html#ansible-galaxy","text":"If you followed previous examples in our Ansible documentation, you might have noticed that we do not do much in the Initialization phase. If it comes to Ansible stacks, during that phase we try to auto-detect the requirements.yml file that will be used to install dependencies. We will look for it in the following locations: requirements.yml in the root directory roles/requirements.yml for roles requirements collections/requirements.yml for collections requirements Tip We also check for the alternative .yaml extension for the paths listed above. As an example, try using an example requirements.yml file. Example requirements.yml file 1 2 3 4 --- collections : - name : community.grafana version : 1.3.1 After our Initialization phase detects this file, it will use Ansible Galaxy to install those dependencies.","title":"Ansible Galaxy"},{"location":"vendors/ansible/getting-started.html","text":"Getting Started \u00bb Prerequisites \u00bb To follow this tutorial, you should have an EC2 instance together with an SSH private key that can be used to access the instance ready. Initial Setup \u00bb Start by forking our Ansible example repository Looking at the code, you'll find that it configures a simple Apache HTTP Server on an EC2 instance. We are also using the AWS EC2 inventory plugin to find the hosts to configure. Feel free to modify aws_ec2.yml inventory file to fit your needs. Also, please take notice of the Spacelift runtime config file , that defines the runner image used on this stack and the ANSIBLE_CONFIG environment variable. Remember you can always define runner image in stack settings , and environment variables within environment settings . Creating a stack \u00bb In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift. In the first tab, you should select the repository you've just forked, as can be seen in the picture. In the next tab, you should choose the Ansible backend. There, fill in the Playbook field with the playbook you want to run. In the case of our example, it is playbook.yml . You could also configure an option to skip the planning phase , but we will leave that disabled for now. On the next tab ( Define Behavior ), setting up a runner image with Ansible dependencies is required. You may use your image (with the Ansible version you choose and all the required dependencies) or use one of our default ones . In this example we are using an AWS-based runner image defined in the runtime configuration file : public.ecr.aws/spacelift/runner-ansible-aws:latest . If you have a private worker pool you'd like to use, you can specify it there instead of the default public one as well. Finally, choose a name for your Spacelift Stack on the last page. We'll use Ansible Example again. Triggering the Stack \u00bb Making sure the inventory plugin works \u00bb You can now click Trigger to create a new Spacelift Run. You should see the run finishing with no hosts matched. This is because the AWS EC2 inventory plugin did not detect valid AWS credentials. Info You need to configure the AWS integration to give Spacelift access to your AWS account. You can find the details here . Configuring SSH keys \u00bb After triggering a run again, you will see we could successfully find EC2 hosts (provided they could be localized with aws_ec2.yml inventory file filters), but we cannot connect to them using SSH. The reason for that is we did not configure SSH keys yet. Let's configure the correct credentials using the Environment . Go to the Environment tab and add the private key that can be used to access the machine as a secret mounted file. You should also specify the location of the SSH private key for Ansible, and you can do that using the ANSIBLE_PRIVATE_KEY_FILE environment variable. Investigating planned changes \u00bb Triggering a run again, you should successfully see it get through the planning phase and end up in the unconfirmed state. In the plan, you can see detailed information about each resource that is supposed to be created. At this point, you can investigate the changes Ansible playbook will apply and to which hosts. When you're happy with the planned changes, click Confirm to apply them. By now, the machine should be configured with a simple Apache HTTP server with a sample website on port 8000. You can switch to the Resources tab to see the hosts you have configured, together with the history of when the host was last created and updated. Conclusion \u00bb That's it! You can find more details about the available configuration settings in the reference . Apart from configuration options available within Spacelift remember you can configure Ansible however you'd like using native Ansible configuration capabilities.","title":"Getting Started"},{"location":"vendors/ansible/getting-started.html#getting-started","text":"","title":"Getting Started"},{"location":"vendors/ansible/getting-started.html#prerequisites","text":"To follow this tutorial, you should have an EC2 instance together with an SSH private key that can be used to access the instance ready.","title":"Prerequisites"},{"location":"vendors/ansible/getting-started.html#initial-setup","text":"Start by forking our Ansible example repository Looking at the code, you'll find that it configures a simple Apache HTTP Server on an EC2 instance. We are also using the AWS EC2 inventory plugin to find the hosts to configure. Feel free to modify aws_ec2.yml inventory file to fit your needs. Also, please take notice of the Spacelift runtime config file , that defines the runner image used on this stack and the ANSIBLE_CONFIG environment variable. Remember you can always define runner image in stack settings , and environment variables within environment settings .","title":"Initial Setup"},{"location":"vendors/ansible/getting-started.html#creating-a-stack","text":"In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift. In the first tab, you should select the repository you've just forked, as can be seen in the picture. In the next tab, you should choose the Ansible backend. There, fill in the Playbook field with the playbook you want to run. In the case of our example, it is playbook.yml . You could also configure an option to skip the planning phase , but we will leave that disabled for now. On the next tab ( Define Behavior ), setting up a runner image with Ansible dependencies is required. You may use your image (with the Ansible version you choose and all the required dependencies) or use one of our default ones . In this example we are using an AWS-based runner image defined in the runtime configuration file : public.ecr.aws/spacelift/runner-ansible-aws:latest . If you have a private worker pool you'd like to use, you can specify it there instead of the default public one as well. Finally, choose a name for your Spacelift Stack on the last page. We'll use Ansible Example again.","title":"Creating a stack"},{"location":"vendors/ansible/getting-started.html#triggering-the-stack","text":"","title":"Triggering the Stack"},{"location":"vendors/ansible/getting-started.html#making-sure-the-inventory-plugin-works","text":"You can now click Trigger to create a new Spacelift Run. You should see the run finishing with no hosts matched. This is because the AWS EC2 inventory plugin did not detect valid AWS credentials. Info You need to configure the AWS integration to give Spacelift access to your AWS account. You can find the details here .","title":"Making sure the inventory plugin works"},{"location":"vendors/ansible/getting-started.html#configuring-ssh-keys","text":"After triggering a run again, you will see we could successfully find EC2 hosts (provided they could be localized with aws_ec2.yml inventory file filters), but we cannot connect to them using SSH. The reason for that is we did not configure SSH keys yet. Let's configure the correct credentials using the Environment . Go to the Environment tab and add the private key that can be used to access the machine as a secret mounted file. You should also specify the location of the SSH private key for Ansible, and you can do that using the ANSIBLE_PRIVATE_KEY_FILE environment variable.","title":"Configuring SSH keys"},{"location":"vendors/ansible/getting-started.html#investigating-planned-changes","text":"Triggering a run again, you should successfully see it get through the planning phase and end up in the unconfirmed state. In the plan, you can see detailed information about each resource that is supposed to be created. At this point, you can investigate the changes Ansible playbook will apply and to which hosts. When you're happy with the planned changes, click Confirm to apply them. By now, the machine should be configured with a simple Apache HTTP server with a sample website on port 8000. You can switch to the Resources tab to see the hosts you have configured, together with the history of when the host was last created and updated.","title":"Investigating planned changes"},{"location":"vendors/ansible/getting-started.html#conclusion","text":"That's it! You can find more details about the available configuration settings in the reference . Apart from configuration options available within Spacelift remember you can configure Ansible however you'd like using native Ansible configuration capabilities.","title":"Conclusion"},{"location":"vendors/ansible/policies.html","text":"Spacelift Policies with Ansible \u00bb Plan Policy with Ansible \u00bb Using our native Plan Policy you could implement advanced handling of different situations that can happen while running playbooks on multiple hosts. One good example is if you'd like to manually review situations when some of the hosts were unreachable, but still be able to apply the change regardless. You could test it using the following Plan Policy: 1 2 3 4 5 6 7 package spacelift warn [ \"Some hosts were unreachable\" ] { input . ansible . dark != {} } sample { true } Once you attach the above Plan Policy to an Ansible stack that is configured in a way not to fail when finding unreachable hosts, you could automatically detect unreachable hosts using a Plan Policy and require approval using the Approval Policy . Please find an example policy evaluation below: Linking Terraform and Ansible workflows \u00bb You can use our Trigger Policy to link multiple stacks together. This applies also to stacks from different vendors. One of the use cases is to link Terraform and Ansible workflows so that you could use Ansible to configure EC2 instances you've just created using Terraform. We provide an extensive example of one way to set something like this up in our Terraform-Ansible workflow demo repository .","title":"Spacelift Policies with Ansible"},{"location":"vendors/ansible/policies.html#spacelift-policies-with-ansible","text":"","title":"Spacelift Policies with Ansible"},{"location":"vendors/ansible/policies.html#plan-policy-with-ansible","text":"Using our native Plan Policy you could implement advanced handling of different situations that can happen while running playbooks on multiple hosts. One good example is if you'd like to manually review situations when some of the hosts were unreachable, but still be able to apply the change regardless. You could test it using the following Plan Policy: 1 2 3 4 5 6 7 package spacelift warn [ \"Some hosts were unreachable\" ] { input . ansible . dark != {} } sample { true } Once you attach the above Plan Policy to an Ansible stack that is configured in a way not to fail when finding unreachable hosts, you could automatically detect unreachable hosts using a Plan Policy and require approval using the Approval Policy . Please find an example policy evaluation below:","title":"Plan Policy with Ansible"},{"location":"vendors/ansible/policies.html#linking-terraform-and-ansible-workflows","text":"You can use our Trigger Policy to link multiple stacks together. This applies also to stacks from different vendors. One of the use cases is to link Terraform and Ansible workflows so that you could use Ansible to configure EC2 instances you've just created using Terraform. We provide an extensive example of one way to set something like this up in our Terraform-Ansible workflow demo repository .","title":"Linking Terraform and Ansible workflows"},{"location":"vendors/ansible/reference.html","text":"Reference \u00bb Stack Settings \u00bb Playbook - A playbook file to run on a stack. Skip Plan - Runs on Spacelift stacks typically have a planning and an applying phase. In Ansible for the planning phase we are running Ansible in check mode . However, not all Ansible modules support check mode and it can result in a run failure. You could configure your playbook to ignore certain errors (e.g. using ignore_errors: \"{{ ansible_check_mode }}\" ) or choose to skip the planning phase entirely (e.g. in situations when handling check failures at the playbook level is not an option). Other settings \u00bb For most of the settings below, there is usually more than one way to configure it (usually either through environment variables or through ansible.cfg file). More on Ansible configuration can be found in official Ansible docs . SSH private key location \u00bb If you want to use SSH to connect to your hosts you will need to provide a path to the SSH private key. You can do that using the ANSIBLE_PRIVATE_KEY_FILE environment variable. Forcing color mode for Ansible \u00bb By default, Ansible will not color the output when running without TTY. You could enable colored output using the ANSIBLE_FORCE_COLOR environment variable. Debugging Ansible runs \u00bb When running into issues with Ansible playbooks a good way to debug the runs is to increase the Ansible verbosity level using the ANSIBLE_VERBOSITY environment variable. Controlling SSH ControlPath parameter \u00bb Ansible uses ControlMaster and ControlPath SSH options to speed up playbook execution. On some occasions, you might want to modify default values to make them compatible with your execution environment. Depending on your exact setup, you might want to adjust some of the SSH settings Ansible uses. The default value for ANSIBLE_SSH_CONTROL_PATH_DIR is /tmp/.ansible/cp . File permissions \u00bb There are a few nuances with certain files' permissions when using Ansible. ansible.cfg \u00bb If you use ansible.cfg file within a repository (or - more generally - within the current working directory) make sure that permissions on that file (and parent directory) are set properly. You can find more details in official Ansible documentation in the section on avoiding security risks with ansible.cfg SSH private key files \u00bb If you are using SSH to connect to your hosts, then you need to make sure that private keys delivered to the worker have the correct permissions. As the ssh man page states: These files contain sensitive data and should be readable by the user but not accessible by others (read/write/execute). ssh will simply ignore a private key file if it is accessible by others. Typically, you would like to deliver private keys directly at the worker level where you can fully manage your environment. If that is not an option, you can always use our read-only mounted files or any other option you find suitable.","title":"Reference"},{"location":"vendors/ansible/reference.html#reference","text":"","title":"Reference"},{"location":"vendors/ansible/reference.html#stack-settings","text":"Playbook - A playbook file to run on a stack. Skip Plan - Runs on Spacelift stacks typically have a planning and an applying phase. In Ansible for the planning phase we are running Ansible in check mode . However, not all Ansible modules support check mode and it can result in a run failure. You could configure your playbook to ignore certain errors (e.g. using ignore_errors: \"{{ ansible_check_mode }}\" ) or choose to skip the planning phase entirely (e.g. in situations when handling check failures at the playbook level is not an option).","title":"Stack Settings"},{"location":"vendors/ansible/reference.html#other-settings","text":"For most of the settings below, there is usually more than one way to configure it (usually either through environment variables or through ansible.cfg file). More on Ansible configuration can be found in official Ansible docs .","title":"Other settings"},{"location":"vendors/ansible/reference.html#ssh-private-key-location","text":"If you want to use SSH to connect to your hosts you will need to provide a path to the SSH private key. You can do that using the ANSIBLE_PRIVATE_KEY_FILE environment variable.","title":"SSH private key location"},{"location":"vendors/ansible/reference.html#forcing-color-mode-for-ansible","text":"By default, Ansible will not color the output when running without TTY. You could enable colored output using the ANSIBLE_FORCE_COLOR environment variable.","title":"Forcing color mode for Ansible"},{"location":"vendors/ansible/reference.html#debugging-ansible-runs","text":"When running into issues with Ansible playbooks a good way to debug the runs is to increase the Ansible verbosity level using the ANSIBLE_VERBOSITY environment variable.","title":"Debugging Ansible runs"},{"location":"vendors/ansible/reference.html#controlling-ssh-controlpath-parameter","text":"Ansible uses ControlMaster and ControlPath SSH options to speed up playbook execution. On some occasions, you might want to modify default values to make them compatible with your execution environment. Depending on your exact setup, you might want to adjust some of the SSH settings Ansible uses. The default value for ANSIBLE_SSH_CONTROL_PATH_DIR is /tmp/.ansible/cp .","title":"Controlling SSH ControlPath parameter"},{"location":"vendors/ansible/reference.html#file-permissions","text":"There are a few nuances with certain files' permissions when using Ansible.","title":"File permissions"},{"location":"vendors/ansible/reference.html#ansiblecfg","text":"If you use ansible.cfg file within a repository (or - more generally - within the current working directory) make sure that permissions on that file (and parent directory) are set properly. You can find more details in official Ansible documentation in the section on avoiding security risks with ansible.cfg","title":"ansible.cfg"},{"location":"vendors/ansible/reference.html#ssh-private-key-files","text":"If you are using SSH to connect to your hosts, then you need to make sure that private keys delivered to the worker have the correct permissions. As the ssh man page states: These files contain sensitive data and should be readable by the user but not accessible by others (read/write/execute). ssh will simply ignore a private key file if it is accessible by others. Typically, you would like to deliver private keys directly at the worker level where you can fully manage your environment. If that is not an option, you can always use our read-only mounted files or any other option you find suitable.","title":"SSH private key files"},{"location":"vendors/cloudformation/index.html","text":"AWS CloudFormation \u00bb You can find more details in the subpages: Getting Started Reference Integrating with AWS Serverless Application Model (SAM) Integrating with the Serverless Framework Why use CloudFormation? \u00bb CloudFormation is an excellent Infrastructure-as-Code tool that supports transactional deploys (automatically rolling back on failure), has a rich construct library, and does not require separate state management like Terraform or Pulumi. Even if you don't want to write YAML/JSON files directly, there are multiple frameworks that let you write your CloudFormation config in more ergonomic, general-purpose languages. Why use Spacelift with CloudFormation? \u00bb Spacelift helps you manage the complexities and compliance challenges of using CloudFormation. It brings with it a GitOps flow, so your infrastructure repository is synced with your CloudFormation Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies , which lets you automate compliance checks and build complex multi-stack workflows . You can also use Spacelift to mix and match Terraform, Pulumi, and CloudFormation Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like an ECS/EKS cluster with all its dependencies) and then connect that to a CloudFormation Stack which then transactionally deploys your services there using trigger policies and the Spacelift provider run resources for workflow orchestration and Contexts to export Terraform outputs as CloudFormation input parameters. Does Spacelift support CloudFormation frameworks? \u00bb Yes! We support AWS CDK , AWS Serverless Application Model (SAM) , and the Serverless Framework . You can read more about it in the relevant subpages of this document.","title":"AWS CloudFormation"},{"location":"vendors/cloudformation/index.html#aws-cloudformation","text":"You can find more details in the subpages: Getting Started Reference Integrating with AWS Serverless Application Model (SAM) Integrating with the Serverless Framework","title":"AWS CloudFormation"},{"location":"vendors/cloudformation/index.html#why-use-cloudformation","text":"CloudFormation is an excellent Infrastructure-as-Code tool that supports transactional deploys (automatically rolling back on failure), has a rich construct library, and does not require separate state management like Terraform or Pulumi. Even if you don't want to write YAML/JSON files directly, there are multiple frameworks that let you write your CloudFormation config in more ergonomic, general-purpose languages.","title":"Why use CloudFormation?"},{"location":"vendors/cloudformation/index.html#why-use-spacelift-with-cloudformation","text":"Spacelift helps you manage the complexities and compliance challenges of using CloudFormation. It brings with it a GitOps flow, so your infrastructure repository is synced with your CloudFormation Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies , which lets you automate compliance checks and build complex multi-stack workflows . You can also use Spacelift to mix and match Terraform, Pulumi, and CloudFormation Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like an ECS/EKS cluster with all its dependencies) and then connect that to a CloudFormation Stack which then transactionally deploys your services there using trigger policies and the Spacelift provider run resources for workflow orchestration and Contexts to export Terraform outputs as CloudFormation input parameters.","title":"Why use Spacelift with CloudFormation?"},{"location":"vendors/cloudformation/index.html#does-spacelift-support-cloudformation-frameworks","text":"Yes! We support AWS CDK , AWS Serverless Application Model (SAM) , and the Serverless Framework . You can read more about it in the relevant subpages of this document.","title":"Does Spacelift support CloudFormation frameworks?"},{"location":"vendors/cloudformation/getting-started.html","text":"Getting Started \u00bb Initial Setup \u00bb Start by forking our AWS CloudFormation example repository Looking at the code, you'll find that it creates two simple Lambda Functions in nested Stacks and a common API Gateway REST API, which provides access to both of them. In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift. In the first screen, you should select the repository you've just forked, as can be seen in the picture. In the next screen, you should choose the CloudFormation backend. There, fill in the Region field with the AWS region you want to create the CloudFormation Stack in. You should also create an Amazon S3 bucket for template storage and provide its name in the Template Bucket field. We won't automatically create this bucket. The Entry Template File should be set to main.yaml (based on the code in our repository) and the Stack Name to a unique CloudFormation Stack name in your AWS account. We'll use cloudformation-example in the pictures. You can leave the settings on the next page ( Define Behavior) unchanged. If you have a private worker pool you'd like to use, specify it there instead of the default public one. Finally, choose a name for your Spacelift Stack on the last page. We'll use cloudformation-example again. You'll also have to configure the AWS integration to give Spacelift access to your AWS account. You can find the details here: AWS Deploying the Stack \u00bb You can now click Trigger to create a new Spacelift Run. And... oh no! It failed! However, the error message is quite straightforward. We're lacking the relevant capability . We can acknowledge this capability by setting the CF_CAPABILITY_IAM environment variable to 1 . There's a bunch of optional settings for CloudFormation Stacks we expose this way. You can read up on all of them in the reference . Triggering a run again, you should successfully see it get through the planning phase and end up in the unconfirmed state. In the plan, you can see detailed information about each resource that is supposed to be created. You can also click the ADD +10 tab to see a concise overview of the resources to be created. When you're happy with the planned changes, click Confirm to apply them. This will show you a feed of the creation and update events happening in the root Stack and all nested Stacks, which will stop after the creation finishes. Great! The resources have successfully been created. You can now switch to the Outputs tab and find the URLBase output. You can curl that URL with a hello1 or hello2 suffix to get responses from your Lambda Functions. You can also switch to the Resources tab to explore the resources you've created. Conclusion \u00bb That's it! You can find more details about the available configuration settings in the reference , or you can check out how to use AWS Serverless Application Model (SAM) or the Serverless Framework to generate your CloudFormation templates.","title":"Getting Started"},{"location":"vendors/cloudformation/getting-started.html#getting-started","text":"","title":"Getting Started"},{"location":"vendors/cloudformation/getting-started.html#initial-setup","text":"Start by forking our AWS CloudFormation example repository Looking at the code, you'll find that it creates two simple Lambda Functions in nested Stacks and a common API Gateway REST API, which provides access to both of them. In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift. In the first screen, you should select the repository you've just forked, as can be seen in the picture. In the next screen, you should choose the CloudFormation backend. There, fill in the Region field with the AWS region you want to create the CloudFormation Stack in. You should also create an Amazon S3 bucket for template storage and provide its name in the Template Bucket field. We won't automatically create this bucket. The Entry Template File should be set to main.yaml (based on the code in our repository) and the Stack Name to a unique CloudFormation Stack name in your AWS account. We'll use cloudformation-example in the pictures. You can leave the settings on the next page ( Define Behavior) unchanged. If you have a private worker pool you'd like to use, specify it there instead of the default public one. Finally, choose a name for your Spacelift Stack on the last page. We'll use cloudformation-example again. You'll also have to configure the AWS integration to give Spacelift access to your AWS account. You can find the details here: AWS","title":"Initial Setup"},{"location":"vendors/cloudformation/getting-started.html#deploying-the-stack","text":"You can now click Trigger to create a new Spacelift Run. And... oh no! It failed! However, the error message is quite straightforward. We're lacking the relevant capability . We can acknowledge this capability by setting the CF_CAPABILITY_IAM environment variable to 1 . There's a bunch of optional settings for CloudFormation Stacks we expose this way. You can read up on all of them in the reference . Triggering a run again, you should successfully see it get through the planning phase and end up in the unconfirmed state. In the plan, you can see detailed information about each resource that is supposed to be created. You can also click the ADD +10 tab to see a concise overview of the resources to be created. When you're happy with the planned changes, click Confirm to apply them. This will show you a feed of the creation and update events happening in the root Stack and all nested Stacks, which will stop after the creation finishes. Great! The resources have successfully been created. You can now switch to the Outputs tab and find the URLBase output. You can curl that URL with a hello1 or hello2 suffix to get responses from your Lambda Functions. You can also switch to the Resources tab to explore the resources you've created.","title":"Deploying the Stack"},{"location":"vendors/cloudformation/getting-started.html#conclusion","text":"That's it! You can find more details about the available configuration settings in the reference , or you can check out how to use AWS Serverless Application Model (SAM) or the Serverless Framework to generate your CloudFormation templates.","title":"Conclusion"},{"location":"vendors/cloudformation/integrating-with-cdk.html","text":"Integrating with AWS Cloud Development Kit (CDK) \u00bb To use AWS Cloud Development Kit in an AWS CloudFormation stack you'll need to do two things: create a Docker image with AWS CDK and invoke it in before_plan hooks. Preparing the image \u00bb Since our base image doesn't support AWS CDK, you will have to build your image that includes it as well as any tooling needed to run whatever language your infrastructure declaration is written in. The example below shows a Dockerfile with attached cdk and go : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM public.ecr.aws/spacelift/runner-terraform:latest USER root # Install packages RUN apk update && apk add --update --no-cache npm # Update NPM RUN npm update -g # Install cdk RUN npm install -g aws-cdk RUN cdk --version # Add Go COPY --from = golang:1.19-alpine /usr/local/go/ /usr/local/go/ ENV PATH = \"/usr/local/go/bin: ${ PATH } \" RUN go version You should build it, push it to a repository, and set it as the Runner Image of your Stack. Adding before_plan hooks \u00bb For the AWS CDK code to be properly interpreted by Spacelift, you have to customize the default stack workflow by enriching them with hooks . To create a CloudFormation template that can be interpreted by Spacelift, you will have to add these hooks to the before_plan stage: cdk bootstrap - to bootstrap your AWS CDK project. cdk synth - to create a CloudFormation template. Limitations \u00bb Multiple template AWS CDK definition \u00bb The default CloudFormation integration in Spacelift uses a single CloudFormation template. That means that AWS CDK definitions that generate multiple templates will only have a single template picked up for further processing. To mitigate this, consider unifying the AWS CDK definition to generate a single template file. Deploying Lambdas \u00bb Our integration doesn't use cdk deploy , but rather uses template definitions created by cdk synth . cdk deploy deploys Lambda assets to S3 which are used to deploy Lambdas by CloudFormation. Our process won't upload assets, so deploying Lambdas via a Spacelift stack configured to handle AWS CDK will result in errors.","title":"Integrating with AWS Cloud Development Kit (CDK)"},{"location":"vendors/cloudformation/integrating-with-cdk.html#integrating-with-aws-cloud-development-kit-cdk","text":"To use AWS Cloud Development Kit in an AWS CloudFormation stack you'll need to do two things: create a Docker image with AWS CDK and invoke it in before_plan hooks.","title":"Integrating with AWS Cloud Development Kit (CDK)"},{"location":"vendors/cloudformation/integrating-with-cdk.html#preparing-the-image","text":"Since our base image doesn't support AWS CDK, you will have to build your image that includes it as well as any tooling needed to run whatever language your infrastructure declaration is written in. The example below shows a Dockerfile with attached cdk and go : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM public.ecr.aws/spacelift/runner-terraform:latest USER root # Install packages RUN apk update && apk add --update --no-cache npm # Update NPM RUN npm update -g # Install cdk RUN npm install -g aws-cdk RUN cdk --version # Add Go COPY --from = golang:1.19-alpine /usr/local/go/ /usr/local/go/ ENV PATH = \"/usr/local/go/bin: ${ PATH } \" RUN go version You should build it, push it to a repository, and set it as the Runner Image of your Stack.","title":"Preparing the image"},{"location":"vendors/cloudformation/integrating-with-cdk.html#adding-before_plan-hooks","text":"For the AWS CDK code to be properly interpreted by Spacelift, you have to customize the default stack workflow by enriching them with hooks . To create a CloudFormation template that can be interpreted by Spacelift, you will have to add these hooks to the before_plan stage: cdk bootstrap - to bootstrap your AWS CDK project. cdk synth - to create a CloudFormation template.","title":"Adding before_plan hooks"},{"location":"vendors/cloudformation/integrating-with-cdk.html#limitations","text":"","title":"Limitations"},{"location":"vendors/cloudformation/integrating-with-cdk.html#multiple-template-aws-cdk-definition","text":"The default CloudFormation integration in Spacelift uses a single CloudFormation template. That means that AWS CDK definitions that generate multiple templates will only have a single template picked up for further processing. To mitigate this, consider unifying the AWS CDK definition to generate a single template file.","title":"Multiple template AWS CDK definition"},{"location":"vendors/cloudformation/integrating-with-cdk.html#deploying-lambdas","text":"Our integration doesn't use cdk deploy , but rather uses template definitions created by cdk synth . cdk deploy deploys Lambda assets to S3 which are used to deploy Lambdas by CloudFormation. Our process won't upload assets, so deploying Lambdas via a Spacelift stack configured to handle AWS CDK will result in errors.","title":"Deploying Lambdas"},{"location":"vendors/cloudformation/integrating-with-sam.html","text":"Integrating with AWS Serverless Application Model (SAM) \u00bb In order to use AWS Serverless Application Model (SAM) in an AWS CloudFormation Stack you'll need to do two things: create a Docker image with SAM included and invoke SAM in before_init hooks. The first one can be done using a Dockerfile akin to this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM public.ecr.aws/spacelift/runner-terraform USER root WORKDIR /home/spacelift RUN apk add --update --no-cache curl py-pip RUN apk -v --no-cache --update add \\ musl-dev \\ gcc \\ python3 \\ python3-dev RUN python3 -m ensurepip --upgrade \\ && pip3 install --upgrade pip RUN pip3 install --upgrade awscli aws-sam-cli RUN pip3 uninstall --yes pip \\ && apk del python3-dev gcc musl-dev RUN sam --version USER spacelift You should build it, push it to a repository and set it as the Runner Image of your Stack. You'll also have to invoke SAM in order to generate raw CloudFormation files and upload Lambda artifacts to S3. You can do this by adding the following to your before initialization hooks: 1 sam package --region ${ CF_METADATA_REGION } --s3-bucket ${ CF_METADATA_TEMPLATE_BUCKET } --s3-prefix sam-artifacts --output-template-file ${ CF_METADATA_ENTRY_TEMPLATE_FILE }","title":"Integrating with AWS Serverless Application Model (SAM)"},{"location":"vendors/cloudformation/integrating-with-sam.html#integrating-with-aws-serverless-application-model-sam","text":"In order to use AWS Serverless Application Model (SAM) in an AWS CloudFormation Stack you'll need to do two things: create a Docker image with SAM included and invoke SAM in before_init hooks. The first one can be done using a Dockerfile akin to this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM public.ecr.aws/spacelift/runner-terraform USER root WORKDIR /home/spacelift RUN apk add --update --no-cache curl py-pip RUN apk -v --no-cache --update add \\ musl-dev \\ gcc \\ python3 \\ python3-dev RUN python3 -m ensurepip --upgrade \\ && pip3 install --upgrade pip RUN pip3 install --upgrade awscli aws-sam-cli RUN pip3 uninstall --yes pip \\ && apk del python3-dev gcc musl-dev RUN sam --version USER spacelift You should build it, push it to a repository and set it as the Runner Image of your Stack. You'll also have to invoke SAM in order to generate raw CloudFormation files and upload Lambda artifacts to S3. You can do this by adding the following to your before initialization hooks: 1 sam package --region ${ CF_METADATA_REGION } --s3-bucket ${ CF_METADATA_TEMPLATE_BUCKET } --s3-prefix sam-artifacts --output-template-file ${ CF_METADATA_ENTRY_TEMPLATE_FILE }","title":"Integrating with AWS Serverless Application Model (SAM)"},{"location":"vendors/cloudformation/integrating-with-the-serverless-framework.html","text":"Integrating with the Serverless Framework \u00bb In order to use the Serverless Framework in an AWS CloudFormation Stack you'll need to do a few things: create a Docker image with the Serverless Framework included, invoke the serverless CLI in before_init hook, sync your artifacts with Amazon S3, and make sure the serverless config has your template bucket configured as the artifact location. The first one can be done using a Dockerfile akin to this one: 1 2 3 4 5 6 7 FROM public.ecr.aws/spacelift/runner-terraform USER root WORKDIR /home/spacelift RUN apk add --update --no-cache curl nodejs npm RUN npm install -g serverless RUN serverless --version USER spacelift You should build it, push it to a repository and set it as the Runner Image of your Stack. You'll also have to invoke the serverless CLI in order to generate raw CloudFormation files. You can do this by adding the following to your before initialization hooks: serverless package --region ${CF_METADATA_REGION} You can add the following script as a mounted file: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash set -eu set -o pipefail STATE_FILE = .serverless/serverless-state.json S3_PREFIX = $( jq -r '.package.artifactDirectoryName' < \" $STATE_FILE \" ) ARTIFACT = $( jq -r '.package.artifact' < \" $STATE_FILE \" ) aws s3 cp .serverless/ $ARTIFACT s3:// $CF_METADATA_TEMPLATE_BUCKET / $S3_PREFIX / $ARTIFACT and invoke it in your before initialization hooks: sh sync.sh Finally, specify the S3 bucket for artifacts in your serverless.yml configuration file: 1 2 provider : deploymentBucket : your-s3-bucket When creating the CloudFormation Stack, make sure that you set the entry template file to .serverless/cloudformation-template-update-stack.json , which is the file generated by the serverless package command.","title":"Integrating with the Serverless Framework"},{"location":"vendors/cloudformation/integrating-with-the-serverless-framework.html#integrating-with-the-serverless-framework","text":"In order to use the Serverless Framework in an AWS CloudFormation Stack you'll need to do a few things: create a Docker image with the Serverless Framework included, invoke the serverless CLI in before_init hook, sync your artifacts with Amazon S3, and make sure the serverless config has your template bucket configured as the artifact location. The first one can be done using a Dockerfile akin to this one: 1 2 3 4 5 6 7 FROM public.ecr.aws/spacelift/runner-terraform USER root WORKDIR /home/spacelift RUN apk add --update --no-cache curl nodejs npm RUN npm install -g serverless RUN serverless --version USER spacelift You should build it, push it to a repository and set it as the Runner Image of your Stack. You'll also have to invoke the serverless CLI in order to generate raw CloudFormation files. You can do this by adding the following to your before initialization hooks: serverless package --region ${CF_METADATA_REGION} You can add the following script as a mounted file: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash set -eu set -o pipefail STATE_FILE = .serverless/serverless-state.json S3_PREFIX = $( jq -r '.package.artifactDirectoryName' < \" $STATE_FILE \" ) ARTIFACT = $( jq -r '.package.artifact' < \" $STATE_FILE \" ) aws s3 cp .serverless/ $ARTIFACT s3:// $CF_METADATA_TEMPLATE_BUCKET / $S3_PREFIX / $ARTIFACT and invoke it in your before initialization hooks: sh sync.sh Finally, specify the S3 bucket for artifacts in your serverless.yml configuration file: 1 2 provider : deploymentBucket : your-s3-bucket When creating the CloudFormation Stack, make sure that you set the entry template file to .serverless/cloudformation-template-update-stack.json , which is the file generated by the serverless package command.","title":"Integrating with the Serverless Framework"},{"location":"vendors/cloudformation/reference.html","text":"Reference \u00bb Stack Settings \u00bb Region - AWS region in which to create and execute the AWS CloudFormation Stack. Stack Name - The name for the CloudFormation Stack controlled by this Spacelift Stack. Entry Template File - The path to the JSON or YAML file describing your root CloudFormation Stack. If you're generating CloudFormation code using a tool like AWS Serverless Application Model (SAM) or AWS Cloud Development Kit (AWS CDK), point this to the file containing the generated template. Template Bucket - Amazon S3 bucket to store CloudFormation templates in. Each created object will be prefixed by the current run ID like this: <run id>/artifact_name Special Environment Variables \u00bb CloudFormation Stack Parameters \u00bb Use this if your CloudFormation template requires parameters to be specified. Each environment variable of the form CF_PARAM_xyz will be interpreted as the value for the parameter xyz . For example, with this template snippet: 1 2 3 4 5 6 7 Parameters : InstanceTypeParameter : Type : String AllowedValues : - t2.micro - t2.small Description : Enter t2.micro or t2.small. In order to specify the InstanceTypeParameter add an environment variable to your Stack CF_PARAM_InstanceTypeParameter and set its value to i.e. t2.micro CloudFormation Stack Capabilities \u00bb Some functionalities available to CloudFormation Stacks need to be explicitly acknowledged using capabilities . You can configure capabilities in Spacelift using environment variables of the form CF_CAPABILITY_xyz and set them to 1. As of the time of writing this page, available capabilities are CF_CAPABILITY_IAM , CF_CAPABILITY_NAMED_IAM, and CF_CAPABILITY_AUTO_EXPAND . Detailed descriptions can be found in the AWS API documentation . Available Computed Environment Variables \u00bb To ease writing reusable scripts and hooks for your CloudFormation Stacks, the following environment variables are computed for each run: CF_METADATA_REGION , CF_METADATA_STACK_NAME , CF_METADATA_ENTRY_TEMPLATE_FILE , CF_METADATA_TEMPLATE_BUCKET . Their values are set to the respective Stack settings . Permissions \u00bb You need to provide Spacelift with access to your AWS account. You can either do this using the AWS Integration , provide ambient credentials on private workers, or pass environment variables directly.","title":"Reference"},{"location":"vendors/cloudformation/reference.html#reference","text":"","title":"Reference"},{"location":"vendors/cloudformation/reference.html#stack-settings","text":"Region - AWS region in which to create and execute the AWS CloudFormation Stack. Stack Name - The name for the CloudFormation Stack controlled by this Spacelift Stack. Entry Template File - The path to the JSON or YAML file describing your root CloudFormation Stack. If you're generating CloudFormation code using a tool like AWS Serverless Application Model (SAM) or AWS Cloud Development Kit (AWS CDK), point this to the file containing the generated template. Template Bucket - Amazon S3 bucket to store CloudFormation templates in. Each created object will be prefixed by the current run ID like this: <run id>/artifact_name","title":"Stack Settings"},{"location":"vendors/cloudformation/reference.html#special-environment-variables","text":"","title":"Special Environment Variables"},{"location":"vendors/cloudformation/reference.html#cloudformation-stack-parameters","text":"Use this if your CloudFormation template requires parameters to be specified. Each environment variable of the form CF_PARAM_xyz will be interpreted as the value for the parameter xyz . For example, with this template snippet: 1 2 3 4 5 6 7 Parameters : InstanceTypeParameter : Type : String AllowedValues : - t2.micro - t2.small Description : Enter t2.micro or t2.small. In order to specify the InstanceTypeParameter add an environment variable to your Stack CF_PARAM_InstanceTypeParameter and set its value to i.e. t2.micro","title":"CloudFormation Stack Parameters"},{"location":"vendors/cloudformation/reference.html#cloudformation-stack-capabilities","text":"Some functionalities available to CloudFormation Stacks need to be explicitly acknowledged using capabilities . You can configure capabilities in Spacelift using environment variables of the form CF_CAPABILITY_xyz and set them to 1. As of the time of writing this page, available capabilities are CF_CAPABILITY_IAM , CF_CAPABILITY_NAMED_IAM, and CF_CAPABILITY_AUTO_EXPAND . Detailed descriptions can be found in the AWS API documentation .","title":"CloudFormation Stack Capabilities"},{"location":"vendors/cloudformation/reference.html#available-computed-environment-variables","text":"To ease writing reusable scripts and hooks for your CloudFormation Stacks, the following environment variables are computed for each run: CF_METADATA_REGION , CF_METADATA_STACK_NAME , CF_METADATA_ENTRY_TEMPLATE_FILE , CF_METADATA_TEMPLATE_BUCKET . Their values are set to the respective Stack settings .","title":"Available Computed Environment Variables"},{"location":"vendors/cloudformation/reference.html#permissions","text":"You need to provide Spacelift with access to your AWS account. You can either do this using the AWS Integration , provide ambient credentials on private workers, or pass environment variables directly.","title":"Permissions"},{"location":"vendors/kubernetes/index.html","text":"Kubernetes \u00bb What is Kubernetes? \u00bb Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. For more information about Kubernetes, see the reference documentation . How does Spacelift work with Kubernetes? \u00bb Spacelift supports Kubernetes via kubectl . What is kubectl ? \u00bb The Kubernetes command-line tool, kubectl , allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. For more information including a complete list of kubectl operations, see the kubectl reference documentation . Why use Spacelift with Kubernetes? \u00bb Spacelift helps you manage the complexities and compliance challenges of using Kubernetes. It brings with it a GitOps flow, so your Kubernetes Deployments are synced with your Kubernetes Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies , which lets you automate compliance checks and build complex multi-stack workflows . You can also use Spacelift to mix and match Terraform, Pulumi, AWS CloudFormation, and Kubernetes Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision the required infrastructure (like an ECS/EKS cluster with all its dependencies) and then deploy the following via a Kubernetes Stack. Anything that can be run via kubectl can be run within a Spacelift stack. To find out more about Kubernetes Workload Resources, read the reference documentation .","title":"Kubernetes"},{"location":"vendors/kubernetes/index.html#kubernetes","text":"","title":"Kubernetes"},{"location":"vendors/kubernetes/index.html#what-is-kubernetes","text":"Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. For more information about Kubernetes, see the reference documentation .","title":"What is Kubernetes?"},{"location":"vendors/kubernetes/index.html#how-does-spacelift-work-with-kubernetes","text":"Spacelift supports Kubernetes via kubectl .","title":"How does Spacelift work with Kubernetes?"},{"location":"vendors/kubernetes/index.html#what-is-kubectl","text":"The Kubernetes command-line tool, kubectl , allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. For more information including a complete list of kubectl operations, see the kubectl reference documentation .","title":"What is kubectl?"},{"location":"vendors/kubernetes/index.html#why-use-spacelift-with-kubernetes","text":"Spacelift helps you manage the complexities and compliance challenges of using Kubernetes. It brings with it a GitOps flow, so your Kubernetes Deployments are synced with your Kubernetes Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies , which lets you automate compliance checks and build complex multi-stack workflows . You can also use Spacelift to mix and match Terraform, Pulumi, AWS CloudFormation, and Kubernetes Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision the required infrastructure (like an ECS/EKS cluster with all its dependencies) and then deploy the following via a Kubernetes Stack. Anything that can be run via kubectl can be run within a Spacelift stack. To find out more about Kubernetes Workload Resources, read the reference documentation .","title":"Why use Spacelift with Kubernetes?"},{"location":"vendors/kubernetes/authenticating.html","text":"Authenticating \u00bb The Kubernetes integration relies on using kubectl 's native authentication to connect to your cluster. You can use the $KUBECONFIG environment variable to find the location of the Kubernetes configuration file, and configure any credentials required. You should perform any custom authentication as part of a before init hook to make sure that kubectl is configured correctly before any commands are run in after init and subsequent hooks. The following sections provide examples of how to configure the integration manually, as well as using cloud provider-specific tooling. Manual Configuration \u00bb Manual configuration allows you to connect to any Kubernetes cluster accessible by your Spacelift workers, regardless of whether your cluster is on-prem or hosted by a cloud provider. The Kubernetes integration automatically sets the $KUBECONFIG environment variable to point at /mnt/workspace/.kube/config , giving you a number of options: You can use a mounted file to mount a pre-prepared config file into your workspace at /mnt/workspace/.kube/config . You can use a before init hook to create a kubeconfig file, or to download it from a trusted location. Please refer to the Kubernetes documentation for more information on configuring kubectl. AWS \u00bb The simplest way to connect to an AWS EKS cluster is using the AWS CLI tool. To do this, add the following before init hook to your Stack: 1 aws eks update-kubeconfig --region $REGION_NAME --name $CLUSTER_NAME Info The $REGION_NAME and $CLUSTER_NAME environment variables must be defined in your Stack's environment. This relies on either using the Spacelift AWS Integration , or ensuring that your workers have permission to access the EKS cluster.","title":"Authenticating"},{"location":"vendors/kubernetes/authenticating.html#authenticating","text":"The Kubernetes integration relies on using kubectl 's native authentication to connect to your cluster. You can use the $KUBECONFIG environment variable to find the location of the Kubernetes configuration file, and configure any credentials required. You should perform any custom authentication as part of a before init hook to make sure that kubectl is configured correctly before any commands are run in after init and subsequent hooks. The following sections provide examples of how to configure the integration manually, as well as using cloud provider-specific tooling.","title":"Authenticating"},{"location":"vendors/kubernetes/authenticating.html#manual-configuration","text":"Manual configuration allows you to connect to any Kubernetes cluster accessible by your Spacelift workers, regardless of whether your cluster is on-prem or hosted by a cloud provider. The Kubernetes integration automatically sets the $KUBECONFIG environment variable to point at /mnt/workspace/.kube/config , giving you a number of options: You can use a mounted file to mount a pre-prepared config file into your workspace at /mnt/workspace/.kube/config . You can use a before init hook to create a kubeconfig file, or to download it from a trusted location. Please refer to the Kubernetes documentation for more information on configuring kubectl.","title":"Manual Configuration"},{"location":"vendors/kubernetes/authenticating.html#aws","text":"The simplest way to connect to an AWS EKS cluster is using the AWS CLI tool. To do this, add the following before init hook to your Stack: 1 aws eks update-kubeconfig --region $REGION_NAME --name $CLUSTER_NAME Info The $REGION_NAME and $CLUSTER_NAME environment variables must be defined in your Stack's environment. This relies on either using the Spacelift AWS Integration , or ensuring that your workers have permission to access the EKS cluster.","title":"AWS"},{"location":"vendors/kubernetes/custom-resources.html","text":"Custom Resources \u00bb Spacelift supports the use of Kubernetes Custom Resources. To find out more about these supported extensions, review the official documentation .","title":"Custom Resources"},{"location":"vendors/kubernetes/custom-resources.html#custom-resources","text":"Spacelift supports the use of Kubernetes Custom Resources. To find out more about these supported extensions, review the official documentation .","title":"Custom Resources"},{"location":"vendors/kubernetes/getting-started.html","text":"Getting Started \u00bb Repository Creation \u00bb Start by creating a new deployment repository and name the file as deployment.yaml with the following code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment spec : selector : matchLabels : app : nginx replicas : 2 # tells deployment to run 2 pods matching the template template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 You can learn more about this example deployment by navigating to the Run a Stateless Application Using a Deployment website from the official Kubernetes documentation. Looking at the code, you will find that it deploys a single instance of Nginx. Create a new Stack \u00bb In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift. Integrate VCS \u00bb Select the repository you created in the initial step, as seen in the picture. Configure Backend \u00bb Choose Kubernetes from the dropdown list and type out the namespace . Info Spacelift does not recommend leaving the Namespace blank due to the elevated level of access privilege required. Define Behavior \u00bb Select the arrow next to Show Advanced Options to expose the advanced configuration options. To ensure the success of a Kubernetes deployment, the following options should be reviewed and validated. Runner Image: Use a custom one that has kubectl installed. Customize workflow: During the initialization phase, you must specify the necessary command to ensure kubeconfig is updated and authenticated to the Kubernetes Cluster you will be authenticating against. Info Spacelift can authenticate against any Kubernetes cluster, including local or cloud provider managed instances. In the example above, I am authenticating to an AWS EKS Cluster and used the following command to update the kubeconfig for the necessary cluster. 1 aws eks update-kubeconfig --region $region -name --name $cluster -name Info Update the previous variables according to your deployment. $region-name: AWS region where your Kubernetes cluster resides $cluster-name: Name of your Kubernetes clusters The above allows the worker to authenticate to the proper cluster before running the specified Kubernetes deployment in the repository that we created earlier. Warning Authentication with a Cloud Provider is required . After you Name the Stack, follow the Cloud Integrations section to ensure Spacelift can authenticate to your Kubernetes Cluster. Name the Stack \u00bb Provide the name of your Stack. Labels and Description are not required but recommended. Saving the Stack will redirect you to its Tracked Runs (Deployment) page. Configure Integrations \u00bb To authenticate against a Kubernetes cluster provided by Cloud Provider managed service, Spacelift requires integration with the associated Cloud Provider. Navigate to the Settings , Integrations page and select the dropdown arrow to access the following selection screen: Warning Necessary permissions to the Kubernetes Cluster are required. The following links will help you set up the necessary integration with your Cloud Provider of choice. AWS OIDC Once you have configured the necessary integration, navigate the Stack landing page and Trigger a Run. Trigger a Run \u00bb To Trigger a Run, select Trigger on the right side of the Stacks view. Spacelift Label To help identify resources deployed to your Kubernetes cluster, Spacelift will add the following label to all resources: spacelift-stack=<stack-slug> Triggered Run Status \u00bb Please review the documentation for a detailed view of each Run Phase and Status associated with Kubernetes. Unconfirmed \u00bb After you manually trigger the Run in the Stack view, Spacelift will deploy a runner image, initialize the Cloud Provider, Authenticate with the Kubernetes Cluster and run the Deployment specified in the repository. After a successful planning phase, you can check the log to see the planned changes. Planning Phase Spacelift utilizes the dry run functionality of kubectl apply to compare your code to the current state of the cluster and output the list of changes to be made. A slightly different dry run mode depending on the scenario: --dry-run=server : Utilized when resources are available --dry-run=client : Utilized when no resources are available To confirm the Triggered run, click the CONFIRM button. Finished Deployment \u00bb The following screen highlights the Finished Run and output from a successful deployment to your Kubernetes cluster. Applying The default timeout is set to 10 minutes. If a Kubernetes Deployment is expected to take longer, you can customize that using the KUBECTL_ROLLOUT_TIMEOUT environment variable. Review the documentation to find out more about Spacelift environment variables.. Default Removal of Deployments \u00bb Info By default; if a YAML file is removed from your repository, the resources with an attached spacelift-stack=<stack-slug> label will be removed from the Kubernetes cluster. The --prune flag will be utilized.","title":"Getting Started"},{"location":"vendors/kubernetes/getting-started.html#getting-started","text":"","title":"Getting Started"},{"location":"vendors/kubernetes/getting-started.html#repository-creation","text":"Start by creating a new deployment repository and name the file as deployment.yaml with the following code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment spec : selector : matchLabels : app : nginx replicas : 2 # tells deployment to run 2 pods matching the template template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 You can learn more about this example deployment by navigating to the Run a Stateless Application Using a Deployment website from the official Kubernetes documentation. Looking at the code, you will find that it deploys a single instance of Nginx.","title":"Repository Creation"},{"location":"vendors/kubernetes/getting-started.html#create-a-new-stack","text":"In Spacelift, go ahead and click the Add Stack button to create a Stack in Spacelift.","title":"Create a new Stack"},{"location":"vendors/kubernetes/getting-started.html#integrate-vcs","text":"Select the repository you created in the initial step, as seen in the picture.","title":"Integrate VCS"},{"location":"vendors/kubernetes/getting-started.html#configure-backend","text":"Choose Kubernetes from the dropdown list and type out the namespace . Info Spacelift does not recommend leaving the Namespace blank due to the elevated level of access privilege required.","title":"Configure Backend"},{"location":"vendors/kubernetes/getting-started.html#define-behavior","text":"Select the arrow next to Show Advanced Options to expose the advanced configuration options. To ensure the success of a Kubernetes deployment, the following options should be reviewed and validated. Runner Image: Use a custom one that has kubectl installed. Customize workflow: During the initialization phase, you must specify the necessary command to ensure kubeconfig is updated and authenticated to the Kubernetes Cluster you will be authenticating against. Info Spacelift can authenticate against any Kubernetes cluster, including local or cloud provider managed instances. In the example above, I am authenticating to an AWS EKS Cluster and used the following command to update the kubeconfig for the necessary cluster. 1 aws eks update-kubeconfig --region $region -name --name $cluster -name Info Update the previous variables according to your deployment. $region-name: AWS region where your Kubernetes cluster resides $cluster-name: Name of your Kubernetes clusters The above allows the worker to authenticate to the proper cluster before running the specified Kubernetes deployment in the repository that we created earlier. Warning Authentication with a Cloud Provider is required . After you Name the Stack, follow the Cloud Integrations section to ensure Spacelift can authenticate to your Kubernetes Cluster.","title":"Define Behavior"},{"location":"vendors/kubernetes/getting-started.html#name-the-stack","text":"Provide the name of your Stack. Labels and Description are not required but recommended. Saving the Stack will redirect you to its Tracked Runs (Deployment) page.","title":"Name the Stack"},{"location":"vendors/kubernetes/getting-started.html#configure-integrations","text":"To authenticate against a Kubernetes cluster provided by Cloud Provider managed service, Spacelift requires integration with the associated Cloud Provider. Navigate to the Settings , Integrations page and select the dropdown arrow to access the following selection screen: Warning Necessary permissions to the Kubernetes Cluster are required. The following links will help you set up the necessary integration with your Cloud Provider of choice. AWS OIDC Once you have configured the necessary integration, navigate the Stack landing page and Trigger a Run.","title":"Configure Integrations"},{"location":"vendors/kubernetes/getting-started.html#trigger-a-run","text":"To Trigger a Run, select Trigger on the right side of the Stacks view. Spacelift Label To help identify resources deployed to your Kubernetes cluster, Spacelift will add the following label to all resources: spacelift-stack=<stack-slug>","title":"Trigger a Run"},{"location":"vendors/kubernetes/getting-started.html#triggered-run-status","text":"Please review the documentation for a detailed view of each Run Phase and Status associated with Kubernetes.","title":"Triggered Run Status"},{"location":"vendors/kubernetes/getting-started.html#unconfirmed","text":"After you manually trigger the Run in the Stack view, Spacelift will deploy a runner image, initialize the Cloud Provider, Authenticate with the Kubernetes Cluster and run the Deployment specified in the repository. After a successful planning phase, you can check the log to see the planned changes. Planning Phase Spacelift utilizes the dry run functionality of kubectl apply to compare your code to the current state of the cluster and output the list of changes to be made. A slightly different dry run mode depending on the scenario: --dry-run=server : Utilized when resources are available --dry-run=client : Utilized when no resources are available To confirm the Triggered run, click the CONFIRM button.","title":"Unconfirmed"},{"location":"vendors/kubernetes/getting-started.html#finished-deployment","text":"The following screen highlights the Finished Run and output from a successful deployment to your Kubernetes cluster. Applying The default timeout is set to 10 minutes. If a Kubernetes Deployment is expected to take longer, you can customize that using the KUBECTL_ROLLOUT_TIMEOUT environment variable. Review the documentation to find out more about Spacelift environment variables..","title":"Finished Deployment"},{"location":"vendors/kubernetes/getting-started.html#default-removal-of-deployments","text":"Info By default; if a YAML file is removed from your repository, the resources with an attached spacelift-stack=<stack-slug> label will be removed from the Kubernetes cluster. The --prune flag will be utilized.","title":"Default Removal of Deployments"},{"location":"vendors/kubernetes/helm.html","text":"Helm \u00bb There is no native support within Spacelift for Helm, but you can use the helm template command in a before plan hook to generate the Kubernetes resource definitions to deploy. Please note, the following caveats apply: Using helm template means that you are not using the full Helm workflow, which may cause limitations or prevent certain Charts from working. You need to use a custom Spacelift worker image that has Helm installed, or alternatively you can install Helm using a before init hook. The rest of this page will go through an example of deploying the Spacelift Workerpool Helm Chart using the Kubernetes integration. See here for an example repository. Prerequisites \u00bb The following prerequisites are required to follow the rest of this guide: A Kubernetes cluster that you can authenticate to from a Spacelift stack. A namespace called spacelift-worker that exists within that cluster. Repository Creation \u00bb Start by creating a new repository for your Helm stack. This repository only needs to contain a single item - a kustomization.yaml file: 1 2 3 4 5 6 7 8 apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namespace : spacelift-worker resources : # spacelift-worker-pool.yaml will be generated in a pre-plan hook - spacelift-worker-pool.yaml The kustomization file is used to tell kubectl where to find the file containing the output of the helm template command, and prevents kubectl from attempting to apply every yaml file in your repository. This is important if you want to commit a values.yaml file to your repository. Info Our example repository contains a values.yaml file used to configure some of the chart values. This isn't required, and is simply there for illustrative purposes. Create a new Stack \u00bb Define Behavior \u00bb Follow the same steps to create your stack as per the Getting Started guide, but when you get to the Define Behavior step, add the following commands as before plan hooks: 1 2 helm repo add spacelift https://downloads.spacelift.io/helm helm template spacelift-worker-pool spacelift/spacelift-worker --values values.yaml --set \"replicaCount= $SPACELIFT_WORKER_REPLICAS \" --set \"credentials.token= $SPACELIFT_WORKER_POOL_TOKEN \" --set \"credentials.privateKey= $SPACELIFT_WORKER_POOL_PRIVATE_KEY \" > spacelift-worker-pool.yaml Also, make sure to specify your custom Runner image that has Helm installed if you are not installing Helm using a before init hook. Once you've completed both steps, you should see something like this: Configure Environment \u00bb Once you have successfully created your Stack, add values for the following environment variables to your Stack environment: SPACELIFT_WORKER_REPLICAS - the number of worker pool replicas to create . SPACELIFT_WORKER_POOL_TOKEN - the token downloaded when creating your worker pool. SPACELIFT_WORKER_POOL_PRIVATE_KEY - your base64-encoded private key. Your Stack environment should look something like this: Configure Integrations \u00bb Configure any required Cloud Provider integrations as per the Getting Started guide. Trigger a Run \u00bb This example assumes that a Kubernetes namespace called spacelift-worker already exists. If it doesn't, create it using kubectl create namespace spacelift-worker before triggering a run. Info You can use a Spacelift Task to run the kubectl create namespace command. Triggering runs works exactly the same as when not using Helm. Once the planning stage has completed, you should see a preview of your changes, showing the Chart resources that will be created: After approving the run, you should see the changes applying, along with a successful rollout of your Chart resources:","title":"Helm"},{"location":"vendors/kubernetes/helm.html#helm","text":"There is no native support within Spacelift for Helm, but you can use the helm template command in a before plan hook to generate the Kubernetes resource definitions to deploy. Please note, the following caveats apply: Using helm template means that you are not using the full Helm workflow, which may cause limitations or prevent certain Charts from working. You need to use a custom Spacelift worker image that has Helm installed, or alternatively you can install Helm using a before init hook. The rest of this page will go through an example of deploying the Spacelift Workerpool Helm Chart using the Kubernetes integration. See here for an example repository.","title":"Helm"},{"location":"vendors/kubernetes/helm.html#prerequisites","text":"The following prerequisites are required to follow the rest of this guide: A Kubernetes cluster that you can authenticate to from a Spacelift stack. A namespace called spacelift-worker that exists within that cluster.","title":"Prerequisites"},{"location":"vendors/kubernetes/helm.html#repository-creation","text":"Start by creating a new repository for your Helm stack. This repository only needs to contain a single item - a kustomization.yaml file: 1 2 3 4 5 6 7 8 apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namespace : spacelift-worker resources : # spacelift-worker-pool.yaml will be generated in a pre-plan hook - spacelift-worker-pool.yaml The kustomization file is used to tell kubectl where to find the file containing the output of the helm template command, and prevents kubectl from attempting to apply every yaml file in your repository. This is important if you want to commit a values.yaml file to your repository. Info Our example repository contains a values.yaml file used to configure some of the chart values. This isn't required, and is simply there for illustrative purposes.","title":"Repository Creation"},{"location":"vendors/kubernetes/helm.html#create-a-new-stack","text":"","title":"Create a new Stack"},{"location":"vendors/kubernetes/helm.html#define-behavior","text":"Follow the same steps to create your stack as per the Getting Started guide, but when you get to the Define Behavior step, add the following commands as before plan hooks: 1 2 helm repo add spacelift https://downloads.spacelift.io/helm helm template spacelift-worker-pool spacelift/spacelift-worker --values values.yaml --set \"replicaCount= $SPACELIFT_WORKER_REPLICAS \" --set \"credentials.token= $SPACELIFT_WORKER_POOL_TOKEN \" --set \"credentials.privateKey= $SPACELIFT_WORKER_POOL_PRIVATE_KEY \" > spacelift-worker-pool.yaml Also, make sure to specify your custom Runner image that has Helm installed if you are not installing Helm using a before init hook. Once you've completed both steps, you should see something like this:","title":"Define Behavior"},{"location":"vendors/kubernetes/helm.html#configure-environment","text":"Once you have successfully created your Stack, add values for the following environment variables to your Stack environment: SPACELIFT_WORKER_REPLICAS - the number of worker pool replicas to create . SPACELIFT_WORKER_POOL_TOKEN - the token downloaded when creating your worker pool. SPACELIFT_WORKER_POOL_PRIVATE_KEY - your base64-encoded private key. Your Stack environment should look something like this:","title":"Configure Environment"},{"location":"vendors/kubernetes/helm.html#configure-integrations","text":"Configure any required Cloud Provider integrations as per the Getting Started guide.","title":"Configure Integrations"},{"location":"vendors/kubernetes/helm.html#trigger-a-run","text":"This example assumes that a Kubernetes namespace called spacelift-worker already exists. If it doesn't, create it using kubectl create namespace spacelift-worker before triggering a run. Info You can use a Spacelift Task to run the kubectl create namespace command. Triggering runs works exactly the same as when not using Helm. Once the planning stage has completed, you should see a preview of your changes, showing the Chart resources that will be created: After approving the run, you should see the changes applying, along with a successful rollout of your Chart resources:","title":"Trigger a Run"},{"location":"vendors/kubernetes/kustomize.html","text":"Kustomize \u00bb Kubernetes support in Spacelift is driven by Kustomize with native support in kubectl . We used Kustomize to make all resources created using the Spacelift Kubernetes support have unique labels attached to them: spacelift-stack: <stack-slug> app.kubernetes.io/managed-by: spacelift All operations Spacelift does will be done only on resources with the spacelift-stack: <stack-slug> . If you are not using Kustomize, Spacelift will transparently create a kustomization.yaml file that will reference all yaml files in the Stack's project root and subdirectories. If you are using Kustomize, then we will only add a label transformer to your kustomization.yaml file.","title":"Kustomize"},{"location":"vendors/kubernetes/kustomize.html#kustomize","text":"Kubernetes support in Spacelift is driven by Kustomize with native support in kubectl . We used Kustomize to make all resources created using the Spacelift Kubernetes support have unique labels attached to them: spacelift-stack: <stack-slug> app.kubernetes.io/managed-by: spacelift All operations Spacelift does will be done only on resources with the spacelift-stack: <stack-slug> . If you are not using Kustomize, Spacelift will transparently create a kustomization.yaml file that will reference all yaml files in the Stack's project root and subdirectories. If you are using Kustomize, then we will only add a label transformer to your kustomization.yaml file.","title":"Kustomize"},{"location":"vendors/pulumi/index.html","text":"Pulumi \u00bb Info Feature previews are subject to change, may contain bugs, and have not yet been ironed out based on real production usage. On a high level, Pulumi has a very similar flow to Terraform. It uses a state backend, provides dry run functionality, reconciles the actual world with the desired state. In this article we'll dive into how each of the concepts in Spacelift translates into working with Pulumi. However, if you're the type that prefers to start with doing, instead of reading too much, there are quickstarts for each of the runtimes supported by Pulumi: C# Go Javascript Python In case you're just getting started with Pulumi, we'd recommend you to start with Javascript. Believe it or not, it's actually the most pleasant experience we had with Pulumi! Later you can also easily switch to languages which compile to Javascript, like TypeScript or ClojureScript. The high level concepts of Spacelift don't change when used with Pulumi. Below, we'll cover a few lower level details, which may be of interest. Run Execution \u00bb Initialization \u00bb Previously described in Run Initializing , in Pulumi the initialization will run: pulumi login with your configured login URL pulumi stack select --create --select with your configured Pulumi stack name (the one you set in vendor-specific settings, not the Spacelift Stack name) It will then commence to run all pre-initialization hooks. Planning \u00bb We run pulumi preview --refresh --diff --show-replacement-steps in order to show planned changes. Applying \u00bb We run pulumi up --refresh --diff --show-replacement-steps in order to apply changes. Policies \u00bb Most policies don't change at all. The one that changes most is the plan policy. Instead of the terraform raw plan in the terraform field, you'll get a pulumi field with the raw Pulumi plan and the following schema: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"pulumi\" : { \"steps\" : [ { \"new\" : { \"custom\" : \"boolean\" , \"id\" : \"string\" , \"inputs\" : \"object - input properties\" , \"outputs\" : \"object - output properties\" , \"parent\" : \"string - parent resource of this resource\" , \"provider\" : \"string - provider this resource stems from\" , \"type\" : \"string - resource type\" , \"urn\" : \"string - urn of this resource\" }, \"old\" : { \"custom\" : \"boolean\" , \"id\" : \"string\" , \"inputs\" : \"object - input properties\" , \"outputs\" : \"object - output properties\" , \"parent\" : \"string - parent resource of this resource\" , \"provider\" : \"string - provider this resource stems from\" , \"type\" : \"string - resource type\" , \"urn\" : \"string - urn of this resource\" }, \"op\" : \"string - same, refresh, create, update, delete, create-replacement or delete-replaced\" , \"provider\" : \"string - provider this resource stems from\" , \"type\" : \"string - resource type\" , \"urn\" : \"string - urn of this resource\" } ] }, \"spacelift\" : { \"...\" : \"...\" } } Pulumi secrets are detected and encoded as [secret] instead of the actual value, that's why there's no other string sanitization going on with Pulumi plans. Modules \u00bb Spacelift module CI/CD isn't currently available for Pulumi.","title":"Pulumi"},{"location":"vendors/pulumi/index.html#pulumi","text":"Info Feature previews are subject to change, may contain bugs, and have not yet been ironed out based on real production usage. On a high level, Pulumi has a very similar flow to Terraform. It uses a state backend, provides dry run functionality, reconciles the actual world with the desired state. In this article we'll dive into how each of the concepts in Spacelift translates into working with Pulumi. However, if you're the type that prefers to start with doing, instead of reading too much, there are quickstarts for each of the runtimes supported by Pulumi: C# Go Javascript Python In case you're just getting started with Pulumi, we'd recommend you to start with Javascript. Believe it or not, it's actually the most pleasant experience we had with Pulumi! Later you can also easily switch to languages which compile to Javascript, like TypeScript or ClojureScript. The high level concepts of Spacelift don't change when used with Pulumi. Below, we'll cover a few lower level details, which may be of interest.","title":"Pulumi"},{"location":"vendors/pulumi/index.html#run-execution","text":"","title":"Run Execution"},{"location":"vendors/pulumi/index.html#initialization","text":"Previously described in Run Initializing , in Pulumi the initialization will run: pulumi login with your configured login URL pulumi stack select --create --select with your configured Pulumi stack name (the one you set in vendor-specific settings, not the Spacelift Stack name) It will then commence to run all pre-initialization hooks.","title":"Initialization"},{"location":"vendors/pulumi/index.html#planning","text":"We run pulumi preview --refresh --diff --show-replacement-steps in order to show planned changes.","title":"Planning"},{"location":"vendors/pulumi/index.html#applying","text":"We run pulumi up --refresh --diff --show-replacement-steps in order to apply changes.","title":"Applying"},{"location":"vendors/pulumi/index.html#policies","text":"Most policies don't change at all. The one that changes most is the plan policy. Instead of the terraform raw plan in the terraform field, you'll get a pulumi field with the raw Pulumi plan and the following schema: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"pulumi\" : { \"steps\" : [ { \"new\" : { \"custom\" : \"boolean\" , \"id\" : \"string\" , \"inputs\" : \"object - input properties\" , \"outputs\" : \"object - output properties\" , \"parent\" : \"string - parent resource of this resource\" , \"provider\" : \"string - provider this resource stems from\" , \"type\" : \"string - resource type\" , \"urn\" : \"string - urn of this resource\" }, \"old\" : { \"custom\" : \"boolean\" , \"id\" : \"string\" , \"inputs\" : \"object - input properties\" , \"outputs\" : \"object - output properties\" , \"parent\" : \"string - parent resource of this resource\" , \"provider\" : \"string - provider this resource stems from\" , \"type\" : \"string - resource type\" , \"urn\" : \"string - urn of this resource\" }, \"op\" : \"string - same, refresh, create, update, delete, create-replacement or delete-replaced\" , \"provider\" : \"string - provider this resource stems from\" , \"type\" : \"string - resource type\" , \"urn\" : \"string - urn of this resource\" } ] }, \"spacelift\" : { \"...\" : \"...\" } } Pulumi secrets are detected and encoded as [secret] instead of the actual value, that's why there's no other string sanitization going on with Pulumi plans.","title":"Policies"},{"location":"vendors/pulumi/index.html#modules","text":"Spacelift module CI/CD isn't currently available for Pulumi.","title":"Modules"},{"location":"vendors/pulumi/c-sharp.html","text":"C# \u00bb In order to follow along with this article, you'll need an AWS account. Start with forking the Pulumi examples repo , we'll be setting up an example directory from there, namely aws-cs-webserver . In the root of the repository (not the aws-cs-webserver directory), add a new file: .spacelift/config.yml 1 2 3 4 5 6 7 version : \"1\" stack_defaults : before_apply : - dotnet clean - rm -rf bin - rm -rf obj before_apply is not yet exposed through the interface like before_init , so you have to set it through the config file. When compiling, the dotnet CLI creates global state which is lost after confirmation. This will mostly clean the workspace before applying, so everything will be cleanly recompiled. Why mostly? This you will see in a sec. Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values: Set the project root to aws-cs-webserver , as we want to run Pulumi in this subdirectory only. Set the runner image to public.ecr.aws/spacelift/runner-pulumi-dotnet:latest Pinning to a specific Pulumi version is possible too, using a tag like v2.15.4 - you can see the available versions here . In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose: A state backend, aka login URL. This can be a cloud storage bucket, like s3://pulumi-state-bucket , but it can also be a Pulumi Service endpoint. A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like my-dotnet-pulumi-spacelift-stack . Info You can use https://api.pulumi.com as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the PULUMI_ACCESS_TOKEN environment variable. You'll now have to set up the AWS integration for the Stack, as is described in AWS . Go into the Environment tab in your screen, add an AWS_REGION environment variable and set it to your region of choice, i.e. eu-central-1 . Previously I said dotnet clean mostly clears the state, this is because you'll also have to add the NUGET_PACKAGES environment variable, and set it to a directory persisted with the workspace, i.e. /mnt/workspace/nuget_packages . You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes. Confirm the run to let it apply the changes, after applying it should look like this: We can see the PublicDns stack output. If we try to curl it, lo and behold: 1 2 ~> curl ec2-18-184-92-34.eu-central-1.compute.amazonaws.com Hello, World! In order to clean up, open the Tasks tab, and perform pulumi destroy --non-interactive --yes there. Which will destroy all created resources.","title":"C#"},{"location":"vendors/pulumi/c-sharp.html#c","text":"In order to follow along with this article, you'll need an AWS account. Start with forking the Pulumi examples repo , we'll be setting up an example directory from there, namely aws-cs-webserver . In the root of the repository (not the aws-cs-webserver directory), add a new file: .spacelift/config.yml 1 2 3 4 5 6 7 version : \"1\" stack_defaults : before_apply : - dotnet clean - rm -rf bin - rm -rf obj before_apply is not yet exposed through the interface like before_init , so you have to set it through the config file. When compiling, the dotnet CLI creates global state which is lost after confirmation. This will mostly clean the workspace before applying, so everything will be cleanly recompiled. Why mostly? This you will see in a sec. Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values: Set the project root to aws-cs-webserver , as we want to run Pulumi in this subdirectory only. Set the runner image to public.ecr.aws/spacelift/runner-pulumi-dotnet:latest Pinning to a specific Pulumi version is possible too, using a tag like v2.15.4 - you can see the available versions here . In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose: A state backend, aka login URL. This can be a cloud storage bucket, like s3://pulumi-state-bucket , but it can also be a Pulumi Service endpoint. A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like my-dotnet-pulumi-spacelift-stack . Info You can use https://api.pulumi.com as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the PULUMI_ACCESS_TOKEN environment variable. You'll now have to set up the AWS integration for the Stack, as is described in AWS . Go into the Environment tab in your screen, add an AWS_REGION environment variable and set it to your region of choice, i.e. eu-central-1 . Previously I said dotnet clean mostly clears the state, this is because you'll also have to add the NUGET_PACKAGES environment variable, and set it to a directory persisted with the workspace, i.e. /mnt/workspace/nuget_packages . You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes. Confirm the run to let it apply the changes, after applying it should look like this: We can see the PublicDns stack output. If we try to curl it, lo and behold: 1 2 ~> curl ec2-18-184-92-34.eu-central-1.compute.amazonaws.com Hello, World! In order to clean up, open the Tasks tab, and perform pulumi destroy --non-interactive --yes there. Which will destroy all created resources.","title":"C#"},{"location":"vendors/pulumi/golang.html","text":"Go \u00bb In order to follow along with this article, you'll need an AWS account. Start with forking the Pulumi examples repo , we'll be setting up an example directory from there, namely aws-go-s3-folder . Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values: Set the project root to aws-go-s3-folder , as we want to run Pulumi in this subdirectory only. Set the runner image to public.ecr.aws/spacelift/runner-pulumi-golang:latest Pinning to a specific Pulumi version is possible too, using a tag like v2.15.4 - you can see the available versions here . In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose: A state backend, aka login URL. This can be a cloud storage bucket, like s3://pulumi-state-bucket , but it can also be a Pulumi Service endpoint. A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like my-golang-pulumi-spacelift-stack . Info You can use https://api.pulumi.com as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the PULUMI_ACCESS_TOKEN environment variable. You'll now have to set up the AWS integration for the Stack, as is described in AWS . Go into the Environment tab in your screen, add an AWS_REGION environment variable and set it to your region of choice, i.e. eu-central-1 . You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes. Confirm the run to let it apply the changes, after applying it should look like this: We can see the websiteUrl stack output. If we try to curl it, lo and behold: 1 2 3 4 5 6 7 ~> curl s3-website-bucket-b47a23a.s3-website.eu-central-1.amazonaws.com <html><head> <title>Hello Amazon S3</title><meta charset = \"UTF-8\" > <link rel = \"shortcut icon\" href = \"/favicon.png\" type = \"image/png\" > </head> <body><p>Hello, world!</p><p>Made with \u2764\ufe0f with <a href = \"https://pulumi.com\" >Pulumi</a></p> </body></html> In order to clean up, open the Tasks tab, and perform pulumi destroy --non-interactive --yes there. Which will destroy all created resources.","title":"Go"},{"location":"vendors/pulumi/golang.html#go","text":"In order to follow along with this article, you'll need an AWS account. Start with forking the Pulumi examples repo , we'll be setting up an example directory from there, namely aws-go-s3-folder . Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values: Set the project root to aws-go-s3-folder , as we want to run Pulumi in this subdirectory only. Set the runner image to public.ecr.aws/spacelift/runner-pulumi-golang:latest Pinning to a specific Pulumi version is possible too, using a tag like v2.15.4 - you can see the available versions here . In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose: A state backend, aka login URL. This can be a cloud storage bucket, like s3://pulumi-state-bucket , but it can also be a Pulumi Service endpoint. A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like my-golang-pulumi-spacelift-stack . Info You can use https://api.pulumi.com as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the PULUMI_ACCESS_TOKEN environment variable. You'll now have to set up the AWS integration for the Stack, as is described in AWS . Go into the Environment tab in your screen, add an AWS_REGION environment variable and set it to your region of choice, i.e. eu-central-1 . You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes. Confirm the run to let it apply the changes, after applying it should look like this: We can see the websiteUrl stack output. If we try to curl it, lo and behold: 1 2 3 4 5 6 7 ~> curl s3-website-bucket-b47a23a.s3-website.eu-central-1.amazonaws.com <html><head> <title>Hello Amazon S3</title><meta charset = \"UTF-8\" > <link rel = \"shortcut icon\" href = \"/favicon.png\" type = \"image/png\" > </head> <body><p>Hello, world!</p><p>Made with \u2764\ufe0f with <a href = \"https://pulumi.com\" >Pulumi</a></p> </body></html> In order to clean up, open the Tasks tab, and perform pulumi destroy --non-interactive --yes there. Which will destroy all created resources.","title":"Go"},{"location":"vendors/pulumi/javascript.html","text":"Javascript \u00bb In order to follow along with this article, you'll need an AWS account. Start with forking the Pulumi examples repo , we'll be setting up an example directory from there, namely aws-js-webserver . Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values: Set the project root to aws-js-webserver , as we want to run Pulumi in this subdirectory only. Add one before init script: npm install , which will install all necessary dependencies, before initializing Pulumi itself. The outputs will be persisted in the workspace and be there for the Planning and Applying phases. Set the runner image to public.ecr.aws/spacelift/runner-pulumi-javascript:latest Pinning to a specific Pulumi version is possible too, using a tag like v2.15.4 - you can see the available versions here . In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose: A state backend, aka login URL. This can be a cloud storage bucket, like s3://pulumi-state-bucket , but it can also be a Pulumi Service endpoint. A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like my-javascript-pulumi-spacelift-stack . Info You can use https://api.pulumi.com as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the PULUMI_ACCESS_TOKEN environment variable. You'll now have to set up the AWS integration for the Stack, as is described in AWS . Go into the Environment tab in your screen, add an AWS_REGION environment variable and set it to your region of choice, i.e. eu-central-1 . You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes. Confirm the run to let it apply the changes, after applying it should look like this: We can see the publicHostName stack output. If we try to curl it, lo and behold: 1 2 ~> curl ec2-18-184-240-9.eu-central-1.compute.amazonaws.com Hello, World! In order to clean up, open the Tasks tab, and perform pulumi destroy --non-interactive --yes there. Which will destroy all created resources.","title":"Javascript"},{"location":"vendors/pulumi/javascript.html#javascript","text":"In order to follow along with this article, you'll need an AWS account. Start with forking the Pulumi examples repo , we'll be setting up an example directory from there, namely aws-js-webserver . Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values: Set the project root to aws-js-webserver , as we want to run Pulumi in this subdirectory only. Add one before init script: npm install , which will install all necessary dependencies, before initializing Pulumi itself. The outputs will be persisted in the workspace and be there for the Planning and Applying phases. Set the runner image to public.ecr.aws/spacelift/runner-pulumi-javascript:latest Pinning to a specific Pulumi version is possible too, using a tag like v2.15.4 - you can see the available versions here . In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose: A state backend, aka login URL. This can be a cloud storage bucket, like s3://pulumi-state-bucket , but it can also be a Pulumi Service endpoint. A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like my-javascript-pulumi-spacelift-stack . Info You can use https://api.pulumi.com as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the PULUMI_ACCESS_TOKEN environment variable. You'll now have to set up the AWS integration for the Stack, as is described in AWS . Go into the Environment tab in your screen, add an AWS_REGION environment variable and set it to your region of choice, i.e. eu-central-1 . You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes. Confirm the run to let it apply the changes, after applying it should look like this: We can see the publicHostName stack output. If we try to curl it, lo and behold: 1 2 ~> curl ec2-18-184-240-9.eu-central-1.compute.amazonaws.com Hello, World! In order to clean up, open the Tasks tab, and perform pulumi destroy --non-interactive --yes there. Which will destroy all created resources.","title":"Javascript"},{"location":"vendors/pulumi/python.html","text":"Python \u00bb In order to follow along with this article, you'll need an AWS account. Start with forking the Pulumi examples repo , we'll be setting up an example directory from there, namely aws-py-webserver . In the root of the repository (not the aws-py-webserver directory), add a new file: .spacelift/config.yml 1 2 3 4 5 version : \"1\" stack_defaults : before_apply : - pip install -r requirements.txt before_apply is not yet exposed through the interface like before_init , so you have to set it through the config file. Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values: Set the project root to aws-py-webserver , as we want to run Pulumi in this subdirectory only. Add one before init script: pip install -r requirements.txt , which will install all necessary dependencies, before initializing Pulumi itself. This will need to run both when initializing and before applying . Set the runner image to public.ecr.aws/spacelift/runner-pulumi-python:latest Pinning to a specific Pulumi version is possible too, using a tag like v2.15.4 - you can see the available versions here . In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose: A state backend, aka login URL. This can be a cloud storage bucket, like s3://pulumi-state-bucket , but it can also be a Pulumi Service endpoint. A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like my-python-pulumi-spacelift-stack . Info You can use https://api.pulumi.com as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the PULUMI_ACCESS_TOKEN environment variable. You'll now have to set up the AWS integration for the Stack, as is described in AWS . Go into the Environment tab in your screen, add an AWS_REGION environment variable and set it to your region of choice, i.e. eu-central-1 . You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes. Confirm the run to let it apply the changes, after applying it should look like this: We can see the public_dns stack output. If we try to curl it, lo and behold: 1 2 ~> curl ec2-3-125-48-55.eu-central-1.compute.amazonaws.com Hello, World! In order to clean up, open the Tasks tab, and perform pulumi destroy --non-interactive --yes there. Which will destroy all created resources.","title":"Python"},{"location":"vendors/pulumi/python.html#python","text":"In order to follow along with this article, you'll need an AWS account. Start with forking the Pulumi examples repo , we'll be setting up an example directory from there, namely aws-py-webserver . In the root of the repository (not the aws-py-webserver directory), add a new file: .spacelift/config.yml 1 2 3 4 5 version : \"1\" stack_defaults : before_apply : - pip install -r requirements.txt before_apply is not yet exposed through the interface like before_init , so you have to set it through the config file. Now let's open Spacelift and create a new Stack, choose the examples repo you just forked. In the second step you'll have to change multiple default values: Set the project root to aws-py-webserver , as we want to run Pulumi in this subdirectory only. Add one before init script: pip install -r requirements.txt , which will install all necessary dependencies, before initializing Pulumi itself. This will need to run both when initializing and before applying . Set the runner image to public.ecr.aws/spacelift/runner-pulumi-python:latest Pinning to a specific Pulumi version is possible too, using a tag like v2.15.4 - you can see the available versions here . In the third step, choose Pulumi as your Infrastructure as Code vendor. You'll have to choose: A state backend, aka login URL. This can be a cloud storage bucket, like s3://pulumi-state-bucket , but it can also be a Pulumi Service endpoint. A stack name, which is how the state for this stack will be namespaced in the state backend. Best to write something close to your stack name, like my-python-pulumi-spacelift-stack . Info You can use https://api.pulumi.com as the Login URL to use the official Pulumi state backend. You'll also need to provide your Pulumi access token through the PULUMI_ACCESS_TOKEN environment variable. You'll now have to set up the AWS integration for the Stack, as is described in AWS . Go into the Environment tab in your screen, add an AWS_REGION environment variable and set it to your region of choice, i.e. eu-central-1 . You can now trigger the Run manually in the Stack view, after the planning phase is over, you can check the log to see the planned changes. Confirm the run to let it apply the changes, after applying it should look like this: We can see the public_dns stack output. If we try to curl it, lo and behold: 1 2 ~> curl ec2-3-125-48-55.eu-central-1.compute.amazonaws.com Hello, World! In order to clean up, open the Tasks tab, and perform pulumi destroy --non-interactive --yes there. Which will destroy all created resources.","title":"Python"},{"location":"vendors/terraform/index.html","text":"Terraform \u00bb Why use Terraform? \u00bb Terraform is a full-featured, battle-tested Infrastructure as Code tool. It has a vast ecosystem of providers to interact with many vendors from cloud providers such as AWS, Azure and GCP to monitoring such as New Relic and Datadog, and many many more. There are also plenty of community-managed modules and tools to get you started in no time. Why use Spacelift with Terraform? \u00bb Spacelift helps you manage the complexities and compliance challenges of using Terraform. It brings with it a GitOps flow, so your infrastructure repository is synced with your Terraform Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies , which lets you automate compliance checks and build complex multi-stack workflows . You can also use Spacelift to mix and match Terraform, Pulumi, and CloudFormation Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like an ECS/EKS cluster with all its dependencies) and then connect that to a CloudFormation Stack which then transactionally deploys your services there using trigger policies and the Spacelift provider run resources for workflow orchestration and Contexts to export Terraform outputs as CloudFormation input parameters. Does Spacelift support Terraform wrappers? \u00bb Yes! We support Terragrunt and Cloud Development Kit for Terraform (CDKTF) . You can read more about it in the relevant subpages of this document. Additional resources \u00bb Module registry Provider registry (beta) External modules Provider State management External state access Terragrunt Version management Handling .tfvars CLI Configuration Cost Estimation Resource Sanitization Storing Complex Variables Debugging Guide Dependency Lock File Cloud Development Kit for Terraform (CDKTF)","title":"Terraform"},{"location":"vendors/terraform/index.html#terraform","text":"","title":"Terraform"},{"location":"vendors/terraform/index.html#why-use-terraform","text":"Terraform is a full-featured, battle-tested Infrastructure as Code tool. It has a vast ecosystem of providers to interact with many vendors from cloud providers such as AWS, Azure and GCP to monitoring such as New Relic and Datadog, and many many more. There are also plenty of community-managed modules and tools to get you started in no time.","title":"Why use Terraform?"},{"location":"vendors/terraform/index.html#why-use-spacelift-with-terraform","text":"Spacelift helps you manage the complexities and compliance challenges of using Terraform. It brings with it a GitOps flow, so your infrastructure repository is synced with your Terraform Stacks, and pull requests show you a preview of what they're planning to change. It also has an extensive selection of policies , which lets you automate compliance checks and build complex multi-stack workflows . You can also use Spacelift to mix and match Terraform, Pulumi, and CloudFormation Stacks and have them talk to one another. For example, you can set up Terraform Stacks to provision required infrastructure (like an ECS/EKS cluster with all its dependencies) and then connect that to a CloudFormation Stack which then transactionally deploys your services there using trigger policies and the Spacelift provider run resources for workflow orchestration and Contexts to export Terraform outputs as CloudFormation input parameters.","title":"Why use Spacelift with Terraform?"},{"location":"vendors/terraform/index.html#does-spacelift-support-terraform-wrappers","text":"Yes! We support Terragrunt and Cloud Development Kit for Terraform (CDKTF) . You can read more about it in the relevant subpages of this document.","title":"Does Spacelift support Terraform wrappers?"},{"location":"vendors/terraform/index.html#additional-resources","text":"Module registry Provider registry (beta) External modules Provider State management External state access Terragrunt Version management Handling .tfvars CLI Configuration Cost Estimation Resource Sanitization Storing Complex Variables Debugging Guide Dependency Lock File Cloud Development Kit for Terraform (CDKTF)","title":"Additional resources"},{"location":"vendors/terraform/cdktf.html","text":"Cloud Development Kit for Terraform (CDKTF) \u00bb The Cloud Development Kit for Terraform (CDKTF) generates JSON Terraform configuration from code in C#, Python, TypeScript, Java, or Go. Spacelift fully supports CDKTF. Building a custom runner image \u00bb CDKTF requires packages and tools that are not included in the default Terraform runner. These dependencies are different for each supported programming language. Luckily, extending the default runner Docker image to include these dependencies is easy. You will need to: Create a Dockerfile file that installs the required tools and packages for the specific programming language you want to use (see below). Build and publish the Docker image. Configure the runner image to use in the stack settings . TypeScript Python Go 1 2 3 4 5 6 FROM public.ecr.aws/spacelift/runner-terraform:latest USER root RUN apk add --no-cache nodejs npm RUN npm install --global cdktf-cli@latest USER spacelift 1 2 3 4 5 6 7 8 9 10 11 FROM public.ecr.aws/spacelift/runner-terraform:latest USER root RUN apk add --no-cache nodejs npm python3 RUN npm install --global cdktf-cli@latest RUN python3 -m ensurepip \\ && python3 -m pip install --upgrade pip setuptools USER spacelift RUN pip3 install --user pipenv ENV PATH = \"/home/spacelift/.local/bin: $PATH \" 1 2 3 4 5 6 FROM public.ecr.aws/spacelift/runner-terraform:latest USER root RUN apk add --no-cache go nodejs npm RUN npm install --global cdktf-cli@latest USER spacelift Synthesizing Terraform code \u00bb Before Terraform can plan and apply changes to your infrastructure, CDKTF must turn your C#, Python, TypeScript, Java, or Go code into Terraform configuration code. That process is called synthesizing. This step needs to happen before the Initializing phase of a run. This can be easily done by adding a few before_init hooks : TypeScript Python Go npm install cdktf synth cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json . pipenv install cdktf synth cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json . cdktf synth cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json . Warning If the Terraform state is managed by Spacelift, make sure to disable the local backend that CDKTF automatically adds if none is configured by adding the following command after the hooks mentioned above: 1 jq '.terraform.backend.local = null' cdk.tf.json > cdk.tf.json.tmp && mv cdk.tf.json.tmp cdk.tf.json","title":"Cloud Development Kit for Terraform (CDKTF)"},{"location":"vendors/terraform/cdktf.html#cloud-development-kit-for-terraform-cdktf","text":"The Cloud Development Kit for Terraform (CDKTF) generates JSON Terraform configuration from code in C#, Python, TypeScript, Java, or Go. Spacelift fully supports CDKTF.","title":"Cloud Development Kit for Terraform (CDKTF)"},{"location":"vendors/terraform/cdktf.html#building-a-custom-runner-image","text":"CDKTF requires packages and tools that are not included in the default Terraform runner. These dependencies are different for each supported programming language. Luckily, extending the default runner Docker image to include these dependencies is easy. You will need to: Create a Dockerfile file that installs the required tools and packages for the specific programming language you want to use (see below). Build and publish the Docker image. Configure the runner image to use in the stack settings . TypeScript Python Go 1 2 3 4 5 6 FROM public.ecr.aws/spacelift/runner-terraform:latest USER root RUN apk add --no-cache nodejs npm RUN npm install --global cdktf-cli@latest USER spacelift 1 2 3 4 5 6 7 8 9 10 11 FROM public.ecr.aws/spacelift/runner-terraform:latest USER root RUN apk add --no-cache nodejs npm python3 RUN npm install --global cdktf-cli@latest RUN python3 -m ensurepip \\ && python3 -m pip install --upgrade pip setuptools USER spacelift RUN pip3 install --user pipenv ENV PATH = \"/home/spacelift/.local/bin: $PATH \" 1 2 3 4 5 6 FROM public.ecr.aws/spacelift/runner-terraform:latest USER root RUN apk add --no-cache go nodejs npm RUN npm install --global cdktf-cli@latest USER spacelift","title":"Building a custom runner image"},{"location":"vendors/terraform/cdktf.html#synthesizing-terraform-code","text":"Before Terraform can plan and apply changes to your infrastructure, CDKTF must turn your C#, Python, TypeScript, Java, or Go code into Terraform configuration code. That process is called synthesizing. This step needs to happen before the Initializing phase of a run. This can be easily done by adding a few before_init hooks : TypeScript Python Go npm install cdktf synth cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json . pipenv install cdktf synth cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json . cdktf synth cp cdktf.out/stacks/${TF_VAR_spacelift_stack_id}/cdk.tf.json . Warning If the Terraform state is managed by Spacelift, make sure to disable the local backend that CDKTF automatically adds if none is configured by adding the following command after the hooks mentioned above: 1 jq '.terraform.backend.local = null' cdk.tf.json > cdk.tf.json.tmp && mv cdk.tf.json.tmp cdk.tf.json","title":"Synthesizing Terraform code"},{"location":"vendors/terraform/cli-configuration.html","text":"CLI Configuration \u00bb For some of our Terraform users, a convenient way to configure the Terraform CLI behavior is through customizing the ~/.terraformrc file. During the preparing phase of a run, Spacelift creates a configuration file at the default location: ~/.terraformrc . This file contains credentials that are needed to communicate with remote services. Extending CLI configuration \u00bb Extending the Terraform CLI behavior can be done by using mounted files : Using mounted files \u00bb Any mounted files with names ending in .terraformrc will be appended to ~/.terraformrc . Info The Terraform CLI configuration file syntax supports these settings","title":"CLI Configuration"},{"location":"vendors/terraform/cli-configuration.html#cli-configuration","text":"For some of our Terraform users, a convenient way to configure the Terraform CLI behavior is through customizing the ~/.terraformrc file. During the preparing phase of a run, Spacelift creates a configuration file at the default location: ~/.terraformrc . This file contains credentials that are needed to communicate with remote services.","title":"CLI Configuration"},{"location":"vendors/terraform/cli-configuration.html#extending-cli-configuration","text":"Extending the Terraform CLI behavior can be done by using mounted files :","title":"Extending CLI configuration"},{"location":"vendors/terraform/cli-configuration.html#using-mounted-files","text":"Any mounted files with names ending in .terraformrc will be appended to ~/.terraformrc . Info The Terraform CLI configuration file syntax supports these settings","title":"Using mounted files"},{"location":"vendors/terraform/debugging-guide.html","text":"Debugging Guide \u00bb Setting Environment Variables \u00bb Environment variables are commonly used for enabling advanced logging levels for Terraform and Terragrunt. There are two ways environment variables can be set for runs. Set Environment Variable(s) directly on a Stack's Environment (easiest method). Set Environment Variable(s) in a Context , and then attach that Context to your Spacelift Stack(s). Terraform Debugging \u00bb Terraform providing an advanced logging mode that can be enabled using the TF_LOG environment variable. As of the writing of this documentation, TF_LOG has 5 different logging levels: TRACE DEBUG INFO WARN and ERROR Please refer to the Setting Environment Variables section for more information on how to set these variables on your Spacelift Stack(s). For more information on Terraform logging, please refer directly to the Terraform documentation . Terragrunt Debugging \u00bb Please refer to the Debugging Terragrunt section of our Terragrunt documentation.","title":"Debugging Guide"},{"location":"vendors/terraform/debugging-guide.html#debugging-guide","text":"","title":"Debugging Guide"},{"location":"vendors/terraform/debugging-guide.html#setting-environment-variables","text":"Environment variables are commonly used for enabling advanced logging levels for Terraform and Terragrunt. There are two ways environment variables can be set for runs. Set Environment Variable(s) directly on a Stack's Environment (easiest method). Set Environment Variable(s) in a Context , and then attach that Context to your Spacelift Stack(s).","title":"Setting Environment Variables"},{"location":"vendors/terraform/debugging-guide.html#terraform-debugging","text":"Terraform providing an advanced logging mode that can be enabled using the TF_LOG environment variable. As of the writing of this documentation, TF_LOG has 5 different logging levels: TRACE DEBUG INFO WARN and ERROR Please refer to the Setting Environment Variables section for more information on how to set these variables on your Spacelift Stack(s). For more information on Terraform logging, please refer directly to the Terraform documentation .","title":"Terraform Debugging"},{"location":"vendors/terraform/debugging-guide.html#terragrunt-debugging","text":"Please refer to the Debugging Terragrunt section of our Terragrunt documentation.","title":"Terragrunt Debugging"},{"location":"vendors/terraform/dependency-lock-file.html","text":"Dependency Lock File \u00bb Recent versions of Terraform can optionally track dependency selections using a Dependency Lock File named .terraform.lock.hcl , in a similar fashion to npm's package-lock.json file. If this file is present in the project root for your stack , Terraform will use it. Otherwise, it will dynamically determine the dependencies to use. Generating & Updating the File \u00bb Terraform recommends including the Dependency Lock File file in your version control repository, alongside your infrastructure code. You can generate or update this file by running terraform init locally and committing it into your repository. An alternative option would be to run the terraform init in a Task , print it to the Task logs, copy/paste the content from the Task logs into the .terraform.lock.hcl file, and commit it into your repository.","title":"Dependency Lock File"},{"location":"vendors/terraform/dependency-lock-file.html#dependency-lock-file","text":"Recent versions of Terraform can optionally track dependency selections using a Dependency Lock File named .terraform.lock.hcl , in a similar fashion to npm's package-lock.json file. If this file is present in the project root for your stack , Terraform will use it. Otherwise, it will dynamically determine the dependencies to use.","title":"Dependency Lock File"},{"location":"vendors/terraform/dependency-lock-file.html#generating-updating-the-file","text":"Terraform recommends including the Dependency Lock File file in your version control repository, alongside your infrastructure code. You can generate or update this file by running terraform init locally and committing it into your repository. An alternative option would be to run the terraform init in a Task , print it to the Task logs, copy/paste the content from the Task logs into the .terraform.lock.hcl file, and commit it into your repository.","title":"Generating &amp; Updating the File"},{"location":"vendors/terraform/external-modules.html","text":"External modules \u00bb Those of our customers who are not yet using our private module registry may want to pull modules from various external sources supported by Terraform. This article discusses a few most popular types of module sources and how to use them in Spacelift. Cloud storage \u00bb The easiest option is accessing modules from Amazon S3 buckets. Access to S3 can be granted using our AWS integration. Alternatively if your worker pool has the correct IAM permissions, you may not require any authentication at all! Git repositories \u00bb Git is by far the most popular external module source. This example will focus on GitHub as the most popular one, but the advice applies to other VCS providers. In general, Terraform retrieves Git-based modules using one of the two supported transports - HTTPS or SSL. Assuming your repository is private, you will need to give Spacelift credentials required to access it. Using HTTPS \u00bb Git with HTTPS is slightly simpler than SSH - all you need is a personal access token, and you need to make sure that it ends up in the ~/.netrc file, which Terraform will use to log in to the host that stores your source code. Assuming you already have a token you can use, create a file like this: 1 2 3 machine github.com login $yourLogin password $yourToken Then, upload this file to your stack's Spacelift environment as a mounted file . In this example, we called that file github.netrc : Add the following commands as \"before init\" hooks to append the content of this file to the ~/.netrc proper: 1 2 cat /mnt/workspace/github.netrc >> ~/.netrc chmod 600 ~/.netrc Using SSH \u00bb Using SSH isn't much more complex, but it requires a bit more preparation. Once you have a public-private key pair (whether it's a personal SSH key or a single-repo deploy key ), you will need to pass it to Spacelift and make sure it's used to access your VCS provider. Once again, we're going to use the mounted file functionality to pass the private key called id_ed25519 to your stack's environment: Add the following commands as \"before init\" hooks to \"teach\" our SSH agent to use this key for GitHub: 1 2 3 4 mkdir -p ~/.ssh cp /mnt/workspace/id_ed25519 ~/.ssh/id_ed25519 chmod 400 ~/.ssh/id_ed25519 ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts The above example warrants a little explanation. First, we're making sure that the ~/.ssh directory exists - otherwise, we won't be able to put anything in there. Then we copy the private key file mounted in our workspace to the SSH configuration directory and give it proper permissions. Last but not least, we're using the ssh-keyscan utility to retrieve the public SSH host key for github.com and add it to the list of known hosts - this will avoid your code checkout failing due to what would otherwise be an interactive prompt asking you whether to trust that key. Dedicated third-party registries \u00bb For users storing their modules in dedicated external private registries, like Terraform Cloud's one , you will need to supply credentials in the .terraformrc file - this approach is documented in the official documentation . In order to faciliate that, we've introduced a special mechanism for extending the CLI configuration that does not even require using before_init hooks. You can read more about it here . To mount or not to mount? \u00bb That is the question. And there isn't a single right answer. Instead, there is a list of questions to consider. By mounting a file, you're giving us access to its content. No, we're not going to read it, and yes, we have it encrypted using a fancy multi-layered mechanism, but still - we have it. So the main question is how sensitive the credentials are . Read-only deploy keys are probably the least sensitive - they only give read access to a single repository, so these are the ones where convenience may outweigh other concerns. On the other hand, personal access tokens may be pretty powerful, even if you generate them from service users. The same thing goes for personal SSH keys. Guard these well. So if you don't want to mount these credentials, what are your options? First, you can put these credentials directly into your private runner image . But that means that anyone in your organization who uses the private runner image gets access to your credentials - and that may or may not be what you wanted. The other option is to store the credentials externally in one of the secrets stores - like AWS Secrets Manager or HashiCorp Vault and retrieve them in one of your before_init scripts before putting them in the right place ( ~/.netrc file, ~/.ssh directory, etc.). Info If you decide to mount, we advise that you store credentials in contexts and attach these to stacks that need them. This way you can avoid credentials sprawl and leak.","title":"External modules"},{"location":"vendors/terraform/external-modules.html#external-modules","text":"Those of our customers who are not yet using our private module registry may want to pull modules from various external sources supported by Terraform. This article discusses a few most popular types of module sources and how to use them in Spacelift.","title":"External modules"},{"location":"vendors/terraform/external-modules.html#cloud-storage","text":"The easiest option is accessing modules from Amazon S3 buckets. Access to S3 can be granted using our AWS integration. Alternatively if your worker pool has the correct IAM permissions, you may not require any authentication at all!","title":"Cloud storage"},{"location":"vendors/terraform/external-modules.html#git-repositories","text":"Git is by far the most popular external module source. This example will focus on GitHub as the most popular one, but the advice applies to other VCS providers. In general, Terraform retrieves Git-based modules using one of the two supported transports - HTTPS or SSL. Assuming your repository is private, you will need to give Spacelift credentials required to access it.","title":"Git repositories"},{"location":"vendors/terraform/external-modules.html#using-https","text":"Git with HTTPS is slightly simpler than SSH - all you need is a personal access token, and you need to make sure that it ends up in the ~/.netrc file, which Terraform will use to log in to the host that stores your source code. Assuming you already have a token you can use, create a file like this: 1 2 3 machine github.com login $yourLogin password $yourToken Then, upload this file to your stack's Spacelift environment as a mounted file . In this example, we called that file github.netrc : Add the following commands as \"before init\" hooks to append the content of this file to the ~/.netrc proper: 1 2 cat /mnt/workspace/github.netrc >> ~/.netrc chmod 600 ~/.netrc","title":"Using HTTPS"},{"location":"vendors/terraform/external-modules.html#using-ssh","text":"Using SSH isn't much more complex, but it requires a bit more preparation. Once you have a public-private key pair (whether it's a personal SSH key or a single-repo deploy key ), you will need to pass it to Spacelift and make sure it's used to access your VCS provider. Once again, we're going to use the mounted file functionality to pass the private key called id_ed25519 to your stack's environment: Add the following commands as \"before init\" hooks to \"teach\" our SSH agent to use this key for GitHub: 1 2 3 4 mkdir -p ~/.ssh cp /mnt/workspace/id_ed25519 ~/.ssh/id_ed25519 chmod 400 ~/.ssh/id_ed25519 ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts The above example warrants a little explanation. First, we're making sure that the ~/.ssh directory exists - otherwise, we won't be able to put anything in there. Then we copy the private key file mounted in our workspace to the SSH configuration directory and give it proper permissions. Last but not least, we're using the ssh-keyscan utility to retrieve the public SSH host key for github.com and add it to the list of known hosts - this will avoid your code checkout failing due to what would otherwise be an interactive prompt asking you whether to trust that key.","title":"Using SSH"},{"location":"vendors/terraform/external-modules.html#dedicated-third-party-registries","text":"For users storing their modules in dedicated external private registries, like Terraform Cloud's one , you will need to supply credentials in the .terraformrc file - this approach is documented in the official documentation . In order to faciliate that, we've introduced a special mechanism for extending the CLI configuration that does not even require using before_init hooks. You can read more about it here .","title":"Dedicated third-party registries"},{"location":"vendors/terraform/external-modules.html#to-mount-or-not-to-mount","text":"That is the question. And there isn't a single right answer. Instead, there is a list of questions to consider. By mounting a file, you're giving us access to its content. No, we're not going to read it, and yes, we have it encrypted using a fancy multi-layered mechanism, but still - we have it. So the main question is how sensitive the credentials are . Read-only deploy keys are probably the least sensitive - they only give read access to a single repository, so these are the ones where convenience may outweigh other concerns. On the other hand, personal access tokens may be pretty powerful, even if you generate them from service users. The same thing goes for personal SSH keys. Guard these well. So if you don't want to mount these credentials, what are your options? First, you can put these credentials directly into your private runner image . But that means that anyone in your organization who uses the private runner image gets access to your credentials - and that may or may not be what you wanted. The other option is to store the credentials externally in one of the secrets stores - like AWS Secrets Manager or HashiCorp Vault and retrieve them in one of your before_init scripts before putting them in the right place ( ~/.netrc file, ~/.ssh directory, etc.). Info If you decide to mount, we advise that you store credentials in contexts and attach these to stacks that need them. This way you can avoid credentials sprawl and leak.","title":"To mount or not to mount?"},{"location":"vendors/terraform/external-state-access.html","text":"External state access \u00bb External state access allows you to read the state of the stack from outside authorized runs and tasks . In particular, this enables sharing the outputs between stacks using the Terraform mechanism of remote state or even accessing the state offline for analytical or compliance purposes. If enabled for a particular stack, any user or stack with Write permission to that stack's space will be able to access its state. This feature is off by default. Enabling external access \u00bb Info Only administrative stacks or users with write permission to this Stack's space can access the state. You can enable the external access in a couple of ways. through the UI using the Terraform provider . 1 2 3 4 5 6 resource \"spacelift_stack\" \"example\" { name = \"example\" repository = \"spacelift-stacks\" branch = \"main\" terraform_external_state_access = true } Sharing outputs between stacks \u00bb Sharing the outputs between stacks can be achieved using the Terraform mechanism of remote state . Given an account called spacecorp , and a stack named deep-thought , having the following Terraform configuration. 1 2 3 output \"answer\" { value = 42 } You can read that output from a different stack by using the terraform_remote_state data source. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 data \"terraform_remote_state\" \"deepthought\" { backend = \"remote\" config = { hostname = \"spacelift.io\" # (1) organization = \"spacecorp\" # (2) workspaces = { name = \"deep-thought\" # (3) } } } output \"ultimate_answer\" { value = data.terraform_remote_state.deepthought.outputs.answer } The hostname of the Spacelift system. The name of the account. The ID of the stack you wish to retrieve outputs from. Offline state access \u00bb Given an account called spacecorp , and a stack named deep-thought , having the following Terraform configuration. 1 2 3 output \"answer\" { value = 42 } Before you will be able to access the state of the stack, you need to retrieve an authentication token for spacelift.io . 1 terraform login spacelift.io Next, you need a remote backend configuration. 1 2 3 4 5 6 7 8 9 10 terraform { backend \"remote\" { hostname = \"spacelift.io\" # (1) organization = \"spacecorp\" # (2) workspaces { name = \"deep-thought\" # (3) } } } The hostname of the Spacelift system. The name of the account. The ID of the stack you wish to retrieve outputs from. Finally, you can download the state of the stack. 1 terraform state pull > terraform.tfstate","title":"External state access"},{"location":"vendors/terraform/external-state-access.html#external-state-access","text":"External state access allows you to read the state of the stack from outside authorized runs and tasks . In particular, this enables sharing the outputs between stacks using the Terraform mechanism of remote state or even accessing the state offline for analytical or compliance purposes. If enabled for a particular stack, any user or stack with Write permission to that stack's space will be able to access its state. This feature is off by default.","title":"External state access"},{"location":"vendors/terraform/external-state-access.html#enabling-external-access","text":"Info Only administrative stacks or users with write permission to this Stack's space can access the state. You can enable the external access in a couple of ways. through the UI using the Terraform provider . 1 2 3 4 5 6 resource \"spacelift_stack\" \"example\" { name = \"example\" repository = \"spacelift-stacks\" branch = \"main\" terraform_external_state_access = true }","title":"Enabling external access"},{"location":"vendors/terraform/external-state-access.html#sharing-outputs-between-stacks","text":"Sharing the outputs between stacks can be achieved using the Terraform mechanism of remote state . Given an account called spacecorp , and a stack named deep-thought , having the following Terraform configuration. 1 2 3 output \"answer\" { value = 42 } You can read that output from a different stack by using the terraform_remote_state data source. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 data \"terraform_remote_state\" \"deepthought\" { backend = \"remote\" config = { hostname = \"spacelift.io\" # (1) organization = \"spacecorp\" # (2) workspaces = { name = \"deep-thought\" # (3) } } } output \"ultimate_answer\" { value = data.terraform_remote_state.deepthought.outputs.answer } The hostname of the Spacelift system. The name of the account. The ID of the stack you wish to retrieve outputs from.","title":"Sharing outputs between stacks"},{"location":"vendors/terraform/external-state-access.html#offline-state-access","text":"Given an account called spacecorp , and a stack named deep-thought , having the following Terraform configuration. 1 2 3 output \"answer\" { value = 42 } Before you will be able to access the state of the stack, you need to retrieve an authentication token for spacelift.io . 1 terraform login spacelift.io Next, you need a remote backend configuration. 1 2 3 4 5 6 7 8 9 10 terraform { backend \"remote\" { hostname = \"spacelift.io\" # (1) organization = \"spacecorp\" # (2) workspaces { name = \"deep-thought\" # (3) } } } The hostname of the Spacelift system. The name of the account. The ID of the stack you wish to retrieve outputs from. Finally, you can download the state of the stack. 1 terraform state pull > terraform.tfstate","title":"Offline state access"},{"location":"vendors/terraform/handling-tfvars.html","text":"Handling .tfvars \u00bb For some of our Terraform users, the most convenient solution to configure a stack is to specify its input values in a variable definitions file that is then passed to Terraform executions in plan and apply phases. Spacelift supports this approach but does not provide a separate mechanism, depending instead on a combination of Terraform's built-in mechanisms and Spacelift-provided primitives like: environment variables ; mounted files before_init scripts; Using environment variables \u00bb In Terraform, special environment variables can be used to pass extra flags to executed commands like plan or apply . These are the more generic TF_CLI_ARGS and TF_CLI_ARGS_name that only affect a specific command. In Spacelift, environment variables can be defined directly on stacks and modules, as well as on contexts attached to those. As an example, let's declare the following environment variable: In our particular case we don't have this file checked in, so the run will fail: But we can supply this file dynamically using mounted files functionality. Using mounted files \u00bb If the variable definitions file is not part of the repo, we can inject it dynamically. The above example can be fixed by supplying the variables file at the requested path: Note that there are \"magical\" names you can give to your variable definitions files that always get autoloaded, without the need to supply extra CLI arguments. According to the documentation , Terraform automatically loads a number of variable definitions files if they are present: Files named exactly terraform.tfvars or terraform.tfvars.json . Any files with names ending in .auto.tfvars or .auto.tfvars.json . The above can be used in conjunction with another Spacelift building block, before_init hooks . Using before_init hooks \u00bb If you need to use different variable definitions files for different projects, would like to have them checked into the repo, but would also want to avoid supplying extra CLI arguments, you could just dynamically move files - whether as a move, copy or a symlink to one of the autoloaded locations. This should happen in one of the before_init steps. Example:","title":"Handling .tfvars"},{"location":"vendors/terraform/handling-tfvars.html#handling-tfvars","text":"For some of our Terraform users, the most convenient solution to configure a stack is to specify its input values in a variable definitions file that is then passed to Terraform executions in plan and apply phases. Spacelift supports this approach but does not provide a separate mechanism, depending instead on a combination of Terraform's built-in mechanisms and Spacelift-provided primitives like: environment variables ; mounted files before_init scripts;","title":"Handling .tfvars"},{"location":"vendors/terraform/handling-tfvars.html#using-environment-variables","text":"In Terraform, special environment variables can be used to pass extra flags to executed commands like plan or apply . These are the more generic TF_CLI_ARGS and TF_CLI_ARGS_name that only affect a specific command. In Spacelift, environment variables can be defined directly on stacks and modules, as well as on contexts attached to those. As an example, let's declare the following environment variable: In our particular case we don't have this file checked in, so the run will fail: But we can supply this file dynamically using mounted files functionality.","title":"Using environment variables"},{"location":"vendors/terraform/handling-tfvars.html#using-mounted-files","text":"If the variable definitions file is not part of the repo, we can inject it dynamically. The above example can be fixed by supplying the variables file at the requested path: Note that there are \"magical\" names you can give to your variable definitions files that always get autoloaded, without the need to supply extra CLI arguments. According to the documentation , Terraform automatically loads a number of variable definitions files if they are present: Files named exactly terraform.tfvars or terraform.tfvars.json . Any files with names ending in .auto.tfvars or .auto.tfvars.json . The above can be used in conjunction with another Spacelift building block, before_init hooks .","title":"Using mounted files"},{"location":"vendors/terraform/handling-tfvars.html#using-before_init-hooks","text":"If you need to use different variable definitions files for different projects, would like to have them checked into the repo, but would also want to avoid supplying extra CLI arguments, you could just dynamically move files - whether as a move, copy or a symlink to one of the autoloaded locations. This should happen in one of the before_init steps. Example:","title":"Using before_init hooks"},{"location":"vendors/terraform/infracost.html","text":"Cost Estimation \u00bb The Infracost integration allows you to run an Infracost breakdown during Spacelift runs, providing feedback on PRs, and allowing you to integrate cost data with plan policies . This allows you to understand how infrastructure changes will impact costs, and to build automatic guards to help prevent costs from spiraling out of control. Setting up the integration \u00bb To enable Infracost on any stack you need to do the following: Add the infracost label to the stack. Add an INFRACOST_API_KEY environment variable containing your Infracost API key. Info Creating a context for your Infracost API key means you can attach your key to any stacks that need to have Infracost enabled. If Infracost has been configured successfully, you should see some messages during the initialization phase of your runs indicating that Infracost is enabled and that the environment variable has been found: Additional CLI Arguments \u00bb If you need to pass any additional CLI arguments to the Infracost breakdown command, you can add them to the INFRACOST_CLI_ARGS environment variable. Anything found in this variable is automatically appended to the command. This allows you to do things like specifying the path to your Infracost usage file . Ignore Failures \u00bb By default, a failure executing Infracost, or a non-zero exit code being returned from the command will cause runs to fail. This behavior can be changed by setting the INFRACOST_WARN_ON_FAILURE environment variable to true . When enabled, Infracost errors will produce a warning message, but will not cause run failures. Using the integration \u00bb Once the integration is configured, Spacelift will automatically run Infracost breakdowns during the planning and applying stages. The following sections explain the functionality provided by the integration. Pull Requests \u00bb Spacelift automatically posts the usage summary to your pull requests once Infracost is enabled: Plan Policies \u00bb Spacelift includes the full Infracost breakdown report in JSON format as part of the input to your plan policies. This is contained in third_party_metadata.infracost . The following shows an example plan input: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \"cloudformation\" : null , \"pulumi\" : null , \"spacelift\" : { ... }, \"terraform\" : { ... }, \"third_party_metadata\" : { \"infracost\" : { \"projects\" : [{ \"breakdown\" : { \"resources\" : [ ... ], \"totalHourlyCost\" : \"0.0321506849315068485\" , \"totalMonthlyCost\" : \"23.47\" }, \"diff\" : { \"resources\" : [ ... ], \"totalHourlyCost\" : \"0.0321506849315068485\" , \"totalMonthlyCost\" : \"23.47\" }, \"metadata\" : {}, \"pastBreakdown\" : { \"resources\" : [], \"totalHourlyCost\" : \"0\" , \"totalMonthlyCost\" : \"0\" }, \"path\" : \"/tmp/spacelift-plan923575332\" }], \"resources\" : [ ... ], \"summary\" : { \"unsupportedResourceCounts\" : {} }, \"timeGenerated\" : \"2021-06-09T14:14:44.146230883Z\" , \"totalHourlyCost\" : \"0.0321506849315068485\" , \"totalMonthlyCost\" : \"23.47\" , \"version\" : \"0.1\" } } } This means that you can take cost information into account when deciding whether to ask for human approval or to block changes entirely. The following policy provides a simple example of this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package spacelift # Prevent any changes that will cause the monthly cost to go above a certain threshold deny [ sprintf ( \"monthly cost greater than $%d ($%.2f)\" , [ threshold , monthly_cost ])] { threshold : = 100 monthly_cost : = to_number ( input . third_party_metadata . infracost . projects [ 0 ]. breakdown . totalMonthlyCost ) monthly_cost > threshold } # Warn if the monthly costs increase more than a certain percentage warn [ sprintf ( \"monthly cost increase greater than %d%% (%.2f%%)\" , [ threshold , percentage_increase ])] { threshold : = 5 previous_cost : = to_number ( input . third_party_metadata . infracost . projects [ 0 ]. pastBreakdown . totalMonthlyCost ) previous_cost > 0 monthly_cost : = to_number ( input . third_party_metadata . infracost . projects [ 0 ]. breakdown . totalMonthlyCost ) percentage_increase : = (( monthly_cost - previous_cost ) / previous_cost ) * 100 percentage_increase > threshold } Resources View \u00bb Infracost provides information about how individual resources contribute to the overall cost of the stack. Spacelift combines this information with our resources view to allow you to view the cost information for each resource:","title":"Cost Estimation"},{"location":"vendors/terraform/infracost.html#cost-estimation","text":"The Infracost integration allows you to run an Infracost breakdown during Spacelift runs, providing feedback on PRs, and allowing you to integrate cost data with plan policies . This allows you to understand how infrastructure changes will impact costs, and to build automatic guards to help prevent costs from spiraling out of control.","title":"Cost Estimation"},{"location":"vendors/terraform/infracost.html#setting-up-the-integration","text":"To enable Infracost on any stack you need to do the following: Add the infracost label to the stack. Add an INFRACOST_API_KEY environment variable containing your Infracost API key. Info Creating a context for your Infracost API key means you can attach your key to any stacks that need to have Infracost enabled. If Infracost has been configured successfully, you should see some messages during the initialization phase of your runs indicating that Infracost is enabled and that the environment variable has been found:","title":"Setting up the integration"},{"location":"vendors/terraform/infracost.html#additional-cli-arguments","text":"If you need to pass any additional CLI arguments to the Infracost breakdown command, you can add them to the INFRACOST_CLI_ARGS environment variable. Anything found in this variable is automatically appended to the command. This allows you to do things like specifying the path to your Infracost usage file .","title":"Additional CLI Arguments"},{"location":"vendors/terraform/infracost.html#ignore-failures","text":"By default, a failure executing Infracost, or a non-zero exit code being returned from the command will cause runs to fail. This behavior can be changed by setting the INFRACOST_WARN_ON_FAILURE environment variable to true . When enabled, Infracost errors will produce a warning message, but will not cause run failures.","title":"Ignore Failures"},{"location":"vendors/terraform/infracost.html#using-the-integration","text":"Once the integration is configured, Spacelift will automatically run Infracost breakdowns during the planning and applying stages. The following sections explain the functionality provided by the integration.","title":"Using the integration"},{"location":"vendors/terraform/infracost.html#pull-requests","text":"Spacelift automatically posts the usage summary to your pull requests once Infracost is enabled:","title":"Pull Requests"},{"location":"vendors/terraform/infracost.html#plan-policies","text":"Spacelift includes the full Infracost breakdown report in JSON format as part of the input to your plan policies. This is contained in third_party_metadata.infracost . The following shows an example plan input: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \"cloudformation\" : null , \"pulumi\" : null , \"spacelift\" : { ... }, \"terraform\" : { ... }, \"third_party_metadata\" : { \"infracost\" : { \"projects\" : [{ \"breakdown\" : { \"resources\" : [ ... ], \"totalHourlyCost\" : \"0.0321506849315068485\" , \"totalMonthlyCost\" : \"23.47\" }, \"diff\" : { \"resources\" : [ ... ], \"totalHourlyCost\" : \"0.0321506849315068485\" , \"totalMonthlyCost\" : \"23.47\" }, \"metadata\" : {}, \"pastBreakdown\" : { \"resources\" : [], \"totalHourlyCost\" : \"0\" , \"totalMonthlyCost\" : \"0\" }, \"path\" : \"/tmp/spacelift-plan923575332\" }], \"resources\" : [ ... ], \"summary\" : { \"unsupportedResourceCounts\" : {} }, \"timeGenerated\" : \"2021-06-09T14:14:44.146230883Z\" , \"totalHourlyCost\" : \"0.0321506849315068485\" , \"totalMonthlyCost\" : \"23.47\" , \"version\" : \"0.1\" } } } This means that you can take cost information into account when deciding whether to ask for human approval or to block changes entirely. The following policy provides a simple example of this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package spacelift # Prevent any changes that will cause the monthly cost to go above a certain threshold deny [ sprintf ( \"monthly cost greater than $%d ($%.2f)\" , [ threshold , monthly_cost ])] { threshold : = 100 monthly_cost : = to_number ( input . third_party_metadata . infracost . projects [ 0 ]. breakdown . totalMonthlyCost ) monthly_cost > threshold } # Warn if the monthly costs increase more than a certain percentage warn [ sprintf ( \"monthly cost increase greater than %d%% (%.2f%%)\" , [ threshold , percentage_increase ])] { threshold : = 5 previous_cost : = to_number ( input . third_party_metadata . infracost . projects [ 0 ]. pastBreakdown . totalMonthlyCost ) previous_cost > 0 monthly_cost : = to_number ( input . third_party_metadata . infracost . projects [ 0 ]. breakdown . totalMonthlyCost ) percentage_increase : = (( monthly_cost - previous_cost ) / previous_cost ) * 100 percentage_increase > threshold }","title":"Plan Policies"},{"location":"vendors/terraform/infracost.html#resources-view","text":"Infracost provides information about how individual resources contribute to the overall cost of the stack. Spacelift combines this information with our resources view to allow you to view the cost information for each resource:","title":"Resources View"},{"location":"vendors/terraform/module-registry.html","text":"Module registry \u00bb Intro \u00bb In Terraform, modules help you abstract away common functionality in your infrastructure. The name of a module managed by Spacelift is of the following form: 1 spacelift.io/<organization>/<module_name>/<provider> In this name we have: The source module registry - spacelift.io is used here; The organization which owns and maintains the module; The module name, this will usually be the best shorthand descriptor of what the module actually does, i.e. it could be starting a machine with an HTTP server running. The main Terraform provider this module is meant to work with, i.e. the provider for the cloud service the resources should be created on. You can use a module in your Terraform configuration this way: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 module \"my-birthday-cake\" { source = \"spacelift.io/spacelift-io/cake/oven\" version = \"4.2.0\" # Inputs. eggs = 5 flour = \"200g\" } output \"my-birthday-cake\" { value = { weight = module.my-birthday-cake.weight allergens = module.my-birthday-cake.allergens } } As you can see, we've explicitly used a module which can make cakes using an oven. We can specify variables the module depends on, and finally use the outputs the cake module exports. Spacelift obviously lets you host modules, but it also does much more, providing you with robust CI/CD for your modules, leading us to the question... Why host your Modules on Spacelift? \u00bb Spacelift provides everything you need to make your module easily maintainable and usable. There is CI/CD for multiple specified versions of Terraform, which \"runs\" your module on each commit. You get an autogenerated page describing your Module and its intricacies, so your users can explore them and gather required information at a glimpse. It's also deeply integrated with all the features Stacks use which you know and love, like Environments , Policies , Contexts and Worker Pools . Setting up a Module \u00bb Git repository structure \u00bb You will have to set up a repository for your module, the structure of the repository should be as follows: 1 2 3 4 5 6 7 . \u251c\u2500\u2500 .spacelift \u2502 \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 output.tf \u2514\u2500\u2500 variables.tf Each module must have a config.yml file in its .spacelift directory, containing information about the module along with any test cases. Details of the format of this file can be found in the module configuration section of this page. You can check out an example module here: https://github.com/spacelift-io/terraform-spacelift-example Info The source code for a module can be stored in a subdirectory of your repository because you can specify the project root when configuring your module in Spacelift. An example of a repository containing multiple modules can be found here: https://github.com/spacelift-io/multimodule Spacelift setup \u00bb In order to add a module to Spacelift, navigate to the Terraform registry section of the account view, and click the Add module button: The setup steps are pretty similar to the ones for stacks . First you you point Spacelift at the right repo and choose the \" tracked \" branch - note that repositories whose name don't follow the convention are filtered out: In the behavior section there are just three settings: administrative , worker pool and project root . You will only need to set administrative to true if your module manages Spacelift resources (and most likely it does not). Setting worker pool to the one you manage yourself makes sense if the module tests will be touching resources or accounts you don't want Spacelift to access directly. Plus, your private workers may have more bandwidth than the shared ones, so you may get feedback faster. The project root let's you specify the module source code root inside of your repository: Last but not least, you will be able to add a name , provider , labels and description . The name and provider will be inferred from your repository name if it follows the terraform-<provider>-<name> convention. However, if it can't be inferred or you want a custom name, then you can specify them directly. The final module slug will then be based on the name. Environment, contexts and policies \u00bb Environment and context management in modules is identical to that for stacks . The only thing worth noting here is the fact that environment variables and mounted files set either through the module environment directly, or via one of its attached contexts will be passed to each of the test cases for the module. Attaching policies works in a similar way. One thing worth pointing out is that the behavior of Trigger policies are slightly different for modules. Instead of being provided with the list of all other accessible stacks, module trigger policies receive a list of the current consumers of the module. This allows you to automatically trigger dependent stacks when new module versions are published. Module configuration \u00bb While by convention a single Git repository hosts a single module, that root module can have multiple submodules. Thus, we've created a way to create a number of test cases: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # The version of the configuration file format version : 1 # Your module version - must be changed to release a new version module_version : 0.1.1 # Any default settings that should be used for all test cases test_defaults : before_init : [ \"terraform fmt -check\" ] runner_image : your/runner:image # The set of tests to run to verify your module works correctly tests : - name : Test the module with 0.12.7 terraform_version : 0.12.7 environment : TF_VAR_bacon : tasty - name : Test the submodule with 0.13.0 # project_root can be set if your test case is not stored in the root directory project_root : submodule terraform_version : 0.13.0 environment : TF_VAR_cabbage : awful - name : Ensure that the submodule can fail # You can use negative to indicate that the test case is expected to fail negative : true project_root : submodule terraform_version : 0.13.0 This configuration is nearly identical to the one described in the Runtime configuration section, with both test_defaults and each test case accepting the same configuration block. Note that settings explicitly specified in each test case will override those in the test_defaults section. Also, notice that each test case has a name , which is a required field . Info While we don't check for name uniqueness, it's always good idea to give your test cases descriptive names, as these are then used to report job status on your commits and pull requests. Tests \u00bb In order to verify that your module is working correctly, Spacelift can run a number of test cases for your module. Note how the configuration above allows you to set up different runtime environment (Docker image, Terraform version) etc. If you want to test the module with different inputs, these can be passed as Terraform variables (starting with TF_VAR_ ) through the test-level environment configuration option - see above for an example. While coverage is not yet calculated or enforced, we suggest that tests set up all resources defined by the module and submodules. It's generally a good idea to provide examples in the examples/ directory of your repository showing users how they can use the module in practice. These examples can then become your test cases, and you can test them against multiple supported Terraform version to maximize compatibility. While running each test case, Spacelift will - as usual, initialize, plan and apply the resource, but also destroy everything in the end, checking for errors. In the meantime, it will also validate that some resources have actually been created by the tests - though as for now it does not care what these are. A test case can be marked as negative , which means that it is expected to fail. In an example above one of the test cases is expected to fail if one of the required Terraform variables is not set. Negative test cases are as useful as positive ones because they can prove that the module will not work under certain - unexpected or erroneous - circumstances. Test cases will be executed in parallel (as much as worker count permits) for each of the test cases version you have specified in the module configuration. Tests run both on proposed and tracked changes . When a tracked change occurs, we create a Version. Versions are described in more detail in the next section . Info Each test case will have its own commit status in GitHub / GitLab. Test case ordering \u00bb You can specify the order in which test cases should be executed by setting the depends_on property on a test case. This property accepts a list of test case id s that must be executed before the current test case. For example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 version : 1 module_version : 1.0.0 tests : # This one is executed first. - name : Test the module with 0.12.7 id : test-0.12.7 terraform_version : 0.12.7 # This is executed second, because it depends on the first test case. - name : Test the submodule with 0.13.0 id : test-0.13.0 project_root : submodule terraform_version : 0.13.0 depends_on : [ \"test-0.12.7\" ] # This is executed third, because it depends on the second test case. - name : Ensure that the submodule can fail depends_on : [ \"test-0.13.0\" ] negative : true project_root : submodule Note that in order to refer to a test case, you need to set a unique id to it. Versions \u00bb Whenever tests succeed on a tracked change , a new Version is created based on the module_version in the configuration. Important thing to note is that Spacelift will not let you reuse the number of a successful version, and will require you to strictly follow semantic versioning - ie. you can't go to from 0.2.0 to 0.4.0 , skipping 0.3.0 entirely. Two proposed git flow are as follows: The first one would be to have a main branch and create feature branches for changes. Whenever you merge to the main branch you bump the version and release it. If you want more control over release schedules, you could go with the following: A release branch A main branch Feature branches Whenever you add a new functionality, you may want to create a feature branch and open Pull Request from it to the main branch. Whenever you want to release a new version, you merge the main branch into the release branch. You can also use Git push policies to further customize this. Info If no test cases are present, the version is immediately marked green. Marking versions as bad \u00bb If you don't want people to use a specific version of your module, you can mark it as bad. Currently, this feature doesn't have any technical implications - it is still downloadable and usable, but it's a good way to communicate to your users that a specific version is not recommended. You need to click on the version number and then click on the Mark as bad button on the right. Make sure to leave a note explaining why the version is bad. Modules in practice \u00bb In order to use modules, you have to source them from the Spacelift module registry. You can generate the necessary snippet, by opening the page of the specific module version, and clicking show instructions . Sharing modules \u00bb Unlike Stacks, modules can be shared between Spacelift accounts in a sense that while they're always managed by a single account, they can be made accessible to an arbitrary number of other accounts. In order to share the module with other accounts, please add their names in subdomain form (all lowercase) in the module settings Sharing section: This can also be accomplished programmatically using our Terraform provider . Using modules outside of Spacelift \u00bb Modules hosted in the private registry can be used outside of Spacelift. The easiest way is to have Terraform retrieve and store the credentials by running the following command in a terminal: 1 terraform login spacelift.io After you confirm that you want to proceed, Terraform will open your default web browser and ask you to log in to your Spacelift account. Once this is done, Terraform will store the credentials in the ~/.terraform.d/credentials.tfrc.json file for use by subsequent commands. Warning The method above requires a web browser which is not always practical, for example on remote server with no GUI. In that case, you can use credentials generated from API keys . The credentials file generated upon the creation of each API key contains a section explaining how a key can be used to set up credentials in the Terraform configuration file ( .terraformrc ). To learn more about this please refer directly to Terraform documentation . Dependabot \u00bb If you want to use Dependabot to automatically update your module versions, you can use the following dependabot.yml configuration : 1 2 3 4 5 6 7 8 9 10 11 12 13 version : 2 registries : spacelift-private-registry : type : terraform-registry url : https://app.spacelift.io token : ${{ secrets.SPACELIFT_TOKEN }} updates : - package-ecosystem : \"terraform\" directory : \"/\" registries : - spacelift-private-registry schedule : interval : \"daily\" It is important for the url to be https://app.spacelift.io and for the token to be a Spacelift API key . Admin access is not required.","title":"Module registry"},{"location":"vendors/terraform/module-registry.html#module-registry","text":"","title":"Module registry"},{"location":"vendors/terraform/module-registry.html#intro","text":"In Terraform, modules help you abstract away common functionality in your infrastructure. The name of a module managed by Spacelift is of the following form: 1 spacelift.io/<organization>/<module_name>/<provider> In this name we have: The source module registry - spacelift.io is used here; The organization which owns and maintains the module; The module name, this will usually be the best shorthand descriptor of what the module actually does, i.e. it could be starting a machine with an HTTP server running. The main Terraform provider this module is meant to work with, i.e. the provider for the cloud service the resources should be created on. You can use a module in your Terraform configuration this way: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 module \"my-birthday-cake\" { source = \"spacelift.io/spacelift-io/cake/oven\" version = \"4.2.0\" # Inputs. eggs = 5 flour = \"200g\" } output \"my-birthday-cake\" { value = { weight = module.my-birthday-cake.weight allergens = module.my-birthday-cake.allergens } } As you can see, we've explicitly used a module which can make cakes using an oven. We can specify variables the module depends on, and finally use the outputs the cake module exports. Spacelift obviously lets you host modules, but it also does much more, providing you with robust CI/CD for your modules, leading us to the question...","title":"Intro"},{"location":"vendors/terraform/module-registry.html#why-host-your-modules-on-spacelift","text":"Spacelift provides everything you need to make your module easily maintainable and usable. There is CI/CD for multiple specified versions of Terraform, which \"runs\" your module on each commit. You get an autogenerated page describing your Module and its intricacies, so your users can explore them and gather required information at a glimpse. It's also deeply integrated with all the features Stacks use which you know and love, like Environments , Policies , Contexts and Worker Pools .","title":"Why host your Modules on Spacelift?"},{"location":"vendors/terraform/module-registry.html#setting-up-a-module","text":"","title":"Setting up a Module"},{"location":"vendors/terraform/module-registry.html#git-repository-structure","text":"You will have to set up a repository for your module, the structure of the repository should be as follows: 1 2 3 4 5 6 7 . \u251c\u2500\u2500 .spacelift \u2502 \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 output.tf \u2514\u2500\u2500 variables.tf Each module must have a config.yml file in its .spacelift directory, containing information about the module along with any test cases. Details of the format of this file can be found in the module configuration section of this page. You can check out an example module here: https://github.com/spacelift-io/terraform-spacelift-example Info The source code for a module can be stored in a subdirectory of your repository because you can specify the project root when configuring your module in Spacelift. An example of a repository containing multiple modules can be found here: https://github.com/spacelift-io/multimodule","title":"Git repository structure"},{"location":"vendors/terraform/module-registry.html#spacelift-setup","text":"In order to add a module to Spacelift, navigate to the Terraform registry section of the account view, and click the Add module button: The setup steps are pretty similar to the ones for stacks . First you you point Spacelift at the right repo and choose the \" tracked \" branch - note that repositories whose name don't follow the convention are filtered out: In the behavior section there are just three settings: administrative , worker pool and project root . You will only need to set administrative to true if your module manages Spacelift resources (and most likely it does not). Setting worker pool to the one you manage yourself makes sense if the module tests will be touching resources or accounts you don't want Spacelift to access directly. Plus, your private workers may have more bandwidth than the shared ones, so you may get feedback faster. The project root let's you specify the module source code root inside of your repository: Last but not least, you will be able to add a name , provider , labels and description . The name and provider will be inferred from your repository name if it follows the terraform-<provider>-<name> convention. However, if it can't be inferred or you want a custom name, then you can specify them directly. The final module slug will then be based on the name.","title":"Spacelift setup"},{"location":"vendors/terraform/module-registry.html#environment-contexts-and-policies","text":"Environment and context management in modules is identical to that for stacks . The only thing worth noting here is the fact that environment variables and mounted files set either through the module environment directly, or via one of its attached contexts will be passed to each of the test cases for the module. Attaching policies works in a similar way. One thing worth pointing out is that the behavior of Trigger policies are slightly different for modules. Instead of being provided with the list of all other accessible stacks, module trigger policies receive a list of the current consumers of the module. This allows you to automatically trigger dependent stacks when new module versions are published.","title":"Environment, contexts and policies"},{"location":"vendors/terraform/module-registry.html#module-configuration","text":"While by convention a single Git repository hosts a single module, that root module can have multiple submodules. Thus, we've created a way to create a number of test cases: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # The version of the configuration file format version : 1 # Your module version - must be changed to release a new version module_version : 0.1.1 # Any default settings that should be used for all test cases test_defaults : before_init : [ \"terraform fmt -check\" ] runner_image : your/runner:image # The set of tests to run to verify your module works correctly tests : - name : Test the module with 0.12.7 terraform_version : 0.12.7 environment : TF_VAR_bacon : tasty - name : Test the submodule with 0.13.0 # project_root can be set if your test case is not stored in the root directory project_root : submodule terraform_version : 0.13.0 environment : TF_VAR_cabbage : awful - name : Ensure that the submodule can fail # You can use negative to indicate that the test case is expected to fail negative : true project_root : submodule terraform_version : 0.13.0 This configuration is nearly identical to the one described in the Runtime configuration section, with both test_defaults and each test case accepting the same configuration block. Note that settings explicitly specified in each test case will override those in the test_defaults section. Also, notice that each test case has a name , which is a required field . Info While we don't check for name uniqueness, it's always good idea to give your test cases descriptive names, as these are then used to report job status on your commits and pull requests.","title":"Module configuration"},{"location":"vendors/terraform/module-registry.html#tests","text":"In order to verify that your module is working correctly, Spacelift can run a number of test cases for your module. Note how the configuration above allows you to set up different runtime environment (Docker image, Terraform version) etc. If you want to test the module with different inputs, these can be passed as Terraform variables (starting with TF_VAR_ ) through the test-level environment configuration option - see above for an example. While coverage is not yet calculated or enforced, we suggest that tests set up all resources defined by the module and submodules. It's generally a good idea to provide examples in the examples/ directory of your repository showing users how they can use the module in practice. These examples can then become your test cases, and you can test them against multiple supported Terraform version to maximize compatibility. While running each test case, Spacelift will - as usual, initialize, plan and apply the resource, but also destroy everything in the end, checking for errors. In the meantime, it will also validate that some resources have actually been created by the tests - though as for now it does not care what these are. A test case can be marked as negative , which means that it is expected to fail. In an example above one of the test cases is expected to fail if one of the required Terraform variables is not set. Negative test cases are as useful as positive ones because they can prove that the module will not work under certain - unexpected or erroneous - circumstances. Test cases will be executed in parallel (as much as worker count permits) for each of the test cases version you have specified in the module configuration. Tests run both on proposed and tracked changes . When a tracked change occurs, we create a Version. Versions are described in more detail in the next section . Info Each test case will have its own commit status in GitHub / GitLab.","title":"Tests"},{"location":"vendors/terraform/module-registry.html#test-case-ordering","text":"You can specify the order in which test cases should be executed by setting the depends_on property on a test case. This property accepts a list of test case id s that must be executed before the current test case. For example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 version : 1 module_version : 1.0.0 tests : # This one is executed first. - name : Test the module with 0.12.7 id : test-0.12.7 terraform_version : 0.12.7 # This is executed second, because it depends on the first test case. - name : Test the submodule with 0.13.0 id : test-0.13.0 project_root : submodule terraform_version : 0.13.0 depends_on : [ \"test-0.12.7\" ] # This is executed third, because it depends on the second test case. - name : Ensure that the submodule can fail depends_on : [ \"test-0.13.0\" ] negative : true project_root : submodule Note that in order to refer to a test case, you need to set a unique id to it.","title":"Test case ordering"},{"location":"vendors/terraform/module-registry.html#versions","text":"Whenever tests succeed on a tracked change , a new Version is created based on the module_version in the configuration. Important thing to note is that Spacelift will not let you reuse the number of a successful version, and will require you to strictly follow semantic versioning - ie. you can't go to from 0.2.0 to 0.4.0 , skipping 0.3.0 entirely. Two proposed git flow are as follows: The first one would be to have a main branch and create feature branches for changes. Whenever you merge to the main branch you bump the version and release it. If you want more control over release schedules, you could go with the following: A release branch A main branch Feature branches Whenever you add a new functionality, you may want to create a feature branch and open Pull Request from it to the main branch. Whenever you want to release a new version, you merge the main branch into the release branch. You can also use Git push policies to further customize this. Info If no test cases are present, the version is immediately marked green.","title":"Versions"},{"location":"vendors/terraform/module-registry.html#marking-versions-as-bad","text":"If you don't want people to use a specific version of your module, you can mark it as bad. Currently, this feature doesn't have any technical implications - it is still downloadable and usable, but it's a good way to communicate to your users that a specific version is not recommended. You need to click on the version number and then click on the Mark as bad button on the right. Make sure to leave a note explaining why the version is bad.","title":"Marking versions as bad"},{"location":"vendors/terraform/module-registry.html#modules-in-practice","text":"In order to use modules, you have to source them from the Spacelift module registry. You can generate the necessary snippet, by opening the page of the specific module version, and clicking show instructions .","title":"Modules in practice"},{"location":"vendors/terraform/module-registry.html#sharing-modules","text":"Unlike Stacks, modules can be shared between Spacelift accounts in a sense that while they're always managed by a single account, they can be made accessible to an arbitrary number of other accounts. In order to share the module with other accounts, please add their names in subdomain form (all lowercase) in the module settings Sharing section: This can also be accomplished programmatically using our Terraform provider .","title":"Sharing modules"},{"location":"vendors/terraform/module-registry.html#using-modules-outside-of-spacelift","text":"Modules hosted in the private registry can be used outside of Spacelift. The easiest way is to have Terraform retrieve and store the credentials by running the following command in a terminal: 1 terraform login spacelift.io After you confirm that you want to proceed, Terraform will open your default web browser and ask you to log in to your Spacelift account. Once this is done, Terraform will store the credentials in the ~/.terraform.d/credentials.tfrc.json file for use by subsequent commands. Warning The method above requires a web browser which is not always practical, for example on remote server with no GUI. In that case, you can use credentials generated from API keys . The credentials file generated upon the creation of each API key contains a section explaining how a key can be used to set up credentials in the Terraform configuration file ( .terraformrc ). To learn more about this please refer directly to Terraform documentation .","title":"Using modules outside of Spacelift"},{"location":"vendors/terraform/module-registry.html#dependabot","text":"If you want to use Dependabot to automatically update your module versions, you can use the following dependabot.yml configuration : 1 2 3 4 5 6 7 8 9 10 11 12 13 version : 2 registries : spacelift-private-registry : type : terraform-registry url : https://app.spacelift.io token : ${{ secrets.SPACELIFT_TOKEN }} updates : - package-ecosystem : \"terraform\" directory : \"/\" registries : - spacelift-private-registry schedule : interval : \"daily\" It is important for the url to be https://app.spacelift.io and for the token to be a Spacelift API key . Admin access is not required.","title":"Dependabot"},{"location":"vendors/terraform/provider-registry.html","text":"Provider registry \u00bb Warning This feature is currently in open public beta and is free to use regardless of your pricing plan. Once GA, it will be available as part of our Enterprise plan . Intro \u00bb While the Terraform ecosystem is vast and growing, sometimes there is no official provider for your use case, especially if you need to interface with internal or niche tooling. This is where the Spacelift provider registry comes in. It's a place where you can publish your own providers. These providers can be used both inside, and outside of Spacelift. Publishing a provider \u00bb Assumptions \u00bb We will not be covering the process of writing a Terraform provider in this article. If you are interested in that, please refer to the official documentation . In this article, we will assume that you already have a provider that you want to publish. We will also focus on providing step-by-step instructions for GitHub Actions users. If you are using a different CI/CD tool, you will need to adapt the steps accordingly based on its documentation. Note that you don't need to use a CI/CD tool to publish a provider - you could do it from your laptop. However, we recommend using a CI/CD tool to automate the process and provide an audit trail of what's been done, when, and by whom. Last but not least, we assume you're going to use GoReleaser to build your provider. This is by far the most common way of managing Terraform providers which need to be available for different operating systems and architectures. If you're not familiar with GoReleaser, please refer to the official documentation . You can also check out Terraform's official terraform-provider-scaffolding template repository for an example of using GoReleaser with Terraform providers. Creating a provider \u00bb To create a provider in Spacelift, you have three options: Use our Terraform provider (preferable) \u00bb This is the easiest way to do it, as it will let you manage your provider declaratively in the future: provider.tf 1 2 3 4 5 6 resource \"spacelift_terraform_provider\" \"provider\" { # This is the type of your provider. type = \"myinternaltool\" space_id = \"root\" description = \"Explain what this is for\" } It is possible to mark the provider as public, which will make it available to everyone. This is generally not recommended, as it will make it easy for others to use your provider without your knowledge. At the same time, this is the only way of sharing a provider between Spacelift accounts. If you're doing that, make sure there is nothing sensitive in your provider. In order to mark the provider as public, you need to set its public attribute to true . Use the API \u00bb To create a Terraform Provider using the GraphQL API, you can use the terraformProviderCreate mutation. This mutation allows you to create a new provider with the specified inputs. After successfully creating the Terraform Provider, you will receive an output of type TerraformProvider , which contains various fields providing information about the provider. These fields include the ID, creation timestamp, description, labels, latest version number, public accessibility, associated space details, update timestamp, specific version details, and a list of all versions of the provider. For more detailed information about the GraphQL API and its integration, please refer to the API documentation . Create the provider manually in the UI \u00bb In order to create a provider in the UI, navigate to the Terraform registry section of the account view, switch to the Providers tab and click the Create provider button: In the Create Terraform provider drawer, you will need to provide the type, space and optionally a description and/or labels: Info Note that we've put the provider in the root space . This is because we want to give everyone access to it. If you want to make it available only to a specific team, you can put it in a team-specific different space. In general, unless providers are made public, they can be accessed by all users and stacks belonging to the same space and its children (assuming they're set to inherit resources). Register a GPG key \u00bb Warning Only Spacelift root admins can manage account GPG keys. If you're not a root admin, you will need to ask one to do it for you. Terraform uses GPG keys to verify the authenticity of providers. Before you can publish a provider version, you need to register a GPG key with Spacelift. Similarly to creating a provider, you have three options to register a GPG key: Use our CLI tool called spacectl \u00bb One reason we do not want to do it declaratively through the Terraform provider is that it would inevitably lead to the private key being stored in some Terraform state, which is not ideal. spacectl will let you register a GPG key without storing the private key anywhere outside of your system. If you have an existing GPG key that you want to use, you can use spacectl to register it: 1 2 3 4 spacectl provider add-gpg-key \\ --import \\ --name = \"My first GPG key\" \\ --path = \"Path to the ASCII-armored private key\" Alternatively, spacectl can generate a new key for you. Note that spacectl generates GPG keys without a passphrase: 1 2 3 4 5 spacectl provider add-gpg-key \\ --generate \\ --name = \"My first GPG key\" \\ --email = \"Your email address\" \\ --path = \"Path to save the ASCII-armored private key to\" You can list all your GPG keys using spacectl : 1 spacectl provider list-gpg-keys Use the API \u00bb To register a GPG Key using the GraphQL API, you can utilize the gpgKeyCreate mutation. This mutation allows you to register a GPG key with the specified inputs. After successfully registering the GPG Key, you will receive an output of type GpgKey , which contains various fields providing information about the key. These fields include the creation timestamp, the user who created the key, the optional description, the ID, the name of the key, the revocation timestamp (if the key has been revoked), the user who revoked the key (if applicable), and the timestamp of the last update to the key. For more detailed information about the GraphQL API and its integration, please refer to the API documentation . Register the GPG key manually in the UI \u00bb In order to register a GPG key in the UI, navigate to the Terraform registry section of the account view, switch to the GPG keys tab and click the Register GPG key button: In the Register GPG key drawer, you will need to provide the name, the ASCII-armored private key and optionally a description: CI/CD setup \u00bb Now that you have a provider and a GPG key, you can set up your CI/CD tool to publish provider versions. First, let's set up the GoReleaser config file in your provider repository: .goreleaser.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 builds : - env : [ 'CGO_ENABLED=0' ] flags : [ '-trimpath' ] ldflags : [ '-s -w -X main.version={{ .Version }} -X main.commit={{ .Commit }}' ] goos : [ darwin , linux ] goarch : [ amd64 , arm64 ] binary : '{{ .ProjectName }}_v{{ .Version }}' archives : - format : zip name_template : '{{ .ProjectName }}_{{ .Version }}_{{ .Os }}_{{ .Arch }}' checksum : name_template : '{{ .ProjectName }}_{{ .Version }}_SHA256SUMS' algorithm : sha256 signs : - artifacts : checksum args : - \"--batch\" - \"--local-user\" - \"{{ .Env.GPG_FINGERPRINT }}\" - \"--output\" - \"${signature}\" - \"--detach-sign\" - \"${artifact}\" release : disable : true This setup assumes that the name of your project (repository) is terraform-provider-$name . If it is not (maybe you're using a monorepo?) then you will need to change the config accordingly, presumably by hardcoding the project name. Next, let's make sure you have an API key for the Spacelift account you want to publish the provider to. You can refer to the API key management section of the API documentation for more information on how to do that. Note that the key you're generating must have admin access to the space that the provider lives in. If the provider is in the root space, then the key must have root admin access. We can now add a GitHub Actions workflow definition to our repository: .github/workflows/release.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 name : release on : push : permissions : contents : write jobs : goreleaser : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v3 - name : Unshallow run : git fetch --prune --unshallow - name : Set up Go uses : actions/setup-go@v3 with : go-version-file : 'go.mod' cache : true - name : Import GPG key uses : crazy-max/ghaction-import-gpg@v5 id : import_gpg with : # The private key must be stored in an environment variable registered # with GitHub. The expected format is ASCII-armored. # # If you need to use a passphrase, you can populate it in this # section, too. gpg_private_key : ${{ secrets.GPG_PRIVATE_KEY }} - name : Install spacectl uses : spacelift-io/setup-spacectl@main env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} - name : Run GoReleaser # We will only run GoReleaser when a tag is pushed. Semantic versioning # is required, but build metadata is not supported. if : startsWith(github.ref, 'refs/tags/') uses : goreleaser/goreleaser-action@v3.2.0 with : version : latest args : release --rm-dist env : GPG_FINGERPRINT : ${{ steps.import_gpg.outputs.fingerprint }} - name : Release new version if : startsWith(github.ref, 'refs/tags/') env : GPG_KEY_ID : ${{ steps.import_gpg.outputs.keyid }} # This is the URL of the Spacelift account hosting the provider. SPACELIFT_API_KEY_ENDPOINT : https://youraccount.app.spacelift.io # This is the ID of the API key you generated earlier. SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} # This is the secret of the API key you generated earlier. SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : # Don't forget to change the provider type! spacectl provider create-version --type=TYPE-change-me!!! If everything is fine, pushing a tag like v0.1.0 should create a new draft version of the provider in Spacelift. You can list the versions of your provider using spacectl : 1 spacectl provider list-versions --type = $YOUR_PROVIDER_TYPE Note the version status. Versions start their life as drafts, and you can publish them by grabbing their ID (first column) and using spacectl : 1 spacectl provider publish-version --version = $YOUR_VERSION_ID Once published, your version is ready to use. See the next section for more information. Using providers \u00bb Terraform providers hosted by Spacelift can be used the same way as providers hosted by the Terraform Registry. The only difference is that you need to specify the Spacelift registry URL in your Terraform configuration. main.tf 1 2 3 4 5 6 7 terraform { required_providers { yourprovider = { source = \"spacelift.io/your-org/yourprovider\" } } } The above example does not refer to a specific version, meaning that you are going to always use the latest available (published) version of your provider. That said, you can use any versioning syntax supported by the Terraform Registry - learn more about it here . Using providers inside Spacelift \u00bb All runs in any of the stacks belonging to the provider's space or one of its children will be able to read the provider from the Spacelift registry, same as with modules This is because the runs are automatically authenticated to the spacelift.io registry while the run is in progress. Using providers outside of Spacelift \u00bb If you need to use the provider outside of Spacelift, you will need to authenticate to the registry first, either interactively (eg. on your machine) or using an API key (eg. in automation). This process is the same as with modules , so please refer to that section for more information. Your access to the required provider will be determined by your access to the space that the provider lives in. If you have read access to the space, you will be able to use the provider. Other management tasks \u00bb Beyond creating and publishing new versions, there are a few other tasks you can perform on your provider. In this section we will cover the most common ones. Revoking GPG keys \u00bb If you lose control over your GPG key, you will want to revoke it. Revoking a key has no automatic impact on provider versions already published, but it will prevent you from publishing new versions signed with that key. You can revoke a key using spacectl : 1 spacectl provider revoke-gpg-key --key = $ID_OF_YOUR_KEY ... or by using the UI. In order to do that, click the Revoke button inside the three dots menu next to the key you want to revoke: If you want to also revoke any of the provider versions signed with this key, refer to the section on revoking provider versions . Deleting provider versions \u00bb You can permanently delete any draft provider version using spacectl : 1 spacectl provider delete-version --version = $ID_OF_YOUR_VERSION You can also do that in the UI, just navigate to the provider page, find the version you want to delete, and click the Delete button inside the three dots menu next to the version you want to delete: A deleted version disappears from the list of versions, and you can reuse its number in the future. You cannot delete published versions. If you want to disable a published version, you will need to revoke it instead. Revoking provider versions \u00bb You can revoke any published provider version using spacectl : 1 spacectl provider revoke-version --version = $ID_OF_YOUR_VERSION Similarly to deleting a draft provider version, you can delete a published one in the UI by clicking the Revoke button inside the three dots menu next to the version you want to revoke: This will prevent anyone from using the version in the future, but it will not delete it. The version will remain on the list of versions, and will never be available again. You will not be able to reuse the version number of a revoked version.","title":"Provider registry"},{"location":"vendors/terraform/provider-registry.html#provider-registry","text":"Warning This feature is currently in open public beta and is free to use regardless of your pricing plan. Once GA, it will be available as part of our Enterprise plan .","title":"Provider registry"},{"location":"vendors/terraform/provider-registry.html#intro","text":"While the Terraform ecosystem is vast and growing, sometimes there is no official provider for your use case, especially if you need to interface with internal or niche tooling. This is where the Spacelift provider registry comes in. It's a place where you can publish your own providers. These providers can be used both inside, and outside of Spacelift.","title":"Intro"},{"location":"vendors/terraform/provider-registry.html#publishing-a-provider","text":"","title":"Publishing a provider"},{"location":"vendors/terraform/provider-registry.html#assumptions","text":"We will not be covering the process of writing a Terraform provider in this article. If you are interested in that, please refer to the official documentation . In this article, we will assume that you already have a provider that you want to publish. We will also focus on providing step-by-step instructions for GitHub Actions users. If you are using a different CI/CD tool, you will need to adapt the steps accordingly based on its documentation. Note that you don't need to use a CI/CD tool to publish a provider - you could do it from your laptop. However, we recommend using a CI/CD tool to automate the process and provide an audit trail of what's been done, when, and by whom. Last but not least, we assume you're going to use GoReleaser to build your provider. This is by far the most common way of managing Terraform providers which need to be available for different operating systems and architectures. If you're not familiar with GoReleaser, please refer to the official documentation . You can also check out Terraform's official terraform-provider-scaffolding template repository for an example of using GoReleaser with Terraform providers.","title":"Assumptions"},{"location":"vendors/terraform/provider-registry.html#creating-a-provider","text":"To create a provider in Spacelift, you have three options:","title":"Creating a provider"},{"location":"vendors/terraform/provider-registry.html#use-our-terraform-provider-preferable","text":"This is the easiest way to do it, as it will let you manage your provider declaratively in the future: provider.tf 1 2 3 4 5 6 resource \"spacelift_terraform_provider\" \"provider\" { # This is the type of your provider. type = \"myinternaltool\" space_id = \"root\" description = \"Explain what this is for\" } It is possible to mark the provider as public, which will make it available to everyone. This is generally not recommended, as it will make it easy for others to use your provider without your knowledge. At the same time, this is the only way of sharing a provider between Spacelift accounts. If you're doing that, make sure there is nothing sensitive in your provider. In order to mark the provider as public, you need to set its public attribute to true .","title":"Use our Terraform provider (preferable)"},{"location":"vendors/terraform/provider-registry.html#use-the-api","text":"To create a Terraform Provider using the GraphQL API, you can use the terraformProviderCreate mutation. This mutation allows you to create a new provider with the specified inputs. After successfully creating the Terraform Provider, you will receive an output of type TerraformProvider , which contains various fields providing information about the provider. These fields include the ID, creation timestamp, description, labels, latest version number, public accessibility, associated space details, update timestamp, specific version details, and a list of all versions of the provider. For more detailed information about the GraphQL API and its integration, please refer to the API documentation .","title":"Use the API"},{"location":"vendors/terraform/provider-registry.html#create-the-provider-manually-in-the-ui","text":"In order to create a provider in the UI, navigate to the Terraform registry section of the account view, switch to the Providers tab and click the Create provider button: In the Create Terraform provider drawer, you will need to provide the type, space and optionally a description and/or labels: Info Note that we've put the provider in the root space . This is because we want to give everyone access to it. If you want to make it available only to a specific team, you can put it in a team-specific different space. In general, unless providers are made public, they can be accessed by all users and stacks belonging to the same space and its children (assuming they're set to inherit resources).","title":"Create the provider manually in the UI"},{"location":"vendors/terraform/provider-registry.html#register-a-gpg-key","text":"Warning Only Spacelift root admins can manage account GPG keys. If you're not a root admin, you will need to ask one to do it for you. Terraform uses GPG keys to verify the authenticity of providers. Before you can publish a provider version, you need to register a GPG key with Spacelift. Similarly to creating a provider, you have three options to register a GPG key:","title":"Register a GPG key"},{"location":"vendors/terraform/provider-registry.html#use-our-cli-tool-called-spacectl","text":"One reason we do not want to do it declaratively through the Terraform provider is that it would inevitably lead to the private key being stored in some Terraform state, which is not ideal. spacectl will let you register a GPG key without storing the private key anywhere outside of your system. If you have an existing GPG key that you want to use, you can use spacectl to register it: 1 2 3 4 spacectl provider add-gpg-key \\ --import \\ --name = \"My first GPG key\" \\ --path = \"Path to the ASCII-armored private key\" Alternatively, spacectl can generate a new key for you. Note that spacectl generates GPG keys without a passphrase: 1 2 3 4 5 spacectl provider add-gpg-key \\ --generate \\ --name = \"My first GPG key\" \\ --email = \"Your email address\" \\ --path = \"Path to save the ASCII-armored private key to\" You can list all your GPG keys using spacectl : 1 spacectl provider list-gpg-keys","title":"Use our CLI tool called spacectl"},{"location":"vendors/terraform/provider-registry.html#use-the-api_1","text":"To register a GPG Key using the GraphQL API, you can utilize the gpgKeyCreate mutation. This mutation allows you to register a GPG key with the specified inputs. After successfully registering the GPG Key, you will receive an output of type GpgKey , which contains various fields providing information about the key. These fields include the creation timestamp, the user who created the key, the optional description, the ID, the name of the key, the revocation timestamp (if the key has been revoked), the user who revoked the key (if applicable), and the timestamp of the last update to the key. For more detailed information about the GraphQL API and its integration, please refer to the API documentation .","title":"Use the API"},{"location":"vendors/terraform/provider-registry.html#register-the-gpg-key-manually-in-the-ui","text":"In order to register a GPG key in the UI, navigate to the Terraform registry section of the account view, switch to the GPG keys tab and click the Register GPG key button: In the Register GPG key drawer, you will need to provide the name, the ASCII-armored private key and optionally a description:","title":"Register the GPG key manually in the UI"},{"location":"vendors/terraform/provider-registry.html#cicd-setup","text":"Now that you have a provider and a GPG key, you can set up your CI/CD tool to publish provider versions. First, let's set up the GoReleaser config file in your provider repository: .goreleaser.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 builds : - env : [ 'CGO_ENABLED=0' ] flags : [ '-trimpath' ] ldflags : [ '-s -w -X main.version={{ .Version }} -X main.commit={{ .Commit }}' ] goos : [ darwin , linux ] goarch : [ amd64 , arm64 ] binary : '{{ .ProjectName }}_v{{ .Version }}' archives : - format : zip name_template : '{{ .ProjectName }}_{{ .Version }}_{{ .Os }}_{{ .Arch }}' checksum : name_template : '{{ .ProjectName }}_{{ .Version }}_SHA256SUMS' algorithm : sha256 signs : - artifacts : checksum args : - \"--batch\" - \"--local-user\" - \"{{ .Env.GPG_FINGERPRINT }}\" - \"--output\" - \"${signature}\" - \"--detach-sign\" - \"${artifact}\" release : disable : true This setup assumes that the name of your project (repository) is terraform-provider-$name . If it is not (maybe you're using a monorepo?) then you will need to change the config accordingly, presumably by hardcoding the project name. Next, let's make sure you have an API key for the Spacelift account you want to publish the provider to. You can refer to the API key management section of the API documentation for more information on how to do that. Note that the key you're generating must have admin access to the space that the provider lives in. If the provider is in the root space, then the key must have root admin access. We can now add a GitHub Actions workflow definition to our repository: .github/workflows/release.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 name : release on : push : permissions : contents : write jobs : goreleaser : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v3 - name : Unshallow run : git fetch --prune --unshallow - name : Set up Go uses : actions/setup-go@v3 with : go-version-file : 'go.mod' cache : true - name : Import GPG key uses : crazy-max/ghaction-import-gpg@v5 id : import_gpg with : # The private key must be stored in an environment variable registered # with GitHub. The expected format is ASCII-armored. # # If you need to use a passphrase, you can populate it in this # section, too. gpg_private_key : ${{ secrets.GPG_PRIVATE_KEY }} - name : Install spacectl uses : spacelift-io/setup-spacectl@main env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} - name : Run GoReleaser # We will only run GoReleaser when a tag is pushed. Semantic versioning # is required, but build metadata is not supported. if : startsWith(github.ref, 'refs/tags/') uses : goreleaser/goreleaser-action@v3.2.0 with : version : latest args : release --rm-dist env : GPG_FINGERPRINT : ${{ steps.import_gpg.outputs.fingerprint }} - name : Release new version if : startsWith(github.ref, 'refs/tags/') env : GPG_KEY_ID : ${{ steps.import_gpg.outputs.keyid }} # This is the URL of the Spacelift account hosting the provider. SPACELIFT_API_KEY_ENDPOINT : https://youraccount.app.spacelift.io # This is the ID of the API key you generated earlier. SPACELIFT_API_KEY_ID : ${{ secrets.SPACELIFT_API_KEY_ID }} # This is the secret of the API key you generated earlier. SPACELIFT_API_KEY_SECRET : ${{ secrets.SPACELIFT_API_KEY_SECRET }} run : # Don't forget to change the provider type! spacectl provider create-version --type=TYPE-change-me!!! If everything is fine, pushing a tag like v0.1.0 should create a new draft version of the provider in Spacelift. You can list the versions of your provider using spacectl : 1 spacectl provider list-versions --type = $YOUR_PROVIDER_TYPE Note the version status. Versions start their life as drafts, and you can publish them by grabbing their ID (first column) and using spacectl : 1 spacectl provider publish-version --version = $YOUR_VERSION_ID Once published, your version is ready to use. See the next section for more information.","title":"CI/CD setup"},{"location":"vendors/terraform/provider-registry.html#using-providers","text":"Terraform providers hosted by Spacelift can be used the same way as providers hosted by the Terraform Registry. The only difference is that you need to specify the Spacelift registry URL in your Terraform configuration. main.tf 1 2 3 4 5 6 7 terraform { required_providers { yourprovider = { source = \"spacelift.io/your-org/yourprovider\" } } } The above example does not refer to a specific version, meaning that you are going to always use the latest available (published) version of your provider. That said, you can use any versioning syntax supported by the Terraform Registry - learn more about it here .","title":"Using providers"},{"location":"vendors/terraform/provider-registry.html#using-providers-inside-spacelift","text":"All runs in any of the stacks belonging to the provider's space or one of its children will be able to read the provider from the Spacelift registry, same as with modules This is because the runs are automatically authenticated to the spacelift.io registry while the run is in progress.","title":"Using providers inside Spacelift"},{"location":"vendors/terraform/provider-registry.html#using-providers-outside-of-spacelift","text":"If you need to use the provider outside of Spacelift, you will need to authenticate to the registry first, either interactively (eg. on your machine) or using an API key (eg. in automation). This process is the same as with modules , so please refer to that section for more information. Your access to the required provider will be determined by your access to the space that the provider lives in. If you have read access to the space, you will be able to use the provider.","title":"Using providers outside of Spacelift"},{"location":"vendors/terraform/provider-registry.html#other-management-tasks","text":"Beyond creating and publishing new versions, there are a few other tasks you can perform on your provider. In this section we will cover the most common ones.","title":"Other management tasks"},{"location":"vendors/terraform/provider-registry.html#revoking-gpg-keys","text":"If you lose control over your GPG key, you will want to revoke it. Revoking a key has no automatic impact on provider versions already published, but it will prevent you from publishing new versions signed with that key. You can revoke a key using spacectl : 1 spacectl provider revoke-gpg-key --key = $ID_OF_YOUR_KEY ... or by using the UI. In order to do that, click the Revoke button inside the three dots menu next to the key you want to revoke: If you want to also revoke any of the provider versions signed with this key, refer to the section on revoking provider versions .","title":"Revoking GPG keys"},{"location":"vendors/terraform/provider-registry.html#deleting-provider-versions","text":"You can permanently delete any draft provider version using spacectl : 1 spacectl provider delete-version --version = $ID_OF_YOUR_VERSION You can also do that in the UI, just navigate to the provider page, find the version you want to delete, and click the Delete button inside the three dots menu next to the version you want to delete: A deleted version disappears from the list of versions, and you can reuse its number in the future. You cannot delete published versions. If you want to disable a published version, you will need to revoke it instead.","title":"Deleting provider versions"},{"location":"vendors/terraform/provider-registry.html#revoking-provider-versions","text":"You can revoke any published provider version using spacectl : 1 spacectl provider revoke-version --version = $ID_OF_YOUR_VERSION Similarly to deleting a draft provider version, you can delete a published one in the UI by clicking the Revoke button inside the three dots menu next to the version you want to revoke: This will prevent anyone from using the version in the future, but it will not delete it. The version will remain on the list of versions, and will never be available again. You will not be able to reuse the version number of a revoked version.","title":"Revoking provider versions"},{"location":"vendors/terraform/resource-sanitization.html","text":"Resource Sanitization \u00bb The Terraform state can contain very sensitive data. Sometimes this is unavoidable because of the design of certain Terraform providers or because the definition of what is sensitive isn't always simple and may vary between individuals and organizations. Spacelift provides two different approaches for sanitizing values when resources are stored or passed to Plan policies : Default Sanitization : All string values are sanitized. Smart Sanitization : Only the values marked as sensitive are sanitized. For example, if we take the following definition for an AWS RDS instance: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 resource \"aws_db_instance\" \"example\" { allocated_storage = 10 db_name = \"exampledb\" engine = \"mysql\" engine_version = \"5.7\" instance_class = var.instance_class username = var.username password = var.password parameter_group_name = \"default.mysql5.7\" skip_final_snapshot = true } variable \"instance_class\" { default = \"db.t3.micro\" description = \"Instance type\" type = string } variable \"username\" { description = \"Admin username\" sensitive = true type = string } variable \"password\" { description = \"Admin password\" sensitive = true type = string } Spacelift will supply something similar to the following to any plan policies: Default Sanitization Smart Sanitization 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \u2026 \"terraform\" : { \"resource_changes\" : [ { \"address\" : \"aws_db_instance.example\" , \"change\" : { \"actions\" : [ \"create\" ], \"after\" : { \"allocated_storage\" : 10 , \"db_name\" : \"59832d41\" , \"engine\" : \"eae35047\" , \"engine_version\" : \"fc40d152\" , \"instance_class\" : \"4f8189cd\" , \"parameter_group_name\" : \"bead1390\" , \"password\" : \"c1707d9d\" , \"skip_final_snapshot\" : true , \"username\" : \"6266e7ae\" , \u2026 }, \u2026 }, \u2026 } ] } \u2026 } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \u2026 \"terraform\" : { \"resource_changes\" : [ { \"address\" : \"aws_db_instance.example\" , \"change\" : { \"actions\" : [ \"create\" ], \"after\" : { \"allocated_storage\" : 10 , \"db_name\" : \"exampledb\" , \"engine\" : \"mysql\" , \"engine_version\" : \"5.7\" , \"instance_class\" : \"db.t3.micro\" , \"parameter_group_name\" : \"default.mysql5.7\" , \"password\" : \"c1707d9d\" , \"skip_final_snapshot\" : true , \"username\" : \"6266e7ae\" , \u2026 }, \u2026 }, \u2026 } ] } \u2026 } As you can see in the example above, with the Default Sanitization all string values are hashed which makes it difficult to comprehend logs, work with outputs/created resources, and write policies against the changes in your stacks. With Smart Sanitization, only sensitive values are hashed. Smart Sanitization allows you to author Plan policies against non-sensitive string values without the need for a call to the sanitized() function. Default Sanitization Smart Sanitization 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package spacelift import future . keywords allowed_instance_classes : = [ \"db.t3.micro\" , \"db.t3.small\" , \"db.t3.medium\" ] # The values have to be sanitized before they can be compared with the resource change value sanitized_allowed_instance_classes : = { c | sanitized ( allowed_instance_classes [ _ ], c )} deny [ \"Instance class is not allowed\" ] { resource : = input . terraform . resource_changes [ _ ] not resource . change . after . instance_class in sanitized_allowed_instance_classes } deny [ \"Username cannot be 'admin'\" ] { resource : = input . terraform . resource_changes [ _ ] # The value has to be sanitized before they can be compared with the resource change value resource . change . after . username = sanitized ( \"admin\" ) } # Enable sampling to help us debug the policy sample { true } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package spacelift import future . keywords # No need to sanitize the values because the argument is not marked as sensitive allowed_instance_classes : = [ \"db.t3.micro\" , \"db.t3.small\" , \"db.t3.medium\" ] deny [ \"Instance class is not allowed\" ] { resource : = input . terraform . resource_changes [ _ ] not resource . change . after . instance_class in allowed_instance_classes } deny [ \"Username cannot be 'admin'\" ] { resource : = input . terraform . resource_changes [ _ ] # The value has to be sanitized before it can be compared because the argument is marked as sensitive resource . change . after . username = sanitized ( \"admin\" ) } # Enable sampling to help us debug the policy sample { true } Info The same sanitization is also applied to resources shown in the resources views. Default Sanitization \u00bb Unless you enable Smart Sanitization, or disable Sanitization altogether, Default Sanitization will be used. Smart Sanitization \u00bb Info Due to limitations in the data output by Terraform, Smart Sanitization can only be used on stacks that use Terraform versions 1.0.1 or above. Using an unsupported version will fail. As we rely on the sensitive argument in Terraform to determine which of your values are sensitive we recommend that you ensure your variables , outputs , and resources have their sensitive arguments set properly. How to enable Smart Sanitization in your stacks \u00bb When using the Spacelift User Interface \u00bb If you're using the Spacelift user interface to create your stacks, you can enable Smart Sanitization in your existing stack settings page under the \"Backend\" section. If you're creating new stacks it will be an option when you select your Terraform version in our wizard. When creating stacks with the Spacelift Terraform Provider \u00bb If you\u2019re using the Spacelift Terraform provider for creating Spacelift stacks, you are able to set the property terraform_smart_sanitization . For example, to create a simple stack you can use the following code: 1 2 3 4 5 6 7 8 9 10 resource \"spacelift_stack\" \"user_dashboard_internal\" { administrative = true autodeploy = true branch = \"main\" description = \u201cCreates and manages the user management internal dashboard\" name = \"User Dashboard Internal\" repository = \"management\" terraform_smart_sanitization = true terraform_version = \"1.3.1\" } Disabling Sanitization \u00bb If you have a situation where the sanitized() helper function doesn't provide you with enough flexibility to create a particular policy, you can disable sanitization completely for a stack. To do this, add the feature:disable_resource_sanitization label to your stack. This will disable sanitization for any future runs.","title":"Resource Sanitization"},{"location":"vendors/terraform/resource-sanitization.html#resource-sanitization","text":"The Terraform state can contain very sensitive data. Sometimes this is unavoidable because of the design of certain Terraform providers or because the definition of what is sensitive isn't always simple and may vary between individuals and organizations. Spacelift provides two different approaches for sanitizing values when resources are stored or passed to Plan policies : Default Sanitization : All string values are sanitized. Smart Sanitization : Only the values marked as sensitive are sanitized. For example, if we take the following definition for an AWS RDS instance: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 resource \"aws_db_instance\" \"example\" { allocated_storage = 10 db_name = \"exampledb\" engine = \"mysql\" engine_version = \"5.7\" instance_class = var.instance_class username = var.username password = var.password parameter_group_name = \"default.mysql5.7\" skip_final_snapshot = true } variable \"instance_class\" { default = \"db.t3.micro\" description = \"Instance type\" type = string } variable \"username\" { description = \"Admin username\" sensitive = true type = string } variable \"password\" { description = \"Admin password\" sensitive = true type = string } Spacelift will supply something similar to the following to any plan policies: Default Sanitization Smart Sanitization 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \u2026 \"terraform\" : { \"resource_changes\" : [ { \"address\" : \"aws_db_instance.example\" , \"change\" : { \"actions\" : [ \"create\" ], \"after\" : { \"allocated_storage\" : 10 , \"db_name\" : \"59832d41\" , \"engine\" : \"eae35047\" , \"engine_version\" : \"fc40d152\" , \"instance_class\" : \"4f8189cd\" , \"parameter_group_name\" : \"bead1390\" , \"password\" : \"c1707d9d\" , \"skip_final_snapshot\" : true , \"username\" : \"6266e7ae\" , \u2026 }, \u2026 }, \u2026 } ] } \u2026 } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \u2026 \"terraform\" : { \"resource_changes\" : [ { \"address\" : \"aws_db_instance.example\" , \"change\" : { \"actions\" : [ \"create\" ], \"after\" : { \"allocated_storage\" : 10 , \"db_name\" : \"exampledb\" , \"engine\" : \"mysql\" , \"engine_version\" : \"5.7\" , \"instance_class\" : \"db.t3.micro\" , \"parameter_group_name\" : \"default.mysql5.7\" , \"password\" : \"c1707d9d\" , \"skip_final_snapshot\" : true , \"username\" : \"6266e7ae\" , \u2026 }, \u2026 }, \u2026 } ] } \u2026 } As you can see in the example above, with the Default Sanitization all string values are hashed which makes it difficult to comprehend logs, work with outputs/created resources, and write policies against the changes in your stacks. With Smart Sanitization, only sensitive values are hashed. Smart Sanitization allows you to author Plan policies against non-sensitive string values without the need for a call to the sanitized() function. Default Sanitization Smart Sanitization 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package spacelift import future . keywords allowed_instance_classes : = [ \"db.t3.micro\" , \"db.t3.small\" , \"db.t3.medium\" ] # The values have to be sanitized before they can be compared with the resource change value sanitized_allowed_instance_classes : = { c | sanitized ( allowed_instance_classes [ _ ], c )} deny [ \"Instance class is not allowed\" ] { resource : = input . terraform . resource_changes [ _ ] not resource . change . after . instance_class in sanitized_allowed_instance_classes } deny [ \"Username cannot be 'admin'\" ] { resource : = input . terraform . resource_changes [ _ ] # The value has to be sanitized before they can be compared with the resource change value resource . change . after . username = sanitized ( \"admin\" ) } # Enable sampling to help us debug the policy sample { true } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package spacelift import future . keywords # No need to sanitize the values because the argument is not marked as sensitive allowed_instance_classes : = [ \"db.t3.micro\" , \"db.t3.small\" , \"db.t3.medium\" ] deny [ \"Instance class is not allowed\" ] { resource : = input . terraform . resource_changes [ _ ] not resource . change . after . instance_class in allowed_instance_classes } deny [ \"Username cannot be 'admin'\" ] { resource : = input . terraform . resource_changes [ _ ] # The value has to be sanitized before it can be compared because the argument is marked as sensitive resource . change . after . username = sanitized ( \"admin\" ) } # Enable sampling to help us debug the policy sample { true } Info The same sanitization is also applied to resources shown in the resources views.","title":"Resource Sanitization"},{"location":"vendors/terraform/resource-sanitization.html#default-sanitization","text":"Unless you enable Smart Sanitization, or disable Sanitization altogether, Default Sanitization will be used.","title":"Default Sanitization"},{"location":"vendors/terraform/resource-sanitization.html#smart-sanitization","text":"Info Due to limitations in the data output by Terraform, Smart Sanitization can only be used on stacks that use Terraform versions 1.0.1 or above. Using an unsupported version will fail. As we rely on the sensitive argument in Terraform to determine which of your values are sensitive we recommend that you ensure your variables , outputs , and resources have their sensitive arguments set properly.","title":"Smart Sanitization"},{"location":"vendors/terraform/resource-sanitization.html#how-to-enable-smart-sanitization-in-your-stacks","text":"","title":"How to enable Smart Sanitization in your stacks"},{"location":"vendors/terraform/resource-sanitization.html#when-using-the-spacelift-user-interface","text":"If you're using the Spacelift user interface to create your stacks, you can enable Smart Sanitization in your existing stack settings page under the \"Backend\" section. If you're creating new stacks it will be an option when you select your Terraform version in our wizard.","title":"When using the Spacelift User Interface"},{"location":"vendors/terraform/resource-sanitization.html#when-creating-stacks-with-the-spacelift-terraform-provider","text":"If you\u2019re using the Spacelift Terraform provider for creating Spacelift stacks, you are able to set the property terraform_smart_sanitization . For example, to create a simple stack you can use the following code: 1 2 3 4 5 6 7 8 9 10 resource \"spacelift_stack\" \"user_dashboard_internal\" { administrative = true autodeploy = true branch = \"main\" description = \u201cCreates and manages the user management internal dashboard\" name = \"User Dashboard Internal\" repository = \"management\" terraform_smart_sanitization = true terraform_version = \"1.3.1\" }","title":"When creating stacks with the Spacelift Terraform Provider"},{"location":"vendors/terraform/resource-sanitization.html#disabling-sanitization","text":"If you have a situation where the sanitized() helper function doesn't provide you with enough flexibility to create a particular policy, you can disable sanitization completely for a stack. To do this, add the feature:disable_resource_sanitization label to your stack. This will disable sanitization for any future runs.","title":"Disabling Sanitization"},{"location":"vendors/terraform/state-management.html","text":"State management \u00bb For those of you who don't want to manage Terraform state, Spacelift offers an optional sophisticated state backend synchronized with the rest of the application to maximize security and convenience. The ability to have Spacelift manage the state for you is only available during stack creation . As you can see, it's also possible to import an existing Terraform state at this point, which is useful for users who want to upgrade their previous Terraform workflow. Info If you're using Spacelift to manage your stack, do not specify any Terraform backend whatsoever. The one-off config will be dynamically injected into every run and task . Do. Or do not. There is no try. \u00bb In this section we'd like to give you a few reasons why it could be useful to trust Spacelift to take care of your Terraform state. To keep things level, we'll also give you a reason not to. Do \u00bb It's super simple - just two clicks during stack setup. Otherwise there's nothing to set up on your end, so one fewer sensitive thing to worry about. Feel free to refer to how it works on our end , but overall we believe it to be a rather sensible and secure setup, at least on par with anything you could set up on your end. It's protected against accidental or malicious access . Again, you can refer to the more technical section on the inner workings of the state server, but the gist is that we're able to map state access and state changes to legitimate Spacelift runs, thus automatically blocking all other unauthorized traffic. As far as we know, no other backend is capable of that, which is one more reason to give us a go. Don't \u00bb We'll let you in on a little secret now - behind the pixie dust it's still Amazon S3 all the way down, and at this stage we store all our data in Ireland . If you're not OK with that, you're better off managing the state on your end. How it works \u00bb S3, like half of the Internet. The pixie dust we're adding on top of it involves generating one-off credentials for every run and task and injecting them directly into the root of your Terraform project as a .tf file. Warning If you have some Terraform state backend already specified in your code, the initialization phase will keep failing until you remove it. The state server is an HTTP endpoint implementing the Terraform standard state management protocol . Our backend always ensures that the credentials belong to one of the runs or tasks that are currently marked as active on our end, and their state indicates that they should be accessing or modifying the state. Once this is established, we just pass the request to S3 with the right parameters. Importing resources into your Terraform State \u00bb So you have an existing resource that was created by other means and would like that resource to be reflected in your terraform state. This is an excellent use case for the terraform import command. When you're managing your own terraform state, you would typically run this command locally to import said resource(s) to your state file, but what do I do when I'm using Spacelift-managed state you might ask? Spacelift Task to the rescue! To do this, use the following steps: Select the Spacelift Stack to which you would like to import state for. Within the navigation, select \"Tasks\" Run the terraform import command needed to import your state file to the Spacelift-managed state by typing the command into the text input and clicking the perform button. Note: If you are using Terragrunt on Spacelift, you will need to run terragrunt import Follow the status of your task's execution to ensure it was executed successfully. When completed, you should see an output similar to the following within the \"Performing\" step of your task. Exporting Spacelift-managed Terraform state file \u00bb Info If you enable external state access , you can export the stack's state from outside of Spacelift. If a Terraform stack's state is managed by Spacelift and you need to export it you can do so by running the following command in a Task : 1 terraform state pull > terraform.tfstate The local workspace is discarded after the Task has finished so you most likely want to combine this command with another one that pushes the terraform.tfstate file to some remote location. Here is an example of pushing the state file to an AWS S3 bucket (without using an intermediary file): 1 terraform state pull | aws s3 cp - s3://example-bucket/folder/sub-folder/terraform.tfstate","title":"State management"},{"location":"vendors/terraform/state-management.html#state-management","text":"For those of you who don't want to manage Terraform state, Spacelift offers an optional sophisticated state backend synchronized with the rest of the application to maximize security and convenience. The ability to have Spacelift manage the state for you is only available during stack creation . As you can see, it's also possible to import an existing Terraform state at this point, which is useful for users who want to upgrade their previous Terraform workflow. Info If you're using Spacelift to manage your stack, do not specify any Terraform backend whatsoever. The one-off config will be dynamically injected into every run and task .","title":"State management"},{"location":"vendors/terraform/state-management.html#do-or-do-not-there-is-no-try","text":"In this section we'd like to give you a few reasons why it could be useful to trust Spacelift to take care of your Terraform state. To keep things level, we'll also give you a reason not to.","title":"Do. Or do not. There is no try."},{"location":"vendors/terraform/state-management.html#do","text":"It's super simple - just two clicks during stack setup. Otherwise there's nothing to set up on your end, so one fewer sensitive thing to worry about. Feel free to refer to how it works on our end , but overall we believe it to be a rather sensible and secure setup, at least on par with anything you could set up on your end. It's protected against accidental or malicious access . Again, you can refer to the more technical section on the inner workings of the state server, but the gist is that we're able to map state access and state changes to legitimate Spacelift runs, thus automatically blocking all other unauthorized traffic. As far as we know, no other backend is capable of that, which is one more reason to give us a go.","title":"Do"},{"location":"vendors/terraform/state-management.html#dont","text":"We'll let you in on a little secret now - behind the pixie dust it's still Amazon S3 all the way down, and at this stage we store all our data in Ireland . If you're not OK with that, you're better off managing the state on your end.","title":"Don't"},{"location":"vendors/terraform/state-management.html#how-it-works","text":"S3, like half of the Internet. The pixie dust we're adding on top of it involves generating one-off credentials for every run and task and injecting them directly into the root of your Terraform project as a .tf file. Warning If you have some Terraform state backend already specified in your code, the initialization phase will keep failing until you remove it. The state server is an HTTP endpoint implementing the Terraform standard state management protocol . Our backend always ensures that the credentials belong to one of the runs or tasks that are currently marked as active on our end, and their state indicates that they should be accessing or modifying the state. Once this is established, we just pass the request to S3 with the right parameters.","title":"How it works"},{"location":"vendors/terraform/state-management.html#importing-resources-into-your-terraform-state","text":"So you have an existing resource that was created by other means and would like that resource to be reflected in your terraform state. This is an excellent use case for the terraform import command. When you're managing your own terraform state, you would typically run this command locally to import said resource(s) to your state file, but what do I do when I'm using Spacelift-managed state you might ask? Spacelift Task to the rescue! To do this, use the following steps: Select the Spacelift Stack to which you would like to import state for. Within the navigation, select \"Tasks\" Run the terraform import command needed to import your state file to the Spacelift-managed state by typing the command into the text input and clicking the perform button. Note: If you are using Terragrunt on Spacelift, you will need to run terragrunt import Follow the status of your task's execution to ensure it was executed successfully. When completed, you should see an output similar to the following within the \"Performing\" step of your task.","title":"Importing resources into your Terraform State"},{"location":"vendors/terraform/state-management.html#exporting-spacelift-managed-terraform-state-file","text":"Info If you enable external state access , you can export the stack's state from outside of Spacelift. If a Terraform stack's state is managed by Spacelift and you need to export it you can do so by running the following command in a Task : 1 terraform state pull > terraform.tfstate The local workspace is discarded after the Task has finished so you most likely want to combine this command with another one that pushes the terraform.tfstate file to some remote location. Here is an example of pushing the state file to an AWS S3 bucket (without using an intermediary file): 1 terraform state pull | aws s3 cp - s3://example-bucket/folder/sub-folder/terraform.tfstate","title":"Exporting Spacelift-managed Terraform state file"},{"location":"vendors/terraform/storing-complex-variables.html","text":"Storing Complex Variables \u00bb Terraform supports a variety of variable types such as string , number , list , bool , and map . The full list of Terraform's variable types can be found in the Terraform documentation here . When using \"complex\" variable types with Spacelift such as map and list you'll need to utilize Terraform's jsonencode function when storing these variables as an environment variable in your Spacelift Stack environment or context . Usage Example \u00bb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 locals { map = { foo = \"bar\" } list = [ \"this\", \"is\", \"a\", \"list\" ] } resource \"spacelift_context\" \"example\" { description = \"Example of storing complex variable types\" name = \"Terraform Complex Variable Types Example\" } resource \"spacelift_environment_variable\" \"map_example\" { context_id = spacelift_context.example.id name = \"map_example\" value = jsonencode ( local.map ) write_only = false } resource \"spacelift_environment_variable\" \"list_example\" { context_id = spacelift_context.example.id name = \"list_example\" value = jsonencode ( local.list ) write_only = false } Notice the use of the jsonencode function when storing these complex variable types. This will allow you to successfully store these variable types within Spacelift. Consuming Stored Variables \u00bb When consuming complex variable types in your environment, there is no need to use the jsondecode() function.","title":"Storing Complex Variables"},{"location":"vendors/terraform/storing-complex-variables.html#storing-complex-variables","text":"Terraform supports a variety of variable types such as string , number , list , bool , and map . The full list of Terraform's variable types can be found in the Terraform documentation here . When using \"complex\" variable types with Spacelift such as map and list you'll need to utilize Terraform's jsonencode function when storing these variables as an environment variable in your Spacelift Stack environment or context .","title":"Storing Complex Variables"},{"location":"vendors/terraform/storing-complex-variables.html#usage-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 locals { map = { foo = \"bar\" } list = [ \"this\", \"is\", \"a\", \"list\" ] } resource \"spacelift_context\" \"example\" { description = \"Example of storing complex variable types\" name = \"Terraform Complex Variable Types Example\" } resource \"spacelift_environment_variable\" \"map_example\" { context_id = spacelift_context.example.id name = \"map_example\" value = jsonencode ( local.map ) write_only = false } resource \"spacelift_environment_variable\" \"list_example\" { context_id = spacelift_context.example.id name = \"list_example\" value = jsonencode ( local.list ) write_only = false } Notice the use of the jsonencode function when storing these complex variable types. This will allow you to successfully store these variable types within Spacelift.","title":"Usage Example"},{"location":"vendors/terraform/storing-complex-variables.html#consuming-stored-variables","text":"When consuming complex variable types in your environment, there is no need to use the jsondecode() function.","title":"Consuming Stored Variables"},{"location":"vendors/terraform/terraform-provider.html","text":"Provider \u00bb What would you say if you could manage Spacelift resources - that is stacks , contexts , integrations , and configuration - using Spacelift? We hate ClickOps as much as anyone, so we designed everything from the ground up to be easily managed using a Terraform provider. We hope that advanced users will define most of their resources programmatically. Taking it for a spin \u00bb Our Terraform provider is open source and its README always contains the latest available documentation. It's also distributed as part of our Docker runner image and available through our own provider registry . The purpose of this article isn't as much to document the provider itself but to show how it can be used to incorporate Spacelift resources into your infra-as-code. So, without further ado, let's define a stack: stack.tf 1 2 3 4 5 6 7 resource \"spacelift_stack\" \"managed-stack\" { name = \"Stack managed by Spacelift\" # Source code. repository = \"testing-spacelift\" branch = \"master\" } That's awesome. But can we put Terraform to good use and integrate it with resources from a completely different provider? Sure we can, and we have a good excuse, too. Stacks accessibility can be managed by GitHub teams , so why don't we define some? stack-and-teams.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 resource \"github_team\" \"stack-readers\" { name = \"managed-stack-readers\" } resource \"github_team\" \"stack-writers\" { name = \"managed-stack-writers\" } resource \"spacelift_stack\" \"managed-stack\" { name = \"Stack managed by Spacelift\" # Source code. repository = \"testing-spacelift\" branch = \"master\" # Access. readers_team = github_team.stack_readers.slug writers_team = github_team.stack_writers.slug } Now that we programmatically combine Spacelift and GitHub resources, let's add AWS to the mix and give our new stack a dedicated IAM role : stack-teams-and-iam.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 resource \"github_team\" \"stack-readers\" { name = \"managed-stack-readers\" } resource \"github_team\" \"stack-writers\" { name = \"managed-stack-writers\" } resource \"spacelift_stack\" \"managed-stack\" { name = \"Stack managed by Spacelift\" # Source code. repository = \"testing-spacelift\" branch = \"master\" # Access. readers_team = github_team.stack_readers.slug writers_team = github_team.stack_writers.slug } # IAM role. resource \"aws_iam_role\" \"managed-stack\" { name = \"spacelift-managed-stack\" assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ jsondecode ( spacelift_stack.managed-stack.aws_assume_role_policy_statement ) ] }) } # Attaching a nice, powerful policy to it. resource \"aws_iam_role_policy_attachment\" \"managed-stack\" { role = aws_iam_role.managed-stack.name policy_arn = \"arn:aws:iam::aws:policy/PowerUserAccess\" } # Telling Spacelift stack to assume it. resource \"spacelift_stack_aws_role\" \"managed-stack\" { stack_id = spacelift_stack.managed-stack.id role_arn = aws_iam_role.managed-stack.arn } Success OK, so who wants to go back to clicking on things in the web GUI? Because you will likely need to do some clicking, too, at least with your first stack . How it works \u00bb Depending on whether you're using Terraform 0.12.x or higher, the Spacelift provider is distributed slightly differently. For 0.12.x users, the provider is distributed as a binary available in the runner Docker image in the same folder we put the Terraform binary. If you're using Terraform 0.13 and above, you can benefit from pulling our provider directly from our own provider registry. In order to do that, just point Terraform to the right location: 1 2 3 4 5 6 7 8 9 provider \"spacelift\" {} terraform { required_providers { spacelift = { source = \"spacelift-io/spacelift\" } } } Using inside Spacelift \u00bb Within Spacelift, the provider is configured by an environment variable SPACELIFT_API_TOKEN injected into each run and task belonging to stacks marked as administrative . This value is a bearer token that contains all the details necessary for the provider to work, including the full address of the API endpoint to talk to. It's technically valid for 3 hours but only when the run responsible for generating it is in Planning , Applying or Performing (for tasks ) state and throughout that time it provides full administrative access to Spacelift entities that can be managed by Terraform within the same Spacelift account. Using outside of Spacelift \u00bb If you want to run the Spacelift provider outside of Spacelift, or you need to manage resources across multiple Spacelift accounts from the same Terraform project, the preferred method is to generate and use dedicated API keys . Note that unless you're just accessing whitelisted data resources, the Terraform use case will normally require marking the API key as administrative. In order to set up the provider to use an API key, you will need the key ID, secret, and the API key endpoint: 1 2 3 4 5 6 7 8 variable \"spacelift_key_id\" {} variable \"spacelift_key_secret\" {} provider \"spacelift\" { api_key_endpoint = \"https://your-account.app.spacelift.io\" api_key_id = var.spacelift_key_id api_key_secret = var.spacelift_key_secret } These values can also be passed using environment variables, though this will only work to set up the provider for a single Spacelift account: SPACELIFT_API_KEY_ENDPOINT for api_key_endpoint ; SPACELIFT_API_KEY_ID for api_key_id ; SPACELIFT_API_KEY_SECRET for api_key_secret ; If you want to talk to multiple Spacelift accounts, you just need to set up provider aliases like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 variable \"spacelift_first_key_id\" {} variable \"spacelift_first_key_secret\" {} variable \"spacelift_second_key_id\" {} variable \"spacelift_second_key_secret\" {} provider \"spacelift\" { alias = \"first\" api_key_endpoint = \"https://first.app.spacelift.io\" api_key_id = var.spacelift_first_key_id api_key_secret = var.spacelift_first_key_secret } provider \"spacelift\" { alias = \"second\" api_key_endpoint = \"https://second.app.spacelift.io\" api_key_id = var.spacelift_second_key_id api_key_secret = var.spacelift_second_key_secret } If you're running from inside Spacelift, you can still use the default, zero-setup provider for the current account with providers for accounts set up through API keys: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 variable \"spacelift_that_key_id\" {} variable \"spacelift_that_key_secret\" {} provider \"spacelift\" { alias = \"this\" } provider \"spacelift\" { alias = \"that\" api_key_endpoint = \"https://that.app.spacelift.io\" api_key_id = var.spacelift_that_key_id api_key_secret = var.spacelift_that_key_secret } Proposed workflow \u00bb We suggest to first manually create a single administrative stack, and then use it to programmatically define other stacks as necessary. If you're using an integration like AWS, you should probably give the role associated with this stack full IAM access too, allowing it to create separate roles and policies for individual stacks. If you want to share data between stacks, please consider programmatically creating and attaching contexts . Info Programmatically generated stacks and contexts can still be manually augmented, for example by setting extra elements of the environment. Thanks to the magic of Terraform, these will simply be invisible to (and thus not disturbed by) your resource definitions. Boundaries of programmatic management \u00bb Spacelift administrative tokens are not like user tokens. Specifically, they allow access to a much smaller subset of the API . They allow managing the lifecycles of stacks , contexts , integrations , and configuration , but they won't allow you to create or even access Terraform state , runs or tasks , or their associated logs. Administrative tokens have no superpowers either. They can't read write-only configuration elements any more than you can as a user. Unlike human users with user tokens, administrative tokens won't allow you to run env in a task and read back the logs. In general, we believe that things like runs or tasks do not fit the (relatively static) Terraform resource lifecycle model and that hiding those parts of the API from Terraform helps us ensure the integrity of potentially sensitive data - just see the example above.","title":"Provider"},{"location":"vendors/terraform/terraform-provider.html#provider","text":"What would you say if you could manage Spacelift resources - that is stacks , contexts , integrations , and configuration - using Spacelift? We hate ClickOps as much as anyone, so we designed everything from the ground up to be easily managed using a Terraform provider. We hope that advanced users will define most of their resources programmatically.","title":"Provider"},{"location":"vendors/terraform/terraform-provider.html#taking-it-for-a-spin","text":"Our Terraform provider is open source and its README always contains the latest available documentation. It's also distributed as part of our Docker runner image and available through our own provider registry . The purpose of this article isn't as much to document the provider itself but to show how it can be used to incorporate Spacelift resources into your infra-as-code. So, without further ado, let's define a stack: stack.tf 1 2 3 4 5 6 7 resource \"spacelift_stack\" \"managed-stack\" { name = \"Stack managed by Spacelift\" # Source code. repository = \"testing-spacelift\" branch = \"master\" } That's awesome. But can we put Terraform to good use and integrate it with resources from a completely different provider? Sure we can, and we have a good excuse, too. Stacks accessibility can be managed by GitHub teams , so why don't we define some? stack-and-teams.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 resource \"github_team\" \"stack-readers\" { name = \"managed-stack-readers\" } resource \"github_team\" \"stack-writers\" { name = \"managed-stack-writers\" } resource \"spacelift_stack\" \"managed-stack\" { name = \"Stack managed by Spacelift\" # Source code. repository = \"testing-spacelift\" branch = \"master\" # Access. readers_team = github_team.stack_readers.slug writers_team = github_team.stack_writers.slug } Now that we programmatically combine Spacelift and GitHub resources, let's add AWS to the mix and give our new stack a dedicated IAM role : stack-teams-and-iam.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 resource \"github_team\" \"stack-readers\" { name = \"managed-stack-readers\" } resource \"github_team\" \"stack-writers\" { name = \"managed-stack-writers\" } resource \"spacelift_stack\" \"managed-stack\" { name = \"Stack managed by Spacelift\" # Source code. repository = \"testing-spacelift\" branch = \"master\" # Access. readers_team = github_team.stack_readers.slug writers_team = github_team.stack_writers.slug } # IAM role. resource \"aws_iam_role\" \"managed-stack\" { name = \"spacelift-managed-stack\" assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ jsondecode ( spacelift_stack.managed-stack.aws_assume_role_policy_statement ) ] }) } # Attaching a nice, powerful policy to it. resource \"aws_iam_role_policy_attachment\" \"managed-stack\" { role = aws_iam_role.managed-stack.name policy_arn = \"arn:aws:iam::aws:policy/PowerUserAccess\" } # Telling Spacelift stack to assume it. resource \"spacelift_stack_aws_role\" \"managed-stack\" { stack_id = spacelift_stack.managed-stack.id role_arn = aws_iam_role.managed-stack.arn } Success OK, so who wants to go back to clicking on things in the web GUI? Because you will likely need to do some clicking, too, at least with your first stack .","title":"Taking it for a spin"},{"location":"vendors/terraform/terraform-provider.html#how-it-works","text":"Depending on whether you're using Terraform 0.12.x or higher, the Spacelift provider is distributed slightly differently. For 0.12.x users, the provider is distributed as a binary available in the runner Docker image in the same folder we put the Terraform binary. If you're using Terraform 0.13 and above, you can benefit from pulling our provider directly from our own provider registry. In order to do that, just point Terraform to the right location: 1 2 3 4 5 6 7 8 9 provider \"spacelift\" {} terraform { required_providers { spacelift = { source = \"spacelift-io/spacelift\" } } }","title":"How it works"},{"location":"vendors/terraform/terraform-provider.html#using-inside-spacelift","text":"Within Spacelift, the provider is configured by an environment variable SPACELIFT_API_TOKEN injected into each run and task belonging to stacks marked as administrative . This value is a bearer token that contains all the details necessary for the provider to work, including the full address of the API endpoint to talk to. It's technically valid for 3 hours but only when the run responsible for generating it is in Planning , Applying or Performing (for tasks ) state and throughout that time it provides full administrative access to Spacelift entities that can be managed by Terraform within the same Spacelift account.","title":"Using inside Spacelift"},{"location":"vendors/terraform/terraform-provider.html#using-outside-of-spacelift","text":"If you want to run the Spacelift provider outside of Spacelift, or you need to manage resources across multiple Spacelift accounts from the same Terraform project, the preferred method is to generate and use dedicated API keys . Note that unless you're just accessing whitelisted data resources, the Terraform use case will normally require marking the API key as administrative. In order to set up the provider to use an API key, you will need the key ID, secret, and the API key endpoint: 1 2 3 4 5 6 7 8 variable \"spacelift_key_id\" {} variable \"spacelift_key_secret\" {} provider \"spacelift\" { api_key_endpoint = \"https://your-account.app.spacelift.io\" api_key_id = var.spacelift_key_id api_key_secret = var.spacelift_key_secret } These values can also be passed using environment variables, though this will only work to set up the provider for a single Spacelift account: SPACELIFT_API_KEY_ENDPOINT for api_key_endpoint ; SPACELIFT_API_KEY_ID for api_key_id ; SPACELIFT_API_KEY_SECRET for api_key_secret ; If you want to talk to multiple Spacelift accounts, you just need to set up provider aliases like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 variable \"spacelift_first_key_id\" {} variable \"spacelift_first_key_secret\" {} variable \"spacelift_second_key_id\" {} variable \"spacelift_second_key_secret\" {} provider \"spacelift\" { alias = \"first\" api_key_endpoint = \"https://first.app.spacelift.io\" api_key_id = var.spacelift_first_key_id api_key_secret = var.spacelift_first_key_secret } provider \"spacelift\" { alias = \"second\" api_key_endpoint = \"https://second.app.spacelift.io\" api_key_id = var.spacelift_second_key_id api_key_secret = var.spacelift_second_key_secret } If you're running from inside Spacelift, you can still use the default, zero-setup provider for the current account with providers for accounts set up through API keys: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 variable \"spacelift_that_key_id\" {} variable \"spacelift_that_key_secret\" {} provider \"spacelift\" { alias = \"this\" } provider \"spacelift\" { alias = \"that\" api_key_endpoint = \"https://that.app.spacelift.io\" api_key_id = var.spacelift_that_key_id api_key_secret = var.spacelift_that_key_secret }","title":"Using outside of Spacelift"},{"location":"vendors/terraform/terraform-provider.html#proposed-workflow","text":"We suggest to first manually create a single administrative stack, and then use it to programmatically define other stacks as necessary. If you're using an integration like AWS, you should probably give the role associated with this stack full IAM access too, allowing it to create separate roles and policies for individual stacks. If you want to share data between stacks, please consider programmatically creating and attaching contexts . Info Programmatically generated stacks and contexts can still be manually augmented, for example by setting extra elements of the environment. Thanks to the magic of Terraform, these will simply be invisible to (and thus not disturbed by) your resource definitions.","title":"Proposed workflow"},{"location":"vendors/terraform/terraform-provider.html#boundaries-of-programmatic-management","text":"Spacelift administrative tokens are not like user tokens. Specifically, they allow access to a much smaller subset of the API . They allow managing the lifecycles of stacks , contexts , integrations , and configuration , but they won't allow you to create or even access Terraform state , runs or tasks , or their associated logs. Administrative tokens have no superpowers either. They can't read write-only configuration elements any more than you can as a user. Unlike human users with user tokens, administrative tokens won't allow you to run env in a task and read back the logs. In general, we believe that things like runs or tasks do not fit the (relatively static) Terraform resource lifecycle model and that hiding those parts of the API from Terraform helps us ensure the integrity of potentially sensitive data - just see the example above.","title":"Boundaries of programmatic management"},{"location":"vendors/terraform/terragrunt.html","text":"Terragrunt \u00bb Using Terragrunt \u00bb Whether a Terraform stack is using Terragrunt or not is controlled by the presence of terragrunt label on the stack: If present, all workloads will use terragrunt instead of terraform as the main command. Since the Terragrunt API is a superset of Terraform's, this is completely transparent to the end user. Terragrunt is installed on our standard runner image . If you're not using our runner image, you can install Terragrunt separately . During the Initialization phase we're showing you the exact binary that will process your job, along with its location: Versioning with Terragrunt \u00bb When working with Terragrunt, you will still specify the Terraform version to be used to process your job. We don't do it for Terragrunt, which is way more relaxed in terms of how it interacts with Terraform versions, especially since we're only using a very stable subset of its API. On our runner image, we install a version of Terragrunt that will work with the latest version of Terraform that we support. If you need a specific version of Terragrunt, feel free to create a custom runner image and install the Terragrunt version of your choosing. Scope of support \u00bb We're currently using Terragrunt the same way we're using Terraform, running init , plan , and apply commands. This means we're not supporting executing Terraform commands on multiple modules at once ( run-all ). This functionality was designed to operate in a very different mode and environment, and is strictly outside our scope. We also support authentication with Spacelift modules by automatically filling the TG_TF_REGISTRY_TOKEN environment variable for each run. Terragrunt uses this variable to authenticate with private module registries. Debugging Terragrunt \u00bb Similar to Terraform, Terragrunt provides an advanced logging mode, and as of the writing of this documentation, there are currently two ways it can be enabled: Using the --terragrunt-log-level debug CLI flag (You'll need to set this flag using the TF_CLI_ARGS environment variable. For example, TF_CLI_ARGS=\"--terragrunt-log-level debug\" Using the TERRAGRUNT_LOG_LEVEL environment variable. Logging levels supported: info (default), panic fatal error warn debug trace Please refer to the Setting Environment Variables section within our Terraform Debugging Guide for more information on how to set these variables on your Spacelift Stack(s). For more information on logging with Terragrunt, please refer to the Terragrunt documentation .","title":"Terragrunt"},{"location":"vendors/terraform/terragrunt.html#terragrunt","text":"","title":"Terragrunt"},{"location":"vendors/terraform/terragrunt.html#using-terragrunt","text":"Whether a Terraform stack is using Terragrunt or not is controlled by the presence of terragrunt label on the stack: If present, all workloads will use terragrunt instead of terraform as the main command. Since the Terragrunt API is a superset of Terraform's, this is completely transparent to the end user. Terragrunt is installed on our standard runner image . If you're not using our runner image, you can install Terragrunt separately . During the Initialization phase we're showing you the exact binary that will process your job, along with its location:","title":"Using Terragrunt"},{"location":"vendors/terraform/terragrunt.html#versioning-with-terragrunt","text":"When working with Terragrunt, you will still specify the Terraform version to be used to process your job. We don't do it for Terragrunt, which is way more relaxed in terms of how it interacts with Terraform versions, especially since we're only using a very stable subset of its API. On our runner image, we install a version of Terragrunt that will work with the latest version of Terraform that we support. If you need a specific version of Terragrunt, feel free to create a custom runner image and install the Terragrunt version of your choosing.","title":"Versioning with Terragrunt"},{"location":"vendors/terraform/terragrunt.html#scope-of-support","text":"We're currently using Terragrunt the same way we're using Terraform, running init , plan , and apply commands. This means we're not supporting executing Terraform commands on multiple modules at once ( run-all ). This functionality was designed to operate in a very different mode and environment, and is strictly outside our scope. We also support authentication with Spacelift modules by automatically filling the TG_TF_REGISTRY_TOKEN environment variable for each run. Terragrunt uses this variable to authenticate with private module registries.","title":"Scope of support"},{"location":"vendors/terraform/terragrunt.html#debugging-terragrunt","text":"Similar to Terraform, Terragrunt provides an advanced logging mode, and as of the writing of this documentation, there are currently two ways it can be enabled: Using the --terragrunt-log-level debug CLI flag (You'll need to set this flag using the TF_CLI_ARGS environment variable. For example, TF_CLI_ARGS=\"--terragrunt-log-level debug\" Using the TERRAGRUNT_LOG_LEVEL environment variable. Logging levels supported: info (default), panic fatal error warn debug trace Please refer to the Setting Environment Variables section within our Terraform Debugging Guide for more information on how to set these variables on your Spacelift Stack(s). For more information on logging with Terragrunt, please refer to the Terragrunt documentation .","title":"Debugging Terragrunt"},{"location":"vendors/terraform/version-management.html","text":"Version management \u00bb Intro to Terraform versioning \u00bb Terraform is an actively developed open-source product with a mighty sponsor . This means frequent releases - in fact, over the last few months (since minor version 0.12 ) we've been seeing nearly weekly releases . While that's all great news for us as Terraform users, we need to be aware of how version management works in order not to be caught off-guard. Historically (until 0.15.x), once the state was written to (applied) with a higher version of Terraform, there was no way back. Hence, you had to be very careful when updating your current Terraform versions. If you're still using an older version of Terraform with Spacelift, you will likely want to use the runtime configuration to preview the intended changes before you make the jump. Currently (since version 0.15.x) the state format is stable so this is no longer a problem and you don't need to be that extra careful with Terraform versions. Terraform versions in Spacelift \u00bb Terraform binaries are neither distributed with Spacelift nor with its runner Docker image . Instead, Spacelift dynamically detects the right version for each workflow ( run or task ), downloads the right binary on demand, verifies it, and mounts it as read-only on the runner Docker container as /bin/terraform to be directly available in the runner's $PATH : There are two ways to tell Spacelift which Terraform version to use. The main one is to set the version directly on the stack. The version can be set in the Backend section of the stack configuration: Note that you can either point to a specific version or define a version range, which is particularly useful if you don't want to update your code every time the Terraform version changes. The exact supported syntax options can be found here . The other way of specifying the Terraform version is to set it through runtime configuration . The runtime configuration is useful if you want to validate your Terraform code with a newer version of the binary before committing to it - which is especially important in older versions where the state format was not yet stable. If you're creating stacks programmatically but intend to make independent changes to the Terraform version (eg. using runtime configuration), we advise you to ignore any subsequent changes . In order to determine the version of the Terraform binary to mount, Spacelift will first look at the runtime configuration . If it does not contain the version setting for the current stack, the stack setting is then considered. If there is no version set on the current stack, the newest supported Terraform version is used. We always advise creating stacks with the newest available version of Terraform, though we realize it may not be the best option if the project is imported from elsewhere or incompatible providers are used. Warning The newest Terraform version supported by Spacelift may lag a bit behind the latest available Terraform release. We err on the side of caution and thus separately verify each version to ensure that it works as expected and that our code is compatible with its output and general behavior. We're trying to catch up roughly within a week but may temporarily blacklist a faulty version . If you need a compatibility check and a bump sooner than that, please get in touch with our support. Once we apply a run with a particular version of Terraform, we set it on the stack to make sure that we don't implicitly attempt to update it using a lower version. Migrating to newer versions \u00bb In order to migrate a stack to a newer version of Terraform, we suggest opening a feature branch bumping the version through runtime configuration . Open a Pull Request in GitHub from the feature branch to the tracked branch to easily get a link to your proposed run and see if everything looks good. If it does, merge your Pull Request and enjoy working with the latest and greatest version of Terraform. Otherwise, try making necessary changes until your code is working or postpone the migration until you have the bandwidth to do so. Info In general, we suggest to try and keep up with the latest Terraform releases. The longer you wait, the more serious is the migration work going to be . Terraform evolves, providers evolve, external APIs evolve and so should your code.","title":"Version management"},{"location":"vendors/terraform/version-management.html#version-management","text":"","title":"Version management"},{"location":"vendors/terraform/version-management.html#intro-to-terraform-versioning","text":"Terraform is an actively developed open-source product with a mighty sponsor . This means frequent releases - in fact, over the last few months (since minor version 0.12 ) we've been seeing nearly weekly releases . While that's all great news for us as Terraform users, we need to be aware of how version management works in order not to be caught off-guard. Historically (until 0.15.x), once the state was written to (applied) with a higher version of Terraform, there was no way back. Hence, you had to be very careful when updating your current Terraform versions. If you're still using an older version of Terraform with Spacelift, you will likely want to use the runtime configuration to preview the intended changes before you make the jump. Currently (since version 0.15.x) the state format is stable so this is no longer a problem and you don't need to be that extra careful with Terraform versions.","title":"Intro to Terraform versioning"},{"location":"vendors/terraform/version-management.html#terraform-versions-in-spacelift","text":"Terraform binaries are neither distributed with Spacelift nor with its runner Docker image . Instead, Spacelift dynamically detects the right version for each workflow ( run or task ), downloads the right binary on demand, verifies it, and mounts it as read-only on the runner Docker container as /bin/terraform to be directly available in the runner's $PATH : There are two ways to tell Spacelift which Terraform version to use. The main one is to set the version directly on the stack. The version can be set in the Backend section of the stack configuration: Note that you can either point to a specific version or define a version range, which is particularly useful if you don't want to update your code every time the Terraform version changes. The exact supported syntax options can be found here . The other way of specifying the Terraform version is to set it through runtime configuration . The runtime configuration is useful if you want to validate your Terraform code with a newer version of the binary before committing to it - which is especially important in older versions where the state format was not yet stable. If you're creating stacks programmatically but intend to make independent changes to the Terraform version (eg. using runtime configuration), we advise you to ignore any subsequent changes . In order to determine the version of the Terraform binary to mount, Spacelift will first look at the runtime configuration . If it does not contain the version setting for the current stack, the stack setting is then considered. If there is no version set on the current stack, the newest supported Terraform version is used. We always advise creating stacks with the newest available version of Terraform, though we realize it may not be the best option if the project is imported from elsewhere or incompatible providers are used. Warning The newest Terraform version supported by Spacelift may lag a bit behind the latest available Terraform release. We err on the side of caution and thus separately verify each version to ensure that it works as expected and that our code is compatible with its output and general behavior. We're trying to catch up roughly within a week but may temporarily blacklist a faulty version . If you need a compatibility check and a bump sooner than that, please get in touch with our support. Once we apply a run with a particular version of Terraform, we set it on the stack to make sure that we don't implicitly attempt to update it using a lower version.","title":"Terraform versions in Spacelift"},{"location":"vendors/terraform/version-management.html#migrating-to-newer-versions","text":"In order to migrate a stack to a newer version of Terraform, we suggest opening a feature branch bumping the version through runtime configuration . Open a Pull Request in GitHub from the feature branch to the tracked branch to easily get a link to your proposed run and see if everything looks good. If it does, merge your Pull Request and enjoy working with the latest and greatest version of Terraform. Otherwise, try making necessary changes until your code is working or postpone the migration until you have the bandwidth to do so. Info In general, we suggest to try and keep up with the latest Terraform releases. The longer you wait, the more serious is the migration work going to be . Terraform evolves, providers evolve, external APIs evolve and so should your code.","title":"Migrating to newer versions"}]}